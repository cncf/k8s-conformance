I0103 01:46:50.815028      23 e2e.go:116] Starting e2e run "e7487ade-25e1-4251-b1f2-f125f73b7c0d" on Ginkgo node 1
Jan  3 01:46:50.831: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1672710410 - will randomize all specs

Will run 362 of 7067 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
{"msg":"Test Suite starting","completed":0,"skipped":0,"failed":0}
Jan  3 01:46:50.898: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
Jan  3 01:46:50.899: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
E0103 01:46:50.899462      23 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
E0103 01:46:50.899462      23 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Jan  3 01:46:50.913: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jan  3 01:46:50.933: INFO: 15 / 15 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jan  3 01:46:50.933: INFO: expected 5 pod replicas in namespace 'kube-system', 5 are Running and Ready.
Jan  3 01:46:50.933: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jan  3 01:46:50.937: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Jan  3 01:46:50.937: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jan  3 01:46:50.937: INFO: e2e test version: v1.25.0
Jan  3 01:46:50.937: INFO: kube-apiserver version: v1.25.0
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
Jan  3 01:46:50.937: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
Jan  3 01:46:50.940: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.042 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Jan  3 01:46:50.898: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    Jan  3 01:46:50.899: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    E0103 01:46:50.899462      23 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
    Jan  3 01:46:50.913: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Jan  3 01:46:50.933: INFO: 15 / 15 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Jan  3 01:46:50.933: INFO: expected 5 pod replicas in namespace 'kube-system', 5 are Running and Ready.
    Jan  3 01:46:50.933: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Jan  3 01:46:50.937: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
    Jan  3 01:46:50.937: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    Jan  3 01:46:50.937: INFO: e2e test version: v1.25.0
    Jan  3 01:46:50.937: INFO: kube-apiserver version: v1.25.0
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Jan  3 01:46:50.937: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    Jan  3 01:46:50.940: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:46:50.961
Jan  3 01:46:50.961: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename crd-publish-openapi 01/03/23 01:46:50.962
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:46:50.979
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:46:50.981
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
Jan  3 01:46:50.984: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/03/23 01:46:56.393
Jan  3 01:46:56.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-8327 --namespace=crd-publish-openapi-8327 create -f -'
Jan  3 01:46:57.379: INFO: stderr: ""
Jan  3 01:46:57.379: INFO: stdout: "e2e-test-crd-publish-openapi-8177-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan  3 01:46:57.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-8327 --namespace=crd-publish-openapi-8327 delete e2e-test-crd-publish-openapi-8177-crds test-cr'
Jan  3 01:46:57.450: INFO: stderr: ""
Jan  3 01:46:57.450: INFO: stdout: "e2e-test-crd-publish-openapi-8177-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jan  3 01:46:57.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-8327 --namespace=crd-publish-openapi-8327 apply -f -'
Jan  3 01:46:57.672: INFO: stderr: ""
Jan  3 01:46:57.672: INFO: stdout: "e2e-test-crd-publish-openapi-8177-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan  3 01:46:57.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-8327 --namespace=crd-publish-openapi-8327 delete e2e-test-crd-publish-openapi-8177-crds test-cr'
Jan  3 01:46:57.741: INFO: stderr: ""
Jan  3 01:46:57.741: INFO: stdout: "e2e-test-crd-publish-openapi-8177-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 01/03/23 01:46:57.741
Jan  3 01:46:57.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-8327 explain e2e-test-crd-publish-openapi-8177-crds'
Jan  3 01:46:57.956: INFO: stderr: ""
Jan  3 01:46:57.956: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8177-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  3 01:47:03.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8327" for this suite. 01/03/23 01:47:03.414
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","completed":1,"skipped":32,"failed":0}
------------------------------
• [SLOW TEST] [12.455 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:46:50.961
    Jan  3 01:46:50.961: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename crd-publish-openapi 01/03/23 01:46:50.962
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:46:50.979
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:46:50.981
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:193
    Jan  3 01:46:50.984: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/03/23 01:46:56.393
    Jan  3 01:46:56.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-8327 --namespace=crd-publish-openapi-8327 create -f -'
    Jan  3 01:46:57.379: INFO: stderr: ""
    Jan  3 01:46:57.379: INFO: stdout: "e2e-test-crd-publish-openapi-8177-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jan  3 01:46:57.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-8327 --namespace=crd-publish-openapi-8327 delete e2e-test-crd-publish-openapi-8177-crds test-cr'
    Jan  3 01:46:57.450: INFO: stderr: ""
    Jan  3 01:46:57.450: INFO: stdout: "e2e-test-crd-publish-openapi-8177-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Jan  3 01:46:57.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-8327 --namespace=crd-publish-openapi-8327 apply -f -'
    Jan  3 01:46:57.672: INFO: stderr: ""
    Jan  3 01:46:57.672: INFO: stdout: "e2e-test-crd-publish-openapi-8177-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jan  3 01:46:57.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-8327 --namespace=crd-publish-openapi-8327 delete e2e-test-crd-publish-openapi-8177-crds test-cr'
    Jan  3 01:46:57.741: INFO: stderr: ""
    Jan  3 01:46:57.741: INFO: stdout: "e2e-test-crd-publish-openapi-8177-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 01/03/23 01:46:57.741
    Jan  3 01:46:57.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-8327 explain e2e-test-crd-publish-openapi-8177-crds'
    Jan  3 01:46:57.956: INFO: stderr: ""
    Jan  3 01:46:57.956: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8177-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  3 01:47:03.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-8327" for this suite. 01/03/23 01:47:03.414
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:47:03.417
Jan  3 01:47:03.417: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename crd-publish-openapi 01/03/23 01:47:03.418
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:47:03.434
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:47:03.436
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
Jan  3 01:47:03.438: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/03/23 01:47:08.868
Jan  3 01:47:08.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-2355 --namespace=crd-publish-openapi-2355 create -f -'
Jan  3 01:47:09.805: INFO: stderr: ""
Jan  3 01:47:09.805: INFO: stdout: "e2e-test-crd-publish-openapi-4899-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan  3 01:47:09.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-2355 --namespace=crd-publish-openapi-2355 delete e2e-test-crd-publish-openapi-4899-crds test-cr'
Jan  3 01:47:09.867: INFO: stderr: ""
Jan  3 01:47:09.867: INFO: stdout: "e2e-test-crd-publish-openapi-4899-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jan  3 01:47:09.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-2355 --namespace=crd-publish-openapi-2355 apply -f -'
Jan  3 01:47:10.087: INFO: stderr: ""
Jan  3 01:47:10.087: INFO: stdout: "e2e-test-crd-publish-openapi-4899-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan  3 01:47:10.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-2355 --namespace=crd-publish-openapi-2355 delete e2e-test-crd-publish-openapi-4899-crds test-cr'
Jan  3 01:47:10.170: INFO: stderr: ""
Jan  3 01:47:10.170: INFO: stdout: "e2e-test-crd-publish-openapi-4899-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 01/03/23 01:47:10.17
Jan  3 01:47:10.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-2355 explain e2e-test-crd-publish-openapi-4899-crds'
Jan  3 01:47:10.386: INFO: stderr: ""
Jan  3 01:47:10.386: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4899-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  3 01:47:15.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2355" for this suite. 01/03/23 01:47:15.704
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","completed":2,"skipped":40,"failed":0}
------------------------------
• [SLOW TEST] [12.289 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:47:03.417
    Jan  3 01:47:03.417: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename crd-publish-openapi 01/03/23 01:47:03.418
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:47:03.434
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:47:03.436
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:235
    Jan  3 01:47:03.438: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/03/23 01:47:08.868
    Jan  3 01:47:08.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-2355 --namespace=crd-publish-openapi-2355 create -f -'
    Jan  3 01:47:09.805: INFO: stderr: ""
    Jan  3 01:47:09.805: INFO: stdout: "e2e-test-crd-publish-openapi-4899-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jan  3 01:47:09.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-2355 --namespace=crd-publish-openapi-2355 delete e2e-test-crd-publish-openapi-4899-crds test-cr'
    Jan  3 01:47:09.867: INFO: stderr: ""
    Jan  3 01:47:09.867: INFO: stdout: "e2e-test-crd-publish-openapi-4899-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Jan  3 01:47:09.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-2355 --namespace=crd-publish-openapi-2355 apply -f -'
    Jan  3 01:47:10.087: INFO: stderr: ""
    Jan  3 01:47:10.087: INFO: stdout: "e2e-test-crd-publish-openapi-4899-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jan  3 01:47:10.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-2355 --namespace=crd-publish-openapi-2355 delete e2e-test-crd-publish-openapi-4899-crds test-cr'
    Jan  3 01:47:10.170: INFO: stderr: ""
    Jan  3 01:47:10.170: INFO: stdout: "e2e-test-crd-publish-openapi-4899-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 01/03/23 01:47:10.17
    Jan  3 01:47:10.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-2355 explain e2e-test-crd-publish-openapi-4899-crds'
    Jan  3 01:47:10.386: INFO: stderr: ""
    Jan  3 01:47:10.386: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4899-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  3 01:47:15.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-2355" for this suite. 01/03/23 01:47:15.704
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:47:15.707
Jan  3 01:47:15.707: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename resourcequota 01/03/23 01:47:15.707
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:47:15.758
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:47:15.759
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
STEP: Creating a ResourceQuota with terminating scope 01/03/23 01:47:15.761
STEP: Ensuring ResourceQuota status is calculated 01/03/23 01:47:15.773
STEP: Creating a ResourceQuota with not terminating scope 01/03/23 01:47:17.777
STEP: Ensuring ResourceQuota status is calculated 01/03/23 01:47:17.781
STEP: Creating a long running pod 01/03/23 01:47:19.783
STEP: Ensuring resource quota with not terminating scope captures the pod usage 01/03/23 01:47:19.798
STEP: Ensuring resource quota with terminating scope ignored the pod usage 01/03/23 01:47:21.8
STEP: Deleting the pod 01/03/23 01:47:23.803
STEP: Ensuring resource quota status released the pod usage 01/03/23 01:47:23.826
STEP: Creating a terminating pod 01/03/23 01:47:25.828
STEP: Ensuring resource quota with terminating scope captures the pod usage 01/03/23 01:47:25.855
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 01/03/23 01:47:27.858
STEP: Deleting the pod 01/03/23 01:47:29.861
STEP: Ensuring resource quota status released the pod usage 01/03/23 01:47:29.873
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan  3 01:47:31.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7016" for this suite. 01/03/23 01:47:31.878
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","completed":3,"skipped":40,"failed":0}
------------------------------
• [SLOW TEST] [16.174 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:47:15.707
    Jan  3 01:47:15.707: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename resourcequota 01/03/23 01:47:15.707
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:47:15.758
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:47:15.759
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:680
    STEP: Creating a ResourceQuota with terminating scope 01/03/23 01:47:15.761
    STEP: Ensuring ResourceQuota status is calculated 01/03/23 01:47:15.773
    STEP: Creating a ResourceQuota with not terminating scope 01/03/23 01:47:17.777
    STEP: Ensuring ResourceQuota status is calculated 01/03/23 01:47:17.781
    STEP: Creating a long running pod 01/03/23 01:47:19.783
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 01/03/23 01:47:19.798
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 01/03/23 01:47:21.8
    STEP: Deleting the pod 01/03/23 01:47:23.803
    STEP: Ensuring resource quota status released the pod usage 01/03/23 01:47:23.826
    STEP: Creating a terminating pod 01/03/23 01:47:25.828
    STEP: Ensuring resource quota with terminating scope captures the pod usage 01/03/23 01:47:25.855
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 01/03/23 01:47:27.858
    STEP: Deleting the pod 01/03/23 01:47:29.861
    STEP: Ensuring resource quota status released the pod usage 01/03/23 01:47:29.873
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan  3 01:47:31.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-7016" for this suite. 01/03/23 01:47:31.878
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:47:31.882
Jan  3 01:47:31.882: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename pods 01/03/23 01:47:31.883
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:47:31.944
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:47:31.947
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
STEP: creating a Pod with a static label 01/03/23 01:47:31.955
STEP: watching for Pod to be ready 01/03/23 01:47:31.961
Jan  3 01:47:31.962: INFO: observed Pod pod-test in namespace pods-2537 in phase Pending with labels: map[test-pod-static:true] & conditions []
Jan  3 01:47:31.964: INFO: observed Pod pod-test in namespace pods-2537 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:47:31 +0000 UTC  }]
Jan  3 01:47:31.987: INFO: observed Pod pod-test in namespace pods-2537 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:47:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:47:31 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:47:31 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:47:31 +0000 UTC  }]
Jan  3 01:47:32.522: INFO: observed Pod pod-test in namespace pods-2537 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:47:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:47:31 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:47:31 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:47:31 +0000 UTC  }]
Jan  3 01:47:33.133: INFO: Found Pod pod-test in namespace pods-2537 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:47:31 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:47:33 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:47:33 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:47:31 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 01/03/23 01:47:33.184
STEP: getting the Pod and ensuring that it's patched 01/03/23 01:47:33.191
STEP: replacing the Pod's status Ready condition to False 01/03/23 01:47:33.195
STEP: check the Pod again to ensure its Ready conditions are False 01/03/23 01:47:33.201
STEP: deleting the Pod via a Collection with a LabelSelector 01/03/23 01:47:33.202
STEP: watching for the Pod to be deleted 01/03/23 01:47:33.208
Jan  3 01:47:33.209: INFO: observed event type MODIFIED
Jan  3 01:47:35.144: INFO: observed event type MODIFIED
Jan  3 01:47:35.396: INFO: observed event type MODIFIED
Jan  3 01:47:36.140: INFO: observed event type MODIFIED
Jan  3 01:47:36.153: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan  3 01:47:36.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2537" for this suite. 01/03/23 01:47:36.167
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","completed":4,"skipped":68,"failed":0}
------------------------------
• [4.288 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:47:31.882
    Jan  3 01:47:31.882: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename pods 01/03/23 01:47:31.883
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:47:31.944
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:47:31.947
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:895
    STEP: creating a Pod with a static label 01/03/23 01:47:31.955
    STEP: watching for Pod to be ready 01/03/23 01:47:31.961
    Jan  3 01:47:31.962: INFO: observed Pod pod-test in namespace pods-2537 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Jan  3 01:47:31.964: INFO: observed Pod pod-test in namespace pods-2537 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:47:31 +0000 UTC  }]
    Jan  3 01:47:31.987: INFO: observed Pod pod-test in namespace pods-2537 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:47:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:47:31 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:47:31 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:47:31 +0000 UTC  }]
    Jan  3 01:47:32.522: INFO: observed Pod pod-test in namespace pods-2537 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:47:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:47:31 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:47:31 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:47:31 +0000 UTC  }]
    Jan  3 01:47:33.133: INFO: Found Pod pod-test in namespace pods-2537 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:47:31 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:47:33 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:47:33 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:47:31 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 01/03/23 01:47:33.184
    STEP: getting the Pod and ensuring that it's patched 01/03/23 01:47:33.191
    STEP: replacing the Pod's status Ready condition to False 01/03/23 01:47:33.195
    STEP: check the Pod again to ensure its Ready conditions are False 01/03/23 01:47:33.201
    STEP: deleting the Pod via a Collection with a LabelSelector 01/03/23 01:47:33.202
    STEP: watching for the Pod to be deleted 01/03/23 01:47:33.208
    Jan  3 01:47:33.209: INFO: observed event type MODIFIED
    Jan  3 01:47:35.144: INFO: observed event type MODIFIED
    Jan  3 01:47:35.396: INFO: observed event type MODIFIED
    Jan  3 01:47:36.140: INFO: observed event type MODIFIED
    Jan  3 01:47:36.153: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan  3 01:47:36.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-2537" for this suite. 01/03/23 01:47:36.167
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:47:36.172
Jan  3 01:47:36.172: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename sched-pred 01/03/23 01:47:36.173
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:47:36.193
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:47:36.195
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan  3 01:47:36.197: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan  3 01:47:36.202: INFO: Waiting for terminating namespaces to be deleted...
Jan  3 01:47:36.204: INFO: 
Logging pods the apiserver thinks is on node cncf-master before test
Jan  3 01:47:36.215: INFO: postgresql-c6b9bb88b-lqpz6 from hyperauth started at 2023-01-03 01:35:41 +0000 UTC (1 container statuses recorded)
Jan  3 01:47:36.215: INFO: 	Container postgresql ready: true, restart count 0
Jan  3 01:47:36.215: INFO: calico-kube-controllers-58dbc876ff-rc9xm from kube-system started at 2022-12-28 00:23:10 +0000 UTC (1 container statuses recorded)
Jan  3 01:47:36.215: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan  3 01:47:36.215: INFO: calico-node-zxb7d from kube-system started at 2022-12-28 00:23:10 +0000 UTC (1 container statuses recorded)
Jan  3 01:47:36.215: INFO: 	Container calico-node ready: true, restart count 0
Jan  3 01:47:36.215: INFO: coredns-565d847f94-lwkgz from kube-system started at 2022-12-28 00:23:10 +0000 UTC (1 container statuses recorded)
Jan  3 01:47:36.215: INFO: 	Container coredns ready: true, restart count 0
Jan  3 01:47:36.215: INFO: coredns-565d847f94-tsqd7 from kube-system started at 2022-12-28 00:23:10 +0000 UTC (1 container statuses recorded)
Jan  3 01:47:36.215: INFO: 	Container coredns ready: true, restart count 0
Jan  3 01:47:36.215: INFO: etcd-cncf-master from kube-system started at 2022-12-28 00:22:57 +0000 UTC (1 container statuses recorded)
Jan  3 01:47:36.215: INFO: 	Container etcd ready: true, restart count 0
Jan  3 01:47:36.215: INFO: kube-apiserver-cncf-master from kube-system started at 2023-01-03 01:37:53 +0000 UTC (1 container statuses recorded)
Jan  3 01:47:36.215: INFO: 	Container kube-apiserver ready: true, restart count 0
Jan  3 01:47:36.215: INFO: kube-controller-manager-cncf-master from kube-system started at 2022-12-28 00:22:57 +0000 UTC (1 container statuses recorded)
Jan  3 01:47:36.215: INFO: 	Container kube-controller-manager ready: true, restart count 2
Jan  3 01:47:36.215: INFO: kube-proxy-qvggp from kube-system started at 2022-12-28 00:23:10 +0000 UTC (1 container statuses recorded)
Jan  3 01:47:36.215: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  3 01:47:36.215: INFO: kube-scheduler-cncf-master from kube-system started at 2022-12-28 00:22:56 +0000 UTC (1 container statuses recorded)
Jan  3 01:47:36.215: INFO: 	Container kube-scheduler ready: true, restart count 2
Jan  3 01:47:36.215: INFO: csi-cephfsplugin-g2vdb from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (2 container statuses recorded)
Jan  3 01:47:36.215: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Jan  3 01:47:36.215: INFO: 	Container driver-registrar ready: true, restart count 0
Jan  3 01:47:36.215: INFO: csi-rbdplugin-krkxg from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (2 container statuses recorded)
Jan  3 01:47:36.215: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jan  3 01:47:36.215: INFO: 	Container driver-registrar ready: true, restart count 0
Jan  3 01:47:36.215: INFO: csi-rbdplugin-provisioner-6f689965cc-56vzc from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (5 container statuses recorded)
Jan  3 01:47:36.215: INFO: 	Container csi-attacher ready: true, restart count 0
Jan  3 01:47:36.215: INFO: 	Container csi-provisioner ready: true, restart count 0
Jan  3 01:47:36.215: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jan  3 01:47:36.215: INFO: 	Container csi-resizer ready: true, restart count 0
Jan  3 01:47:36.215: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jan  3 01:47:36.215: INFO: rook-ceph-crashcollector-cncf-master-65b6767fc8-xjlw5 from rook-ceph started at 2023-01-02 01:35:37 +0000 UTC (1 container statuses recorded)
Jan  3 01:47:36.215: INFO: 	Container ceph-crash ready: true, restart count 0
Jan  3 01:47:36.215: INFO: rook-ceph-mds-myfs-b-65b48b8c9d-2w82f from rook-ceph started at 2023-01-02 01:35:37 +0000 UTC (1 container statuses recorded)
Jan  3 01:47:36.215: INFO: 	Container mds ready: true, restart count 0
Jan  3 01:47:36.215: INFO: rook-ceph-mgr-b-856c746f6-wkx97 from rook-ceph started at 2022-12-29 08:25:47 +0000 UTC (2 container statuses recorded)
Jan  3 01:47:36.215: INFO: 	Container mgr ready: true, restart count 0
Jan  3 01:47:36.215: INFO: 	Container watch-active ready: true, restart count 0
Jan  3 01:47:36.215: INFO: rook-ceph-mon-b-66ffc4d4d9-sm6t5 from rook-ceph started at 2022-12-29 08:25:22 +0000 UTC (1 container statuses recorded)
Jan  3 01:47:36.215: INFO: 	Container mon ready: true, restart count 0
Jan  3 01:47:36.215: INFO: rook-ceph-osd-1-58d74d96db-9ww4l from rook-ceph started at 2023-01-02 01:35:03 +0000 UTC (1 container statuses recorded)
Jan  3 01:47:36.215: INFO: 	Container osd ready: true, restart count 0
Jan  3 01:47:36.215: INFO: rook-ceph-osd-prepare-cncf-master-j6dcd from rook-ceph started at 2023-01-03 01:40:53 +0000 UTC (1 container statuses recorded)
Jan  3 01:47:36.215: INFO: 	Container provision ready: false, restart count 0
Jan  3 01:47:36.215: INFO: sonobuoy-systemd-logs-daemon-set-58995b31e0564399-fxv6h from sonobuoy started at 2023-01-03 01:46:49 +0000 UTC (2 container statuses recorded)
Jan  3 01:47:36.215: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  3 01:47:36.215: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  3 01:47:36.215: INFO: 
Logging pods the apiserver thinks is on node cncf-node1 before test
Jan  3 01:47:36.230: INFO: traefik-7c456f74d8-b7b6n from api-gateway-system started at 2023-01-03 01:41:21 +0000 UTC (1 container statuses recorded)
Jan  3 01:47:36.230: INFO: 	Container traefik ready: true, restart count 0
Jan  3 01:47:36.230: INFO: cert-manager-754886db57-z7pm7 from cert-manager started at 2023-01-03 01:35:12 +0000 UTC (1 container statuses recorded)
Jan  3 01:47:36.230: INFO: 	Container cert-manager ready: true, restart count 1
Jan  3 01:47:36.230: INFO: cert-manager-cainjector-66b4c7c78d-vc87n from cert-manager started at 2023-01-03 01:35:12 +0000 UTC (1 container statuses recorded)
Jan  3 01:47:36.230: INFO: 	Container cert-manager ready: true, restart count 4
Jan  3 01:47:36.230: INFO: cert-manager-webhook-78cdb5fdf-frlxh from cert-manager started at 2023-01-03 01:35:12 +0000 UTC (1 container statuses recorded)
Jan  3 01:47:36.230: INFO: 	Container cert-manager ready: true, restart count 0
Jan  3 01:47:36.230: INFO: console-59c484c98b-mcqpt from console-system started at 2023-01-03 01:41:24 +0000 UTC (1 container statuses recorded)
Jan  3 01:47:36.230: INFO: 	Container console ready: true, restart count 0
Jan  3 01:47:36.230: INFO: hyperauth-5cbb4d58b8-qpc2d from hyperauth started at 2023-01-03 01:37:01 +0000 UTC (1 container statuses recorded)
Jan  3 01:47:36.230: INFO: 	Container hyperauth ready: true, restart count 0
Jan  3 01:47:36.230: INFO: hypercloud5-api-server-8c944885b-mk6vb from hypercloud5-system started at 2023-01-03 01:38:50 +0000 UTC (1 container statuses recorded)
Jan  3 01:47:36.230: INFO: 	Container hypercloud5-api-server ready: true, restart count 4
Jan  3 01:47:36.230: INFO: timescaledb-0 from hypercloud5-system started at 2023-01-03 01:38:50 +0000 UTC (1 container statuses recorded)
Jan  3 01:47:36.230: INFO: 	Container timescaledb ready: true, restart count 0
Jan  3 01:47:36.230: INFO: calico-node-9p9ff from kube-system started at 2022-12-28 00:54:44 +0000 UTC (1 container statuses recorded)
Jan  3 01:47:36.230: INFO: 	Container calico-node ready: true, restart count 0
Jan  3 01:47:36.230: INFO: kube-proxy-jplgg from kube-system started at 2022-12-28 00:54:44 +0000 UTC (1 container statuses recorded)
Jan  3 01:47:36.230: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  3 01:47:36.230: INFO: snapshot-controller-545d7f8994-vg2ld from kube-system started at 2022-12-29 08:24:10 +0000 UTC (1 container statuses recorded)
Jan  3 01:47:36.230: INFO: 	Container snapshot-controller ready: true, restart count 3
Jan  3 01:47:36.230: INFO: csi-cephfsplugin-provisioner-69f54fb566-jxcpx from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (5 container statuses recorded)
Jan  3 01:47:36.230: INFO: 	Container csi-attacher ready: true, restart count 0
Jan  3 01:47:36.231: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Jan  3 01:47:36.231: INFO: 	Container csi-provisioner ready: true, restart count 0
Jan  3 01:47:36.231: INFO: 	Container csi-resizer ready: true, restart count 0
Jan  3 01:47:36.231: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jan  3 01:47:36.231: INFO: csi-cephfsplugin-stnqm from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (2 container statuses recorded)
Jan  3 01:47:36.231: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Jan  3 01:47:36.231: INFO: 	Container driver-registrar ready: true, restart count 0
Jan  3 01:47:36.231: INFO: csi-rbdplugin-provisioner-6f689965cc-wl454 from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (5 container statuses recorded)
Jan  3 01:47:36.231: INFO: 	Container csi-attacher ready: true, restart count 0
Jan  3 01:47:36.231: INFO: 	Container csi-provisioner ready: true, restart count 0
Jan  3 01:47:36.231: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jan  3 01:47:36.231: INFO: 	Container csi-resizer ready: true, restart count 0
Jan  3 01:47:36.231: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jan  3 01:47:36.231: INFO: csi-rbdplugin-t2hxd from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (2 container statuses recorded)
Jan  3 01:47:36.231: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jan  3 01:47:36.231: INFO: 	Container driver-registrar ready: true, restart count 0
Jan  3 01:47:36.231: INFO: rook-ceph-crashcollector-cncf-node1-846d9dbb97-scfg5 from rook-ceph started at 2023-01-02 01:35:36 +0000 UTC (1 container statuses recorded)
Jan  3 01:47:36.231: INFO: 	Container ceph-crash ready: true, restart count 0
Jan  3 01:47:36.231: INFO: rook-ceph-mds-myfs-a-58f7884986-6th9v from rook-ceph started at 2023-01-02 01:35:36 +0000 UTC (1 container statuses recorded)
Jan  3 01:47:36.231: INFO: 	Container mds ready: true, restart count 0
Jan  3 01:47:36.231: INFO: rook-ceph-mgr-a-7949c65bd-kjzgc from rook-ceph started at 2022-12-29 08:25:47 +0000 UTC (2 container statuses recorded)
Jan  3 01:47:36.231: INFO: 	Container mgr ready: true, restart count 0
Jan  3 01:47:36.231: INFO: 	Container watch-active ready: true, restart count 0
Jan  3 01:47:36.231: INFO: rook-ceph-mon-a-bdd44bcfb-s5hjx from rook-ceph started at 2022-12-29 08:24:58 +0000 UTC (1 container statuses recorded)
Jan  3 01:47:36.231: INFO: 	Container mon ready: true, restart count 0
Jan  3 01:47:36.231: INFO: rook-ceph-operator-7f56d78d46-z2j6r from rook-ceph started at 2023-01-02 01:29:14 +0000 UTC (1 container statuses recorded)
Jan  3 01:47:36.231: INFO: 	Container rook-ceph-operator ready: true, restart count 0
Jan  3 01:47:36.231: INFO: rook-ceph-osd-2-5d8bc49787-jd84z from rook-ceph started at 2023-01-02 01:35:03 +0000 UTC (1 container statuses recorded)
Jan  3 01:47:36.231: INFO: 	Container osd ready: true, restart count 0
Jan  3 01:47:36.231: INFO: rook-ceph-osd-prepare-cncf-node1-89n8c from rook-ceph started at 2023-01-03 01:40:56 +0000 UTC (1 container statuses recorded)
Jan  3 01:47:36.231: INFO: 	Container provision ready: false, restart count 0
Jan  3 01:47:36.231: INFO: rook-ceph-tools-75fb64866c-bcxp7 from rook-ceph started at 2022-12-30 02:06:28 +0000 UTC (1 container statuses recorded)
Jan  3 01:47:36.231: INFO: 	Container rook-ceph-tools ready: true, restart count 0
Jan  3 01:47:36.231: INFO: sonobuoy from sonobuoy started at 2023-01-03 01:46:49 +0000 UTC (1 container statuses recorded)
Jan  3 01:47:36.231: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan  3 01:47:36.231: INFO: sonobuoy-e2e-job-a15b13ec9aa249f2 from sonobuoy started at 2023-01-03 01:46:49 +0000 UTC (2 container statuses recorded)
Jan  3 01:47:36.231: INFO: 	Container e2e ready: true, restart count 0
Jan  3 01:47:36.231: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  3 01:47:36.231: INFO: sonobuoy-systemd-logs-daemon-set-58995b31e0564399-gndlm from sonobuoy started at 2023-01-03 01:46:49 +0000 UTC (2 container statuses recorded)
Jan  3 01:47:36.231: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  3 01:47:36.231: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  3 01:47:36.231: INFO: 
Logging pods the apiserver thinks is on node cncf-node2 before test
Jan  3 01:47:36.248: INFO: hyperauth-5cbb4d58b8-pqhc5 from hyperauth started at 2023-01-03 01:37:01 +0000 UTC (1 container statuses recorded)
Jan  3 01:47:36.248: INFO: 	Container hyperauth ready: true, restart count 0
Jan  3 01:47:36.248: INFO: calico-node-5f5qs from kube-system started at 2022-12-29 05:30:03 +0000 UTC (1 container statuses recorded)
Jan  3 01:47:36.248: INFO: 	Container calico-node ready: true, restart count 0
Jan  3 01:47:36.248: INFO: kube-proxy-ddlsm from kube-system started at 2022-12-29 05:30:03 +0000 UTC (1 container statuses recorded)
Jan  3 01:47:36.248: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  3 01:47:36.248: INFO: snapshot-controller-545d7f8994-qgm6l from kube-system started at 2022-12-29 08:24:10 +0000 UTC (1 container statuses recorded)
Jan  3 01:47:36.248: INFO: 	Container snapshot-controller ready: true, restart count 0
Jan  3 01:47:36.248: INFO: csi-cephfsplugin-hxtjf from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (2 container statuses recorded)
Jan  3 01:47:36.248: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Jan  3 01:47:36.248: INFO: 	Container driver-registrar ready: true, restart count 0
Jan  3 01:47:36.248: INFO: csi-cephfsplugin-provisioner-69f54fb566-xhc7t from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (5 container statuses recorded)
Jan  3 01:47:36.248: INFO: 	Container csi-attacher ready: true, restart count 0
Jan  3 01:47:36.248: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Jan  3 01:47:36.248: INFO: 	Container csi-provisioner ready: true, restart count 0
Jan  3 01:47:36.248: INFO: 	Container csi-resizer ready: true, restart count 0
Jan  3 01:47:36.248: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jan  3 01:47:36.248: INFO: csi-rbdplugin-qk6jk from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (2 container statuses recorded)
Jan  3 01:47:36.248: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jan  3 01:47:36.248: INFO: 	Container driver-registrar ready: true, restart count 0
Jan  3 01:47:36.248: INFO: rook-ceph-crashcollector-cncf-node2-898ff4558-twdv7 from rook-ceph started at 2022-12-29 08:26:03 +0000 UTC (1 container statuses recorded)
Jan  3 01:47:36.248: INFO: 	Container ceph-crash ready: true, restart count 0
Jan  3 01:47:36.248: INFO: rook-ceph-mon-c-6c468d6b45-m7rqj from rook-ceph started at 2022-12-29 08:25:33 +0000 UTC (1 container statuses recorded)
Jan  3 01:47:36.248: INFO: 	Container mon ready: true, restart count 0
Jan  3 01:47:36.248: INFO: rook-ceph-osd-0-7d877c8d99-hq5fw from rook-ceph started at 2023-01-02 01:35:02 +0000 UTC (1 container statuses recorded)
Jan  3 01:47:36.248: INFO: 	Container osd ready: true, restart count 0
Jan  3 01:47:36.248: INFO: rook-ceph-osd-prepare-cncf-node2-rnwb6 from rook-ceph started at 2023-01-03 01:40:59 +0000 UTC (1 container statuses recorded)
Jan  3 01:47:36.248: INFO: 	Container provision ready: false, restart count 0
Jan  3 01:47:36.248: INFO: sonobuoy-systemd-logs-daemon-set-58995b31e0564399-wjpkt from sonobuoy started at 2023-01-03 01:46:49 +0000 UTC (2 container statuses recorded)
Jan  3 01:47:36.248: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  3 01:47:36.248: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
STEP: verifying the node has the label node cncf-master 01/03/23 01:47:36.28
STEP: verifying the node has the label node cncf-node1 01/03/23 01:47:36.295
STEP: verifying the node has the label node cncf-node2 01/03/23 01:47:36.315
Jan  3 01:47:36.342: INFO: Pod traefik-7c456f74d8-b7b6n requesting resource cpu=0m on Node cncf-node1
Jan  3 01:47:36.342: INFO: Pod cert-manager-754886db57-z7pm7 requesting resource cpu=0m on Node cncf-node1
Jan  3 01:47:36.342: INFO: Pod cert-manager-cainjector-66b4c7c78d-vc87n requesting resource cpu=0m on Node cncf-node1
Jan  3 01:47:36.342: INFO: Pod cert-manager-webhook-78cdb5fdf-frlxh requesting resource cpu=0m on Node cncf-node1
Jan  3 01:47:36.342: INFO: Pod console-59c484c98b-mcqpt requesting resource cpu=0m on Node cncf-node1
Jan  3 01:47:36.342: INFO: Pod hyperauth-5cbb4d58b8-pqhc5 requesting resource cpu=1000m on Node cncf-node2
Jan  3 01:47:36.342: INFO: Pod hyperauth-5cbb4d58b8-qpc2d requesting resource cpu=1000m on Node cncf-node1
Jan  3 01:47:36.342: INFO: Pod postgresql-c6b9bb88b-lqpz6 requesting resource cpu=1000m on Node cncf-master
Jan  3 01:47:36.342: INFO: Pod hypercloud5-api-server-8c944885b-mk6vb requesting resource cpu=300m on Node cncf-node1
Jan  3 01:47:36.342: INFO: Pod timescaledb-0 requesting resource cpu=100m on Node cncf-node1
Jan  3 01:47:36.342: INFO: Pod calico-kube-controllers-58dbc876ff-rc9xm requesting resource cpu=0m on Node cncf-master
Jan  3 01:47:36.342: INFO: Pod calico-node-5f5qs requesting resource cpu=250m on Node cncf-node2
Jan  3 01:47:36.342: INFO: Pod calico-node-9p9ff requesting resource cpu=250m on Node cncf-node1
Jan  3 01:47:36.342: INFO: Pod calico-node-zxb7d requesting resource cpu=250m on Node cncf-master
Jan  3 01:47:36.342: INFO: Pod coredns-565d847f94-lwkgz requesting resource cpu=100m on Node cncf-master
Jan  3 01:47:36.342: INFO: Pod coredns-565d847f94-tsqd7 requesting resource cpu=100m on Node cncf-master
Jan  3 01:47:36.342: INFO: Pod etcd-cncf-master requesting resource cpu=100m on Node cncf-master
Jan  3 01:47:36.342: INFO: Pod kube-apiserver-cncf-master requesting resource cpu=250m on Node cncf-master
Jan  3 01:47:36.342: INFO: Pod kube-controller-manager-cncf-master requesting resource cpu=200m on Node cncf-master
Jan  3 01:47:36.342: INFO: Pod kube-proxy-ddlsm requesting resource cpu=0m on Node cncf-node2
Jan  3 01:47:36.342: INFO: Pod kube-proxy-jplgg requesting resource cpu=0m on Node cncf-node1
Jan  3 01:47:36.342: INFO: Pod kube-proxy-qvggp requesting resource cpu=0m on Node cncf-master
Jan  3 01:47:36.342: INFO: Pod kube-scheduler-cncf-master requesting resource cpu=100m on Node cncf-master
Jan  3 01:47:36.342: INFO: Pod snapshot-controller-545d7f8994-qgm6l requesting resource cpu=0m on Node cncf-node2
Jan  3 01:47:36.342: INFO: Pod snapshot-controller-545d7f8994-vg2ld requesting resource cpu=0m on Node cncf-node1
Jan  3 01:47:36.342: INFO: Pod csi-cephfsplugin-g2vdb requesting resource cpu=0m on Node cncf-master
Jan  3 01:47:36.342: INFO: Pod csi-cephfsplugin-hxtjf requesting resource cpu=0m on Node cncf-node2
Jan  3 01:47:36.342: INFO: Pod csi-cephfsplugin-provisioner-69f54fb566-jxcpx requesting resource cpu=0m on Node cncf-node1
Jan  3 01:47:36.342: INFO: Pod csi-cephfsplugin-provisioner-69f54fb566-xhc7t requesting resource cpu=0m on Node cncf-node2
Jan  3 01:47:36.342: INFO: Pod csi-cephfsplugin-stnqm requesting resource cpu=0m on Node cncf-node1
Jan  3 01:47:36.342: INFO: Pod csi-rbdplugin-krkxg requesting resource cpu=0m on Node cncf-master
Jan  3 01:47:36.342: INFO: Pod csi-rbdplugin-provisioner-6f689965cc-56vzc requesting resource cpu=0m on Node cncf-master
Jan  3 01:47:36.342: INFO: Pod csi-rbdplugin-provisioner-6f689965cc-wl454 requesting resource cpu=0m on Node cncf-node1
Jan  3 01:47:36.342: INFO: Pod csi-rbdplugin-qk6jk requesting resource cpu=0m on Node cncf-node2
Jan  3 01:47:36.342: INFO: Pod csi-rbdplugin-t2hxd requesting resource cpu=0m on Node cncf-node1
Jan  3 01:47:36.342: INFO: Pod rook-ceph-crashcollector-cncf-master-65b6767fc8-xjlw5 requesting resource cpu=0m on Node cncf-master
Jan  3 01:47:36.342: INFO: Pod rook-ceph-crashcollector-cncf-node1-846d9dbb97-scfg5 requesting resource cpu=0m on Node cncf-node1
Jan  3 01:47:36.342: INFO: Pod rook-ceph-crashcollector-cncf-node2-898ff4558-twdv7 requesting resource cpu=0m on Node cncf-node2
Jan  3 01:47:36.342: INFO: Pod rook-ceph-mds-myfs-a-58f7884986-6th9v requesting resource cpu=0m on Node cncf-node1
Jan  3 01:47:36.342: INFO: Pod rook-ceph-mds-myfs-b-65b48b8c9d-2w82f requesting resource cpu=0m on Node cncf-master
Jan  3 01:47:36.342: INFO: Pod rook-ceph-mgr-a-7949c65bd-kjzgc requesting resource cpu=0m on Node cncf-node1
Jan  3 01:47:36.342: INFO: Pod rook-ceph-mgr-b-856c746f6-wkx97 requesting resource cpu=0m on Node cncf-master
Jan  3 01:47:36.342: INFO: Pod rook-ceph-mon-a-bdd44bcfb-s5hjx requesting resource cpu=0m on Node cncf-node1
Jan  3 01:47:36.342: INFO: Pod rook-ceph-mon-b-66ffc4d4d9-sm6t5 requesting resource cpu=0m on Node cncf-master
Jan  3 01:47:36.342: INFO: Pod rook-ceph-mon-c-6c468d6b45-m7rqj requesting resource cpu=0m on Node cncf-node2
Jan  3 01:47:36.342: INFO: Pod rook-ceph-operator-7f56d78d46-z2j6r requesting resource cpu=0m on Node cncf-node1
Jan  3 01:47:36.342: INFO: Pod rook-ceph-osd-0-7d877c8d99-hq5fw requesting resource cpu=0m on Node cncf-node2
Jan  3 01:47:36.342: INFO: Pod rook-ceph-osd-1-58d74d96db-9ww4l requesting resource cpu=0m on Node cncf-master
Jan  3 01:47:36.342: INFO: Pod rook-ceph-osd-2-5d8bc49787-jd84z requesting resource cpu=0m on Node cncf-node1
Jan  3 01:47:36.342: INFO: Pod rook-ceph-tools-75fb64866c-bcxp7 requesting resource cpu=0m on Node cncf-node1
Jan  3 01:47:36.342: INFO: Pod sonobuoy requesting resource cpu=0m on Node cncf-node1
Jan  3 01:47:36.342: INFO: Pod sonobuoy-e2e-job-a15b13ec9aa249f2 requesting resource cpu=0m on Node cncf-node1
Jan  3 01:47:36.342: INFO: Pod sonobuoy-systemd-logs-daemon-set-58995b31e0564399-fxv6h requesting resource cpu=0m on Node cncf-master
Jan  3 01:47:36.342: INFO: Pod sonobuoy-systemd-logs-daemon-set-58995b31e0564399-gndlm requesting resource cpu=0m on Node cncf-node1
Jan  3 01:47:36.342: INFO: Pod sonobuoy-systemd-logs-daemon-set-58995b31e0564399-wjpkt requesting resource cpu=0m on Node cncf-node2
STEP: Starting Pods to consume most of the cluster CPU. 01/03/23 01:47:36.342
Jan  3 01:47:36.342: INFO: Creating a pod which consumes cpu=9730m on Node cncf-master
Jan  3 01:47:36.359: INFO: Creating a pod which consumes cpu=10045m on Node cncf-node1
Jan  3 01:47:36.374: INFO: Creating a pod which consumes cpu=4725m on Node cncf-node2
Jan  3 01:47:36.379: INFO: Waiting up to 5m0s for pod "filler-pod-46bd6373-9873-4ef5-ad45-4fa7144eb477" in namespace "sched-pred-5211" to be "running"
Jan  3 01:47:36.385: INFO: Pod "filler-pod-46bd6373-9873-4ef5-ad45-4fa7144eb477": Phase="Pending", Reason="", readiness=false. Elapsed: 5.713995ms
Jan  3 01:47:38.388: INFO: Pod "filler-pod-46bd6373-9873-4ef5-ad45-4fa7144eb477": Phase="Running", Reason="", readiness=true. Elapsed: 2.008965931s
Jan  3 01:47:38.388: INFO: Pod "filler-pod-46bd6373-9873-4ef5-ad45-4fa7144eb477" satisfied condition "running"
Jan  3 01:47:38.388: INFO: Waiting up to 5m0s for pod "filler-pod-8f9abc79-a63a-42b6-879c-9443fc702154" in namespace "sched-pred-5211" to be "running"
Jan  3 01:47:38.390: INFO: Pod "filler-pod-8f9abc79-a63a-42b6-879c-9443fc702154": Phase="Running", Reason="", readiness=true. Elapsed: 1.778931ms
Jan  3 01:47:38.390: INFO: Pod "filler-pod-8f9abc79-a63a-42b6-879c-9443fc702154" satisfied condition "running"
Jan  3 01:47:38.390: INFO: Waiting up to 5m0s for pod "filler-pod-8d072cab-72e0-487a-8f83-ae3f35bc7821" in namespace "sched-pred-5211" to be "running"
Jan  3 01:47:38.392: INFO: Pod "filler-pod-8d072cab-72e0-487a-8f83-ae3f35bc7821": Phase="Running", Reason="", readiness=true. Elapsed: 1.627551ms
Jan  3 01:47:38.392: INFO: Pod "filler-pod-8d072cab-72e0-487a-8f83-ae3f35bc7821" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 01/03/23 01:47:38.392
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-46bd6373-9873-4ef5-ad45-4fa7144eb477.1736a94c343e10e1], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5211/filler-pod-46bd6373-9873-4ef5-ad45-4fa7144eb477 to cncf-master] 01/03/23 01:47:38.395
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-46bd6373-9873-4ef5-ad45-4fa7144eb477.1736a94c55d46b48], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/03/23 01:47:38.395
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-46bd6373-9873-4ef5-ad45-4fa7144eb477.1736a94c6249120e], Reason = [Created], Message = [Created container filler-pod-46bd6373-9873-4ef5-ad45-4fa7144eb477] 01/03/23 01:47:38.395
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-46bd6373-9873-4ef5-ad45-4fa7144eb477.1736a94c6453cb67], Reason = [Started], Message = [Started container filler-pod-46bd6373-9873-4ef5-ad45-4fa7144eb477] 01/03/23 01:47:38.395
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8d072cab-72e0-487a-8f83-ae3f35bc7821.1736a94c34eed93c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5211/filler-pod-8d072cab-72e0-487a-8f83-ae3f35bc7821 to cncf-node2] 01/03/23 01:47:38.395
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8d072cab-72e0-487a-8f83-ae3f35bc7821.1736a94c5a14a66c], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/03/23 01:47:38.395
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8d072cab-72e0-487a-8f83-ae3f35bc7821.1736a94c688771e0], Reason = [Created], Message = [Created container filler-pod-8d072cab-72e0-487a-8f83-ae3f35bc7821] 01/03/23 01:47:38.395
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8d072cab-72e0-487a-8f83-ae3f35bc7821.1736a94c6aff24bf], Reason = [Started], Message = [Started container filler-pod-8d072cab-72e0-487a-8f83-ae3f35bc7821] 01/03/23 01:47:38.395
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8f9abc79-a63a-42b6-879c-9443fc702154.1736a94c3494fcb5], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5211/filler-pod-8f9abc79-a63a-42b6-879c-9443fc702154 to cncf-node1] 01/03/23 01:47:38.395
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8f9abc79-a63a-42b6-879c-9443fc702154.1736a94c5a706367], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/03/23 01:47:38.395
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8f9abc79-a63a-42b6-879c-9443fc702154.1736a94c6944d6e3], Reason = [Created], Message = [Created container filler-pod-8f9abc79-a63a-42b6-879c-9443fc702154] 01/03/23 01:47:38.395
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8f9abc79-a63a-42b6-879c-9443fc702154.1736a94c6b51922e], Reason = [Started], Message = [Started container filler-pod-8f9abc79-a63a-42b6-879c-9443fc702154] 01/03/23 01:47:38.395
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1736a94cad09323d], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod.] 01/03/23 01:47:38.41
STEP: removing the label node off the node cncf-master 01/03/23 01:47:39.404
STEP: verifying the node doesn't have the label node 01/03/23 01:47:39.411
STEP: removing the label node off the node cncf-node1 01/03/23 01:47:39.422
STEP: verifying the node doesn't have the label node 01/03/23 01:47:39.431
STEP: removing the label node off the node cncf-node2 01/03/23 01:47:39.453
STEP: verifying the node doesn't have the label node 01/03/23 01:47:39.461
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jan  3 01:47:39.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5211" for this suite. 01/03/23 01:47:39.476
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","completed":5,"skipped":144,"failed":0}
------------------------------
• [3.306 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:47:36.172
    Jan  3 01:47:36.172: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename sched-pred 01/03/23 01:47:36.173
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:47:36.193
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:47:36.195
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jan  3 01:47:36.197: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan  3 01:47:36.202: INFO: Waiting for terminating namespaces to be deleted...
    Jan  3 01:47:36.204: INFO: 
    Logging pods the apiserver thinks is on node cncf-master before test
    Jan  3 01:47:36.215: INFO: postgresql-c6b9bb88b-lqpz6 from hyperauth started at 2023-01-03 01:35:41 +0000 UTC (1 container statuses recorded)
    Jan  3 01:47:36.215: INFO: 	Container postgresql ready: true, restart count 0
    Jan  3 01:47:36.215: INFO: calico-kube-controllers-58dbc876ff-rc9xm from kube-system started at 2022-12-28 00:23:10 +0000 UTC (1 container statuses recorded)
    Jan  3 01:47:36.215: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Jan  3 01:47:36.215: INFO: calico-node-zxb7d from kube-system started at 2022-12-28 00:23:10 +0000 UTC (1 container statuses recorded)
    Jan  3 01:47:36.215: INFO: 	Container calico-node ready: true, restart count 0
    Jan  3 01:47:36.215: INFO: coredns-565d847f94-lwkgz from kube-system started at 2022-12-28 00:23:10 +0000 UTC (1 container statuses recorded)
    Jan  3 01:47:36.215: INFO: 	Container coredns ready: true, restart count 0
    Jan  3 01:47:36.215: INFO: coredns-565d847f94-tsqd7 from kube-system started at 2022-12-28 00:23:10 +0000 UTC (1 container statuses recorded)
    Jan  3 01:47:36.215: INFO: 	Container coredns ready: true, restart count 0
    Jan  3 01:47:36.215: INFO: etcd-cncf-master from kube-system started at 2022-12-28 00:22:57 +0000 UTC (1 container statuses recorded)
    Jan  3 01:47:36.215: INFO: 	Container etcd ready: true, restart count 0
    Jan  3 01:47:36.215: INFO: kube-apiserver-cncf-master from kube-system started at 2023-01-03 01:37:53 +0000 UTC (1 container statuses recorded)
    Jan  3 01:47:36.215: INFO: 	Container kube-apiserver ready: true, restart count 0
    Jan  3 01:47:36.215: INFO: kube-controller-manager-cncf-master from kube-system started at 2022-12-28 00:22:57 +0000 UTC (1 container statuses recorded)
    Jan  3 01:47:36.215: INFO: 	Container kube-controller-manager ready: true, restart count 2
    Jan  3 01:47:36.215: INFO: kube-proxy-qvggp from kube-system started at 2022-12-28 00:23:10 +0000 UTC (1 container statuses recorded)
    Jan  3 01:47:36.215: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  3 01:47:36.215: INFO: kube-scheduler-cncf-master from kube-system started at 2022-12-28 00:22:56 +0000 UTC (1 container statuses recorded)
    Jan  3 01:47:36.215: INFO: 	Container kube-scheduler ready: true, restart count 2
    Jan  3 01:47:36.215: INFO: csi-cephfsplugin-g2vdb from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (2 container statuses recorded)
    Jan  3 01:47:36.215: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
    Jan  3 01:47:36.215: INFO: 	Container driver-registrar ready: true, restart count 0
    Jan  3 01:47:36.215: INFO: csi-rbdplugin-krkxg from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (2 container statuses recorded)
    Jan  3 01:47:36.215: INFO: 	Container csi-rbdplugin ready: true, restart count 0
    Jan  3 01:47:36.215: INFO: 	Container driver-registrar ready: true, restart count 0
    Jan  3 01:47:36.215: INFO: csi-rbdplugin-provisioner-6f689965cc-56vzc from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (5 container statuses recorded)
    Jan  3 01:47:36.215: INFO: 	Container csi-attacher ready: true, restart count 0
    Jan  3 01:47:36.215: INFO: 	Container csi-provisioner ready: true, restart count 0
    Jan  3 01:47:36.215: INFO: 	Container csi-rbdplugin ready: true, restart count 0
    Jan  3 01:47:36.215: INFO: 	Container csi-resizer ready: true, restart count 0
    Jan  3 01:47:36.215: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Jan  3 01:47:36.215: INFO: rook-ceph-crashcollector-cncf-master-65b6767fc8-xjlw5 from rook-ceph started at 2023-01-02 01:35:37 +0000 UTC (1 container statuses recorded)
    Jan  3 01:47:36.215: INFO: 	Container ceph-crash ready: true, restart count 0
    Jan  3 01:47:36.215: INFO: rook-ceph-mds-myfs-b-65b48b8c9d-2w82f from rook-ceph started at 2023-01-02 01:35:37 +0000 UTC (1 container statuses recorded)
    Jan  3 01:47:36.215: INFO: 	Container mds ready: true, restart count 0
    Jan  3 01:47:36.215: INFO: rook-ceph-mgr-b-856c746f6-wkx97 from rook-ceph started at 2022-12-29 08:25:47 +0000 UTC (2 container statuses recorded)
    Jan  3 01:47:36.215: INFO: 	Container mgr ready: true, restart count 0
    Jan  3 01:47:36.215: INFO: 	Container watch-active ready: true, restart count 0
    Jan  3 01:47:36.215: INFO: rook-ceph-mon-b-66ffc4d4d9-sm6t5 from rook-ceph started at 2022-12-29 08:25:22 +0000 UTC (1 container statuses recorded)
    Jan  3 01:47:36.215: INFO: 	Container mon ready: true, restart count 0
    Jan  3 01:47:36.215: INFO: rook-ceph-osd-1-58d74d96db-9ww4l from rook-ceph started at 2023-01-02 01:35:03 +0000 UTC (1 container statuses recorded)
    Jan  3 01:47:36.215: INFO: 	Container osd ready: true, restart count 0
    Jan  3 01:47:36.215: INFO: rook-ceph-osd-prepare-cncf-master-j6dcd from rook-ceph started at 2023-01-03 01:40:53 +0000 UTC (1 container statuses recorded)
    Jan  3 01:47:36.215: INFO: 	Container provision ready: false, restart count 0
    Jan  3 01:47:36.215: INFO: sonobuoy-systemd-logs-daemon-set-58995b31e0564399-fxv6h from sonobuoy started at 2023-01-03 01:46:49 +0000 UTC (2 container statuses recorded)
    Jan  3 01:47:36.215: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  3 01:47:36.215: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  3 01:47:36.215: INFO: 
    Logging pods the apiserver thinks is on node cncf-node1 before test
    Jan  3 01:47:36.230: INFO: traefik-7c456f74d8-b7b6n from api-gateway-system started at 2023-01-03 01:41:21 +0000 UTC (1 container statuses recorded)
    Jan  3 01:47:36.230: INFO: 	Container traefik ready: true, restart count 0
    Jan  3 01:47:36.230: INFO: cert-manager-754886db57-z7pm7 from cert-manager started at 2023-01-03 01:35:12 +0000 UTC (1 container statuses recorded)
    Jan  3 01:47:36.230: INFO: 	Container cert-manager ready: true, restart count 1
    Jan  3 01:47:36.230: INFO: cert-manager-cainjector-66b4c7c78d-vc87n from cert-manager started at 2023-01-03 01:35:12 +0000 UTC (1 container statuses recorded)
    Jan  3 01:47:36.230: INFO: 	Container cert-manager ready: true, restart count 4
    Jan  3 01:47:36.230: INFO: cert-manager-webhook-78cdb5fdf-frlxh from cert-manager started at 2023-01-03 01:35:12 +0000 UTC (1 container statuses recorded)
    Jan  3 01:47:36.230: INFO: 	Container cert-manager ready: true, restart count 0
    Jan  3 01:47:36.230: INFO: console-59c484c98b-mcqpt from console-system started at 2023-01-03 01:41:24 +0000 UTC (1 container statuses recorded)
    Jan  3 01:47:36.230: INFO: 	Container console ready: true, restart count 0
    Jan  3 01:47:36.230: INFO: hyperauth-5cbb4d58b8-qpc2d from hyperauth started at 2023-01-03 01:37:01 +0000 UTC (1 container statuses recorded)
    Jan  3 01:47:36.230: INFO: 	Container hyperauth ready: true, restart count 0
    Jan  3 01:47:36.230: INFO: hypercloud5-api-server-8c944885b-mk6vb from hypercloud5-system started at 2023-01-03 01:38:50 +0000 UTC (1 container statuses recorded)
    Jan  3 01:47:36.230: INFO: 	Container hypercloud5-api-server ready: true, restart count 4
    Jan  3 01:47:36.230: INFO: timescaledb-0 from hypercloud5-system started at 2023-01-03 01:38:50 +0000 UTC (1 container statuses recorded)
    Jan  3 01:47:36.230: INFO: 	Container timescaledb ready: true, restart count 0
    Jan  3 01:47:36.230: INFO: calico-node-9p9ff from kube-system started at 2022-12-28 00:54:44 +0000 UTC (1 container statuses recorded)
    Jan  3 01:47:36.230: INFO: 	Container calico-node ready: true, restart count 0
    Jan  3 01:47:36.230: INFO: kube-proxy-jplgg from kube-system started at 2022-12-28 00:54:44 +0000 UTC (1 container statuses recorded)
    Jan  3 01:47:36.230: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  3 01:47:36.230: INFO: snapshot-controller-545d7f8994-vg2ld from kube-system started at 2022-12-29 08:24:10 +0000 UTC (1 container statuses recorded)
    Jan  3 01:47:36.230: INFO: 	Container snapshot-controller ready: true, restart count 3
    Jan  3 01:47:36.230: INFO: csi-cephfsplugin-provisioner-69f54fb566-jxcpx from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (5 container statuses recorded)
    Jan  3 01:47:36.230: INFO: 	Container csi-attacher ready: true, restart count 0
    Jan  3 01:47:36.231: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
    Jan  3 01:47:36.231: INFO: 	Container csi-provisioner ready: true, restart count 0
    Jan  3 01:47:36.231: INFO: 	Container csi-resizer ready: true, restart count 0
    Jan  3 01:47:36.231: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Jan  3 01:47:36.231: INFO: csi-cephfsplugin-stnqm from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (2 container statuses recorded)
    Jan  3 01:47:36.231: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
    Jan  3 01:47:36.231: INFO: 	Container driver-registrar ready: true, restart count 0
    Jan  3 01:47:36.231: INFO: csi-rbdplugin-provisioner-6f689965cc-wl454 from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (5 container statuses recorded)
    Jan  3 01:47:36.231: INFO: 	Container csi-attacher ready: true, restart count 0
    Jan  3 01:47:36.231: INFO: 	Container csi-provisioner ready: true, restart count 0
    Jan  3 01:47:36.231: INFO: 	Container csi-rbdplugin ready: true, restart count 0
    Jan  3 01:47:36.231: INFO: 	Container csi-resizer ready: true, restart count 0
    Jan  3 01:47:36.231: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Jan  3 01:47:36.231: INFO: csi-rbdplugin-t2hxd from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (2 container statuses recorded)
    Jan  3 01:47:36.231: INFO: 	Container csi-rbdplugin ready: true, restart count 0
    Jan  3 01:47:36.231: INFO: 	Container driver-registrar ready: true, restart count 0
    Jan  3 01:47:36.231: INFO: rook-ceph-crashcollector-cncf-node1-846d9dbb97-scfg5 from rook-ceph started at 2023-01-02 01:35:36 +0000 UTC (1 container statuses recorded)
    Jan  3 01:47:36.231: INFO: 	Container ceph-crash ready: true, restart count 0
    Jan  3 01:47:36.231: INFO: rook-ceph-mds-myfs-a-58f7884986-6th9v from rook-ceph started at 2023-01-02 01:35:36 +0000 UTC (1 container statuses recorded)
    Jan  3 01:47:36.231: INFO: 	Container mds ready: true, restart count 0
    Jan  3 01:47:36.231: INFO: rook-ceph-mgr-a-7949c65bd-kjzgc from rook-ceph started at 2022-12-29 08:25:47 +0000 UTC (2 container statuses recorded)
    Jan  3 01:47:36.231: INFO: 	Container mgr ready: true, restart count 0
    Jan  3 01:47:36.231: INFO: 	Container watch-active ready: true, restart count 0
    Jan  3 01:47:36.231: INFO: rook-ceph-mon-a-bdd44bcfb-s5hjx from rook-ceph started at 2022-12-29 08:24:58 +0000 UTC (1 container statuses recorded)
    Jan  3 01:47:36.231: INFO: 	Container mon ready: true, restart count 0
    Jan  3 01:47:36.231: INFO: rook-ceph-operator-7f56d78d46-z2j6r from rook-ceph started at 2023-01-02 01:29:14 +0000 UTC (1 container statuses recorded)
    Jan  3 01:47:36.231: INFO: 	Container rook-ceph-operator ready: true, restart count 0
    Jan  3 01:47:36.231: INFO: rook-ceph-osd-2-5d8bc49787-jd84z from rook-ceph started at 2023-01-02 01:35:03 +0000 UTC (1 container statuses recorded)
    Jan  3 01:47:36.231: INFO: 	Container osd ready: true, restart count 0
    Jan  3 01:47:36.231: INFO: rook-ceph-osd-prepare-cncf-node1-89n8c from rook-ceph started at 2023-01-03 01:40:56 +0000 UTC (1 container statuses recorded)
    Jan  3 01:47:36.231: INFO: 	Container provision ready: false, restart count 0
    Jan  3 01:47:36.231: INFO: rook-ceph-tools-75fb64866c-bcxp7 from rook-ceph started at 2022-12-30 02:06:28 +0000 UTC (1 container statuses recorded)
    Jan  3 01:47:36.231: INFO: 	Container rook-ceph-tools ready: true, restart count 0
    Jan  3 01:47:36.231: INFO: sonobuoy from sonobuoy started at 2023-01-03 01:46:49 +0000 UTC (1 container statuses recorded)
    Jan  3 01:47:36.231: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan  3 01:47:36.231: INFO: sonobuoy-e2e-job-a15b13ec9aa249f2 from sonobuoy started at 2023-01-03 01:46:49 +0000 UTC (2 container statuses recorded)
    Jan  3 01:47:36.231: INFO: 	Container e2e ready: true, restart count 0
    Jan  3 01:47:36.231: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  3 01:47:36.231: INFO: sonobuoy-systemd-logs-daemon-set-58995b31e0564399-gndlm from sonobuoy started at 2023-01-03 01:46:49 +0000 UTC (2 container statuses recorded)
    Jan  3 01:47:36.231: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  3 01:47:36.231: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  3 01:47:36.231: INFO: 
    Logging pods the apiserver thinks is on node cncf-node2 before test
    Jan  3 01:47:36.248: INFO: hyperauth-5cbb4d58b8-pqhc5 from hyperauth started at 2023-01-03 01:37:01 +0000 UTC (1 container statuses recorded)
    Jan  3 01:47:36.248: INFO: 	Container hyperauth ready: true, restart count 0
    Jan  3 01:47:36.248: INFO: calico-node-5f5qs from kube-system started at 2022-12-29 05:30:03 +0000 UTC (1 container statuses recorded)
    Jan  3 01:47:36.248: INFO: 	Container calico-node ready: true, restart count 0
    Jan  3 01:47:36.248: INFO: kube-proxy-ddlsm from kube-system started at 2022-12-29 05:30:03 +0000 UTC (1 container statuses recorded)
    Jan  3 01:47:36.248: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  3 01:47:36.248: INFO: snapshot-controller-545d7f8994-qgm6l from kube-system started at 2022-12-29 08:24:10 +0000 UTC (1 container statuses recorded)
    Jan  3 01:47:36.248: INFO: 	Container snapshot-controller ready: true, restart count 0
    Jan  3 01:47:36.248: INFO: csi-cephfsplugin-hxtjf from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (2 container statuses recorded)
    Jan  3 01:47:36.248: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
    Jan  3 01:47:36.248: INFO: 	Container driver-registrar ready: true, restart count 0
    Jan  3 01:47:36.248: INFO: csi-cephfsplugin-provisioner-69f54fb566-xhc7t from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (5 container statuses recorded)
    Jan  3 01:47:36.248: INFO: 	Container csi-attacher ready: true, restart count 0
    Jan  3 01:47:36.248: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
    Jan  3 01:47:36.248: INFO: 	Container csi-provisioner ready: true, restart count 0
    Jan  3 01:47:36.248: INFO: 	Container csi-resizer ready: true, restart count 0
    Jan  3 01:47:36.248: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Jan  3 01:47:36.248: INFO: csi-rbdplugin-qk6jk from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (2 container statuses recorded)
    Jan  3 01:47:36.248: INFO: 	Container csi-rbdplugin ready: true, restart count 0
    Jan  3 01:47:36.248: INFO: 	Container driver-registrar ready: true, restart count 0
    Jan  3 01:47:36.248: INFO: rook-ceph-crashcollector-cncf-node2-898ff4558-twdv7 from rook-ceph started at 2022-12-29 08:26:03 +0000 UTC (1 container statuses recorded)
    Jan  3 01:47:36.248: INFO: 	Container ceph-crash ready: true, restart count 0
    Jan  3 01:47:36.248: INFO: rook-ceph-mon-c-6c468d6b45-m7rqj from rook-ceph started at 2022-12-29 08:25:33 +0000 UTC (1 container statuses recorded)
    Jan  3 01:47:36.248: INFO: 	Container mon ready: true, restart count 0
    Jan  3 01:47:36.248: INFO: rook-ceph-osd-0-7d877c8d99-hq5fw from rook-ceph started at 2023-01-02 01:35:02 +0000 UTC (1 container statuses recorded)
    Jan  3 01:47:36.248: INFO: 	Container osd ready: true, restart count 0
    Jan  3 01:47:36.248: INFO: rook-ceph-osd-prepare-cncf-node2-rnwb6 from rook-ceph started at 2023-01-03 01:40:59 +0000 UTC (1 container statuses recorded)
    Jan  3 01:47:36.248: INFO: 	Container provision ready: false, restart count 0
    Jan  3 01:47:36.248: INFO: sonobuoy-systemd-logs-daemon-set-58995b31e0564399-wjpkt from sonobuoy started at 2023-01-03 01:46:49 +0000 UTC (2 container statuses recorded)
    Jan  3 01:47:36.248: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  3 01:47:36.248: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:326
    STEP: verifying the node has the label node cncf-master 01/03/23 01:47:36.28
    STEP: verifying the node has the label node cncf-node1 01/03/23 01:47:36.295
    STEP: verifying the node has the label node cncf-node2 01/03/23 01:47:36.315
    Jan  3 01:47:36.342: INFO: Pod traefik-7c456f74d8-b7b6n requesting resource cpu=0m on Node cncf-node1
    Jan  3 01:47:36.342: INFO: Pod cert-manager-754886db57-z7pm7 requesting resource cpu=0m on Node cncf-node1
    Jan  3 01:47:36.342: INFO: Pod cert-manager-cainjector-66b4c7c78d-vc87n requesting resource cpu=0m on Node cncf-node1
    Jan  3 01:47:36.342: INFO: Pod cert-manager-webhook-78cdb5fdf-frlxh requesting resource cpu=0m on Node cncf-node1
    Jan  3 01:47:36.342: INFO: Pod console-59c484c98b-mcqpt requesting resource cpu=0m on Node cncf-node1
    Jan  3 01:47:36.342: INFO: Pod hyperauth-5cbb4d58b8-pqhc5 requesting resource cpu=1000m on Node cncf-node2
    Jan  3 01:47:36.342: INFO: Pod hyperauth-5cbb4d58b8-qpc2d requesting resource cpu=1000m on Node cncf-node1
    Jan  3 01:47:36.342: INFO: Pod postgresql-c6b9bb88b-lqpz6 requesting resource cpu=1000m on Node cncf-master
    Jan  3 01:47:36.342: INFO: Pod hypercloud5-api-server-8c944885b-mk6vb requesting resource cpu=300m on Node cncf-node1
    Jan  3 01:47:36.342: INFO: Pod timescaledb-0 requesting resource cpu=100m on Node cncf-node1
    Jan  3 01:47:36.342: INFO: Pod calico-kube-controllers-58dbc876ff-rc9xm requesting resource cpu=0m on Node cncf-master
    Jan  3 01:47:36.342: INFO: Pod calico-node-5f5qs requesting resource cpu=250m on Node cncf-node2
    Jan  3 01:47:36.342: INFO: Pod calico-node-9p9ff requesting resource cpu=250m on Node cncf-node1
    Jan  3 01:47:36.342: INFO: Pod calico-node-zxb7d requesting resource cpu=250m on Node cncf-master
    Jan  3 01:47:36.342: INFO: Pod coredns-565d847f94-lwkgz requesting resource cpu=100m on Node cncf-master
    Jan  3 01:47:36.342: INFO: Pod coredns-565d847f94-tsqd7 requesting resource cpu=100m on Node cncf-master
    Jan  3 01:47:36.342: INFO: Pod etcd-cncf-master requesting resource cpu=100m on Node cncf-master
    Jan  3 01:47:36.342: INFO: Pod kube-apiserver-cncf-master requesting resource cpu=250m on Node cncf-master
    Jan  3 01:47:36.342: INFO: Pod kube-controller-manager-cncf-master requesting resource cpu=200m on Node cncf-master
    Jan  3 01:47:36.342: INFO: Pod kube-proxy-ddlsm requesting resource cpu=0m on Node cncf-node2
    Jan  3 01:47:36.342: INFO: Pod kube-proxy-jplgg requesting resource cpu=0m on Node cncf-node1
    Jan  3 01:47:36.342: INFO: Pod kube-proxy-qvggp requesting resource cpu=0m on Node cncf-master
    Jan  3 01:47:36.342: INFO: Pod kube-scheduler-cncf-master requesting resource cpu=100m on Node cncf-master
    Jan  3 01:47:36.342: INFO: Pod snapshot-controller-545d7f8994-qgm6l requesting resource cpu=0m on Node cncf-node2
    Jan  3 01:47:36.342: INFO: Pod snapshot-controller-545d7f8994-vg2ld requesting resource cpu=0m on Node cncf-node1
    Jan  3 01:47:36.342: INFO: Pod csi-cephfsplugin-g2vdb requesting resource cpu=0m on Node cncf-master
    Jan  3 01:47:36.342: INFO: Pod csi-cephfsplugin-hxtjf requesting resource cpu=0m on Node cncf-node2
    Jan  3 01:47:36.342: INFO: Pod csi-cephfsplugin-provisioner-69f54fb566-jxcpx requesting resource cpu=0m on Node cncf-node1
    Jan  3 01:47:36.342: INFO: Pod csi-cephfsplugin-provisioner-69f54fb566-xhc7t requesting resource cpu=0m on Node cncf-node2
    Jan  3 01:47:36.342: INFO: Pod csi-cephfsplugin-stnqm requesting resource cpu=0m on Node cncf-node1
    Jan  3 01:47:36.342: INFO: Pod csi-rbdplugin-krkxg requesting resource cpu=0m on Node cncf-master
    Jan  3 01:47:36.342: INFO: Pod csi-rbdplugin-provisioner-6f689965cc-56vzc requesting resource cpu=0m on Node cncf-master
    Jan  3 01:47:36.342: INFO: Pod csi-rbdplugin-provisioner-6f689965cc-wl454 requesting resource cpu=0m on Node cncf-node1
    Jan  3 01:47:36.342: INFO: Pod csi-rbdplugin-qk6jk requesting resource cpu=0m on Node cncf-node2
    Jan  3 01:47:36.342: INFO: Pod csi-rbdplugin-t2hxd requesting resource cpu=0m on Node cncf-node1
    Jan  3 01:47:36.342: INFO: Pod rook-ceph-crashcollector-cncf-master-65b6767fc8-xjlw5 requesting resource cpu=0m on Node cncf-master
    Jan  3 01:47:36.342: INFO: Pod rook-ceph-crashcollector-cncf-node1-846d9dbb97-scfg5 requesting resource cpu=0m on Node cncf-node1
    Jan  3 01:47:36.342: INFO: Pod rook-ceph-crashcollector-cncf-node2-898ff4558-twdv7 requesting resource cpu=0m on Node cncf-node2
    Jan  3 01:47:36.342: INFO: Pod rook-ceph-mds-myfs-a-58f7884986-6th9v requesting resource cpu=0m on Node cncf-node1
    Jan  3 01:47:36.342: INFO: Pod rook-ceph-mds-myfs-b-65b48b8c9d-2w82f requesting resource cpu=0m on Node cncf-master
    Jan  3 01:47:36.342: INFO: Pod rook-ceph-mgr-a-7949c65bd-kjzgc requesting resource cpu=0m on Node cncf-node1
    Jan  3 01:47:36.342: INFO: Pod rook-ceph-mgr-b-856c746f6-wkx97 requesting resource cpu=0m on Node cncf-master
    Jan  3 01:47:36.342: INFO: Pod rook-ceph-mon-a-bdd44bcfb-s5hjx requesting resource cpu=0m on Node cncf-node1
    Jan  3 01:47:36.342: INFO: Pod rook-ceph-mon-b-66ffc4d4d9-sm6t5 requesting resource cpu=0m on Node cncf-master
    Jan  3 01:47:36.342: INFO: Pod rook-ceph-mon-c-6c468d6b45-m7rqj requesting resource cpu=0m on Node cncf-node2
    Jan  3 01:47:36.342: INFO: Pod rook-ceph-operator-7f56d78d46-z2j6r requesting resource cpu=0m on Node cncf-node1
    Jan  3 01:47:36.342: INFO: Pod rook-ceph-osd-0-7d877c8d99-hq5fw requesting resource cpu=0m on Node cncf-node2
    Jan  3 01:47:36.342: INFO: Pod rook-ceph-osd-1-58d74d96db-9ww4l requesting resource cpu=0m on Node cncf-master
    Jan  3 01:47:36.342: INFO: Pod rook-ceph-osd-2-5d8bc49787-jd84z requesting resource cpu=0m on Node cncf-node1
    Jan  3 01:47:36.342: INFO: Pod rook-ceph-tools-75fb64866c-bcxp7 requesting resource cpu=0m on Node cncf-node1
    Jan  3 01:47:36.342: INFO: Pod sonobuoy requesting resource cpu=0m on Node cncf-node1
    Jan  3 01:47:36.342: INFO: Pod sonobuoy-e2e-job-a15b13ec9aa249f2 requesting resource cpu=0m on Node cncf-node1
    Jan  3 01:47:36.342: INFO: Pod sonobuoy-systemd-logs-daemon-set-58995b31e0564399-fxv6h requesting resource cpu=0m on Node cncf-master
    Jan  3 01:47:36.342: INFO: Pod sonobuoy-systemd-logs-daemon-set-58995b31e0564399-gndlm requesting resource cpu=0m on Node cncf-node1
    Jan  3 01:47:36.342: INFO: Pod sonobuoy-systemd-logs-daemon-set-58995b31e0564399-wjpkt requesting resource cpu=0m on Node cncf-node2
    STEP: Starting Pods to consume most of the cluster CPU. 01/03/23 01:47:36.342
    Jan  3 01:47:36.342: INFO: Creating a pod which consumes cpu=9730m on Node cncf-master
    Jan  3 01:47:36.359: INFO: Creating a pod which consumes cpu=10045m on Node cncf-node1
    Jan  3 01:47:36.374: INFO: Creating a pod which consumes cpu=4725m on Node cncf-node2
    Jan  3 01:47:36.379: INFO: Waiting up to 5m0s for pod "filler-pod-46bd6373-9873-4ef5-ad45-4fa7144eb477" in namespace "sched-pred-5211" to be "running"
    Jan  3 01:47:36.385: INFO: Pod "filler-pod-46bd6373-9873-4ef5-ad45-4fa7144eb477": Phase="Pending", Reason="", readiness=false. Elapsed: 5.713995ms
    Jan  3 01:47:38.388: INFO: Pod "filler-pod-46bd6373-9873-4ef5-ad45-4fa7144eb477": Phase="Running", Reason="", readiness=true. Elapsed: 2.008965931s
    Jan  3 01:47:38.388: INFO: Pod "filler-pod-46bd6373-9873-4ef5-ad45-4fa7144eb477" satisfied condition "running"
    Jan  3 01:47:38.388: INFO: Waiting up to 5m0s for pod "filler-pod-8f9abc79-a63a-42b6-879c-9443fc702154" in namespace "sched-pred-5211" to be "running"
    Jan  3 01:47:38.390: INFO: Pod "filler-pod-8f9abc79-a63a-42b6-879c-9443fc702154": Phase="Running", Reason="", readiness=true. Elapsed: 1.778931ms
    Jan  3 01:47:38.390: INFO: Pod "filler-pod-8f9abc79-a63a-42b6-879c-9443fc702154" satisfied condition "running"
    Jan  3 01:47:38.390: INFO: Waiting up to 5m0s for pod "filler-pod-8d072cab-72e0-487a-8f83-ae3f35bc7821" in namespace "sched-pred-5211" to be "running"
    Jan  3 01:47:38.392: INFO: Pod "filler-pod-8d072cab-72e0-487a-8f83-ae3f35bc7821": Phase="Running", Reason="", readiness=true. Elapsed: 1.627551ms
    Jan  3 01:47:38.392: INFO: Pod "filler-pod-8d072cab-72e0-487a-8f83-ae3f35bc7821" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 01/03/23 01:47:38.392
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-46bd6373-9873-4ef5-ad45-4fa7144eb477.1736a94c343e10e1], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5211/filler-pod-46bd6373-9873-4ef5-ad45-4fa7144eb477 to cncf-master] 01/03/23 01:47:38.395
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-46bd6373-9873-4ef5-ad45-4fa7144eb477.1736a94c55d46b48], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/03/23 01:47:38.395
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-46bd6373-9873-4ef5-ad45-4fa7144eb477.1736a94c6249120e], Reason = [Created], Message = [Created container filler-pod-46bd6373-9873-4ef5-ad45-4fa7144eb477] 01/03/23 01:47:38.395
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-46bd6373-9873-4ef5-ad45-4fa7144eb477.1736a94c6453cb67], Reason = [Started], Message = [Started container filler-pod-46bd6373-9873-4ef5-ad45-4fa7144eb477] 01/03/23 01:47:38.395
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-8d072cab-72e0-487a-8f83-ae3f35bc7821.1736a94c34eed93c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5211/filler-pod-8d072cab-72e0-487a-8f83-ae3f35bc7821 to cncf-node2] 01/03/23 01:47:38.395
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-8d072cab-72e0-487a-8f83-ae3f35bc7821.1736a94c5a14a66c], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/03/23 01:47:38.395
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-8d072cab-72e0-487a-8f83-ae3f35bc7821.1736a94c688771e0], Reason = [Created], Message = [Created container filler-pod-8d072cab-72e0-487a-8f83-ae3f35bc7821] 01/03/23 01:47:38.395
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-8d072cab-72e0-487a-8f83-ae3f35bc7821.1736a94c6aff24bf], Reason = [Started], Message = [Started container filler-pod-8d072cab-72e0-487a-8f83-ae3f35bc7821] 01/03/23 01:47:38.395
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-8f9abc79-a63a-42b6-879c-9443fc702154.1736a94c3494fcb5], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5211/filler-pod-8f9abc79-a63a-42b6-879c-9443fc702154 to cncf-node1] 01/03/23 01:47:38.395
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-8f9abc79-a63a-42b6-879c-9443fc702154.1736a94c5a706367], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/03/23 01:47:38.395
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-8f9abc79-a63a-42b6-879c-9443fc702154.1736a94c6944d6e3], Reason = [Created], Message = [Created container filler-pod-8f9abc79-a63a-42b6-879c-9443fc702154] 01/03/23 01:47:38.395
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-8f9abc79-a63a-42b6-879c-9443fc702154.1736a94c6b51922e], Reason = [Started], Message = [Started container filler-pod-8f9abc79-a63a-42b6-879c-9443fc702154] 01/03/23 01:47:38.395
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.1736a94cad09323d], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod.] 01/03/23 01:47:38.41
    STEP: removing the label node off the node cncf-master 01/03/23 01:47:39.404
    STEP: verifying the node doesn't have the label node 01/03/23 01:47:39.411
    STEP: removing the label node off the node cncf-node1 01/03/23 01:47:39.422
    STEP: verifying the node doesn't have the label node 01/03/23 01:47:39.431
    STEP: removing the label node off the node cncf-node2 01/03/23 01:47:39.453
    STEP: verifying the node doesn't have the label node 01/03/23 01:47:39.461
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jan  3 01:47:39.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-5211" for this suite. 01/03/23 01:47:39.476
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:47:39.48
Jan  3 01:47:39.480: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename csistoragecapacity 01/03/23 01:47:39.48
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:47:39.497
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:47:39.499
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 01/03/23 01:47:39.501
STEP: getting /apis/storage.k8s.io 01/03/23 01:47:39.502
STEP: getting /apis/storage.k8s.io/v1 01/03/23 01:47:39.503
STEP: creating 01/03/23 01:47:39.504
STEP: watching 01/03/23 01:47:39.523
Jan  3 01:47:39.523: INFO: starting watch
STEP: getting 01/03/23 01:47:39.527
STEP: listing in namespace 01/03/23 01:47:39.531
STEP: listing across namespaces 01/03/23 01:47:39.533
STEP: patching 01/03/23 01:47:39.534
STEP: updating 01/03/23 01:47:39.537
Jan  3 01:47:39.567: INFO: waiting for watch events with expected annotations in namespace
Jan  3 01:47:39.567: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 01/03/23 01:47:39.567
STEP: deleting a collection 01/03/23 01:47:39.584
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:187
Jan  3 01:47:39.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "csistoragecapacity-6349" for this suite. 01/03/23 01:47:39.597
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]","completed":6,"skipped":157,"failed":0}
------------------------------
• [0.119 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:47:39.48
    Jan  3 01:47:39.480: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename csistoragecapacity 01/03/23 01:47:39.48
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:47:39.497
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:47:39.499
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 01/03/23 01:47:39.501
    STEP: getting /apis/storage.k8s.io 01/03/23 01:47:39.502
    STEP: getting /apis/storage.k8s.io/v1 01/03/23 01:47:39.503
    STEP: creating 01/03/23 01:47:39.504
    STEP: watching 01/03/23 01:47:39.523
    Jan  3 01:47:39.523: INFO: starting watch
    STEP: getting 01/03/23 01:47:39.527
    STEP: listing in namespace 01/03/23 01:47:39.531
    STEP: listing across namespaces 01/03/23 01:47:39.533
    STEP: patching 01/03/23 01:47:39.534
    STEP: updating 01/03/23 01:47:39.537
    Jan  3 01:47:39.567: INFO: waiting for watch events with expected annotations in namespace
    Jan  3 01:47:39.567: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 01/03/23 01:47:39.567
    STEP: deleting a collection 01/03/23 01:47:39.584
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:187
    Jan  3 01:47:39.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "csistoragecapacity-6349" for this suite. 01/03/23 01:47:39.597
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:47:39.599
Jan  3 01:47:39.600: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename job 01/03/23 01:47:39.601
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:47:39.617
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:47:39.619
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
STEP: Creating a job 01/03/23 01:47:39.62
STEP: Ensure pods equal to paralellism count is attached to the job 01/03/23 01:47:39.625
STEP: patching /status 01/03/23 01:47:41.628
STEP: updating /status 01/03/23 01:47:41.631
STEP: get /status 01/03/23 01:47:41.657
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan  3 01:47:41.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6617" for this suite. 01/03/23 01:47:41.661
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Job should apply changes to a job status [Conformance]","completed":7,"skipped":164,"failed":0}
------------------------------
• [2.064 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:47:39.599
    Jan  3 01:47:39.600: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename job 01/03/23 01:47:39.601
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:47:39.617
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:47:39.619
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:464
    STEP: Creating a job 01/03/23 01:47:39.62
    STEP: Ensure pods equal to paralellism count is attached to the job 01/03/23 01:47:39.625
    STEP: patching /status 01/03/23 01:47:41.628
    STEP: updating /status 01/03/23 01:47:41.631
    STEP: get /status 01/03/23 01:47:41.657
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan  3 01:47:41.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-6617" for this suite. 01/03/23 01:47:41.661
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:47:41.665
Jan  3 01:47:41.665: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename projected 01/03/23 01:47:41.665
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:47:41.704
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:47:41.706
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
STEP: Creating a pod to test downward API volume plugin 01/03/23 01:47:41.708
Jan  3 01:47:41.714: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c15c58ce-b1e4-49b3-b56c-5055faa66b52" in namespace "projected-260" to be "Succeeded or Failed"
Jan  3 01:47:41.724: INFO: Pod "downwardapi-volume-c15c58ce-b1e4-49b3-b56c-5055faa66b52": Phase="Pending", Reason="", readiness=false. Elapsed: 9.600171ms
Jan  3 01:47:43.727: INFO: Pod "downwardapi-volume-c15c58ce-b1e4-49b3-b56c-5055faa66b52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013148259s
Jan  3 01:47:45.726: INFO: Pod "downwardapi-volume-c15c58ce-b1e4-49b3-b56c-5055faa66b52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011976327s
STEP: Saw pod success 01/03/23 01:47:45.726
Jan  3 01:47:45.726: INFO: Pod "downwardapi-volume-c15c58ce-b1e4-49b3-b56c-5055faa66b52" satisfied condition "Succeeded or Failed"
Jan  3 01:47:45.728: INFO: Trying to get logs from node cncf-master pod downwardapi-volume-c15c58ce-b1e4-49b3-b56c-5055faa66b52 container client-container: <nil>
STEP: delete the pod 01/03/23 01:47:45.74
Jan  3 01:47:45.779: INFO: Waiting for pod downwardapi-volume-c15c58ce-b1e4-49b3-b56c-5055faa66b52 to disappear
Jan  3 01:47:45.784: INFO: Pod downwardapi-volume-c15c58ce-b1e4-49b3-b56c-5055faa66b52 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan  3 01:47:45.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-260" for this suite. 01/03/23 01:47:45.787
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":8,"skipped":168,"failed":0}
------------------------------
• [4.125 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:47:41.665
    Jan  3 01:47:41.665: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename projected 01/03/23 01:47:41.665
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:47:41.704
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:47:41.706
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:260
    STEP: Creating a pod to test downward API volume plugin 01/03/23 01:47:41.708
    Jan  3 01:47:41.714: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c15c58ce-b1e4-49b3-b56c-5055faa66b52" in namespace "projected-260" to be "Succeeded or Failed"
    Jan  3 01:47:41.724: INFO: Pod "downwardapi-volume-c15c58ce-b1e4-49b3-b56c-5055faa66b52": Phase="Pending", Reason="", readiness=false. Elapsed: 9.600171ms
    Jan  3 01:47:43.727: INFO: Pod "downwardapi-volume-c15c58ce-b1e4-49b3-b56c-5055faa66b52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013148259s
    Jan  3 01:47:45.726: INFO: Pod "downwardapi-volume-c15c58ce-b1e4-49b3-b56c-5055faa66b52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011976327s
    STEP: Saw pod success 01/03/23 01:47:45.726
    Jan  3 01:47:45.726: INFO: Pod "downwardapi-volume-c15c58ce-b1e4-49b3-b56c-5055faa66b52" satisfied condition "Succeeded or Failed"
    Jan  3 01:47:45.728: INFO: Trying to get logs from node cncf-master pod downwardapi-volume-c15c58ce-b1e4-49b3-b56c-5055faa66b52 container client-container: <nil>
    STEP: delete the pod 01/03/23 01:47:45.74
    Jan  3 01:47:45.779: INFO: Waiting for pod downwardapi-volume-c15c58ce-b1e4-49b3-b56c-5055faa66b52 to disappear
    Jan  3 01:47:45.784: INFO: Pod downwardapi-volume-c15c58ce-b1e4-49b3-b56c-5055faa66b52 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan  3 01:47:45.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-260" for this suite. 01/03/23 01:47:45.787
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:47:45.79
Jan  3 01:47:45.790: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename configmap 01/03/23 01:47:45.791
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:47:45.82
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:47:45.822
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
STEP: Creating configMap configmap-709/configmap-test-e2dc724d-25cd-431b-9315-32f5271820aa 01/03/23 01:47:45.824
STEP: Creating a pod to test consume configMaps 01/03/23 01:47:45.827
Jan  3 01:47:45.846: INFO: Waiting up to 5m0s for pod "pod-configmaps-c530bbbc-ffc8-4ee6-8140-8ca3f3a91f4c" in namespace "configmap-709" to be "Succeeded or Failed"
Jan  3 01:47:45.889: INFO: Pod "pod-configmaps-c530bbbc-ffc8-4ee6-8140-8ca3f3a91f4c": Phase="Pending", Reason="", readiness=false. Elapsed: 43.093756ms
Jan  3 01:47:47.892: INFO: Pod "pod-configmaps-c530bbbc-ffc8-4ee6-8140-8ca3f3a91f4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046073603s
Jan  3 01:47:49.891: INFO: Pod "pod-configmaps-c530bbbc-ffc8-4ee6-8140-8ca3f3a91f4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045207293s
STEP: Saw pod success 01/03/23 01:47:49.891
Jan  3 01:47:49.891: INFO: Pod "pod-configmaps-c530bbbc-ffc8-4ee6-8140-8ca3f3a91f4c" satisfied condition "Succeeded or Failed"
Jan  3 01:47:49.893: INFO: Trying to get logs from node cncf-master pod pod-configmaps-c530bbbc-ffc8-4ee6-8140-8ca3f3a91f4c container env-test: <nil>
STEP: delete the pod 01/03/23 01:47:49.896
Jan  3 01:47:49.922: INFO: Waiting for pod pod-configmaps-c530bbbc-ffc8-4ee6-8140-8ca3f3a91f4c to disappear
Jan  3 01:47:49.939: INFO: Pod pod-configmaps-c530bbbc-ffc8-4ee6-8140-8ca3f3a91f4c no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jan  3 01:47:49.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-709" for this suite. 01/03/23 01:47:49.942
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","completed":9,"skipped":170,"failed":0}
------------------------------
• [4.155 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:47:45.79
    Jan  3 01:47:45.790: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename configmap 01/03/23 01:47:45.791
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:47:45.82
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:47:45.822
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:44
    STEP: Creating configMap configmap-709/configmap-test-e2dc724d-25cd-431b-9315-32f5271820aa 01/03/23 01:47:45.824
    STEP: Creating a pod to test consume configMaps 01/03/23 01:47:45.827
    Jan  3 01:47:45.846: INFO: Waiting up to 5m0s for pod "pod-configmaps-c530bbbc-ffc8-4ee6-8140-8ca3f3a91f4c" in namespace "configmap-709" to be "Succeeded or Failed"
    Jan  3 01:47:45.889: INFO: Pod "pod-configmaps-c530bbbc-ffc8-4ee6-8140-8ca3f3a91f4c": Phase="Pending", Reason="", readiness=false. Elapsed: 43.093756ms
    Jan  3 01:47:47.892: INFO: Pod "pod-configmaps-c530bbbc-ffc8-4ee6-8140-8ca3f3a91f4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046073603s
    Jan  3 01:47:49.891: INFO: Pod "pod-configmaps-c530bbbc-ffc8-4ee6-8140-8ca3f3a91f4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045207293s
    STEP: Saw pod success 01/03/23 01:47:49.891
    Jan  3 01:47:49.891: INFO: Pod "pod-configmaps-c530bbbc-ffc8-4ee6-8140-8ca3f3a91f4c" satisfied condition "Succeeded or Failed"
    Jan  3 01:47:49.893: INFO: Trying to get logs from node cncf-master pod pod-configmaps-c530bbbc-ffc8-4ee6-8140-8ca3f3a91f4c container env-test: <nil>
    STEP: delete the pod 01/03/23 01:47:49.896
    Jan  3 01:47:49.922: INFO: Waiting for pod pod-configmaps-c530bbbc-ffc8-4ee6-8140-8ca3f3a91f4c to disappear
    Jan  3 01:47:49.939: INFO: Pod pod-configmaps-c530bbbc-ffc8-4ee6-8140-8ca3f3a91f4c no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jan  3 01:47:49.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-709" for this suite. 01/03/23 01:47:49.942
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:47:49.946
Jan  3 01:47:49.946: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename configmap 01/03/23 01:47:49.947
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:47:49.991
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:47:49.994
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
STEP: creating a ConfigMap 01/03/23 01:47:49.996
STEP: fetching the ConfigMap 01/03/23 01:47:50.002
STEP: patching the ConfigMap 01/03/23 01:47:50.008
STEP: listing all ConfigMaps in all namespaces with a label selector 01/03/23 01:47:50.01
STEP: deleting the ConfigMap by collection with a label selector 01/03/23 01:47:50.016
STEP: listing all ConfigMaps in test namespace 01/03/23 01:47:50.019
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jan  3 01:47:50.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9464" for this suite. 01/03/23 01:47:50.033
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","completed":10,"skipped":196,"failed":0}
------------------------------
• [0.089 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:47:49.946
    Jan  3 01:47:49.946: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename configmap 01/03/23 01:47:49.947
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:47:49.991
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:47:49.994
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:168
    STEP: creating a ConfigMap 01/03/23 01:47:49.996
    STEP: fetching the ConfigMap 01/03/23 01:47:50.002
    STEP: patching the ConfigMap 01/03/23 01:47:50.008
    STEP: listing all ConfigMaps in all namespaces with a label selector 01/03/23 01:47:50.01
    STEP: deleting the ConfigMap by collection with a label selector 01/03/23 01:47:50.016
    STEP: listing all ConfigMaps in test namespace 01/03/23 01:47:50.019
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jan  3 01:47:50.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9464" for this suite. 01/03/23 01:47:50.033
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:47:50.035
Jan  3 01:47:50.035: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename webhook 01/03/23 01:47:50.036
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:47:50.048
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:47:50.05
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/03/23 01:47:50.072
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/23 01:47:50.606
STEP: Deploying the webhook pod 01/03/23 01:47:50.611
STEP: Wait for the deployment to be ready 01/03/23 01:47:50.658
Jan  3 01:47:50.665: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/03/23 01:47:52.675
STEP: Verifying the service has paired with the endpoint 01/03/23 01:47:52.691
Jan  3 01:47:53.691: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
STEP: Setting timeout (1s) shorter than webhook latency (5s) 01/03/23 01:47:53.694
STEP: Registering slow webhook via the AdmissionRegistration API 01/03/23 01:47:53.694
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 01/03/23 01:47:53.704
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 01/03/23 01:47:54.712
STEP: Registering slow webhook via the AdmissionRegistration API 01/03/23 01:47:54.712
STEP: Having no error when timeout is longer than webhook latency 01/03/23 01:47:55.758
STEP: Registering slow webhook via the AdmissionRegistration API 01/03/23 01:47:55.758
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 01/03/23 01:48:00.827
STEP: Registering slow webhook via the AdmissionRegistration API 01/03/23 01:48:00.827
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  3 01:48:05.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1706" for this suite. 01/03/23 01:48:05.867
STEP: Destroying namespace "webhook-1706-markers" for this suite. 01/03/23 01:48:05.87
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","completed":11,"skipped":196,"failed":0}
------------------------------
• [SLOW TEST] [15.922 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:47:50.035
    Jan  3 01:47:50.035: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename webhook 01/03/23 01:47:50.036
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:47:50.048
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:47:50.05
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/03/23 01:47:50.072
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/23 01:47:50.606
    STEP: Deploying the webhook pod 01/03/23 01:47:50.611
    STEP: Wait for the deployment to be ready 01/03/23 01:47:50.658
    Jan  3 01:47:50.665: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/03/23 01:47:52.675
    STEP: Verifying the service has paired with the endpoint 01/03/23 01:47:52.691
    Jan  3 01:47:53.691: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:380
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 01/03/23 01:47:53.694
    STEP: Registering slow webhook via the AdmissionRegistration API 01/03/23 01:47:53.694
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 01/03/23 01:47:53.704
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 01/03/23 01:47:54.712
    STEP: Registering slow webhook via the AdmissionRegistration API 01/03/23 01:47:54.712
    STEP: Having no error when timeout is longer than webhook latency 01/03/23 01:47:55.758
    STEP: Registering slow webhook via the AdmissionRegistration API 01/03/23 01:47:55.758
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 01/03/23 01:48:00.827
    STEP: Registering slow webhook via the AdmissionRegistration API 01/03/23 01:48:00.827
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  3 01:48:05.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1706" for this suite. 01/03/23 01:48:05.867
    STEP: Destroying namespace "webhook-1706-markers" for this suite. 01/03/23 01:48:05.87
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:48:05.958
Jan  3 01:48:05.959: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename svcaccounts 01/03/23 01:48:05.96
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:48:05.969
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:48:05.971
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
Jan  3 01:48:06.003: INFO: Waiting up to 5m0s for pod "pod-service-account-16342d95-a4bf-41bf-bc1e-f45f802d57e3" in namespace "svcaccounts-2083" to be "running"
Jan  3 01:48:06.041: INFO: Pod "pod-service-account-16342d95-a4bf-41bf-bc1e-f45f802d57e3": Phase="Pending", Reason="", readiness=false. Elapsed: 37.575507ms
Jan  3 01:48:08.044: INFO: Pod "pod-service-account-16342d95-a4bf-41bf-bc1e-f45f802d57e3": Phase="Running", Reason="", readiness=true. Elapsed: 2.040586225s
Jan  3 01:48:08.044: INFO: Pod "pod-service-account-16342d95-a4bf-41bf-bc1e-f45f802d57e3" satisfied condition "running"
STEP: reading a file in the container 01/03/23 01:48:08.044
Jan  3 01:48:08.044: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2083 pod-service-account-16342d95-a4bf-41bf-bc1e-f45f802d57e3 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 01/03/23 01:48:08.159
Jan  3 01:48:08.159: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2083 pod-service-account-16342d95-a4bf-41bf-bc1e-f45f802d57e3 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 01/03/23 01:48:08.285
Jan  3 01:48:08.285: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2083 pod-service-account-16342d95-a4bf-41bf-bc1e-f45f802d57e3 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Jan  3 01:48:08.422: INFO: Got root ca configmap in namespace "svcaccounts-2083"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan  3 01:48:08.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2083" for this suite. 01/03/23 01:48:08.426
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","completed":12,"skipped":200,"failed":0}
------------------------------
• [2.470 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:48:05.958
    Jan  3 01:48:05.959: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename svcaccounts 01/03/23 01:48:05.96
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:48:05.969
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:48:05.971
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:75
    Jan  3 01:48:06.003: INFO: Waiting up to 5m0s for pod "pod-service-account-16342d95-a4bf-41bf-bc1e-f45f802d57e3" in namespace "svcaccounts-2083" to be "running"
    Jan  3 01:48:06.041: INFO: Pod "pod-service-account-16342d95-a4bf-41bf-bc1e-f45f802d57e3": Phase="Pending", Reason="", readiness=false. Elapsed: 37.575507ms
    Jan  3 01:48:08.044: INFO: Pod "pod-service-account-16342d95-a4bf-41bf-bc1e-f45f802d57e3": Phase="Running", Reason="", readiness=true. Elapsed: 2.040586225s
    Jan  3 01:48:08.044: INFO: Pod "pod-service-account-16342d95-a4bf-41bf-bc1e-f45f802d57e3" satisfied condition "running"
    STEP: reading a file in the container 01/03/23 01:48:08.044
    Jan  3 01:48:08.044: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2083 pod-service-account-16342d95-a4bf-41bf-bc1e-f45f802d57e3 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 01/03/23 01:48:08.159
    Jan  3 01:48:08.159: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2083 pod-service-account-16342d95-a4bf-41bf-bc1e-f45f802d57e3 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 01/03/23 01:48:08.285
    Jan  3 01:48:08.285: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2083 pod-service-account-16342d95-a4bf-41bf-bc1e-f45f802d57e3 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Jan  3 01:48:08.422: INFO: Got root ca configmap in namespace "svcaccounts-2083"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan  3 01:48:08.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-2083" for this suite. 01/03/23 01:48:08.426
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:48:08.429
Jan  3 01:48:08.429: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename secrets 01/03/23 01:48:08.43
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:48:08.45
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:48:08.452
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan  3 01:48:08.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8995" for this suite. 01/03/23 01:48:08.525
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","completed":13,"skipped":204,"failed":0}
------------------------------
• [0.098 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:48:08.429
    Jan  3 01:48:08.429: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename secrets 01/03/23 01:48:08.43
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:48:08.45
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:48:08.452
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:385
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan  3 01:48:08.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8995" for this suite. 01/03/23 01:48:08.525
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:48:08.528
Jan  3 01:48:08.528: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename subpath 01/03/23 01:48:08.529
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:48:08.536
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:48:08.537
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/03/23 01:48:08.541
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-zn52 01/03/23 01:48:08.564
STEP: Creating a pod to test atomic-volume-subpath 01/03/23 01:48:08.564
Jan  3 01:48:08.573: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-zn52" in namespace "subpath-6347" to be "Succeeded or Failed"
Jan  3 01:48:08.588: INFO: Pod "pod-subpath-test-configmap-zn52": Phase="Pending", Reason="", readiness=false. Elapsed: 15.242824ms
Jan  3 01:48:10.590: INFO: Pod "pod-subpath-test-configmap-zn52": Phase="Running", Reason="", readiness=true. Elapsed: 2.017693456s
Jan  3 01:48:12.592: INFO: Pod "pod-subpath-test-configmap-zn52": Phase="Running", Reason="", readiness=true. Elapsed: 4.018946784s
Jan  3 01:48:14.591: INFO: Pod "pod-subpath-test-configmap-zn52": Phase="Running", Reason="", readiness=true. Elapsed: 6.018703981s
Jan  3 01:48:16.590: INFO: Pod "pod-subpath-test-configmap-zn52": Phase="Running", Reason="", readiness=true. Elapsed: 8.017712308s
Jan  3 01:48:18.591: INFO: Pod "pod-subpath-test-configmap-zn52": Phase="Running", Reason="", readiness=true. Elapsed: 10.018248645s
Jan  3 01:48:20.592: INFO: Pod "pod-subpath-test-configmap-zn52": Phase="Running", Reason="", readiness=true. Elapsed: 12.019168368s
Jan  3 01:48:22.591: INFO: Pod "pod-subpath-test-configmap-zn52": Phase="Running", Reason="", readiness=true. Elapsed: 14.018441791s
Jan  3 01:48:24.591: INFO: Pod "pod-subpath-test-configmap-zn52": Phase="Running", Reason="", readiness=true. Elapsed: 16.018321648s
Jan  3 01:48:26.590: INFO: Pod "pod-subpath-test-configmap-zn52": Phase="Running", Reason="", readiness=true. Elapsed: 18.017267825s
Jan  3 01:48:28.591: INFO: Pod "pod-subpath-test-configmap-zn52": Phase="Running", Reason="", readiness=true. Elapsed: 20.018493216s
Jan  3 01:48:30.591: INFO: Pod "pod-subpath-test-configmap-zn52": Phase="Running", Reason="", readiness=false. Elapsed: 22.018149488s
Jan  3 01:48:32.594: INFO: Pod "pod-subpath-test-configmap-zn52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.02084527s
STEP: Saw pod success 01/03/23 01:48:32.594
Jan  3 01:48:32.594: INFO: Pod "pod-subpath-test-configmap-zn52" satisfied condition "Succeeded or Failed"
Jan  3 01:48:32.595: INFO: Trying to get logs from node cncf-master pod pod-subpath-test-configmap-zn52 container test-container-subpath-configmap-zn52: <nil>
STEP: delete the pod 01/03/23 01:48:32.599
Jan  3 01:48:32.622: INFO: Waiting for pod pod-subpath-test-configmap-zn52 to disappear
Jan  3 01:48:32.628: INFO: Pod pod-subpath-test-configmap-zn52 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-zn52 01/03/23 01:48:32.628
Jan  3 01:48:32.628: INFO: Deleting pod "pod-subpath-test-configmap-zn52" in namespace "subpath-6347"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan  3 01:48:32.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6347" for this suite. 01/03/23 01:48:32.632
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]","completed":14,"skipped":233,"failed":0}
------------------------------
• [SLOW TEST] [24.124 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:48:08.528
    Jan  3 01:48:08.528: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename subpath 01/03/23 01:48:08.529
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:48:08.536
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:48:08.537
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/03/23 01:48:08.541
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-zn52 01/03/23 01:48:08.564
    STEP: Creating a pod to test atomic-volume-subpath 01/03/23 01:48:08.564
    Jan  3 01:48:08.573: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-zn52" in namespace "subpath-6347" to be "Succeeded or Failed"
    Jan  3 01:48:08.588: INFO: Pod "pod-subpath-test-configmap-zn52": Phase="Pending", Reason="", readiness=false. Elapsed: 15.242824ms
    Jan  3 01:48:10.590: INFO: Pod "pod-subpath-test-configmap-zn52": Phase="Running", Reason="", readiness=true. Elapsed: 2.017693456s
    Jan  3 01:48:12.592: INFO: Pod "pod-subpath-test-configmap-zn52": Phase="Running", Reason="", readiness=true. Elapsed: 4.018946784s
    Jan  3 01:48:14.591: INFO: Pod "pod-subpath-test-configmap-zn52": Phase="Running", Reason="", readiness=true. Elapsed: 6.018703981s
    Jan  3 01:48:16.590: INFO: Pod "pod-subpath-test-configmap-zn52": Phase="Running", Reason="", readiness=true. Elapsed: 8.017712308s
    Jan  3 01:48:18.591: INFO: Pod "pod-subpath-test-configmap-zn52": Phase="Running", Reason="", readiness=true. Elapsed: 10.018248645s
    Jan  3 01:48:20.592: INFO: Pod "pod-subpath-test-configmap-zn52": Phase="Running", Reason="", readiness=true. Elapsed: 12.019168368s
    Jan  3 01:48:22.591: INFO: Pod "pod-subpath-test-configmap-zn52": Phase="Running", Reason="", readiness=true. Elapsed: 14.018441791s
    Jan  3 01:48:24.591: INFO: Pod "pod-subpath-test-configmap-zn52": Phase="Running", Reason="", readiness=true. Elapsed: 16.018321648s
    Jan  3 01:48:26.590: INFO: Pod "pod-subpath-test-configmap-zn52": Phase="Running", Reason="", readiness=true. Elapsed: 18.017267825s
    Jan  3 01:48:28.591: INFO: Pod "pod-subpath-test-configmap-zn52": Phase="Running", Reason="", readiness=true. Elapsed: 20.018493216s
    Jan  3 01:48:30.591: INFO: Pod "pod-subpath-test-configmap-zn52": Phase="Running", Reason="", readiness=false. Elapsed: 22.018149488s
    Jan  3 01:48:32.594: INFO: Pod "pod-subpath-test-configmap-zn52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.02084527s
    STEP: Saw pod success 01/03/23 01:48:32.594
    Jan  3 01:48:32.594: INFO: Pod "pod-subpath-test-configmap-zn52" satisfied condition "Succeeded or Failed"
    Jan  3 01:48:32.595: INFO: Trying to get logs from node cncf-master pod pod-subpath-test-configmap-zn52 container test-container-subpath-configmap-zn52: <nil>
    STEP: delete the pod 01/03/23 01:48:32.599
    Jan  3 01:48:32.622: INFO: Waiting for pod pod-subpath-test-configmap-zn52 to disappear
    Jan  3 01:48:32.628: INFO: Pod pod-subpath-test-configmap-zn52 no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-zn52 01/03/23 01:48:32.628
    Jan  3 01:48:32.628: INFO: Deleting pod "pod-subpath-test-configmap-zn52" in namespace "subpath-6347"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan  3 01:48:32.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-6347" for this suite. 01/03/23 01:48:32.632
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:48:32.655
Jan  3 01:48:32.655: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename endpointslice 01/03/23 01:48:32.656
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:48:32.672
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:48:32.674
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
STEP: getting /apis 01/03/23 01:48:32.676
STEP: getting /apis/discovery.k8s.io 01/03/23 01:48:32.677
STEP: getting /apis/discovery.k8s.iov1 01/03/23 01:48:32.678
STEP: creating 01/03/23 01:48:32.679
STEP: getting 01/03/23 01:48:32.708
STEP: listing 01/03/23 01:48:32.713
STEP: watching 01/03/23 01:48:32.715
Jan  3 01:48:32.715: INFO: starting watch
STEP: cluster-wide listing 01/03/23 01:48:32.716
STEP: cluster-wide watching 01/03/23 01:48:32.718
Jan  3 01:48:32.718: INFO: starting watch
STEP: patching 01/03/23 01:48:32.719
STEP: updating 01/03/23 01:48:32.722
Jan  3 01:48:32.727: INFO: waiting for watch events with expected annotations
Jan  3 01:48:32.727: INFO: saw patched and updated annotations
STEP: deleting 01/03/23 01:48:32.727
STEP: deleting a collection 01/03/23 01:48:32.738
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jan  3 01:48:32.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-3892" for this suite. 01/03/23 01:48:32.775
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","completed":15,"skipped":267,"failed":0}
------------------------------
• [0.128 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:48:32.655
    Jan  3 01:48:32.655: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename endpointslice 01/03/23 01:48:32.656
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:48:32.672
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:48:32.674
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:352
    STEP: getting /apis 01/03/23 01:48:32.676
    STEP: getting /apis/discovery.k8s.io 01/03/23 01:48:32.677
    STEP: getting /apis/discovery.k8s.iov1 01/03/23 01:48:32.678
    STEP: creating 01/03/23 01:48:32.679
    STEP: getting 01/03/23 01:48:32.708
    STEP: listing 01/03/23 01:48:32.713
    STEP: watching 01/03/23 01:48:32.715
    Jan  3 01:48:32.715: INFO: starting watch
    STEP: cluster-wide listing 01/03/23 01:48:32.716
    STEP: cluster-wide watching 01/03/23 01:48:32.718
    Jan  3 01:48:32.718: INFO: starting watch
    STEP: patching 01/03/23 01:48:32.719
    STEP: updating 01/03/23 01:48:32.722
    Jan  3 01:48:32.727: INFO: waiting for watch events with expected annotations
    Jan  3 01:48:32.727: INFO: saw patched and updated annotations
    STEP: deleting 01/03/23 01:48:32.727
    STEP: deleting a collection 01/03/23 01:48:32.738
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jan  3 01:48:32.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-3892" for this suite. 01/03/23 01:48:32.775
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:48:32.784
Jan  3 01:48:32.784: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename kubectl 01/03/23 01:48:32.785
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:48:32.793
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:48:32.795
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
Jan  3 01:48:32.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-4657 create -f -'
Jan  3 01:48:33.760: INFO: stderr: ""
Jan  3 01:48:33.760: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jan  3 01:48:33.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-4657 create -f -'
Jan  3 01:48:34.019: INFO: stderr: ""
Jan  3 01:48:34.019: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/03/23 01:48:34.019
Jan  3 01:48:35.021: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  3 01:48:35.021: INFO: Found 0 / 1
Jan  3 01:48:36.021: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  3 01:48:36.021: INFO: Found 1 / 1
Jan  3 01:48:36.021: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan  3 01:48:36.023: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  3 01:48:36.023: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan  3 01:48:36.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-4657 describe pod agnhost-primary-zqhjs'
Jan  3 01:48:36.085: INFO: stderr: ""
Jan  3 01:48:36.085: INFO: stdout: "Name:             agnhost-primary-zqhjs\nNamespace:        kubectl-4657\nPriority:         0\nService Account:  default\nNode:             cncf-master/172.21.7.5\nStart Time:       Tue, 03 Jan 2023 01:48:33 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: b42695f49c1ab12f730ac58c35854a887eefce4cd9787b063ef5a9744d2e2f5e\n                  cni.projectcalico.org/podIP: 10.244.246.94/32\n                  cni.projectcalico.org/podIPs: 10.244.246.94/32\n                  createdTime: 2023-01-03T10:48:33.784263498+09:00\n                  creator: system:serviceaccount:kube-system:replication-controller\n                  updatedTime: 2023-01-03T10:48:33.784263498+09:00\n                  updater: system:serviceaccount:kube-system:replication-controller\nStatus:           Running\nIP:               10.244.246.94\nIPs:\n  IP:           10.244.246.94\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://6965c5c73bd8eafd77c946fad26f3af01e910cd2954a1737195a436020eded13\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 03 Jan 2023 01:48:34 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4swnm (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-4swnm:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-4657/agnhost-primary-zqhjs to cncf-master\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
Jan  3 01:48:36.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-4657 describe rc agnhost-primary'
Jan  3 01:48:36.175: INFO: stderr: ""
Jan  3 01:48:36.175: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-4657\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  createdTime: 2023-01-03T10:48:33.753980849+09:00\n              creator: system:serviceaccount:sonobuoy:sonobuoy-serviceaccount\n              updatedTime: 2023-01-03T10:48:33.753980849+09:00\n              updater: system:serviceaccount:sonobuoy:sonobuoy-serviceaccount\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-zqhjs\n"
Jan  3 01:48:36.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-4657 describe service agnhost-primary'
Jan  3 01:48:36.235: INFO: stderr: ""
Jan  3 01:48:36.235: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-4657\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       createdTime: 2023-01-03T10:48:34.003827829+09:00\n                   creator: system:serviceaccount:sonobuoy:sonobuoy-serviceaccount\n                   updatedTime: 2023-01-03T10:48:34.003827829+09:00\n                   updater: system:serviceaccount:sonobuoy:sonobuoy-serviceaccount\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.96.201.220\nIPs:               10.96.201.220\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.244.246.94:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jan  3 01:48:36.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-4657 describe node cncf-master'
Jan  3 01:48:36.341: INFO: stderr: ""
Jan  3 01:48:36.341: INFO: stdout: "Name:               cncf-master\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=cncf-master\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"rook-ceph.cephfs.csi.ceph.com\":\"cncf-master\",\"rook-ceph.rbd.csi.ceph.com\":\"cncf-master\"}\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/crio/crio.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 172.21.7.5/24\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.244.246.64\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 28 Dec 2022 00:22:52 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  cncf-master\n  AcquireTime:     <unset>\n  RenewTime:       Tue, 03 Jan 2023 01:48:31 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 28 Dec 2022 00:23:15 +0000   Wed, 28 Dec 2022 00:23:15 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Tue, 03 Jan 2023 01:45:55 +0000   Wed, 28 Dec 2022 00:22:50 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Tue, 03 Jan 2023 01:45:55 +0000   Wed, 28 Dec 2022 00:22:50 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Tue, 03 Jan 2023 01:45:55 +0000   Wed, 28 Dec 2022 00:22:50 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Tue, 03 Jan 2023 01:45:55 +0000   Wed, 28 Dec 2022 00:22:56 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  172.21.7.5\n  Hostname:    cncf-master\nCapacity:\n  cpu:                16\n  ephemeral-storage:  71645Mi\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             31753980Ki\n  pods:               110\nAllocatable:\n  cpu:                16\n  ephemeral-storage:  67612704657\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             31651580Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 c06fc91a84194193b08977166fac634e\n  System UUID:                da26fd9e-9ed5-11e7-91be-0894ef504a92\n  Boot ID:                    126447f4-7ae4-4264-b9dd-20887edd5cdb\n  Kernel Version:             5.14.0-70.13.1.el9_0.hl.1.x86_64\n  OS Image:                   HyperLinux 9.0 (Gorani)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  cri-o://1.25.0\n  Kubelet Version:            v1.25.0\n  Kube-Proxy Version:         v1.25.0\nPodCIDR:                      10.244.0.0/24\nPodCIDRs:                     10.244.0.0/24\nNon-terminated Pods:          (20 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  hyperauth                   postgresql-c6b9bb88b-lqpz6                                 1 (6%)        1 (6%)      5Gi (16%)        5Gi (16%)      12m\n  kube-system                 calico-kube-controllers-58dbc876ff-rc9xm                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         6d1h\n  kube-system                 calico-node-zxb7d                                          250m (1%)     0 (0%)      0 (0%)           0 (0%)         6d1h\n  kube-system                 coredns-565d847f94-lwkgz                                   100m (0%)     0 (0%)      70Mi (0%)        170Mi (0%)     6d1h\n  kube-system                 coredns-565d847f94-tsqd7                                   100m (0%)     0 (0%)      70Mi (0%)        170Mi (0%)     6d1h\n  kube-system                 etcd-cncf-master                                           100m (0%)     0 (0%)      100Mi (0%)       0 (0%)         6d1h\n  kube-system                 kube-apiserver-cncf-master                                 250m (1%)     0 (0%)      0 (0%)           0 (0%)         7m23s\n  kube-system                 kube-controller-manager-cncf-master                        200m (1%)     0 (0%)      0 (0%)           0 (0%)         6d1h\n  kube-system                 kube-proxy-qvggp                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         6d1h\n  kube-system                 kube-scheduler-cncf-master                                 100m (0%)     0 (0%)      0 (0%)           0 (0%)         6d1h\n  kubectl-4657                agnhost-primary-zqhjs                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         3s\n  rook-ceph                   csi-cephfsplugin-g2vdb                                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         4d17h\n  rook-ceph                   csi-rbdplugin-krkxg                                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         4d17h\n  rook-ceph                   csi-rbdplugin-provisioner-6f689965cc-56vzc                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         4d17h\n  rook-ceph                   rook-ceph-crashcollector-cncf-master-65b6767fc8-xjlw5      0 (0%)        0 (0%)      0 (0%)           0 (0%)         24h\n  rook-ceph                   rook-ceph-mds-myfs-b-65b48b8c9d-2w82f                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         24h\n  rook-ceph                   rook-ceph-mgr-b-856c746f6-wkx97                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         4d17h\n  rook-ceph                   rook-ceph-mon-b-66ffc4d4d9-sm6t5                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         4d17h\n  rook-ceph                   rook-ceph-osd-1-58d74d96db-9ww4l                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         24h\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-58995b31e0564399-fxv6h    0 (0%)        0 (0%)      0 (0%)           0 (0%)         107s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests      Limits\n  --------           --------      ------\n  cpu                2100m (13%)   1 (6%)\n  memory             5360Mi (17%)  5460Mi (17%)\n  ephemeral-storage  0 (0%)        0 (0%)\n  hugepages-1Gi      0 (0%)        0 (0%)\n  hugepages-2Mi      0 (0%)        0 (0%)\nEvents:\n  Type    Reason          Age   From             Message\n  ----    ------          ----  ----             -------\n  Normal  RegisteredNode  10m   node-controller  Node cncf-master event: Registered Node cncf-master in Controller\n"
Jan  3 01:48:36.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-4657 describe namespace kubectl-4657'
Jan  3 01:48:36.403: INFO: stderr: ""
Jan  3 01:48:36.403: INFO: stdout: "Name:         kubectl-4657\nLabels:       e2e-framework=kubectl\n              e2e-run=e7487ade-25e1-4251-b1f2-f125f73b7c0d\n              kubernetes.io/metadata.name=kubectl-4657\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  createdTime: 2023-01-03T10:48:32.786643093+09:00\n              creator: system:serviceaccount:sonobuoy:sonobuoy-serviceaccount\n              updatedTime: 2023-01-03T10:48:32.786643093+09:00\n              updater: system:serviceaccount:sonobuoy:sonobuoy-serviceaccount\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan  3 01:48:36.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4657" for this suite. 01/03/23 01:48:36.405
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","completed":16,"skipped":286,"failed":0}
------------------------------
• [3.624 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1268
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1274

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:48:32.784
    Jan  3 01:48:32.784: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename kubectl 01/03/23 01:48:32.785
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:48:32.793
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:48:32.795
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1274
    Jan  3 01:48:32.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-4657 create -f -'
    Jan  3 01:48:33.760: INFO: stderr: ""
    Jan  3 01:48:33.760: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Jan  3 01:48:33.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-4657 create -f -'
    Jan  3 01:48:34.019: INFO: stderr: ""
    Jan  3 01:48:34.019: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/03/23 01:48:34.019
    Jan  3 01:48:35.021: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  3 01:48:35.021: INFO: Found 0 / 1
    Jan  3 01:48:36.021: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  3 01:48:36.021: INFO: Found 1 / 1
    Jan  3 01:48:36.021: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jan  3 01:48:36.023: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  3 01:48:36.023: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan  3 01:48:36.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-4657 describe pod agnhost-primary-zqhjs'
    Jan  3 01:48:36.085: INFO: stderr: ""
    Jan  3 01:48:36.085: INFO: stdout: "Name:             agnhost-primary-zqhjs\nNamespace:        kubectl-4657\nPriority:         0\nService Account:  default\nNode:             cncf-master/172.21.7.5\nStart Time:       Tue, 03 Jan 2023 01:48:33 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: b42695f49c1ab12f730ac58c35854a887eefce4cd9787b063ef5a9744d2e2f5e\n                  cni.projectcalico.org/podIP: 10.244.246.94/32\n                  cni.projectcalico.org/podIPs: 10.244.246.94/32\n                  createdTime: 2023-01-03T10:48:33.784263498+09:00\n                  creator: system:serviceaccount:kube-system:replication-controller\n                  updatedTime: 2023-01-03T10:48:33.784263498+09:00\n                  updater: system:serviceaccount:kube-system:replication-controller\nStatus:           Running\nIP:               10.244.246.94\nIPs:\n  IP:           10.244.246.94\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://6965c5c73bd8eafd77c946fad26f3af01e910cd2954a1737195a436020eded13\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 03 Jan 2023 01:48:34 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4swnm (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-4swnm:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-4657/agnhost-primary-zqhjs to cncf-master\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
    Jan  3 01:48:36.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-4657 describe rc agnhost-primary'
    Jan  3 01:48:36.175: INFO: stderr: ""
    Jan  3 01:48:36.175: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-4657\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  createdTime: 2023-01-03T10:48:33.753980849+09:00\n              creator: system:serviceaccount:sonobuoy:sonobuoy-serviceaccount\n              updatedTime: 2023-01-03T10:48:33.753980849+09:00\n              updater: system:serviceaccount:sonobuoy:sonobuoy-serviceaccount\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-zqhjs\n"
    Jan  3 01:48:36.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-4657 describe service agnhost-primary'
    Jan  3 01:48:36.235: INFO: stderr: ""
    Jan  3 01:48:36.235: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-4657\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       createdTime: 2023-01-03T10:48:34.003827829+09:00\n                   creator: system:serviceaccount:sonobuoy:sonobuoy-serviceaccount\n                   updatedTime: 2023-01-03T10:48:34.003827829+09:00\n                   updater: system:serviceaccount:sonobuoy:sonobuoy-serviceaccount\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.96.201.220\nIPs:               10.96.201.220\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.244.246.94:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Jan  3 01:48:36.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-4657 describe node cncf-master'
    Jan  3 01:48:36.341: INFO: stderr: ""
    Jan  3 01:48:36.341: INFO: stdout: "Name:               cncf-master\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=cncf-master\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"rook-ceph.cephfs.csi.ceph.com\":\"cncf-master\",\"rook-ceph.rbd.csi.ceph.com\":\"cncf-master\"}\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/crio/crio.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 172.21.7.5/24\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.244.246.64\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 28 Dec 2022 00:22:52 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  cncf-master\n  AcquireTime:     <unset>\n  RenewTime:       Tue, 03 Jan 2023 01:48:31 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 28 Dec 2022 00:23:15 +0000   Wed, 28 Dec 2022 00:23:15 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Tue, 03 Jan 2023 01:45:55 +0000   Wed, 28 Dec 2022 00:22:50 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Tue, 03 Jan 2023 01:45:55 +0000   Wed, 28 Dec 2022 00:22:50 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Tue, 03 Jan 2023 01:45:55 +0000   Wed, 28 Dec 2022 00:22:50 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Tue, 03 Jan 2023 01:45:55 +0000   Wed, 28 Dec 2022 00:22:56 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  172.21.7.5\n  Hostname:    cncf-master\nCapacity:\n  cpu:                16\n  ephemeral-storage:  71645Mi\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             31753980Ki\n  pods:               110\nAllocatable:\n  cpu:                16\n  ephemeral-storage:  67612704657\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             31651580Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 c06fc91a84194193b08977166fac634e\n  System UUID:                da26fd9e-9ed5-11e7-91be-0894ef504a92\n  Boot ID:                    126447f4-7ae4-4264-b9dd-20887edd5cdb\n  Kernel Version:             5.14.0-70.13.1.el9_0.hl.1.x86_64\n  OS Image:                   HyperLinux 9.0 (Gorani)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  cri-o://1.25.0\n  Kubelet Version:            v1.25.0\n  Kube-Proxy Version:         v1.25.0\nPodCIDR:                      10.244.0.0/24\nPodCIDRs:                     10.244.0.0/24\nNon-terminated Pods:          (20 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  hyperauth                   postgresql-c6b9bb88b-lqpz6                                 1 (6%)        1 (6%)      5Gi (16%)        5Gi (16%)      12m\n  kube-system                 calico-kube-controllers-58dbc876ff-rc9xm                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         6d1h\n  kube-system                 calico-node-zxb7d                                          250m (1%)     0 (0%)      0 (0%)           0 (0%)         6d1h\n  kube-system                 coredns-565d847f94-lwkgz                                   100m (0%)     0 (0%)      70Mi (0%)        170Mi (0%)     6d1h\n  kube-system                 coredns-565d847f94-tsqd7                                   100m (0%)     0 (0%)      70Mi (0%)        170Mi (0%)     6d1h\n  kube-system                 etcd-cncf-master                                           100m (0%)     0 (0%)      100Mi (0%)       0 (0%)         6d1h\n  kube-system                 kube-apiserver-cncf-master                                 250m (1%)     0 (0%)      0 (0%)           0 (0%)         7m23s\n  kube-system                 kube-controller-manager-cncf-master                        200m (1%)     0 (0%)      0 (0%)           0 (0%)         6d1h\n  kube-system                 kube-proxy-qvggp                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         6d1h\n  kube-system                 kube-scheduler-cncf-master                                 100m (0%)     0 (0%)      0 (0%)           0 (0%)         6d1h\n  kubectl-4657                agnhost-primary-zqhjs                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         3s\n  rook-ceph                   csi-cephfsplugin-g2vdb                                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         4d17h\n  rook-ceph                   csi-rbdplugin-krkxg                                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         4d17h\n  rook-ceph                   csi-rbdplugin-provisioner-6f689965cc-56vzc                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         4d17h\n  rook-ceph                   rook-ceph-crashcollector-cncf-master-65b6767fc8-xjlw5      0 (0%)        0 (0%)      0 (0%)           0 (0%)         24h\n  rook-ceph                   rook-ceph-mds-myfs-b-65b48b8c9d-2w82f                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         24h\n  rook-ceph                   rook-ceph-mgr-b-856c746f6-wkx97                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         4d17h\n  rook-ceph                   rook-ceph-mon-b-66ffc4d4d9-sm6t5                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         4d17h\n  rook-ceph                   rook-ceph-osd-1-58d74d96db-9ww4l                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         24h\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-58995b31e0564399-fxv6h    0 (0%)        0 (0%)      0 (0%)           0 (0%)         107s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests      Limits\n  --------           --------      ------\n  cpu                2100m (13%)   1 (6%)\n  memory             5360Mi (17%)  5460Mi (17%)\n  ephemeral-storage  0 (0%)        0 (0%)\n  hugepages-1Gi      0 (0%)        0 (0%)\n  hugepages-2Mi      0 (0%)        0 (0%)\nEvents:\n  Type    Reason          Age   From             Message\n  ----    ------          ----  ----             -------\n  Normal  RegisteredNode  10m   node-controller  Node cncf-master event: Registered Node cncf-master in Controller\n"
    Jan  3 01:48:36.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-4657 describe namespace kubectl-4657'
    Jan  3 01:48:36.403: INFO: stderr: ""
    Jan  3 01:48:36.403: INFO: stdout: "Name:         kubectl-4657\nLabels:       e2e-framework=kubectl\n              e2e-run=e7487ade-25e1-4251-b1f2-f125f73b7c0d\n              kubernetes.io/metadata.name=kubectl-4657\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  createdTime: 2023-01-03T10:48:32.786643093+09:00\n              creator: system:serviceaccount:sonobuoy:sonobuoy-serviceaccount\n              updatedTime: 2023-01-03T10:48:32.786643093+09:00\n              updater: system:serviceaccount:sonobuoy:sonobuoy-serviceaccount\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan  3 01:48:36.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4657" for this suite. 01/03/23 01:48:36.405
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:48:36.409
Jan  3 01:48:36.409: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename replication-controller 01/03/23 01:48:36.409
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:48:36.423
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:48:36.424
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
STEP: Creating replication controller my-hostname-basic-28751012-cd10-422d-8ce4-66c04d1e1934 01/03/23 01:48:36.426
Jan  3 01:48:36.452: INFO: Pod name my-hostname-basic-28751012-cd10-422d-8ce4-66c04d1e1934: Found 0 pods out of 1
Jan  3 01:48:41.454: INFO: Pod name my-hostname-basic-28751012-cd10-422d-8ce4-66c04d1e1934: Found 1 pods out of 1
Jan  3 01:48:41.454: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-28751012-cd10-422d-8ce4-66c04d1e1934" are running
Jan  3 01:48:41.454: INFO: Waiting up to 5m0s for pod "my-hostname-basic-28751012-cd10-422d-8ce4-66c04d1e1934-5nwsw" in namespace "replication-controller-3912" to be "running"
Jan  3 01:48:41.457: INFO: Pod "my-hostname-basic-28751012-cd10-422d-8ce4-66c04d1e1934-5nwsw": Phase="Running", Reason="", readiness=true. Elapsed: 3.469005ms
Jan  3 01:48:41.457: INFO: Pod "my-hostname-basic-28751012-cd10-422d-8ce4-66c04d1e1934-5nwsw" satisfied condition "running"
Jan  3 01:48:41.457: INFO: Pod "my-hostname-basic-28751012-cd10-422d-8ce4-66c04d1e1934-5nwsw" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-03 01:48:36 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-03 01:48:37 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-03 01:48:37 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-03 01:48:36 +0000 UTC Reason: Message:}])
Jan  3 01:48:41.457: INFO: Trying to dial the pod
Jan  3 01:48:46.464: INFO: Controller my-hostname-basic-28751012-cd10-422d-8ce4-66c04d1e1934: Got expected result from replica 1 [my-hostname-basic-28751012-cd10-422d-8ce4-66c04d1e1934-5nwsw]: "my-hostname-basic-28751012-cd10-422d-8ce4-66c04d1e1934-5nwsw", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan  3 01:48:46.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3912" for this suite. 01/03/23 01:48:46.468
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","completed":17,"skipped":301,"failed":0}
------------------------------
• [SLOW TEST] [10.062 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:48:36.409
    Jan  3 01:48:36.409: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename replication-controller 01/03/23 01:48:36.409
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:48:36.423
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:48:36.424
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:66
    STEP: Creating replication controller my-hostname-basic-28751012-cd10-422d-8ce4-66c04d1e1934 01/03/23 01:48:36.426
    Jan  3 01:48:36.452: INFO: Pod name my-hostname-basic-28751012-cd10-422d-8ce4-66c04d1e1934: Found 0 pods out of 1
    Jan  3 01:48:41.454: INFO: Pod name my-hostname-basic-28751012-cd10-422d-8ce4-66c04d1e1934: Found 1 pods out of 1
    Jan  3 01:48:41.454: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-28751012-cd10-422d-8ce4-66c04d1e1934" are running
    Jan  3 01:48:41.454: INFO: Waiting up to 5m0s for pod "my-hostname-basic-28751012-cd10-422d-8ce4-66c04d1e1934-5nwsw" in namespace "replication-controller-3912" to be "running"
    Jan  3 01:48:41.457: INFO: Pod "my-hostname-basic-28751012-cd10-422d-8ce4-66c04d1e1934-5nwsw": Phase="Running", Reason="", readiness=true. Elapsed: 3.469005ms
    Jan  3 01:48:41.457: INFO: Pod "my-hostname-basic-28751012-cd10-422d-8ce4-66c04d1e1934-5nwsw" satisfied condition "running"
    Jan  3 01:48:41.457: INFO: Pod "my-hostname-basic-28751012-cd10-422d-8ce4-66c04d1e1934-5nwsw" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-03 01:48:36 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-03 01:48:37 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-03 01:48:37 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-03 01:48:36 +0000 UTC Reason: Message:}])
    Jan  3 01:48:41.457: INFO: Trying to dial the pod
    Jan  3 01:48:46.464: INFO: Controller my-hostname-basic-28751012-cd10-422d-8ce4-66c04d1e1934: Got expected result from replica 1 [my-hostname-basic-28751012-cd10-422d-8ce4-66c04d1e1934-5nwsw]: "my-hostname-basic-28751012-cd10-422d-8ce4-66c04d1e1934-5nwsw", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan  3 01:48:46.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-3912" for this suite. 01/03/23 01:48:46.468
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:48:46.471
Jan  3 01:48:46.471: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename projected 01/03/23 01:48:46.472
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:48:46.486
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:48:46.489
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
STEP: Creating configMap with name configmap-projected-all-test-volume-a87b5a68-aa30-4ab2-8424-87d1a31e823a 01/03/23 01:48:46.491
STEP: Creating secret with name secret-projected-all-test-volume-ebb6a9ee-c483-4312-a5c1-c6f131bbfaf3 01/03/23 01:48:46.514
STEP: Creating a pod to test Check all projections for projected volume plugin 01/03/23 01:48:46.521
Jan  3 01:48:46.529: INFO: Waiting up to 5m0s for pod "projected-volume-5a95c3d2-e1e1-4236-9da4-10fdbfae3b85" in namespace "projected-1729" to be "Succeeded or Failed"
Jan  3 01:48:46.561: INFO: Pod "projected-volume-5a95c3d2-e1e1-4236-9da4-10fdbfae3b85": Phase="Pending", Reason="", readiness=false. Elapsed: 31.227599ms
Jan  3 01:48:48.564: INFO: Pod "projected-volume-5a95c3d2-e1e1-4236-9da4-10fdbfae3b85": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034185098s
Jan  3 01:48:50.565: INFO: Pod "projected-volume-5a95c3d2-e1e1-4236-9da4-10fdbfae3b85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035366141s
STEP: Saw pod success 01/03/23 01:48:50.565
Jan  3 01:48:50.565: INFO: Pod "projected-volume-5a95c3d2-e1e1-4236-9da4-10fdbfae3b85" satisfied condition "Succeeded or Failed"
Jan  3 01:48:50.567: INFO: Trying to get logs from node cncf-master pod projected-volume-5a95c3d2-e1e1-4236-9da4-10fdbfae3b85 container projected-all-volume-test: <nil>
STEP: delete the pod 01/03/23 01:48:50.57
Jan  3 01:48:50.623: INFO: Waiting for pod projected-volume-5a95c3d2-e1e1-4236-9da4-10fdbfae3b85 to disappear
Jan  3 01:48:50.624: INFO: Pod projected-volume-5a95c3d2-e1e1-4236-9da4-10fdbfae3b85 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:187
Jan  3 01:48:50.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1729" for this suite. 01/03/23 01:48:50.627
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","completed":18,"skipped":302,"failed":0}
------------------------------
• [4.158 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:48:46.471
    Jan  3 01:48:46.471: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename projected 01/03/23 01:48:46.472
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:48:46.486
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:48:46.489
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:43
    STEP: Creating configMap with name configmap-projected-all-test-volume-a87b5a68-aa30-4ab2-8424-87d1a31e823a 01/03/23 01:48:46.491
    STEP: Creating secret with name secret-projected-all-test-volume-ebb6a9ee-c483-4312-a5c1-c6f131bbfaf3 01/03/23 01:48:46.514
    STEP: Creating a pod to test Check all projections for projected volume plugin 01/03/23 01:48:46.521
    Jan  3 01:48:46.529: INFO: Waiting up to 5m0s for pod "projected-volume-5a95c3d2-e1e1-4236-9da4-10fdbfae3b85" in namespace "projected-1729" to be "Succeeded or Failed"
    Jan  3 01:48:46.561: INFO: Pod "projected-volume-5a95c3d2-e1e1-4236-9da4-10fdbfae3b85": Phase="Pending", Reason="", readiness=false. Elapsed: 31.227599ms
    Jan  3 01:48:48.564: INFO: Pod "projected-volume-5a95c3d2-e1e1-4236-9da4-10fdbfae3b85": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034185098s
    Jan  3 01:48:50.565: INFO: Pod "projected-volume-5a95c3d2-e1e1-4236-9da4-10fdbfae3b85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035366141s
    STEP: Saw pod success 01/03/23 01:48:50.565
    Jan  3 01:48:50.565: INFO: Pod "projected-volume-5a95c3d2-e1e1-4236-9da4-10fdbfae3b85" satisfied condition "Succeeded or Failed"
    Jan  3 01:48:50.567: INFO: Trying to get logs from node cncf-master pod projected-volume-5a95c3d2-e1e1-4236-9da4-10fdbfae3b85 container projected-all-volume-test: <nil>
    STEP: delete the pod 01/03/23 01:48:50.57
    Jan  3 01:48:50.623: INFO: Waiting for pod projected-volume-5a95c3d2-e1e1-4236-9da4-10fdbfae3b85 to disappear
    Jan  3 01:48:50.624: INFO: Pod projected-volume-5a95c3d2-e1e1-4236-9da4-10fdbfae3b85 no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:187
    Jan  3 01:48:50.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1729" for this suite. 01/03/23 01:48:50.627
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:48:50.63
Jan  3 01:48:50.630: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename dns 01/03/23 01:48:50.631
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:48:50.646
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:48:50.648
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6862.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6862.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 01/03/23 01:48:50.65
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6862.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6862.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 01/03/23 01:48:50.65
STEP: creating a pod to probe /etc/hosts 01/03/23 01:48:50.65
STEP: submitting the pod to kubernetes 01/03/23 01:48:50.651
Jan  3 01:48:50.656: INFO: Waiting up to 15m0s for pod "dns-test-8731dd77-232e-44f9-acce-55c522d65b05" in namespace "dns-6862" to be "running"
Jan  3 01:48:50.665: INFO: Pod "dns-test-8731dd77-232e-44f9-acce-55c522d65b05": Phase="Pending", Reason="", readiness=false. Elapsed: 8.579299ms
Jan  3 01:48:52.668: INFO: Pod "dns-test-8731dd77-232e-44f9-acce-55c522d65b05": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011168225s
Jan  3 01:48:54.669: INFO: Pod "dns-test-8731dd77-232e-44f9-acce-55c522d65b05": Phase="Running", Reason="", readiness=true. Elapsed: 4.012029728s
Jan  3 01:48:54.669: INFO: Pod "dns-test-8731dd77-232e-44f9-acce-55c522d65b05" satisfied condition "running"
STEP: retrieving the pod 01/03/23 01:48:54.669
STEP: looking for the results for each expected name from probers 01/03/23 01:48:54.67
Jan  3 01:48:54.680: INFO: DNS probes using dns-6862/dns-test-8731dd77-232e-44f9-acce-55c522d65b05 succeeded

STEP: deleting the pod 01/03/23 01:48:54.68
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan  3 01:48:54.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6862" for this suite. 01/03/23 01:48:54.7
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]","completed":19,"skipped":306,"failed":0}
------------------------------
• [4.073 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:48:50.63
    Jan  3 01:48:50.630: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename dns 01/03/23 01:48:50.631
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:48:50.646
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:48:50.648
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6862.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6862.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     01/03/23 01:48:50.65
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6862.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6862.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     01/03/23 01:48:50.65
    STEP: creating a pod to probe /etc/hosts 01/03/23 01:48:50.65
    STEP: submitting the pod to kubernetes 01/03/23 01:48:50.651
    Jan  3 01:48:50.656: INFO: Waiting up to 15m0s for pod "dns-test-8731dd77-232e-44f9-acce-55c522d65b05" in namespace "dns-6862" to be "running"
    Jan  3 01:48:50.665: INFO: Pod "dns-test-8731dd77-232e-44f9-acce-55c522d65b05": Phase="Pending", Reason="", readiness=false. Elapsed: 8.579299ms
    Jan  3 01:48:52.668: INFO: Pod "dns-test-8731dd77-232e-44f9-acce-55c522d65b05": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011168225s
    Jan  3 01:48:54.669: INFO: Pod "dns-test-8731dd77-232e-44f9-acce-55c522d65b05": Phase="Running", Reason="", readiness=true. Elapsed: 4.012029728s
    Jan  3 01:48:54.669: INFO: Pod "dns-test-8731dd77-232e-44f9-acce-55c522d65b05" satisfied condition "running"
    STEP: retrieving the pod 01/03/23 01:48:54.669
    STEP: looking for the results for each expected name from probers 01/03/23 01:48:54.67
    Jan  3 01:48:54.680: INFO: DNS probes using dns-6862/dns-test-8731dd77-232e-44f9-acce-55c522d65b05 succeeded

    STEP: deleting the pod 01/03/23 01:48:54.68
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan  3 01:48:54.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-6862" for this suite. 01/03/23 01:48:54.7
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:48:54.703
Jan  3 01:48:54.703: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename statefulset 01/03/23 01:48:54.704
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:48:54.721
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:48:54.723
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-3260 01/03/23 01:48:54.725
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
STEP: Creating stateful set ss in namespace statefulset-3260 01/03/23 01:48:54.73
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3260 01/03/23 01:48:54.742
Jan  3 01:48:54.747: INFO: Found 0 stateful pods, waiting for 1
Jan  3 01:49:04.750: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 01/03/23 01:49:04.75
Jan  3 01:49:04.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=statefulset-3260 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  3 01:49:04.898: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  3 01:49:04.898: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  3 01:49:04.898: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  3 01:49:04.906: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan  3 01:49:14.912: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan  3 01:49:14.912: INFO: Waiting for statefulset status.replicas updated to 0
Jan  3 01:49:14.948: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Jan  3 01:49:14.948: INFO: ss-0  cncf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:48:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:49:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:49:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:48:54 +0000 UTC  }]
Jan  3 01:49:14.948: INFO: 
Jan  3 01:49:14.948: INFO: StatefulSet ss has not reached scale 3, at 1
Jan  3 01:49:15.954: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.969106211s
Jan  3 01:49:16.957: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.963423146s
Jan  3 01:49:17.960: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.960398112s
Jan  3 01:49:18.964: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.95624972s
Jan  3 01:49:19.966: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.9535854s
Jan  3 01:49:20.969: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.950789916s
Jan  3 01:49:21.973: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.947404379s
Jan  3 01:49:22.988: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.944750691s
Jan  3 01:49:23.991: INFO: Verifying statefulset ss doesn't scale past 3 for another 929.562321ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3260 01/03/23 01:49:24.991
Jan  3 01:49:25.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=statefulset-3260 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  3 01:49:25.157: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan  3 01:49:25.157: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  3 01:49:25.157: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  3 01:49:25.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=statefulset-3260 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  3 01:49:25.304: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan  3 01:49:25.305: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  3 01:49:25.305: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  3 01:49:25.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=statefulset-3260 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  3 01:49:25.446: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan  3 01:49:25.446: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  3 01:49:25.446: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  3 01:49:25.449: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Jan  3 01:49:35.455: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 01:49:35.455: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 01:49:35.455: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 01/03/23 01:49:35.455
Jan  3 01:49:35.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=statefulset-3260 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  3 01:49:35.600: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  3 01:49:35.600: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  3 01:49:35.600: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  3 01:49:35.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=statefulset-3260 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  3 01:49:35.739: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  3 01:49:35.739: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  3 01:49:35.739: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  3 01:49:35.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=statefulset-3260 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  3 01:49:35.950: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  3 01:49:35.950: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  3 01:49:35.950: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  3 01:49:35.950: INFO: Waiting for statefulset status.replicas updated to 0
Jan  3 01:49:35.959: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jan  3 01:49:45.965: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan  3 01:49:45.965: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan  3 01:49:45.965: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan  3 01:49:45.976: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Jan  3 01:49:45.976: INFO: ss-0  cncf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:48:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:49:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:49:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:48:54 +0000 UTC  }]
Jan  3 01:49:45.976: INFO: ss-1  cncf-node1   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:49:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:49:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:49:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:49:14 +0000 UTC  }]
Jan  3 01:49:45.976: INFO: ss-2  cncf-node2   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:49:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:49:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:49:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:49:14 +0000 UTC  }]
Jan  3 01:49:45.976: INFO: 
Jan  3 01:49:45.976: INFO: StatefulSet ss has not reached scale 0, at 3
Jan  3 01:49:46.987: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Jan  3 01:49:46.988: INFO: ss-0  cncf-master  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:48:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:49:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:49:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:48:54 +0000 UTC  }]
Jan  3 01:49:46.988: INFO: ss-2  cncf-node2   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:49:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:49:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:49:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:49:14 +0000 UTC  }]
Jan  3 01:49:46.988: INFO: 
Jan  3 01:49:46.988: INFO: StatefulSet ss has not reached scale 0, at 2
Jan  3 01:49:47.990: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.982125005s
Jan  3 01:49:49.016: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.980036093s
Jan  3 01:49:50.018: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.954202592s
Jan  3 01:49:51.021: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.951938055s
Jan  3 01:49:52.023: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.94975207s
Jan  3 01:49:53.025: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.947556014s
Jan  3 01:49:54.027: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.945112699s
Jan  3 01:49:55.038: INFO: Verifying statefulset ss doesn't scale past 0 for another 943.202094ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3260 01/03/23 01:49:56.039
Jan  3 01:49:56.041: INFO: Scaling statefulset ss to 0
Jan  3 01:49:56.048: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan  3 01:49:56.049: INFO: Deleting all statefulset in ns statefulset-3260
Jan  3 01:49:56.051: INFO: Scaling statefulset ss to 0
Jan  3 01:49:56.056: INFO: Waiting for statefulset status.replicas updated to 0
Jan  3 01:49:56.058: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan  3 01:49:56.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3260" for this suite. 01/03/23 01:49:56.097
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","completed":20,"skipped":337,"failed":0}
------------------------------
• [SLOW TEST] [61.397 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:695

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:48:54.703
    Jan  3 01:48:54.703: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename statefulset 01/03/23 01:48:54.704
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:48:54.721
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:48:54.723
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-3260 01/03/23 01:48:54.725
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:695
    STEP: Creating stateful set ss in namespace statefulset-3260 01/03/23 01:48:54.73
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3260 01/03/23 01:48:54.742
    Jan  3 01:48:54.747: INFO: Found 0 stateful pods, waiting for 1
    Jan  3 01:49:04.750: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 01/03/23 01:49:04.75
    Jan  3 01:49:04.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=statefulset-3260 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  3 01:49:04.898: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  3 01:49:04.898: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  3 01:49:04.898: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  3 01:49:04.906: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jan  3 01:49:14.912: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan  3 01:49:14.912: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  3 01:49:14.948: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
    Jan  3 01:49:14.948: INFO: ss-0  cncf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:48:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:49:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:49:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:48:54 +0000 UTC  }]
    Jan  3 01:49:14.948: INFO: 
    Jan  3 01:49:14.948: INFO: StatefulSet ss has not reached scale 3, at 1
    Jan  3 01:49:15.954: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.969106211s
    Jan  3 01:49:16.957: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.963423146s
    Jan  3 01:49:17.960: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.960398112s
    Jan  3 01:49:18.964: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.95624972s
    Jan  3 01:49:19.966: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.9535854s
    Jan  3 01:49:20.969: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.950789916s
    Jan  3 01:49:21.973: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.947404379s
    Jan  3 01:49:22.988: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.944750691s
    Jan  3 01:49:23.991: INFO: Verifying statefulset ss doesn't scale past 3 for another 929.562321ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3260 01/03/23 01:49:24.991
    Jan  3 01:49:25.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=statefulset-3260 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  3 01:49:25.157: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan  3 01:49:25.157: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan  3 01:49:25.157: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan  3 01:49:25.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=statefulset-3260 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  3 01:49:25.304: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jan  3 01:49:25.305: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan  3 01:49:25.305: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan  3 01:49:25.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=statefulset-3260 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  3 01:49:25.446: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jan  3 01:49:25.446: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan  3 01:49:25.446: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan  3 01:49:25.449: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
    Jan  3 01:49:35.455: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan  3 01:49:35.455: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan  3 01:49:35.455: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 01/03/23 01:49:35.455
    Jan  3 01:49:35.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=statefulset-3260 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  3 01:49:35.600: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  3 01:49:35.600: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  3 01:49:35.600: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  3 01:49:35.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=statefulset-3260 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  3 01:49:35.739: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  3 01:49:35.739: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  3 01:49:35.739: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  3 01:49:35.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=statefulset-3260 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  3 01:49:35.950: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  3 01:49:35.950: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  3 01:49:35.950: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  3 01:49:35.950: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  3 01:49:35.959: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Jan  3 01:49:45.965: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan  3 01:49:45.965: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jan  3 01:49:45.965: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jan  3 01:49:45.976: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
    Jan  3 01:49:45.976: INFO: ss-0  cncf-master  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:48:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:49:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:49:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:48:54 +0000 UTC  }]
    Jan  3 01:49:45.976: INFO: ss-1  cncf-node1   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:49:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:49:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:49:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:49:14 +0000 UTC  }]
    Jan  3 01:49:45.976: INFO: ss-2  cncf-node2   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:49:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:49:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:49:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:49:14 +0000 UTC  }]
    Jan  3 01:49:45.976: INFO: 
    Jan  3 01:49:45.976: INFO: StatefulSet ss has not reached scale 0, at 3
    Jan  3 01:49:46.987: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
    Jan  3 01:49:46.988: INFO: ss-0  cncf-master  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:48:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:49:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:49:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:48:54 +0000 UTC  }]
    Jan  3 01:49:46.988: INFO: ss-2  cncf-node2   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:49:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:49:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:49:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 01:49:14 +0000 UTC  }]
    Jan  3 01:49:46.988: INFO: 
    Jan  3 01:49:46.988: INFO: StatefulSet ss has not reached scale 0, at 2
    Jan  3 01:49:47.990: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.982125005s
    Jan  3 01:49:49.016: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.980036093s
    Jan  3 01:49:50.018: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.954202592s
    Jan  3 01:49:51.021: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.951938055s
    Jan  3 01:49:52.023: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.94975207s
    Jan  3 01:49:53.025: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.947556014s
    Jan  3 01:49:54.027: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.945112699s
    Jan  3 01:49:55.038: INFO: Verifying statefulset ss doesn't scale past 0 for another 943.202094ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3260 01/03/23 01:49:56.039
    Jan  3 01:49:56.041: INFO: Scaling statefulset ss to 0
    Jan  3 01:49:56.048: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan  3 01:49:56.049: INFO: Deleting all statefulset in ns statefulset-3260
    Jan  3 01:49:56.051: INFO: Scaling statefulset ss to 0
    Jan  3 01:49:56.056: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  3 01:49:56.058: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan  3 01:49:56.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-3260" for this suite. 01/03/23 01:49:56.097
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:49:56.1
Jan  3 01:49:56.100: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename statefulset 01/03/23 01:49:56.101
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:49:56.119
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:49:56.121
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-4737 01/03/23 01:49:56.125
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
STEP: Looking for a node to schedule stateful set and pod 01/03/23 01:49:56.143
STEP: Creating pod with conflicting port in namespace statefulset-4737 01/03/23 01:49:56.149
STEP: Waiting until pod test-pod will start running in namespace statefulset-4737 01/03/23 01:49:56.157
Jan  3 01:49:56.157: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-4737" to be "running"
Jan  3 01:49:56.178: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 20.768657ms
Jan  3 01:49:58.181: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.023777886s
Jan  3 01:49:58.181: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-4737 01/03/23 01:49:58.181
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-4737 01/03/23 01:49:58.185
Jan  3 01:49:58.223: INFO: Observed stateful pod in namespace: statefulset-4737, name: ss-0, uid: bd28f770-0de0-4b36-bf05-4fdfbcf7d47f, status phase: Pending. Waiting for statefulset controller to delete.
Jan  3 01:49:58.245: INFO: Observed stateful pod in namespace: statefulset-4737, name: ss-0, uid: bd28f770-0de0-4b36-bf05-4fdfbcf7d47f, status phase: Failed. Waiting for statefulset controller to delete.
Jan  3 01:49:58.268: INFO: Observed stateful pod in namespace: statefulset-4737, name: ss-0, uid: bd28f770-0de0-4b36-bf05-4fdfbcf7d47f, status phase: Failed. Waiting for statefulset controller to delete.
Jan  3 01:49:58.331: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-4737
STEP: Removing pod with conflicting port in namespace statefulset-4737 01/03/23 01:49:58.331
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-4737 and will be in running state 01/03/23 01:49:58.367
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan  3 01:50:02.376: INFO: Deleting all statefulset in ns statefulset-4737
Jan  3 01:50:02.378: INFO: Scaling statefulset ss to 0
Jan  3 01:50:12.394: INFO: Waiting for statefulset status.replicas updated to 0
Jan  3 01:50:12.395: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan  3 01:50:12.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4737" for this suite. 01/03/23 01:50:12.407
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","completed":21,"skipped":337,"failed":0}
------------------------------
• [SLOW TEST] [16.309 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:737

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:49:56.1
    Jan  3 01:49:56.100: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename statefulset 01/03/23 01:49:56.101
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:49:56.119
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:49:56.121
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-4737 01/03/23 01:49:56.125
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:737
    STEP: Looking for a node to schedule stateful set and pod 01/03/23 01:49:56.143
    STEP: Creating pod with conflicting port in namespace statefulset-4737 01/03/23 01:49:56.149
    STEP: Waiting until pod test-pod will start running in namespace statefulset-4737 01/03/23 01:49:56.157
    Jan  3 01:49:56.157: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-4737" to be "running"
    Jan  3 01:49:56.178: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 20.768657ms
    Jan  3 01:49:58.181: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.023777886s
    Jan  3 01:49:58.181: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-4737 01/03/23 01:49:58.181
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-4737 01/03/23 01:49:58.185
    Jan  3 01:49:58.223: INFO: Observed stateful pod in namespace: statefulset-4737, name: ss-0, uid: bd28f770-0de0-4b36-bf05-4fdfbcf7d47f, status phase: Pending. Waiting for statefulset controller to delete.
    Jan  3 01:49:58.245: INFO: Observed stateful pod in namespace: statefulset-4737, name: ss-0, uid: bd28f770-0de0-4b36-bf05-4fdfbcf7d47f, status phase: Failed. Waiting for statefulset controller to delete.
    Jan  3 01:49:58.268: INFO: Observed stateful pod in namespace: statefulset-4737, name: ss-0, uid: bd28f770-0de0-4b36-bf05-4fdfbcf7d47f, status phase: Failed. Waiting for statefulset controller to delete.
    Jan  3 01:49:58.331: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-4737
    STEP: Removing pod with conflicting port in namespace statefulset-4737 01/03/23 01:49:58.331
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-4737 and will be in running state 01/03/23 01:49:58.367
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan  3 01:50:02.376: INFO: Deleting all statefulset in ns statefulset-4737
    Jan  3 01:50:02.378: INFO: Scaling statefulset ss to 0
    Jan  3 01:50:12.394: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  3 01:50:12.395: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan  3 01:50:12.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-4737" for this suite. 01/03/23 01:50:12.407
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:50:12.41
Jan  3 01:50:12.411: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename deployment 01/03/23 01:50:12.412
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:50:12.425
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:50:12.427
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Jan  3 01:50:12.429: INFO: Creating deployment "webserver-deployment"
Jan  3 01:50:12.432: INFO: Waiting for observed generation 1
Jan  3 01:50:14.450: INFO: Waiting for all required pods to come up
Jan  3 01:50:14.453: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 01/03/23 01:50:14.453
Jan  3 01:50:14.453: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-bbjcp" in namespace "deployment-995" to be "running"
Jan  3 01:50:14.453: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-6jgxv" in namespace "deployment-995" to be "running"
Jan  3 01:50:14.453: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-r56wc" in namespace "deployment-995" to be "running"
Jan  3 01:50:14.455: INFO: Pod "webserver-deployment-845c8977d9-r56wc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.643532ms
Jan  3 01:50:14.455: INFO: Pod "webserver-deployment-845c8977d9-6jgxv": Phase="Pending", Reason="", readiness=false. Elapsed: 1.786354ms
Jan  3 01:50:14.455: INFO: Pod "webserver-deployment-845c8977d9-bbjcp": Phase="Pending", Reason="", readiness=false. Elapsed: 1.913868ms
Jan  3 01:50:16.458: INFO: Pod "webserver-deployment-845c8977d9-6jgxv": Phase="Running", Reason="", readiness=true. Elapsed: 2.004037156s
Jan  3 01:50:16.458: INFO: Pod "webserver-deployment-845c8977d9-6jgxv" satisfied condition "running"
Jan  3 01:50:16.458: INFO: Pod "webserver-deployment-845c8977d9-r56wc": Phase="Running", Reason="", readiness=true. Elapsed: 2.004012676s
Jan  3 01:50:16.458: INFO: Pod "webserver-deployment-845c8977d9-r56wc" satisfied condition "running"
Jan  3 01:50:16.458: INFO: Pod "webserver-deployment-845c8977d9-bbjcp": Phase="Running", Reason="", readiness=true. Elapsed: 2.004049423s
Jan  3 01:50:16.458: INFO: Pod "webserver-deployment-845c8977d9-bbjcp" satisfied condition "running"
Jan  3 01:50:16.458: INFO: Waiting for deployment "webserver-deployment" to complete
Jan  3 01:50:16.461: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jan  3 01:50:16.465: INFO: Updating deployment webserver-deployment
Jan  3 01:50:16.465: INFO: Waiting for observed generation 2
Jan  3 01:50:18.514: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jan  3 01:50:18.516: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jan  3 01:50:18.518: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan  3 01:50:18.523: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jan  3 01:50:18.523: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jan  3 01:50:18.524: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan  3 01:50:18.527: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jan  3 01:50:18.527: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jan  3 01:50:18.531: INFO: Updating deployment webserver-deployment
Jan  3 01:50:18.531: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jan  3 01:50:18.579: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jan  3 01:50:18.597: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan  3 01:50:20.658: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-995  226113e7-e926-422e-a315-6dd8dcbafc1a 975725 3 2023-01-03 01:50:12 +0000 UTC <nil> <nil> map[name:httpd] map[createdTime:2023-01-03T10:50:12.430486675+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:2 updatedTime:2023-01-03T10:50:12.430486675+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update apps/v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00007b748 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-03 01:50:18 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-01-03 01:50:18 +0000 UTC,LastTransitionTime:2023-01-03 01:50:12 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jan  3 01:50:20.669: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-995  dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c 975723 3 2023-01-03 01:50:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[createdTime:2023-01-03T10:50:12.430486675+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2 updatedTime:2023-01-03T10:50:12.430486675+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [{apps/v1 Deployment webserver-deployment 226113e7-e926-422e-a315-6dd8dcbafc1a 0xc003bf045e 0xc003bf045f}] [] [{kube-controller-manager Update apps/v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:createdTime":{},"f:creator":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{},"f:updatedTime":{},"f:updater":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"226113e7-e926-422e-a315-6dd8dcbafc1a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003bf0508 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan  3 01:50:20.669: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jan  3 01:50:20.669: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-995  811d1f78-ed35-43fe-843b-34029d2be9be 975831 3 2023-01-03 01:50:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[createdTime:2023-01-03T10:50:12.430486675+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T10:50:12.430486675+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [{apps/v1 Deployment webserver-deployment 226113e7-e926-422e-a315-6dd8dcbafc1a 0xc003bf058e 0xc003bf058f}] [] [{kube-controller-manager Update apps/v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:createdTime":{},"f:creator":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{},"f:updatedTime":{},"f:updater":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"226113e7-e926-422e-a315-6dd8dcbafc1a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-03 01:50:20 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003bf0628 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:9,AvailableReplicas:9,Conditions:[]ReplicaSetCondition{},},}
Jan  3 01:50:20.685: INFO: Pod "webserver-deployment-69b7448995-2dckt" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-2dckt webserver-deployment-69b7448995- deployment-995  68bec98d-ae6d-486f-9e9f-20c543b85719 975782 0 2023-01-03 01:50:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:e1f2f8064d08518f8c590f3a3cf6aebb412d0af2105759b468233d4ba8c953bc cni.projectcalico.org/podIP:10.244.89.121/32 cni.projectcalico.org/podIPs:10.244.89.121/32 createdTime:2023-01-03T10:50:18.572051284+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:18.572051284+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c 0xc000dbc6e7 0xc000dbc6e8}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-03 01:50:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9ws2n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9ws2n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.13,PodIP:,StartTime:2023-01-03 01:50:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 01:50:20.685: INFO: Pod "webserver-deployment-69b7448995-bm4h8" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-bm4h8 webserver-deployment-69b7448995- deployment-995  9225acee-c288-451a-b31a-73361f7e4d4a 975604 0 2023-01-03 01:50:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:54b356d9922e1d5b6a5c9143b5d80f8ce8140211789cc7899cf8e9fafc17d25f cni.projectcalico.org/podIP:10.244.89.66/32 cni.projectcalico.org/podIPs:10.244.89.66/32 createdTime:2023-01-03T10:50:16.519710551+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:16.519710551+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c 0xc000dbc97e 0xc000dbc97f}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-03 01:50:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-03 01:50:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5pjxn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5pjxn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.13,PodIP:,StartTime:2023-01-03 01:50:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 01:50:20.685: INFO: Pod "webserver-deployment-69b7448995-c9n8t" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-c9n8t webserver-deployment-69b7448995- deployment-995  4500c4d5-d7dc-43b4-bf35-eb7ae7d1e4ed 975786 0 2023-01-03 01:50:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:f401c62eb1ccf72fd3fa7a1ceeb40a6221334dbe9fe3cb4c900946cc7cba1b57 cni.projectcalico.org/podIP:10.244.246.105/32 cni.projectcalico.org/podIPs:10.244.246.105/32 createdTime:2023-01-03T10:50:18.582162377+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:18.582162377+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c 0xc000dbcc4e 0xc000dbcc4f}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-03 01:50:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ntdbs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ntdbs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-master,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.5,PodIP:,StartTime:2023-01-03 01:50:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 01:50:20.685: INFO: Pod "webserver-deployment-69b7448995-krbgn" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-krbgn webserver-deployment-69b7448995- deployment-995  5dfb8e35-c88b-479b-9964-c0d53b264c9f 975813 0 2023-01-03 01:50:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:d611f71833c8869bf3a0db9242e592ce032926dab9c91c4ef36db27ba14afc11 cni.projectcalico.org/podIP:10.244.246.107/32 cni.projectcalico.org/podIPs:10.244.246.107/32 createdTime:2023-01-03T10:50:18.594399868+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:18.594399868+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c 0xc000dbda1e 0xc000dbda1f}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-03 01:50:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tkbkh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tkbkh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-master,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.5,PodIP:,StartTime:2023-01-03 01:50:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 01:50:20.685: INFO: Pod "webserver-deployment-69b7448995-qcg65" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-qcg65 webserver-deployment-69b7448995- deployment-995  32db6e48-55cc-4bcb-8936-fbaedac7d61d 975600 0 2023-01-03 01:50:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:2afbce58b21639496523ca6060f701aef988c727ef7ff5e9f59fde72df71f396 cni.projectcalico.org/podIP:10.244.232.52/32 cni.projectcalico.org/podIPs:10.244.232.52/32 createdTime:2023-01-03T10:50:16.519372652+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:16.519372652+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c 0xc0073d423e 0xc0073d423f}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-03 01:50:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-03 01:50:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hmh9n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hmh9n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.7,PodIP:,StartTime:2023-01-03 01:50:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 01:50:20.686: INFO: Pod "webserver-deployment-69b7448995-qcj8d" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-qcj8d webserver-deployment-69b7448995- deployment-995  ef7b8cba-cb21-4737-86e4-81c8b44824af 975820 0 2023-01-03 01:50:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:f720b25d5ff4163f2117ad5d10c7ee2b0cbad60595e29f045593cc39f159e9cf cni.projectcalico.org/podIP:10.244.89.89/32 cni.projectcalico.org/podIPs:10.244.89.89/32 createdTime:2023-01-03T10:50:18.59643788+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:18.59643788+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c 0xc0073d460e 0xc0073d460f}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-03 01:50:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2qz5h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2qz5h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.13,PodIP:,StartTime:2023-01-03 01:50:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 01:50:20.686: INFO: Pod "webserver-deployment-69b7448995-qrnf8" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-qrnf8 webserver-deployment-69b7448995- deployment-995  4691d76a-86a1-4fa1-8379-33a6cfe71248 975616 0 2023-01-03 01:50:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:ce179a3be625cf0ce784b2fac78fd0a3d83c3a5a528bd9c914fabc06e6934e18 cni.projectcalico.org/podIP:10.244.246.103/32 cni.projectcalico.org/podIPs:10.244.246.103/32 createdTime:2023-01-03T10:50:16.589361061+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:16.589361061+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c 0xc0073d484e 0xc0073d484f}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-03 01:50:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-03 01:50:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6qp9p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6qp9p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-master,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.5,PodIP:,StartTime:2023-01-03 01:50:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 01:50:20.686: INFO: Pod "webserver-deployment-69b7448995-shjl8" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-shjl8 webserver-deployment-69b7448995- deployment-995  c28298f9-bb33-4709-9800-bc09976c46a7 975611 0 2023-01-03 01:50:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:8b873762bb83245599fa766ba38e10fce7922edf3b432454a46ce36852662a46 cni.projectcalico.org/podIP:10.244.232.50/32 cni.projectcalico.org/podIPs:10.244.232.50/32 createdTime:2023-01-03T10:50:16.600701804+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:16.600701804+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c 0xc0073d4a9e 0xc0073d4a9f}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-03 01:50:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-03 01:50:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nmsc2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nmsc2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.7,PodIP:,StartTime:2023-01-03 01:50:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 01:50:20.686: INFO: Pod "webserver-deployment-69b7448995-shzch" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-shzch webserver-deployment-69b7448995- deployment-995  7bc2bb6c-ba2a-46bf-82ed-9d8c8dd308e4 975719 0 2023-01-03 01:50:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[createdTime:2023-01-03T10:50:18.639612728+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:18.639612728+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c 0xc0073d4cce 0xc0073d4ccf}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bjvzr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bjvzr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-master,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 01:50:20.686: INFO: Pod "webserver-deployment-69b7448995-svk27" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-svk27 webserver-deployment-69b7448995- deployment-995  e626c3a5-dba1-4292-a4cc-e33d367b0b61 975610 0 2023-01-03 01:50:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:906372a3c00b12c587062b527a85a84420c185b0731733a69b0cba28943fd1b2 cni.projectcalico.org/podIP:10.244.246.102/32 cni.projectcalico.org/podIPs:10.244.246.102/32 createdTime:2023-01-03T10:50:16.514420229+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:16.514420229+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c 0xc0073d4e6e 0xc0073d4e6f}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-03 01:50:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-03 01:50:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ch6js,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ch6js,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-master,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.5,PodIP:,StartTime:2023-01-03 01:50:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 01:50:20.686: INFO: Pod "webserver-deployment-69b7448995-tbpbn" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-tbpbn webserver-deployment-69b7448995- deployment-995  76004de2-b2a7-4d2f-8049-75cd7ba0eb63 975767 0 2023-01-03 01:50:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:0d9b563e0a510625266c4170f8644cab5f70650d12082f0012dba73b93efab51 cni.projectcalico.org/podIP:10.244.232.27/32 cni.projectcalico.org/podIPs:10.244.232.27/32 createdTime:2023-01-03T10:50:18.59542773+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:18.59542773+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c 0xc0073d50be 0xc0073d50bf}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-03 01:50:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nmw69,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nmw69,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.7,PodIP:,StartTime:2023-01-03 01:50:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 01:50:20.686: INFO: Pod "webserver-deployment-69b7448995-xpjch" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-xpjch webserver-deployment-69b7448995- deployment-995  c8fdd708-da8b-4c39-b296-559358a169ad 975801 0 2023-01-03 01:50:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:17a5168aa7e9f9498f22c2b4cbeb05322a4499b88be69dc330b9dc3a6fa1d29e cni.projectcalico.org/podIP:10.244.232.3/32 cni.projectcalico.org/podIPs:10.244.232.3/32 createdTime:2023-01-03T10:50:18.58245745+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:18.58245745+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c 0xc0073d530e 0xc0073d530f}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-03 01:50:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pzn97,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pzn97,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.7,PodIP:,StartTime:2023-01-03 01:50:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 01:50:20.686: INFO: Pod "webserver-deployment-69b7448995-zgfng" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-zgfng webserver-deployment-69b7448995- deployment-995  a8681391-9706-4245-8039-b1e351b83271 975773 0 2023-01-03 01:50:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:915d01ac7fce125c0cdb45d64def07f7aaab66752ed87327bbe30725a6e21f66 cni.projectcalico.org/podIP:10.244.89.112/32 cni.projectcalico.org/podIPs:10.244.89.112/32 createdTime:2023-01-03T10:50:18.594007671+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:18.594007671+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c 0xc0073d555e 0xc0073d555f}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-03 01:50:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8tq6k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8tq6k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.13,PodIP:,StartTime:2023-01-03 01:50:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 01:50:20.687: INFO: Pod "webserver-deployment-845c8977d9-4dzzs" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-4dzzs webserver-deployment-845c8977d9- deployment-995  905a8688-cb7b-4346-b967-371e300f1329 975749 0 2023-01-03 01:50:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[createdTime:2023-01-03T10:50:18.637820277+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:18.637820277+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 811d1f78-ed35-43fe-843b-34029d2be9be 0xc0073d578e 0xc0073d578f}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"811d1f78-ed35-43fe-843b-34029d2be9be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8t6q6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8t6q6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.13,PodIP:,StartTime:2023-01-03 01:50:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 01:50:20.687: INFO: Pod "webserver-deployment-845c8977d9-5lcd4" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-5lcd4 webserver-deployment-845c8977d9- deployment-995  ed580428-1262-493e-bd8d-7551d495b927 975509 0 2023-01-03 01:50:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:15b0b332a385a65fa97550717841710fd611fd2148c8936f3856addd773be6eb cni.projectcalico.org/podIP:10.244.89.87/32 cni.projectcalico.org/podIPs:10.244.89.87/32 createdTime:2023-01-03T10:50:12.477356082+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:12.477356082+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 811d1f78-ed35-43fe-843b-34029d2be9be 0xc0073d59b7 0xc0073d59b8}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"811d1f78-ed35-43fe-843b-34029d2be9be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-03 01:50:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-03 01:50:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.89.87\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-psn5j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-psn5j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.13,PodIP:10.244.89.87,StartTime:2023-01-03 01:50:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-03 01:50:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://1822882188f355ced71e3c5ffc03d574a0322189c914f3c9aae0ccc94c1554ed,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.89.87,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 01:50:20.687: INFO: Pod "webserver-deployment-845c8977d9-7hfkg" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-7hfkg webserver-deployment-845c8977d9- deployment-995  765561e3-3827-4f5d-9ef9-a4a8c03a8efe 975514 0 2023-01-03 01:50:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:4bf66a29f1bd1bf8d6556b7ae317049b5a20e506e3a78bf070bc7808b7dcf2b7 cni.projectcalico.org/podIP:10.244.89.124/32 cni.projectcalico.org/podIPs:10.244.89.124/32 createdTime:2023-01-03T10:50:12.548789715+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:12.548789715+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 811d1f78-ed35-43fe-843b-34029d2be9be 0xc0073d5c27 0xc0073d5c28}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"811d1f78-ed35-43fe-843b-34029d2be9be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-03 01:50:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-03 01:50:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.89.124\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q78j8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q78j8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.13,PodIP:10.244.89.124,StartTime:2023-01-03 01:50:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-03 01:50:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://926244a0fa41a2456ff6b936b1f163224b1ea21f8b5406c5391ec631826982b7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.89.124,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 01:50:20.687: INFO: Pod "webserver-deployment-845c8977d9-7hmmx" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-7hmmx webserver-deployment-845c8977d9- deployment-995  09bbab4f-57ec-4203-ad11-772eb875c0fe 975510 0 2023-01-03 01:50:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:2f35dc2accc96f4193755e2ad40d97dcb8cfb54c2e64d34ff566bc9bf6dcc049 cni.projectcalico.org/podIP:10.244.232.48/32 cni.projectcalico.org/podIPs:10.244.232.48/32 createdTime:2023-01-03T10:50:12.52686285+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:12.52686285+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 811d1f78-ed35-43fe-843b-34029d2be9be 0xc0073d5e97 0xc0073d5e98}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"811d1f78-ed35-43fe-843b-34029d2be9be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-03 01:50:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-03 01:50:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.232.48\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6mh6x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6mh6x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.7,PodIP:10.244.232.48,StartTime:2023-01-03 01:50:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-03 01:50:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://8c4fec9c113902785c81d2a2855f16102d136b25e0df7918a34289cf05327bf7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.232.48,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 01:50:20.687: INFO: Pod "webserver-deployment-845c8977d9-7rx4z" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-7rx4z webserver-deployment-845c8977d9- deployment-995  08c61f27-7cab-4700-81f2-292665771107 975498 0 2023-01-03 01:50:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:b742c570556004ac65a73a43087209d2a1e3f0bb123546f7893ba9c8fae34475 cni.projectcalico.org/podIP:10.244.232.49/32 cni.projectcalico.org/podIPs:10.244.232.49/32 createdTime:2023-01-03T10:50:12.476871995+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:12.476871995+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 811d1f78-ed35-43fe-843b-34029d2be9be 0xc0028162ee 0xc0028162ef}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"811d1f78-ed35-43fe-843b-34029d2be9be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-03 01:50:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-03 01:50:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.232.49\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kwpld,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kwpld,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.7,PodIP:10.244.232.49,StartTime:2023-01-03 01:50:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-03 01:50:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://7ac867fd421b4230dae0c3873368c21bf13a4a33636c578ba1e3886bc9f1d333,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.232.49,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 01:50:20.687: INFO: Pod "webserver-deployment-845c8977d9-7xq7g" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-7xq7g webserver-deployment-845c8977d9- deployment-995  06625b68-9818-4168-9d1b-ec0306f55aaa 975700 0 2023-01-03 01:50:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[createdTime:2023-01-03T10:50:18.637182405+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:18.637182405+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 811d1f78-ed35-43fe-843b-34029d2be9be 0xc00281660e 0xc00281660f}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"811d1f78-ed35-43fe-843b-34029d2be9be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n4lkq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n4lkq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-master,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 01:50:20.687: INFO: Pod "webserver-deployment-845c8977d9-8mjt8" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-8mjt8 webserver-deployment-845c8977d9- deployment-995  1427d114-ea04-4d95-9c3f-ab43cc7badaf 975809 0 2023-01-03 01:50:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:b4dd9fb3d6cf21c2e0707f24c3bd1f318dbb301bfc57a248deed77d35b767df5 cni.projectcalico.org/podIP:10.244.89.96/32 cni.projectcalico.org/podIPs:10.244.89.96/32 createdTime:2023-01-03T10:50:18.583920699+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:18.583920699+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 811d1f78-ed35-43fe-843b-34029d2be9be 0xc002816a3e 0xc002816a3f}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"811d1f78-ed35-43fe-843b-34029d2be9be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-03 01:50:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9pjkd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9pjkd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.13,PodIP:,StartTime:2023-01-03 01:50:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 01:50:20.687: INFO: Pod "webserver-deployment-845c8977d9-96fpp" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-96fpp webserver-deployment-845c8977d9- deployment-995  f1ee2de2-f320-43b1-9dbc-072e56fbd4d1 975823 0 2023-01-03 01:50:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:64e1ecd360448503d3c3abd405c9efe6c231d99a1983606cca9cac1c1e424deb cni.projectcalico.org/podIP:10.244.232.54/32 cni.projectcalico.org/podIPs:10.244.232.54/32 createdTime:2023-01-03T10:50:18.639248696+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:18.639248696+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 811d1f78-ed35-43fe-843b-34029d2be9be 0xc002816ef7 0xc002816ef8}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"811d1f78-ed35-43fe-843b-34029d2be9be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-03 01:50:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-03 01:50:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dmngs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dmngs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.7,PodIP:,StartTime:2023-01-03 01:50:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 01:50:20.688: INFO: Pod "webserver-deployment-845c8977d9-bkb5w" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-bkb5w webserver-deployment-845c8977d9- deployment-995  a14aabb6-2c41-45d8-8931-c09889fcc4c8 975830 0 2023-01-03 01:50:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:025b09dc2a43b503f95dbce2d8cb8e03a6d295d10cfe813425f3396fc33ff8dd cni.projectcalico.org/podIP:10.244.232.47/32 cni.projectcalico.org/podIPs:10.244.232.47/32 createdTime:2023-01-03T10:50:18.572780289+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:18.572780289+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 811d1f78-ed35-43fe-843b-34029d2be9be 0xc00281711e 0xc00281711f}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"811d1f78-ed35-43fe-843b-34029d2be9be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-03 01:50:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-03 01:50:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.232.47\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9kx7s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9kx7s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.7,PodIP:10.244.232.47,StartTime:2023-01-03 01:50:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-03 01:50:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://c2770c2fb28ca3bee7789f613ebabbe809b70477b6ec6d0af68d18c4377dd0ee,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.232.47,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 01:50:20.688: INFO: Pod "webserver-deployment-845c8977d9-bmnjg" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-bmnjg webserver-deployment-845c8977d9- deployment-995  3d04a4dd-1b0b-42e5-9052-ff35c6387fc4 975764 0 2023-01-03 01:50:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:7f6e877e676eaabecb28ebc75cccd8c3f43b7e15e2ac49a5602a3acfe51775be cni.projectcalico.org/podIP:10.244.246.104/32 cni.projectcalico.org/podIPs:10.244.246.104/32 createdTime:2023-01-03T10:50:18.572195123+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:18.572195123+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 811d1f78-ed35-43fe-843b-34029d2be9be 0xc00281734e 0xc00281734f}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"811d1f78-ed35-43fe-843b-34029d2be9be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-03 01:50:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sjt7w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sjt7w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-master,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.5,PodIP:,StartTime:2023-01-03 01:50:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 01:50:20.688: INFO: Pod "webserver-deployment-845c8977d9-nmqnx" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-nmqnx webserver-deployment-845c8977d9- deployment-995  4413e76b-fa7f-46fe-b025-2022235690e0 975789 0 2023-01-03 01:50:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:b8779485a32350a92de6958455f25ccb209790c2483c7dee05f4ba0672b4ba8d cni.projectcalico.org/podIP:10.244.232.20/32 cni.projectcalico.org/podIPs:10.244.232.20/32 createdTime:2023-01-03T10:50:18.583549803+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:18.583549803+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 811d1f78-ed35-43fe-843b-34029d2be9be 0xc00281757e 0xc00281757f}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"811d1f78-ed35-43fe-843b-34029d2be9be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-03 01:50:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5qcx8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5qcx8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.7,PodIP:,StartTime:2023-01-03 01:50:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 01:50:20.688: INFO: Pod "webserver-deployment-845c8977d9-pg28p" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-pg28p webserver-deployment-845c8977d9- deployment-995  0110efdd-7217-4fce-98aa-af5f5692f6a8 975825 0 2023-01-03 01:50:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:0f52021e9f1fbfa5b5b53b6296de2780c3a551fdfa3c8f7ef3ca19900bdce3b7 cni.projectcalico.org/podIP:10.244.246.108/32 cni.projectcalico.org/podIPs:10.244.246.108/32 createdTime:2023-01-03T10:50:18.583134768+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:18.583134768+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 811d1f78-ed35-43fe-843b-34029d2be9be 0xc00281778e 0xc00281778f}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"811d1f78-ed35-43fe-843b-34029d2be9be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-03 01:50:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p6dcj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p6dcj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-master,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.5,PodIP:,StartTime:2023-01-03 01:50:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 01:50:20.688: INFO: Pod "webserver-deployment-845c8977d9-qxs22" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-qxs22 webserver-deployment-845c8977d9- deployment-995  793429cb-30d9-4aeb-ba2a-d61f2024407e 975802 0 2023-01-03 01:50:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:a7e9d11c082f25a619ec4f1d12f89592a87f9a65b623acd843932758a1855265 cni.projectcalico.org/podIP:10.244.246.106/32 cni.projectcalico.org/podIPs:10.244.246.106/32 createdTime:2023-01-03T10:50:18.566071812+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:18.566071812+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 811d1f78-ed35-43fe-843b-34029d2be9be 0xc00281799e 0xc00281799f}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"811d1f78-ed35-43fe-843b-34029d2be9be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-03 01:50:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ths8v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ths8v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-master,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.5,PodIP:,StartTime:2023-01-03 01:50:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 01:50:20.688: INFO: Pod "webserver-deployment-845c8977d9-r2cvg" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-r2cvg webserver-deployment-845c8977d9- deployment-995  83881765-b5fa-45df-bda9-cca30f1daf3b 975715 0 2023-01-03 01:50:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[createdTime:2023-01-03T10:50:18.596554212+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:18.596554212+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 811d1f78-ed35-43fe-843b-34029d2be9be 0xc002817bae 0xc002817baf}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"811d1f78-ed35-43fe-843b-34029d2be9be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vqbqv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vqbqv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.7,PodIP:,StartTime:2023-01-03 01:50:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 01:50:20.688: INFO: Pod "webserver-deployment-845c8977d9-r56wc" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-r56wc webserver-deployment-845c8977d9- deployment-995  b5d653d1-af08-4acd-8b36-5444050caccf 975526 0 2023-01-03 01:50:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:04a36eff1f9f1714a652ac9c341f80f6e470e45de49ea9b68c6846dfbe57e892 cni.projectcalico.org/podIP:10.244.246.101/32 cni.projectcalico.org/podIPs:10.244.246.101/32 createdTime:2023-01-03T10:50:12.54839738+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:12.54839738+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 811d1f78-ed35-43fe-843b-34029d2be9be 0xc002817d9e 0xc002817d9f}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"811d1f78-ed35-43fe-843b-34029d2be9be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-03 01:50:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-03 01:50:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.246.101\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4pfhs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4pfhs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-master,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.5,PodIP:10.244.246.101,StartTime:2023-01-03 01:50:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-03 01:50:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://ccac5aec29ed602390f6ea134f84ae9d68f8d793e76895ff88dea842f6504448,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.246.101,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 01:50:20.688: INFO: Pod "webserver-deployment-845c8977d9-r65tn" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-r65tn webserver-deployment-845c8977d9- deployment-995  5ac2f094-da40-4706-8a68-5c901122d062 975748 0 2023-01-03 01:50:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[createdTime:2023-01-03T10:50:18.594854505+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:18.594854505+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 811d1f78-ed35-43fe-843b-34029d2be9be 0xc002817fce 0xc002817fcf}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"811d1f78-ed35-43fe-843b-34029d2be9be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dkvwb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dkvwb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-master,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.5,PodIP:,StartTime:2023-01-03 01:50:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 01:50:20.689: INFO: Pod "webserver-deployment-845c8977d9-w585r" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-w585r webserver-deployment-845c8977d9- deployment-995  81d7c415-6bcc-4277-a3bc-140d6ac58388 975472 0 2023-01-03 01:50:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:80490eb37833a794c7a48280f0a615cc00194ba177e8a98bdbdad7c0d71e29b0 cni.projectcalico.org/podIP:10.244.246.98/32 cni.projectcalico.org/podIPs:10.244.246.98/32 createdTime:2023-01-03T10:50:12.47166505+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:12.47166505+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 811d1f78-ed35-43fe-843b-34029d2be9be 0xc00397c29e 0xc00397c29f}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"811d1f78-ed35-43fe-843b-34029d2be9be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-03 01:50:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-03 01:50:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.246.98\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-njhvn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-njhvn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-master,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.5,PodIP:10.244.246.98,StartTime:2023-01-03 01:50:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-03 01:50:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://7e806235446ea357493d76d7eb27d050eb1ae1d5bbde04ae15276cfaffa3ea40,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.246.98,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 01:50:20.689: INFO: Pod "webserver-deployment-845c8977d9-wcqj5" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-wcqj5 webserver-deployment-845c8977d9- deployment-995  7602afb6-c49c-4817-a0fd-a992feaaf987 975800 0 2023-01-03 01:50:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:a477032fec5bbc540f030ac68e85cd13812e54e5a0977e61f2b714377bf7e298 cni.projectcalico.org/podIP:10.244.89.118/32 cni.projectcalico.org/podIPs:10.244.89.118/32 createdTime:2023-01-03T10:50:18.582755856+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:18.582755856+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 811d1f78-ed35-43fe-843b-34029d2be9be 0xc00397c80e 0xc00397c80f}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"811d1f78-ed35-43fe-843b-34029d2be9be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-03 01:50:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-95h8w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-95h8w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.13,PodIP:,StartTime:2023-01-03 01:50:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 01:50:20.689: INFO: Pod "webserver-deployment-845c8977d9-wkqzk" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-wkqzk webserver-deployment-845c8977d9- deployment-995  a16f2833-3796-400c-a3b3-097a89e916bc 975518 0 2023-01-03 01:50:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:06db7e879f1277d99353972ed1413ef09dea08b336c2d8d15749f676653b4e12 cni.projectcalico.org/podIP:10.244.89.67/32 cni.projectcalico.org/podIPs:10.244.89.67/32 createdTime:2023-01-03T10:50:12.527335408+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:12.527335408+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 811d1f78-ed35-43fe-843b-34029d2be9be 0xc00397ca57 0xc00397ca58}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"811d1f78-ed35-43fe-843b-34029d2be9be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-03 01:50:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-03 01:50:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.89.67\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-btjxm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-btjxm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.13,PodIP:10.244.89.67,StartTime:2023-01-03 01:50:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-03 01:50:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://8352aec94e2e6d313f5df9ea21fb058cc4e653136852c6a1ba6631643868ada4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.89.67,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 01:50:20.689: INFO: Pod "webserver-deployment-845c8977d9-ztsbz" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-ztsbz webserver-deployment-845c8977d9- deployment-995  dac84318-a574-470b-a922-0135695ee56c 975502 0 2023-01-03 01:50:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:bd8c23bc82c018e8ce5a6ea55821b5b15039376701f13099088ce128bdfadbc9 cni.projectcalico.org/podIP:10.244.232.44/32 cni.projectcalico.org/podIPs:10.244.232.44/32 createdTime:2023-01-03T10:50:12.548000546+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:12.548000546+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 811d1f78-ed35-43fe-843b-34029d2be9be 0xc00397ccb7 0xc00397ccb8}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"811d1f78-ed35-43fe-843b-34029d2be9be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-03 01:50:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-03 01:50:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.232.44\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dgblv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dgblv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.7,PodIP:10.244.232.44,StartTime:2023-01-03 01:50:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-03 01:50:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://59c92eedc09da5980477796ec6ca87ac5aa55bb0302ecfe0e9fde5bf31e6ee41,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.232.44,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan  3 01:50:20.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-995" for this suite. 01/03/23 01:50:20.695
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","completed":22,"skipped":350,"failed":0}
------------------------------
• [SLOW TEST] [8.287 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:50:12.41
    Jan  3 01:50:12.411: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename deployment 01/03/23 01:50:12.412
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:50:12.425
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:50:12.427
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Jan  3 01:50:12.429: INFO: Creating deployment "webserver-deployment"
    Jan  3 01:50:12.432: INFO: Waiting for observed generation 1
    Jan  3 01:50:14.450: INFO: Waiting for all required pods to come up
    Jan  3 01:50:14.453: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 01/03/23 01:50:14.453
    Jan  3 01:50:14.453: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-bbjcp" in namespace "deployment-995" to be "running"
    Jan  3 01:50:14.453: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-6jgxv" in namespace "deployment-995" to be "running"
    Jan  3 01:50:14.453: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-r56wc" in namespace "deployment-995" to be "running"
    Jan  3 01:50:14.455: INFO: Pod "webserver-deployment-845c8977d9-r56wc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.643532ms
    Jan  3 01:50:14.455: INFO: Pod "webserver-deployment-845c8977d9-6jgxv": Phase="Pending", Reason="", readiness=false. Elapsed: 1.786354ms
    Jan  3 01:50:14.455: INFO: Pod "webserver-deployment-845c8977d9-bbjcp": Phase="Pending", Reason="", readiness=false. Elapsed: 1.913868ms
    Jan  3 01:50:16.458: INFO: Pod "webserver-deployment-845c8977d9-6jgxv": Phase="Running", Reason="", readiness=true. Elapsed: 2.004037156s
    Jan  3 01:50:16.458: INFO: Pod "webserver-deployment-845c8977d9-6jgxv" satisfied condition "running"
    Jan  3 01:50:16.458: INFO: Pod "webserver-deployment-845c8977d9-r56wc": Phase="Running", Reason="", readiness=true. Elapsed: 2.004012676s
    Jan  3 01:50:16.458: INFO: Pod "webserver-deployment-845c8977d9-r56wc" satisfied condition "running"
    Jan  3 01:50:16.458: INFO: Pod "webserver-deployment-845c8977d9-bbjcp": Phase="Running", Reason="", readiness=true. Elapsed: 2.004049423s
    Jan  3 01:50:16.458: INFO: Pod "webserver-deployment-845c8977d9-bbjcp" satisfied condition "running"
    Jan  3 01:50:16.458: INFO: Waiting for deployment "webserver-deployment" to complete
    Jan  3 01:50:16.461: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Jan  3 01:50:16.465: INFO: Updating deployment webserver-deployment
    Jan  3 01:50:16.465: INFO: Waiting for observed generation 2
    Jan  3 01:50:18.514: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Jan  3 01:50:18.516: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Jan  3 01:50:18.518: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jan  3 01:50:18.523: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Jan  3 01:50:18.523: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Jan  3 01:50:18.524: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jan  3 01:50:18.527: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Jan  3 01:50:18.527: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Jan  3 01:50:18.531: INFO: Updating deployment webserver-deployment
    Jan  3 01:50:18.531: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Jan  3 01:50:18.579: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Jan  3 01:50:18.597: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan  3 01:50:20.658: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-995  226113e7-e926-422e-a315-6dd8dcbafc1a 975725 3 2023-01-03 01:50:12 +0000 UTC <nil> <nil> map[name:httpd] map[createdTime:2023-01-03T10:50:12.430486675+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:2 updatedTime:2023-01-03T10:50:12.430486675+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update apps/v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00007b748 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-03 01:50:18 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-01-03 01:50:18 +0000 UTC,LastTransitionTime:2023-01-03 01:50:12 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Jan  3 01:50:20.669: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-995  dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c 975723 3 2023-01-03 01:50:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[createdTime:2023-01-03T10:50:12.430486675+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2 updatedTime:2023-01-03T10:50:12.430486675+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [{apps/v1 Deployment webserver-deployment 226113e7-e926-422e-a315-6dd8dcbafc1a 0xc003bf045e 0xc003bf045f}] [] [{kube-controller-manager Update apps/v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:createdTime":{},"f:creator":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{},"f:updatedTime":{},"f:updater":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"226113e7-e926-422e-a315-6dd8dcbafc1a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003bf0508 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan  3 01:50:20.669: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Jan  3 01:50:20.669: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-995  811d1f78-ed35-43fe-843b-34029d2be9be 975831 3 2023-01-03 01:50:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[createdTime:2023-01-03T10:50:12.430486675+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T10:50:12.430486675+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [{apps/v1 Deployment webserver-deployment 226113e7-e926-422e-a315-6dd8dcbafc1a 0xc003bf058e 0xc003bf058f}] [] [{kube-controller-manager Update apps/v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:createdTime":{},"f:creator":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{},"f:updatedTime":{},"f:updater":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"226113e7-e926-422e-a315-6dd8dcbafc1a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-03 01:50:20 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003bf0628 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:9,AvailableReplicas:9,Conditions:[]ReplicaSetCondition{},},}
    Jan  3 01:50:20.685: INFO: Pod "webserver-deployment-69b7448995-2dckt" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-2dckt webserver-deployment-69b7448995- deployment-995  68bec98d-ae6d-486f-9e9f-20c543b85719 975782 0 2023-01-03 01:50:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:e1f2f8064d08518f8c590f3a3cf6aebb412d0af2105759b468233d4ba8c953bc cni.projectcalico.org/podIP:10.244.89.121/32 cni.projectcalico.org/podIPs:10.244.89.121/32 createdTime:2023-01-03T10:50:18.572051284+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:18.572051284+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c 0xc000dbc6e7 0xc000dbc6e8}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-03 01:50:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9ws2n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9ws2n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.13,PodIP:,StartTime:2023-01-03 01:50:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 01:50:20.685: INFO: Pod "webserver-deployment-69b7448995-bm4h8" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-bm4h8 webserver-deployment-69b7448995- deployment-995  9225acee-c288-451a-b31a-73361f7e4d4a 975604 0 2023-01-03 01:50:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:54b356d9922e1d5b6a5c9143b5d80f8ce8140211789cc7899cf8e9fafc17d25f cni.projectcalico.org/podIP:10.244.89.66/32 cni.projectcalico.org/podIPs:10.244.89.66/32 createdTime:2023-01-03T10:50:16.519710551+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:16.519710551+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c 0xc000dbc97e 0xc000dbc97f}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-03 01:50:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-03 01:50:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5pjxn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5pjxn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.13,PodIP:,StartTime:2023-01-03 01:50:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 01:50:20.685: INFO: Pod "webserver-deployment-69b7448995-c9n8t" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-c9n8t webserver-deployment-69b7448995- deployment-995  4500c4d5-d7dc-43b4-bf35-eb7ae7d1e4ed 975786 0 2023-01-03 01:50:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:f401c62eb1ccf72fd3fa7a1ceeb40a6221334dbe9fe3cb4c900946cc7cba1b57 cni.projectcalico.org/podIP:10.244.246.105/32 cni.projectcalico.org/podIPs:10.244.246.105/32 createdTime:2023-01-03T10:50:18.582162377+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:18.582162377+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c 0xc000dbcc4e 0xc000dbcc4f}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-03 01:50:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ntdbs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ntdbs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-master,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.5,PodIP:,StartTime:2023-01-03 01:50:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 01:50:20.685: INFO: Pod "webserver-deployment-69b7448995-krbgn" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-krbgn webserver-deployment-69b7448995- deployment-995  5dfb8e35-c88b-479b-9964-c0d53b264c9f 975813 0 2023-01-03 01:50:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:d611f71833c8869bf3a0db9242e592ce032926dab9c91c4ef36db27ba14afc11 cni.projectcalico.org/podIP:10.244.246.107/32 cni.projectcalico.org/podIPs:10.244.246.107/32 createdTime:2023-01-03T10:50:18.594399868+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:18.594399868+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c 0xc000dbda1e 0xc000dbda1f}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-03 01:50:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tkbkh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tkbkh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-master,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.5,PodIP:,StartTime:2023-01-03 01:50:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 01:50:20.685: INFO: Pod "webserver-deployment-69b7448995-qcg65" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-qcg65 webserver-deployment-69b7448995- deployment-995  32db6e48-55cc-4bcb-8936-fbaedac7d61d 975600 0 2023-01-03 01:50:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:2afbce58b21639496523ca6060f701aef988c727ef7ff5e9f59fde72df71f396 cni.projectcalico.org/podIP:10.244.232.52/32 cni.projectcalico.org/podIPs:10.244.232.52/32 createdTime:2023-01-03T10:50:16.519372652+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:16.519372652+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c 0xc0073d423e 0xc0073d423f}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-03 01:50:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-03 01:50:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hmh9n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hmh9n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.7,PodIP:,StartTime:2023-01-03 01:50:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 01:50:20.686: INFO: Pod "webserver-deployment-69b7448995-qcj8d" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-qcj8d webserver-deployment-69b7448995- deployment-995  ef7b8cba-cb21-4737-86e4-81c8b44824af 975820 0 2023-01-03 01:50:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:f720b25d5ff4163f2117ad5d10c7ee2b0cbad60595e29f045593cc39f159e9cf cni.projectcalico.org/podIP:10.244.89.89/32 cni.projectcalico.org/podIPs:10.244.89.89/32 createdTime:2023-01-03T10:50:18.59643788+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:18.59643788+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c 0xc0073d460e 0xc0073d460f}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-03 01:50:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2qz5h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2qz5h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.13,PodIP:,StartTime:2023-01-03 01:50:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 01:50:20.686: INFO: Pod "webserver-deployment-69b7448995-qrnf8" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-qrnf8 webserver-deployment-69b7448995- deployment-995  4691d76a-86a1-4fa1-8379-33a6cfe71248 975616 0 2023-01-03 01:50:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:ce179a3be625cf0ce784b2fac78fd0a3d83c3a5a528bd9c914fabc06e6934e18 cni.projectcalico.org/podIP:10.244.246.103/32 cni.projectcalico.org/podIPs:10.244.246.103/32 createdTime:2023-01-03T10:50:16.589361061+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:16.589361061+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c 0xc0073d484e 0xc0073d484f}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-03 01:50:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-03 01:50:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6qp9p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6qp9p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-master,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.5,PodIP:,StartTime:2023-01-03 01:50:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 01:50:20.686: INFO: Pod "webserver-deployment-69b7448995-shjl8" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-shjl8 webserver-deployment-69b7448995- deployment-995  c28298f9-bb33-4709-9800-bc09976c46a7 975611 0 2023-01-03 01:50:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:8b873762bb83245599fa766ba38e10fce7922edf3b432454a46ce36852662a46 cni.projectcalico.org/podIP:10.244.232.50/32 cni.projectcalico.org/podIPs:10.244.232.50/32 createdTime:2023-01-03T10:50:16.600701804+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:16.600701804+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c 0xc0073d4a9e 0xc0073d4a9f}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-03 01:50:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-03 01:50:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nmsc2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nmsc2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.7,PodIP:,StartTime:2023-01-03 01:50:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 01:50:20.686: INFO: Pod "webserver-deployment-69b7448995-shzch" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-shzch webserver-deployment-69b7448995- deployment-995  7bc2bb6c-ba2a-46bf-82ed-9d8c8dd308e4 975719 0 2023-01-03 01:50:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[createdTime:2023-01-03T10:50:18.639612728+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:18.639612728+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c 0xc0073d4cce 0xc0073d4ccf}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bjvzr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bjvzr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-master,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 01:50:20.686: INFO: Pod "webserver-deployment-69b7448995-svk27" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-svk27 webserver-deployment-69b7448995- deployment-995  e626c3a5-dba1-4292-a4cc-e33d367b0b61 975610 0 2023-01-03 01:50:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:906372a3c00b12c587062b527a85a84420c185b0731733a69b0cba28943fd1b2 cni.projectcalico.org/podIP:10.244.246.102/32 cni.projectcalico.org/podIPs:10.244.246.102/32 createdTime:2023-01-03T10:50:16.514420229+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:16.514420229+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c 0xc0073d4e6e 0xc0073d4e6f}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-03 01:50:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-03 01:50:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ch6js,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ch6js,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-master,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.5,PodIP:,StartTime:2023-01-03 01:50:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 01:50:20.686: INFO: Pod "webserver-deployment-69b7448995-tbpbn" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-tbpbn webserver-deployment-69b7448995- deployment-995  76004de2-b2a7-4d2f-8049-75cd7ba0eb63 975767 0 2023-01-03 01:50:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:0d9b563e0a510625266c4170f8644cab5f70650d12082f0012dba73b93efab51 cni.projectcalico.org/podIP:10.244.232.27/32 cni.projectcalico.org/podIPs:10.244.232.27/32 createdTime:2023-01-03T10:50:18.59542773+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:18.59542773+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c 0xc0073d50be 0xc0073d50bf}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-03 01:50:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nmw69,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nmw69,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.7,PodIP:,StartTime:2023-01-03 01:50:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 01:50:20.686: INFO: Pod "webserver-deployment-69b7448995-xpjch" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-xpjch webserver-deployment-69b7448995- deployment-995  c8fdd708-da8b-4c39-b296-559358a169ad 975801 0 2023-01-03 01:50:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:17a5168aa7e9f9498f22c2b4cbeb05322a4499b88be69dc330b9dc3a6fa1d29e cni.projectcalico.org/podIP:10.244.232.3/32 cni.projectcalico.org/podIPs:10.244.232.3/32 createdTime:2023-01-03T10:50:18.58245745+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:18.58245745+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c 0xc0073d530e 0xc0073d530f}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-03 01:50:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pzn97,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pzn97,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.7,PodIP:,StartTime:2023-01-03 01:50:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 01:50:20.686: INFO: Pod "webserver-deployment-69b7448995-zgfng" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-zgfng webserver-deployment-69b7448995- deployment-995  a8681391-9706-4245-8039-b1e351b83271 975773 0 2023-01-03 01:50:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:915d01ac7fce125c0cdb45d64def07f7aaab66752ed87327bbe30725a6e21f66 cni.projectcalico.org/podIP:10.244.89.112/32 cni.projectcalico.org/podIPs:10.244.89.112/32 createdTime:2023-01-03T10:50:18.594007671+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:18.594007671+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c 0xc0073d555e 0xc0073d555f}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dfebbaa0-d0b8-4a7a-be7c-ae51de843a2c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-03 01:50:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8tq6k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8tq6k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.13,PodIP:,StartTime:2023-01-03 01:50:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 01:50:20.687: INFO: Pod "webserver-deployment-845c8977d9-4dzzs" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-4dzzs webserver-deployment-845c8977d9- deployment-995  905a8688-cb7b-4346-b967-371e300f1329 975749 0 2023-01-03 01:50:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[createdTime:2023-01-03T10:50:18.637820277+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:18.637820277+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 811d1f78-ed35-43fe-843b-34029d2be9be 0xc0073d578e 0xc0073d578f}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"811d1f78-ed35-43fe-843b-34029d2be9be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8t6q6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8t6q6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.13,PodIP:,StartTime:2023-01-03 01:50:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 01:50:20.687: INFO: Pod "webserver-deployment-845c8977d9-5lcd4" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-5lcd4 webserver-deployment-845c8977d9- deployment-995  ed580428-1262-493e-bd8d-7551d495b927 975509 0 2023-01-03 01:50:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:15b0b332a385a65fa97550717841710fd611fd2148c8936f3856addd773be6eb cni.projectcalico.org/podIP:10.244.89.87/32 cni.projectcalico.org/podIPs:10.244.89.87/32 createdTime:2023-01-03T10:50:12.477356082+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:12.477356082+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 811d1f78-ed35-43fe-843b-34029d2be9be 0xc0073d59b7 0xc0073d59b8}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"811d1f78-ed35-43fe-843b-34029d2be9be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-03 01:50:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-03 01:50:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.89.87\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-psn5j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-psn5j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.13,PodIP:10.244.89.87,StartTime:2023-01-03 01:50:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-03 01:50:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://1822882188f355ced71e3c5ffc03d574a0322189c914f3c9aae0ccc94c1554ed,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.89.87,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 01:50:20.687: INFO: Pod "webserver-deployment-845c8977d9-7hfkg" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-7hfkg webserver-deployment-845c8977d9- deployment-995  765561e3-3827-4f5d-9ef9-a4a8c03a8efe 975514 0 2023-01-03 01:50:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:4bf66a29f1bd1bf8d6556b7ae317049b5a20e506e3a78bf070bc7808b7dcf2b7 cni.projectcalico.org/podIP:10.244.89.124/32 cni.projectcalico.org/podIPs:10.244.89.124/32 createdTime:2023-01-03T10:50:12.548789715+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:12.548789715+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 811d1f78-ed35-43fe-843b-34029d2be9be 0xc0073d5c27 0xc0073d5c28}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"811d1f78-ed35-43fe-843b-34029d2be9be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-03 01:50:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-03 01:50:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.89.124\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q78j8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q78j8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.13,PodIP:10.244.89.124,StartTime:2023-01-03 01:50:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-03 01:50:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://926244a0fa41a2456ff6b936b1f163224b1ea21f8b5406c5391ec631826982b7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.89.124,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 01:50:20.687: INFO: Pod "webserver-deployment-845c8977d9-7hmmx" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-7hmmx webserver-deployment-845c8977d9- deployment-995  09bbab4f-57ec-4203-ad11-772eb875c0fe 975510 0 2023-01-03 01:50:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:2f35dc2accc96f4193755e2ad40d97dcb8cfb54c2e64d34ff566bc9bf6dcc049 cni.projectcalico.org/podIP:10.244.232.48/32 cni.projectcalico.org/podIPs:10.244.232.48/32 createdTime:2023-01-03T10:50:12.52686285+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:12.52686285+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 811d1f78-ed35-43fe-843b-34029d2be9be 0xc0073d5e97 0xc0073d5e98}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"811d1f78-ed35-43fe-843b-34029d2be9be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-03 01:50:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-03 01:50:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.232.48\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6mh6x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6mh6x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.7,PodIP:10.244.232.48,StartTime:2023-01-03 01:50:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-03 01:50:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://8c4fec9c113902785c81d2a2855f16102d136b25e0df7918a34289cf05327bf7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.232.48,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 01:50:20.687: INFO: Pod "webserver-deployment-845c8977d9-7rx4z" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-7rx4z webserver-deployment-845c8977d9- deployment-995  08c61f27-7cab-4700-81f2-292665771107 975498 0 2023-01-03 01:50:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:b742c570556004ac65a73a43087209d2a1e3f0bb123546f7893ba9c8fae34475 cni.projectcalico.org/podIP:10.244.232.49/32 cni.projectcalico.org/podIPs:10.244.232.49/32 createdTime:2023-01-03T10:50:12.476871995+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:12.476871995+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 811d1f78-ed35-43fe-843b-34029d2be9be 0xc0028162ee 0xc0028162ef}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"811d1f78-ed35-43fe-843b-34029d2be9be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-03 01:50:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-03 01:50:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.232.49\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kwpld,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kwpld,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.7,PodIP:10.244.232.49,StartTime:2023-01-03 01:50:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-03 01:50:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://7ac867fd421b4230dae0c3873368c21bf13a4a33636c578ba1e3886bc9f1d333,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.232.49,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 01:50:20.687: INFO: Pod "webserver-deployment-845c8977d9-7xq7g" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-7xq7g webserver-deployment-845c8977d9- deployment-995  06625b68-9818-4168-9d1b-ec0306f55aaa 975700 0 2023-01-03 01:50:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[createdTime:2023-01-03T10:50:18.637182405+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:18.637182405+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 811d1f78-ed35-43fe-843b-34029d2be9be 0xc00281660e 0xc00281660f}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"811d1f78-ed35-43fe-843b-34029d2be9be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n4lkq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n4lkq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-master,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 01:50:20.687: INFO: Pod "webserver-deployment-845c8977d9-8mjt8" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-8mjt8 webserver-deployment-845c8977d9- deployment-995  1427d114-ea04-4d95-9c3f-ab43cc7badaf 975809 0 2023-01-03 01:50:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:b4dd9fb3d6cf21c2e0707f24c3bd1f318dbb301bfc57a248deed77d35b767df5 cni.projectcalico.org/podIP:10.244.89.96/32 cni.projectcalico.org/podIPs:10.244.89.96/32 createdTime:2023-01-03T10:50:18.583920699+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:18.583920699+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 811d1f78-ed35-43fe-843b-34029d2be9be 0xc002816a3e 0xc002816a3f}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"811d1f78-ed35-43fe-843b-34029d2be9be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-03 01:50:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9pjkd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9pjkd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.13,PodIP:,StartTime:2023-01-03 01:50:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 01:50:20.687: INFO: Pod "webserver-deployment-845c8977d9-96fpp" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-96fpp webserver-deployment-845c8977d9- deployment-995  f1ee2de2-f320-43b1-9dbc-072e56fbd4d1 975823 0 2023-01-03 01:50:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:64e1ecd360448503d3c3abd405c9efe6c231d99a1983606cca9cac1c1e424deb cni.projectcalico.org/podIP:10.244.232.54/32 cni.projectcalico.org/podIPs:10.244.232.54/32 createdTime:2023-01-03T10:50:18.639248696+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:18.639248696+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 811d1f78-ed35-43fe-843b-34029d2be9be 0xc002816ef7 0xc002816ef8}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"811d1f78-ed35-43fe-843b-34029d2be9be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-03 01:50:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-03 01:50:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dmngs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dmngs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.7,PodIP:,StartTime:2023-01-03 01:50:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 01:50:20.688: INFO: Pod "webserver-deployment-845c8977d9-bkb5w" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-bkb5w webserver-deployment-845c8977d9- deployment-995  a14aabb6-2c41-45d8-8931-c09889fcc4c8 975830 0 2023-01-03 01:50:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:025b09dc2a43b503f95dbce2d8cb8e03a6d295d10cfe813425f3396fc33ff8dd cni.projectcalico.org/podIP:10.244.232.47/32 cni.projectcalico.org/podIPs:10.244.232.47/32 createdTime:2023-01-03T10:50:18.572780289+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:18.572780289+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 811d1f78-ed35-43fe-843b-34029d2be9be 0xc00281711e 0xc00281711f}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"811d1f78-ed35-43fe-843b-34029d2be9be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-03 01:50:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-03 01:50:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.232.47\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9kx7s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9kx7s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.7,PodIP:10.244.232.47,StartTime:2023-01-03 01:50:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-03 01:50:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://c2770c2fb28ca3bee7789f613ebabbe809b70477b6ec6d0af68d18c4377dd0ee,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.232.47,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 01:50:20.688: INFO: Pod "webserver-deployment-845c8977d9-bmnjg" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-bmnjg webserver-deployment-845c8977d9- deployment-995  3d04a4dd-1b0b-42e5-9052-ff35c6387fc4 975764 0 2023-01-03 01:50:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:7f6e877e676eaabecb28ebc75cccd8c3f43b7e15e2ac49a5602a3acfe51775be cni.projectcalico.org/podIP:10.244.246.104/32 cni.projectcalico.org/podIPs:10.244.246.104/32 createdTime:2023-01-03T10:50:18.572195123+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:18.572195123+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 811d1f78-ed35-43fe-843b-34029d2be9be 0xc00281734e 0xc00281734f}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"811d1f78-ed35-43fe-843b-34029d2be9be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-03 01:50:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sjt7w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sjt7w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-master,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.5,PodIP:,StartTime:2023-01-03 01:50:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 01:50:20.688: INFO: Pod "webserver-deployment-845c8977d9-nmqnx" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-nmqnx webserver-deployment-845c8977d9- deployment-995  4413e76b-fa7f-46fe-b025-2022235690e0 975789 0 2023-01-03 01:50:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:b8779485a32350a92de6958455f25ccb209790c2483c7dee05f4ba0672b4ba8d cni.projectcalico.org/podIP:10.244.232.20/32 cni.projectcalico.org/podIPs:10.244.232.20/32 createdTime:2023-01-03T10:50:18.583549803+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:18.583549803+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 811d1f78-ed35-43fe-843b-34029d2be9be 0xc00281757e 0xc00281757f}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"811d1f78-ed35-43fe-843b-34029d2be9be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-03 01:50:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5qcx8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5qcx8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.7,PodIP:,StartTime:2023-01-03 01:50:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 01:50:20.688: INFO: Pod "webserver-deployment-845c8977d9-pg28p" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-pg28p webserver-deployment-845c8977d9- deployment-995  0110efdd-7217-4fce-98aa-af5f5692f6a8 975825 0 2023-01-03 01:50:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:0f52021e9f1fbfa5b5b53b6296de2780c3a551fdfa3c8f7ef3ca19900bdce3b7 cni.projectcalico.org/podIP:10.244.246.108/32 cni.projectcalico.org/podIPs:10.244.246.108/32 createdTime:2023-01-03T10:50:18.583134768+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:18.583134768+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 811d1f78-ed35-43fe-843b-34029d2be9be 0xc00281778e 0xc00281778f}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"811d1f78-ed35-43fe-843b-34029d2be9be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-03 01:50:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p6dcj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p6dcj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-master,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.5,PodIP:,StartTime:2023-01-03 01:50:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 01:50:20.688: INFO: Pod "webserver-deployment-845c8977d9-qxs22" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-qxs22 webserver-deployment-845c8977d9- deployment-995  793429cb-30d9-4aeb-ba2a-d61f2024407e 975802 0 2023-01-03 01:50:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:a7e9d11c082f25a619ec4f1d12f89592a87f9a65b623acd843932758a1855265 cni.projectcalico.org/podIP:10.244.246.106/32 cni.projectcalico.org/podIPs:10.244.246.106/32 createdTime:2023-01-03T10:50:18.566071812+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:18.566071812+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 811d1f78-ed35-43fe-843b-34029d2be9be 0xc00281799e 0xc00281799f}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"811d1f78-ed35-43fe-843b-34029d2be9be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-03 01:50:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ths8v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ths8v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-master,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.5,PodIP:,StartTime:2023-01-03 01:50:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 01:50:20.688: INFO: Pod "webserver-deployment-845c8977d9-r2cvg" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-r2cvg webserver-deployment-845c8977d9- deployment-995  83881765-b5fa-45df-bda9-cca30f1daf3b 975715 0 2023-01-03 01:50:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[createdTime:2023-01-03T10:50:18.596554212+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:18.596554212+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 811d1f78-ed35-43fe-843b-34029d2be9be 0xc002817bae 0xc002817baf}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"811d1f78-ed35-43fe-843b-34029d2be9be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vqbqv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vqbqv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.7,PodIP:,StartTime:2023-01-03 01:50:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 01:50:20.688: INFO: Pod "webserver-deployment-845c8977d9-r56wc" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-r56wc webserver-deployment-845c8977d9- deployment-995  b5d653d1-af08-4acd-8b36-5444050caccf 975526 0 2023-01-03 01:50:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:04a36eff1f9f1714a652ac9c341f80f6e470e45de49ea9b68c6846dfbe57e892 cni.projectcalico.org/podIP:10.244.246.101/32 cni.projectcalico.org/podIPs:10.244.246.101/32 createdTime:2023-01-03T10:50:12.54839738+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:12.54839738+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 811d1f78-ed35-43fe-843b-34029d2be9be 0xc002817d9e 0xc002817d9f}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"811d1f78-ed35-43fe-843b-34029d2be9be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-03 01:50:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-03 01:50:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.246.101\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4pfhs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4pfhs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-master,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.5,PodIP:10.244.246.101,StartTime:2023-01-03 01:50:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-03 01:50:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://ccac5aec29ed602390f6ea134f84ae9d68f8d793e76895ff88dea842f6504448,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.246.101,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 01:50:20.688: INFO: Pod "webserver-deployment-845c8977d9-r65tn" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-r65tn webserver-deployment-845c8977d9- deployment-995  5ac2f094-da40-4706-8a68-5c901122d062 975748 0 2023-01-03 01:50:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[createdTime:2023-01-03T10:50:18.594854505+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:18.594854505+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 811d1f78-ed35-43fe-843b-34029d2be9be 0xc002817fce 0xc002817fcf}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"811d1f78-ed35-43fe-843b-34029d2be9be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dkvwb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dkvwb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-master,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.5,PodIP:,StartTime:2023-01-03 01:50:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 01:50:20.689: INFO: Pod "webserver-deployment-845c8977d9-w585r" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-w585r webserver-deployment-845c8977d9- deployment-995  81d7c415-6bcc-4277-a3bc-140d6ac58388 975472 0 2023-01-03 01:50:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:80490eb37833a794c7a48280f0a615cc00194ba177e8a98bdbdad7c0d71e29b0 cni.projectcalico.org/podIP:10.244.246.98/32 cni.projectcalico.org/podIPs:10.244.246.98/32 createdTime:2023-01-03T10:50:12.47166505+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:12.47166505+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 811d1f78-ed35-43fe-843b-34029d2be9be 0xc00397c29e 0xc00397c29f}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"811d1f78-ed35-43fe-843b-34029d2be9be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-03 01:50:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-03 01:50:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.246.98\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-njhvn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-njhvn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-master,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.5,PodIP:10.244.246.98,StartTime:2023-01-03 01:50:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-03 01:50:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://7e806235446ea357493d76d7eb27d050eb1ae1d5bbde04ae15276cfaffa3ea40,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.246.98,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 01:50:20.689: INFO: Pod "webserver-deployment-845c8977d9-wcqj5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-wcqj5 webserver-deployment-845c8977d9- deployment-995  7602afb6-c49c-4817-a0fd-a992feaaf987 975800 0 2023-01-03 01:50:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:a477032fec5bbc540f030ac68e85cd13812e54e5a0977e61f2b714377bf7e298 cni.projectcalico.org/podIP:10.244.89.118/32 cni.projectcalico.org/podIPs:10.244.89.118/32 createdTime:2023-01-03T10:50:18.582755856+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:18.582755856+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 811d1f78-ed35-43fe-843b-34029d2be9be 0xc00397c80e 0xc00397c80f}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"811d1f78-ed35-43fe-843b-34029d2be9be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-03 01:50:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-03 01:50:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-95h8w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-95h8w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.13,PodIP:,StartTime:2023-01-03 01:50:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 01:50:20.689: INFO: Pod "webserver-deployment-845c8977d9-wkqzk" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-wkqzk webserver-deployment-845c8977d9- deployment-995  a16f2833-3796-400c-a3b3-097a89e916bc 975518 0 2023-01-03 01:50:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:06db7e879f1277d99353972ed1413ef09dea08b336c2d8d15749f676653b4e12 cni.projectcalico.org/podIP:10.244.89.67/32 cni.projectcalico.org/podIPs:10.244.89.67/32 createdTime:2023-01-03T10:50:12.527335408+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:12.527335408+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 811d1f78-ed35-43fe-843b-34029d2be9be 0xc00397ca57 0xc00397ca58}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"811d1f78-ed35-43fe-843b-34029d2be9be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-03 01:50:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-03 01:50:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.89.67\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-btjxm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-btjxm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.13,PodIP:10.244.89.67,StartTime:2023-01-03 01:50:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-03 01:50:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://8352aec94e2e6d313f5df9ea21fb058cc4e653136852c6a1ba6631643868ada4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.89.67,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 01:50:20.689: INFO: Pod "webserver-deployment-845c8977d9-ztsbz" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-ztsbz webserver-deployment-845c8977d9- deployment-995  dac84318-a574-470b-a922-0135695ee56c 975502 0 2023-01-03 01:50:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:bd8c23bc82c018e8ce5a6ea55821b5b15039376701f13099088ce128bdfadbc9 cni.projectcalico.org/podIP:10.244.232.44/32 cni.projectcalico.org/podIPs:10.244.232.44/32 createdTime:2023-01-03T10:50:12.548000546+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:50:12.548000546+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 811d1f78-ed35-43fe-843b-34029d2be9be 0xc00397ccb7 0xc00397ccb8}] [] [{kube-controller-manager Update v1 2023-01-03 01:50:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"811d1f78-ed35-43fe-843b-34029d2be9be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-03 01:50:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-03 01:50:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.232.44\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dgblv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dgblv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:50:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.7,PodIP:10.244.232.44,StartTime:2023-01-03 01:50:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-03 01:50:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://59c92eedc09da5980477796ec6ca87ac5aa55bb0302ecfe0e9fde5bf31e6ee41,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.232.44,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan  3 01:50:20.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-995" for this suite. 01/03/23 01:50:20.695
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:50:20.699
Jan  3 01:50:20.699: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename secrets 01/03/23 01:50:20.7
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:50:20.722
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:50:20.724
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
STEP: creating secret secrets-3218/secret-test-4aaf8693-39b4-4443-a74c-f3f55069c5b6 01/03/23 01:50:20.726
STEP: Creating a pod to test consume secrets 01/03/23 01:50:20.743
Jan  3 01:50:20.796: INFO: Waiting up to 5m0s for pod "pod-configmaps-2fed5805-d780-4d8c-a2ef-c36c978d3d0c" in namespace "secrets-3218" to be "Succeeded or Failed"
Jan  3 01:50:20.826: INFO: Pod "pod-configmaps-2fed5805-d780-4d8c-a2ef-c36c978d3d0c": Phase="Pending", Reason="", readiness=false. Elapsed: 29.849409ms
Jan  3 01:50:22.829: INFO: Pod "pod-configmaps-2fed5805-d780-4d8c-a2ef-c36c978d3d0c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033156138s
Jan  3 01:50:24.828: INFO: Pod "pod-configmaps-2fed5805-d780-4d8c-a2ef-c36c978d3d0c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032399188s
Jan  3 01:50:26.832: INFO: Pod "pod-configmaps-2fed5805-d780-4d8c-a2ef-c36c978d3d0c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.036392836s
STEP: Saw pod success 01/03/23 01:50:26.832
Jan  3 01:50:26.832: INFO: Pod "pod-configmaps-2fed5805-d780-4d8c-a2ef-c36c978d3d0c" satisfied condition "Succeeded or Failed"
Jan  3 01:50:26.838: INFO: Trying to get logs from node cncf-master pod pod-configmaps-2fed5805-d780-4d8c-a2ef-c36c978d3d0c container env-test: <nil>
STEP: delete the pod 01/03/23 01:50:26.853
Jan  3 01:50:26.869: INFO: Waiting for pod pod-configmaps-2fed5805-d780-4d8c-a2ef-c36c978d3d0c to disappear
Jan  3 01:50:26.901: INFO: Pod pod-configmaps-2fed5805-d780-4d8c-a2ef-c36c978d3d0c no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jan  3 01:50:26.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3218" for this suite. 01/03/23 01:50:26.907
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","completed":23,"skipped":352,"failed":0}
------------------------------
• [SLOW TEST] [6.214 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:50:20.699
    Jan  3 01:50:20.699: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename secrets 01/03/23 01:50:20.7
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:50:20.722
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:50:20.724
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:94
    STEP: creating secret secrets-3218/secret-test-4aaf8693-39b4-4443-a74c-f3f55069c5b6 01/03/23 01:50:20.726
    STEP: Creating a pod to test consume secrets 01/03/23 01:50:20.743
    Jan  3 01:50:20.796: INFO: Waiting up to 5m0s for pod "pod-configmaps-2fed5805-d780-4d8c-a2ef-c36c978d3d0c" in namespace "secrets-3218" to be "Succeeded or Failed"
    Jan  3 01:50:20.826: INFO: Pod "pod-configmaps-2fed5805-d780-4d8c-a2ef-c36c978d3d0c": Phase="Pending", Reason="", readiness=false. Elapsed: 29.849409ms
    Jan  3 01:50:22.829: INFO: Pod "pod-configmaps-2fed5805-d780-4d8c-a2ef-c36c978d3d0c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033156138s
    Jan  3 01:50:24.828: INFO: Pod "pod-configmaps-2fed5805-d780-4d8c-a2ef-c36c978d3d0c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032399188s
    Jan  3 01:50:26.832: INFO: Pod "pod-configmaps-2fed5805-d780-4d8c-a2ef-c36c978d3d0c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.036392836s
    STEP: Saw pod success 01/03/23 01:50:26.832
    Jan  3 01:50:26.832: INFO: Pod "pod-configmaps-2fed5805-d780-4d8c-a2ef-c36c978d3d0c" satisfied condition "Succeeded or Failed"
    Jan  3 01:50:26.838: INFO: Trying to get logs from node cncf-master pod pod-configmaps-2fed5805-d780-4d8c-a2ef-c36c978d3d0c container env-test: <nil>
    STEP: delete the pod 01/03/23 01:50:26.853
    Jan  3 01:50:26.869: INFO: Waiting for pod pod-configmaps-2fed5805-d780-4d8c-a2ef-c36c978d3d0c to disappear
    Jan  3 01:50:26.901: INFO: Pod pod-configmaps-2fed5805-d780-4d8c-a2ef-c36c978d3d0c no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jan  3 01:50:26.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-3218" for this suite. 01/03/23 01:50:26.907
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:50:26.914
Jan  3 01:50:26.914: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename emptydir 01/03/23 01:50:26.915
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:50:26.935
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:50:26.937
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
STEP: Creating a pod to test emptydir 0644 on node default medium 01/03/23 01:50:26.939
Jan  3 01:50:26.952: INFO: Waiting up to 5m0s for pod "pod-a6592e17-5417-4d6e-a6d8-ba443f4f18a7" in namespace "emptydir-1752" to be "Succeeded or Failed"
Jan  3 01:50:26.972: INFO: Pod "pod-a6592e17-5417-4d6e-a6d8-ba443f4f18a7": Phase="Pending", Reason="", readiness=false. Elapsed: 19.319834ms
Jan  3 01:50:29.001: INFO: Pod "pod-a6592e17-5417-4d6e-a6d8-ba443f4f18a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048740401s
Jan  3 01:50:30.974: INFO: Pod "pod-a6592e17-5417-4d6e-a6d8-ba443f4f18a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021958664s
STEP: Saw pod success 01/03/23 01:50:30.974
Jan  3 01:50:30.974: INFO: Pod "pod-a6592e17-5417-4d6e-a6d8-ba443f4f18a7" satisfied condition "Succeeded or Failed"
Jan  3 01:50:30.976: INFO: Trying to get logs from node cncf-master pod pod-a6592e17-5417-4d6e-a6d8-ba443f4f18a7 container test-container: <nil>
STEP: delete the pod 01/03/23 01:50:30.979
Jan  3 01:50:31.003: INFO: Waiting for pod pod-a6592e17-5417-4d6e-a6d8-ba443f4f18a7 to disappear
Jan  3 01:50:31.008: INFO: Pod pod-a6592e17-5417-4d6e-a6d8-ba443f4f18a7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan  3 01:50:31.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1752" for this suite. 01/03/23 01:50:31.011
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":24,"skipped":382,"failed":0}
------------------------------
• [4.101 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:50:26.914
    Jan  3 01:50:26.914: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename emptydir 01/03/23 01:50:26.915
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:50:26.935
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:50:26.937
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:166
    STEP: Creating a pod to test emptydir 0644 on node default medium 01/03/23 01:50:26.939
    Jan  3 01:50:26.952: INFO: Waiting up to 5m0s for pod "pod-a6592e17-5417-4d6e-a6d8-ba443f4f18a7" in namespace "emptydir-1752" to be "Succeeded or Failed"
    Jan  3 01:50:26.972: INFO: Pod "pod-a6592e17-5417-4d6e-a6d8-ba443f4f18a7": Phase="Pending", Reason="", readiness=false. Elapsed: 19.319834ms
    Jan  3 01:50:29.001: INFO: Pod "pod-a6592e17-5417-4d6e-a6d8-ba443f4f18a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048740401s
    Jan  3 01:50:30.974: INFO: Pod "pod-a6592e17-5417-4d6e-a6d8-ba443f4f18a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021958664s
    STEP: Saw pod success 01/03/23 01:50:30.974
    Jan  3 01:50:30.974: INFO: Pod "pod-a6592e17-5417-4d6e-a6d8-ba443f4f18a7" satisfied condition "Succeeded or Failed"
    Jan  3 01:50:30.976: INFO: Trying to get logs from node cncf-master pod pod-a6592e17-5417-4d6e-a6d8-ba443f4f18a7 container test-container: <nil>
    STEP: delete the pod 01/03/23 01:50:30.979
    Jan  3 01:50:31.003: INFO: Waiting for pod pod-a6592e17-5417-4d6e-a6d8-ba443f4f18a7 to disappear
    Jan  3 01:50:31.008: INFO: Pod pod-a6592e17-5417-4d6e-a6d8-ba443f4f18a7 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan  3 01:50:31.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1752" for this suite. 01/03/23 01:50:31.011
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:50:31.017
Jan  3 01:50:31.017: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename svcaccounts 01/03/23 01:50:31.018
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:50:31.035
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:50:31.037
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
Jan  3 01:50:31.042: INFO: Got root ca configmap in namespace "svcaccounts-1149"
Jan  3 01:50:31.045: INFO: Deleted root ca configmap in namespace "svcaccounts-1149"
STEP: waiting for a new root ca configmap created 01/03/23 01:50:31.545
Jan  3 01:50:31.547: INFO: Recreated root ca configmap in namespace "svcaccounts-1149"
Jan  3 01:50:31.549: INFO: Updated root ca configmap in namespace "svcaccounts-1149"
STEP: waiting for the root ca configmap reconciled 01/03/23 01:50:32.05
Jan  3 01:50:32.052: INFO: Reconciled root ca configmap in namespace "svcaccounts-1149"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan  3 01:50:32.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1149" for this suite. 01/03/23 01:50:32.055
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","completed":25,"skipped":417,"failed":0}
------------------------------
• [1.040 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:50:31.017
    Jan  3 01:50:31.017: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename svcaccounts 01/03/23 01:50:31.018
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:50:31.035
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:50:31.037
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:739
    Jan  3 01:50:31.042: INFO: Got root ca configmap in namespace "svcaccounts-1149"
    Jan  3 01:50:31.045: INFO: Deleted root ca configmap in namespace "svcaccounts-1149"
    STEP: waiting for a new root ca configmap created 01/03/23 01:50:31.545
    Jan  3 01:50:31.547: INFO: Recreated root ca configmap in namespace "svcaccounts-1149"
    Jan  3 01:50:31.549: INFO: Updated root ca configmap in namespace "svcaccounts-1149"
    STEP: waiting for the root ca configmap reconciled 01/03/23 01:50:32.05
    Jan  3 01:50:32.052: INFO: Reconciled root ca configmap in namespace "svcaccounts-1149"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan  3 01:50:32.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-1149" for this suite. 01/03/23 01:50:32.055
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:50:32.059
Jan  3 01:50:32.059: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename certificates 01/03/23 01:50:32.059
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:50:32.135
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:50:32.137
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 01/03/23 01:50:32.615
STEP: getting /apis/certificates.k8s.io 01/03/23 01:50:32.617
STEP: getting /apis/certificates.k8s.io/v1 01/03/23 01:50:32.618
STEP: creating 01/03/23 01:50:32.619
STEP: getting 01/03/23 01:50:32.667
STEP: listing 01/03/23 01:50:32.668
STEP: watching 01/03/23 01:50:32.67
Jan  3 01:50:32.670: INFO: starting watch
STEP: patching 01/03/23 01:50:32.671
STEP: updating 01/03/23 01:50:32.674
Jan  3 01:50:32.681: INFO: waiting for watch events with expected annotations
Jan  3 01:50:32.681: INFO: saw patched and updated annotations
STEP: getting /approval 01/03/23 01:50:32.681
STEP: patching /approval 01/03/23 01:50:32.689
STEP: updating /approval 01/03/23 01:50:32.692
STEP: getting /status 01/03/23 01:50:32.714
STEP: patching /status 01/03/23 01:50:32.733
STEP: updating /status 01/03/23 01:50:32.737
STEP: deleting 01/03/23 01:50:32.744
STEP: deleting a collection 01/03/23 01:50:32.807
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  3 01:50:32.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-6947" for this suite. 01/03/23 01:50:32.833
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","completed":26,"skipped":466,"failed":0}
------------------------------
• [0.777 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:50:32.059
    Jan  3 01:50:32.059: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename certificates 01/03/23 01:50:32.059
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:50:32.135
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:50:32.137
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 01/03/23 01:50:32.615
    STEP: getting /apis/certificates.k8s.io 01/03/23 01:50:32.617
    STEP: getting /apis/certificates.k8s.io/v1 01/03/23 01:50:32.618
    STEP: creating 01/03/23 01:50:32.619
    STEP: getting 01/03/23 01:50:32.667
    STEP: listing 01/03/23 01:50:32.668
    STEP: watching 01/03/23 01:50:32.67
    Jan  3 01:50:32.670: INFO: starting watch
    STEP: patching 01/03/23 01:50:32.671
    STEP: updating 01/03/23 01:50:32.674
    Jan  3 01:50:32.681: INFO: waiting for watch events with expected annotations
    Jan  3 01:50:32.681: INFO: saw patched and updated annotations
    STEP: getting /approval 01/03/23 01:50:32.681
    STEP: patching /approval 01/03/23 01:50:32.689
    STEP: updating /approval 01/03/23 01:50:32.692
    STEP: getting /status 01/03/23 01:50:32.714
    STEP: patching /status 01/03/23 01:50:32.733
    STEP: updating /status 01/03/23 01:50:32.737
    STEP: deleting 01/03/23 01:50:32.744
    STEP: deleting a collection 01/03/23 01:50:32.807
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  3 01:50:32.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "certificates-6947" for this suite. 01/03/23 01:50:32.833
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:50:32.837
Jan  3 01:50:32.837: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename container-probe 01/03/23 01:50:32.838
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:50:32.866
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:50:32.868
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
STEP: Creating pod busybox-37cea75e-20b5-4185-841f-b72780ccd73a in namespace container-probe-1101 01/03/23 01:50:32.87
Jan  3 01:50:32.875: INFO: Waiting up to 5m0s for pod "busybox-37cea75e-20b5-4185-841f-b72780ccd73a" in namespace "container-probe-1101" to be "not pending"
Jan  3 01:50:32.888: INFO: Pod "busybox-37cea75e-20b5-4185-841f-b72780ccd73a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.09089ms
Jan  3 01:50:34.890: INFO: Pod "busybox-37cea75e-20b5-4185-841f-b72780ccd73a": Phase="Running", Reason="", readiness=true. Elapsed: 2.014682577s
Jan  3 01:50:34.890: INFO: Pod "busybox-37cea75e-20b5-4185-841f-b72780ccd73a" satisfied condition "not pending"
Jan  3 01:50:34.890: INFO: Started pod busybox-37cea75e-20b5-4185-841f-b72780ccd73a in namespace container-probe-1101
STEP: checking the pod's current state and verifying that restartCount is present 01/03/23 01:50:34.89
Jan  3 01:50:34.892: INFO: Initial restart count of pod busybox-37cea75e-20b5-4185-841f-b72780ccd73a is 0
STEP: deleting the pod 01/03/23 01:54:35.421
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan  3 01:54:35.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1101" for this suite. 01/03/23 01:54:35.44
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":27,"skipped":514,"failed":0}
------------------------------
• [SLOW TEST] [242.608 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:50:32.837
    Jan  3 01:50:32.837: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename container-probe 01/03/23 01:50:32.838
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:50:32.866
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:50:32.868
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:148
    STEP: Creating pod busybox-37cea75e-20b5-4185-841f-b72780ccd73a in namespace container-probe-1101 01/03/23 01:50:32.87
    Jan  3 01:50:32.875: INFO: Waiting up to 5m0s for pod "busybox-37cea75e-20b5-4185-841f-b72780ccd73a" in namespace "container-probe-1101" to be "not pending"
    Jan  3 01:50:32.888: INFO: Pod "busybox-37cea75e-20b5-4185-841f-b72780ccd73a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.09089ms
    Jan  3 01:50:34.890: INFO: Pod "busybox-37cea75e-20b5-4185-841f-b72780ccd73a": Phase="Running", Reason="", readiness=true. Elapsed: 2.014682577s
    Jan  3 01:50:34.890: INFO: Pod "busybox-37cea75e-20b5-4185-841f-b72780ccd73a" satisfied condition "not pending"
    Jan  3 01:50:34.890: INFO: Started pod busybox-37cea75e-20b5-4185-841f-b72780ccd73a in namespace container-probe-1101
    STEP: checking the pod's current state and verifying that restartCount is present 01/03/23 01:50:34.89
    Jan  3 01:50:34.892: INFO: Initial restart count of pod busybox-37cea75e-20b5-4185-841f-b72780ccd73a is 0
    STEP: deleting the pod 01/03/23 01:54:35.421
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan  3 01:54:35.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-1101" for this suite. 01/03/23 01:54:35.44
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:54:35.446
Jan  3 01:54:35.446: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename job 01/03/23 01:54:35.447
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:54:35.456
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:54:35.458
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
STEP: Creating a suspended job 01/03/23 01:54:35.473
STEP: Patching the Job 01/03/23 01:54:35.477
STEP: Watching for Job to be patched 01/03/23 01:54:35.53
Jan  3 01:54:35.531: INFO: Event ADDED observed for Job e2e-fwjv5 in namespace job-7149 with labels: map[e2e-job-label:e2e-fwjv5] and annotations: map[batch.kubernetes.io/job-tracking: createdTime:2023-01-03T10:54:35.475205858+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T10:54:35.475205858+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount]
Jan  3 01:54:35.531: INFO: Event MODIFIED observed for Job e2e-fwjv5 in namespace job-7149 with labels: map[e2e-job-label:e2e-fwjv5] and annotations: map[batch.kubernetes.io/job-tracking: createdTime:2023-01-03T10:54:35.475205858+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T10:54:35.475205858+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount]
Jan  3 01:54:35.531: INFO: Event MODIFIED found for Job e2e-fwjv5 in namespace job-7149 with labels: map[e2e-fwjv5:patched e2e-job-label:e2e-fwjv5] and annotations: map[batch.kubernetes.io/job-tracking: createdTime:2023-01-03T10:54:35.475205858+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T10:54:35.475205858+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount]
STEP: Updating the job 01/03/23 01:54:35.532
STEP: Watching for Job to be updated 01/03/23 01:54:35.536
Jan  3 01:54:35.537: INFO: Event MODIFIED found for Job e2e-fwjv5 in namespace job-7149 with labels: map[e2e-fwjv5:patched e2e-job-label:e2e-fwjv5] and annotations: map[batch.kubernetes.io/job-tracking: createdTime:2023-01-03T10:54:35.475205858+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updated:true updatedTime:2023-01-03T10:54:35.475205858+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount]
Jan  3 01:54:35.537: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "createdTime":"2023-01-03T10:54:35.475205858+09:00", "creator":"system:serviceaccount:sonobuoy:sonobuoy-serviceaccount", "updated":"true", "updatedTime":"2023-01-03T10:54:35.475205858+09:00", "updater":"system:serviceaccount:sonobuoy:sonobuoy-serviceaccount"}
STEP: Listing all Jobs with LabelSelector 01/03/23 01:54:35.537
Jan  3 01:54:35.548: INFO: Job: e2e-fwjv5 as labels: map[e2e-fwjv5:patched e2e-job-label:e2e-fwjv5]
STEP: Waiting for job to complete 01/03/23 01:54:35.548
STEP: Delete a job collection with a labelselector 01/03/23 01:54:43.551
STEP: Watching for Job to be deleted 01/03/23 01:54:43.554
Jan  3 01:54:43.556: INFO: Event MODIFIED observed for Job e2e-fwjv5 in namespace job-7149 with labels: map[e2e-fwjv5:patched e2e-job-label:e2e-fwjv5] and annotations: map[batch.kubernetes.io/job-tracking: createdTime:2023-01-03T10:54:35.475205858+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updated:true updatedTime:2023-01-03T10:54:35.475205858+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount]
Jan  3 01:54:43.556: INFO: Event MODIFIED observed for Job e2e-fwjv5 in namespace job-7149 with labels: map[e2e-fwjv5:patched e2e-job-label:e2e-fwjv5] and annotations: map[batch.kubernetes.io/job-tracking: createdTime:2023-01-03T10:54:35.475205858+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updated:true updatedTime:2023-01-03T10:54:35.475205858+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount]
Jan  3 01:54:43.556: INFO: Event MODIFIED observed for Job e2e-fwjv5 in namespace job-7149 with labels: map[e2e-fwjv5:patched e2e-job-label:e2e-fwjv5] and annotations: map[batch.kubernetes.io/job-tracking: createdTime:2023-01-03T10:54:35.475205858+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updated:true updatedTime:2023-01-03T10:54:35.475205858+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount]
Jan  3 01:54:43.556: INFO: Event MODIFIED observed for Job e2e-fwjv5 in namespace job-7149 with labels: map[e2e-fwjv5:patched e2e-job-label:e2e-fwjv5] and annotations: map[batch.kubernetes.io/job-tracking: createdTime:2023-01-03T10:54:35.475205858+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updated:true updatedTime:2023-01-03T10:54:35.475205858+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount]
Jan  3 01:54:43.556: INFO: Event MODIFIED observed for Job e2e-fwjv5 in namespace job-7149 with labels: map[e2e-fwjv5:patched e2e-job-label:e2e-fwjv5] and annotations: map[batch.kubernetes.io/job-tracking: createdTime:2023-01-03T10:54:35.475205858+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updated:true updatedTime:2023-01-03T10:54:35.475205858+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount]
Jan  3 01:54:43.556: INFO: Event MODIFIED observed for Job e2e-fwjv5 in namespace job-7149 with labels: map[e2e-fwjv5:patched e2e-job-label:e2e-fwjv5] and annotations: map[batch.kubernetes.io/job-tracking: createdTime:2023-01-03T10:54:35.475205858+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updated:true updatedTime:2023-01-03T10:54:35.475205858+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount]
Jan  3 01:54:43.556: INFO: Event MODIFIED observed for Job e2e-fwjv5 in namespace job-7149 with labels: map[e2e-fwjv5:patched e2e-job-label:e2e-fwjv5] and annotations: map[batch.kubernetes.io/job-tracking: createdTime:2023-01-03T10:54:35.475205858+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updated:true updatedTime:2023-01-03T10:54:35.475205858+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount]
Jan  3 01:54:43.556: INFO: Event DELETED found for Job e2e-fwjv5 in namespace job-7149 with labels: map[e2e-fwjv5:patched e2e-job-label:e2e-fwjv5] and annotations: map[batch.kubernetes.io/job-tracking: createdTime:2023-01-03T10:54:35.475205858+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updated:true updatedTime:2023-01-03T10:54:35.475205858+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount]
STEP: Relist jobs to confirm deletion 01/03/23 01:54:43.556
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan  3 01:54:43.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7149" for this suite. 01/03/23 01:54:43.562
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Job should manage the lifecycle of a job [Conformance]","completed":28,"skipped":517,"failed":0}
------------------------------
• [SLOW TEST] [8.119 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:54:35.446
    Jan  3 01:54:35.446: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename job 01/03/23 01:54:35.447
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:54:35.456
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:54:35.458
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:531
    STEP: Creating a suspended job 01/03/23 01:54:35.473
    STEP: Patching the Job 01/03/23 01:54:35.477
    STEP: Watching for Job to be patched 01/03/23 01:54:35.53
    Jan  3 01:54:35.531: INFO: Event ADDED observed for Job e2e-fwjv5 in namespace job-7149 with labels: map[e2e-job-label:e2e-fwjv5] and annotations: map[batch.kubernetes.io/job-tracking: createdTime:2023-01-03T10:54:35.475205858+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T10:54:35.475205858+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount]
    Jan  3 01:54:35.531: INFO: Event MODIFIED observed for Job e2e-fwjv5 in namespace job-7149 with labels: map[e2e-job-label:e2e-fwjv5] and annotations: map[batch.kubernetes.io/job-tracking: createdTime:2023-01-03T10:54:35.475205858+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T10:54:35.475205858+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount]
    Jan  3 01:54:35.531: INFO: Event MODIFIED found for Job e2e-fwjv5 in namespace job-7149 with labels: map[e2e-fwjv5:patched e2e-job-label:e2e-fwjv5] and annotations: map[batch.kubernetes.io/job-tracking: createdTime:2023-01-03T10:54:35.475205858+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T10:54:35.475205858+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount]
    STEP: Updating the job 01/03/23 01:54:35.532
    STEP: Watching for Job to be updated 01/03/23 01:54:35.536
    Jan  3 01:54:35.537: INFO: Event MODIFIED found for Job e2e-fwjv5 in namespace job-7149 with labels: map[e2e-fwjv5:patched e2e-job-label:e2e-fwjv5] and annotations: map[batch.kubernetes.io/job-tracking: createdTime:2023-01-03T10:54:35.475205858+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updated:true updatedTime:2023-01-03T10:54:35.475205858+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount]
    Jan  3 01:54:35.537: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "createdTime":"2023-01-03T10:54:35.475205858+09:00", "creator":"system:serviceaccount:sonobuoy:sonobuoy-serviceaccount", "updated":"true", "updatedTime":"2023-01-03T10:54:35.475205858+09:00", "updater":"system:serviceaccount:sonobuoy:sonobuoy-serviceaccount"}
    STEP: Listing all Jobs with LabelSelector 01/03/23 01:54:35.537
    Jan  3 01:54:35.548: INFO: Job: e2e-fwjv5 as labels: map[e2e-fwjv5:patched e2e-job-label:e2e-fwjv5]
    STEP: Waiting for job to complete 01/03/23 01:54:35.548
    STEP: Delete a job collection with a labelselector 01/03/23 01:54:43.551
    STEP: Watching for Job to be deleted 01/03/23 01:54:43.554
    Jan  3 01:54:43.556: INFO: Event MODIFIED observed for Job e2e-fwjv5 in namespace job-7149 with labels: map[e2e-fwjv5:patched e2e-job-label:e2e-fwjv5] and annotations: map[batch.kubernetes.io/job-tracking: createdTime:2023-01-03T10:54:35.475205858+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updated:true updatedTime:2023-01-03T10:54:35.475205858+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount]
    Jan  3 01:54:43.556: INFO: Event MODIFIED observed for Job e2e-fwjv5 in namespace job-7149 with labels: map[e2e-fwjv5:patched e2e-job-label:e2e-fwjv5] and annotations: map[batch.kubernetes.io/job-tracking: createdTime:2023-01-03T10:54:35.475205858+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updated:true updatedTime:2023-01-03T10:54:35.475205858+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount]
    Jan  3 01:54:43.556: INFO: Event MODIFIED observed for Job e2e-fwjv5 in namespace job-7149 with labels: map[e2e-fwjv5:patched e2e-job-label:e2e-fwjv5] and annotations: map[batch.kubernetes.io/job-tracking: createdTime:2023-01-03T10:54:35.475205858+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updated:true updatedTime:2023-01-03T10:54:35.475205858+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount]
    Jan  3 01:54:43.556: INFO: Event MODIFIED observed for Job e2e-fwjv5 in namespace job-7149 with labels: map[e2e-fwjv5:patched e2e-job-label:e2e-fwjv5] and annotations: map[batch.kubernetes.io/job-tracking: createdTime:2023-01-03T10:54:35.475205858+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updated:true updatedTime:2023-01-03T10:54:35.475205858+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount]
    Jan  3 01:54:43.556: INFO: Event MODIFIED observed for Job e2e-fwjv5 in namespace job-7149 with labels: map[e2e-fwjv5:patched e2e-job-label:e2e-fwjv5] and annotations: map[batch.kubernetes.io/job-tracking: createdTime:2023-01-03T10:54:35.475205858+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updated:true updatedTime:2023-01-03T10:54:35.475205858+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount]
    Jan  3 01:54:43.556: INFO: Event MODIFIED observed for Job e2e-fwjv5 in namespace job-7149 with labels: map[e2e-fwjv5:patched e2e-job-label:e2e-fwjv5] and annotations: map[batch.kubernetes.io/job-tracking: createdTime:2023-01-03T10:54:35.475205858+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updated:true updatedTime:2023-01-03T10:54:35.475205858+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount]
    Jan  3 01:54:43.556: INFO: Event MODIFIED observed for Job e2e-fwjv5 in namespace job-7149 with labels: map[e2e-fwjv5:patched e2e-job-label:e2e-fwjv5] and annotations: map[batch.kubernetes.io/job-tracking: createdTime:2023-01-03T10:54:35.475205858+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updated:true updatedTime:2023-01-03T10:54:35.475205858+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount]
    Jan  3 01:54:43.556: INFO: Event DELETED found for Job e2e-fwjv5 in namespace job-7149 with labels: map[e2e-fwjv5:patched e2e-job-label:e2e-fwjv5] and annotations: map[batch.kubernetes.io/job-tracking: createdTime:2023-01-03T10:54:35.475205858+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updated:true updatedTime:2023-01-03T10:54:35.475205858+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount]
    STEP: Relist jobs to confirm deletion 01/03/23 01:54:43.556
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan  3 01:54:43.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-7149" for this suite. 01/03/23 01:54:43.562
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:54:43.566
Jan  3 01:54:43.566: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename watch 01/03/23 01:54:43.566
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:54:43.576
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:54:43.578
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 01/03/23 01:54:43.58
STEP: creating a new configmap 01/03/23 01:54:43.581
STEP: modifying the configmap once 01/03/23 01:54:43.593
STEP: changing the label value of the configmap 01/03/23 01:54:43.605
STEP: Expecting to observe a delete notification for the watched object 01/03/23 01:54:43.641
Jan  3 01:54:43.641: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9800  2a4edf67-c2b7-494c-bd97-0b83c3ef986c 977452 0 2023-01-03 01:54:43 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[createdTime:2023-01-03T10:54:43.582893078+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T10:54:43.582893078+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update v1 2023-01-03 01:54:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  3 01:54:43.641: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9800  2a4edf67-c2b7-494c-bd97-0b83c3ef986c 977457 0 2023-01-03 01:54:43 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[createdTime:2023-01-03T10:54:43.582893078+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T10:54:43.582893078+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update v1 2023-01-03 01:54:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  3 01:54:43.642: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9800  2a4edf67-c2b7-494c-bd97-0b83c3ef986c 977458 0 2023-01-03 01:54:43 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[createdTime:2023-01-03T10:54:43.582893078+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T10:54:43.582893078+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update v1 2023-01-03 01:54:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 01/03/23 01:54:43.642
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 01/03/23 01:54:43.657
STEP: changing the label value of the configmap back 01/03/23 01:54:53.659
STEP: modifying the configmap a third time 01/03/23 01:54:53.663
STEP: deleting the configmap 01/03/23 01:54:53.671
STEP: Expecting to observe an add notification for the watched object when the label value was restored 01/03/23 01:54:53.68
Jan  3 01:54:53.680: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9800  2a4edf67-c2b7-494c-bd97-0b83c3ef986c 977527 0 2023-01-03 01:54:43 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[createdTime:2023-01-03T10:54:43.582893078+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T10:54:43.582893078+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update v1 2023-01-03 01:54:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  3 01:54:53.681: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9800  2a4edf67-c2b7-494c-bd97-0b83c3ef986c 977528 0 2023-01-03 01:54:43 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[createdTime:2023-01-03T10:54:43.582893078+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T10:54:43.582893078+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update v1 2023-01-03 01:54:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  3 01:54:53.681: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9800  2a4edf67-c2b7-494c-bd97-0b83c3ef986c 977529 0 2023-01-03 01:54:43 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[createdTime:2023-01-03T10:54:43.582893078+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T10:54:43.582893078+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update v1 2023-01-03 01:54:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan  3 01:54:53.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9800" for this suite. 01/03/23 01:54:53.687
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","completed":29,"skipped":542,"failed":0}
------------------------------
• [SLOW TEST] [10.126 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:54:43.566
    Jan  3 01:54:43.566: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename watch 01/03/23 01:54:43.566
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:54:43.576
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:54:43.578
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 01/03/23 01:54:43.58
    STEP: creating a new configmap 01/03/23 01:54:43.581
    STEP: modifying the configmap once 01/03/23 01:54:43.593
    STEP: changing the label value of the configmap 01/03/23 01:54:43.605
    STEP: Expecting to observe a delete notification for the watched object 01/03/23 01:54:43.641
    Jan  3 01:54:43.641: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9800  2a4edf67-c2b7-494c-bd97-0b83c3ef986c 977452 0 2023-01-03 01:54:43 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[createdTime:2023-01-03T10:54:43.582893078+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T10:54:43.582893078+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update v1 2023-01-03 01:54:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  3 01:54:43.641: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9800  2a4edf67-c2b7-494c-bd97-0b83c3ef986c 977457 0 2023-01-03 01:54:43 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[createdTime:2023-01-03T10:54:43.582893078+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T10:54:43.582893078+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update v1 2023-01-03 01:54:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  3 01:54:43.642: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9800  2a4edf67-c2b7-494c-bd97-0b83c3ef986c 977458 0 2023-01-03 01:54:43 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[createdTime:2023-01-03T10:54:43.582893078+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T10:54:43.582893078+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update v1 2023-01-03 01:54:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 01/03/23 01:54:43.642
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 01/03/23 01:54:43.657
    STEP: changing the label value of the configmap back 01/03/23 01:54:53.659
    STEP: modifying the configmap a third time 01/03/23 01:54:53.663
    STEP: deleting the configmap 01/03/23 01:54:53.671
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 01/03/23 01:54:53.68
    Jan  3 01:54:53.680: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9800  2a4edf67-c2b7-494c-bd97-0b83c3ef986c 977527 0 2023-01-03 01:54:43 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[createdTime:2023-01-03T10:54:43.582893078+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T10:54:43.582893078+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update v1 2023-01-03 01:54:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  3 01:54:53.681: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9800  2a4edf67-c2b7-494c-bd97-0b83c3ef986c 977528 0 2023-01-03 01:54:43 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[createdTime:2023-01-03T10:54:43.582893078+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T10:54:43.582893078+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update v1 2023-01-03 01:54:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  3 01:54:53.681: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9800  2a4edf67-c2b7-494c-bd97-0b83c3ef986c 977529 0 2023-01-03 01:54:43 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[createdTime:2023-01-03T10:54:43.582893078+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T10:54:43.582893078+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update v1 2023-01-03 01:54:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan  3 01:54:53.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-9800" for this suite. 01/03/23 01:54:53.687
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:54:53.693
Jan  3 01:54:53.693: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename container-runtime 01/03/23 01:54:53.694
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:54:53.714
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:54:53.716
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
STEP: create the container 01/03/23 01:54:53.718
STEP: wait for the container to reach Failed 01/03/23 01:54:53.723
STEP: get the container status 01/03/23 01:54:56.78
STEP: the container should be terminated 01/03/23 01:54:56.782
STEP: the termination message should be set 01/03/23 01:54:56.782
Jan  3 01:54:56.782: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 01/03/23 01:54:56.782
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan  3 01:54:56.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7041" for this suite. 01/03/23 01:54:56.797
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":30,"skipped":560,"failed":0}
------------------------------
• [3.106 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:54:53.693
    Jan  3 01:54:53.693: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename container-runtime 01/03/23 01:54:53.694
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:54:53.714
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:54:53.716
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215
    STEP: create the container 01/03/23 01:54:53.718
    STEP: wait for the container to reach Failed 01/03/23 01:54:53.723
    STEP: get the container status 01/03/23 01:54:56.78
    STEP: the container should be terminated 01/03/23 01:54:56.782
    STEP: the termination message should be set 01/03/23 01:54:56.782
    Jan  3 01:54:56.782: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 01/03/23 01:54:56.782
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan  3 01:54:56.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-7041" for this suite. 01/03/23 01:54:56.797
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:54:56.8
Jan  3 01:54:56.800: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename deployment 01/03/23 01:54:56.801
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:54:56.82
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:54:56.822
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Jan  3 01:54:56.832: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jan  3 01:55:01.835: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/03/23 01:55:01.835
Jan  3 01:55:01.835: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jan  3 01:55:03.837: INFO: Creating deployment "test-rollover-deployment"
Jan  3 01:55:03.846: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jan  3 01:55:05.851: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jan  3 01:55:05.854: INFO: Ensure that both replica sets have 1 created replica
Jan  3 01:55:05.857: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jan  3 01:55:05.862: INFO: Updating deployment test-rollover-deployment
Jan  3 01:55:05.862: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jan  3 01:55:07.874: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jan  3 01:55:07.877: INFO: Make sure deployment "test-rollover-deployment" is complete
Jan  3 01:55:07.880: INFO: all replica sets need to contain the pod-template-hash label
Jan  3 01:55:07.880: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 1, 55, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 1, 55, 3, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 1, 55, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 1, 55, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 01:55:09.885: INFO: all replica sets need to contain the pod-template-hash label
Jan  3 01:55:09.885: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 1, 55, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 1, 55, 3, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 1, 55, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 1, 55, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 01:55:11.885: INFO: all replica sets need to contain the pod-template-hash label
Jan  3 01:55:11.885: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 1, 55, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 1, 55, 3, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 1, 55, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 1, 55, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 01:55:13.886: INFO: all replica sets need to contain the pod-template-hash label
Jan  3 01:55:13.886: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 1, 55, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 1, 55, 3, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 1, 55, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 1, 55, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 01:55:15.885: INFO: all replica sets need to contain the pod-template-hash label
Jan  3 01:55:15.885: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 1, 55, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 1, 55, 3, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 1, 55, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 1, 55, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 01:55:17.886: INFO: 
Jan  3 01:55:17.886: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan  3 01:55:17.891: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-3163  07988573-0c09-4e9f-82bd-4b72b28d2129 977742 2 2023-01-03 01:55:03 +0000 UTC <nil> <nil> map[name:rollover-pod] map[createdTime:2023-01-03T10:55:03.840418613+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:2 updatedTime:2023-01-03T10:55:03.840418613+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update apps/v1 2023-01-03 01:55:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-03 01:55:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0073d58c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-03 01:55:03 +0000 UTC,LastTransitionTime:2023-01-03 01:55:03 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-01-03 01:55:17 +0000 UTC,LastTransitionTime:2023-01-03 01:55:03 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan  3 01:55:17.893: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-3163  ada3916a-01f9-4580-a09e-9fe4d3eba317 977732 2 2023-01-03 01:55:05 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[createdTime:2023-01-03T10:55:03.840418613+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2 updatedTime:2023-01-03T10:55:03.840418613+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [{apps/v1 Deployment test-rollover-deployment 07988573-0c09-4e9f-82bd-4b72b28d2129 0xc0073d5f67 0xc0073d5f68}] [] [{kube-controller-manager Update apps/v1 2023-01-03 01:55:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:createdTime":{},"f:creator":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{},"f:updatedTime":{},"f:updater":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07988573-0c09-4e9f-82bd-4b72b28d2129\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-03 01:55:17 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0006f80d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan  3 01:55:17.893: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jan  3 01:55:17.893: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-3163  945968a4-7e93-46c5-8fe5-37c6f0ec54c3 977741 2 2023-01-03 01:54:56 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[createdTime:2023-01-03T10:54:56.82581287+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 updatedTime:2023-01-03T10:54:56.82581287+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [{apps/v1 Deployment test-rollover-deployment 07988573-0c09-4e9f-82bd-4b72b28d2129 0xc0073d5ca7 0xc0073d5ca8}] [] [{e2e.test Update apps/v1 2023-01-03 01:54:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-03 01:55:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07988573-0c09-4e9f-82bd-4b72b28d2129\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-03 01:55:17 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0073d5d78 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan  3 01:55:17.893: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-3163  75e12ec4-2d29-4c63-ba72-ff89d37c40f5 977662 2 2023-01-03 01:55:03 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[createdTime:2023-01-03T10:55:03.840418613+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T10:55:03.840418613+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [{apps/v1 Deployment test-rollover-deployment 07988573-0c09-4e9f-82bd-4b72b28d2129 0xc0073d5e17 0xc0073d5e18}] [] [{kube-controller-manager Update apps/v1 2023-01-03 01:55:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:createdTime":{},"f:creator":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{},"f:updatedTime":{},"f:updater":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07988573-0c09-4e9f-82bd-4b72b28d2129\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-03 01:55:05 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0073d5ec8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan  3 01:55:17.895: INFO: Pod "test-rollover-deployment-6d45fd857b-vttpv" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-vttpv test-rollover-deployment-6d45fd857b- deployment-3163  55ad4881-726a-4018-aee9-bbe78ca601e4 977685 0 2023-01-03 01:55:05 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cni.projectcalico.org/containerID:e6dff4478eda85352adb2e4a9829659927b0335ba931e94a7756bf80f0714df1 cni.projectcalico.org/podIP:10.244.246.122/32 cni.projectcalico.org/podIPs:10.244.246.122/32 createdTime:2023-01-03T10:55:05.976375211+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:55:05.976375211+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b ada3916a-01f9-4580-a09e-9fe4d3eba317 0xc002817977 0xc002817978}] [] [{kube-controller-manager Update v1 2023-01-03 01:55:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ada3916a-01f9-4580-a09e-9fe4d3eba317\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-03 01:55:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-03 01:55:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.246.122\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wdx7b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wdx7b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-master,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:55:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:55:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:55:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:55:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.5,PodIP:10.244.246.122,StartTime:2023-01-03 01:55:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-03 01:55:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787,ContainerID:cri-o://0cc989823657b92f4bb2e3ef8cef214235e4fd4d20a4632e9a53ad56e7728fec,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.246.122,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan  3 01:55:17.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3163" for this suite. 01/03/23 01:55:17.898
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","completed":31,"skipped":598,"failed":0}
------------------------------
• [SLOW TEST] [21.100 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:54:56.8
    Jan  3 01:54:56.800: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename deployment 01/03/23 01:54:56.801
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:54:56.82
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:54:56.822
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Jan  3 01:54:56.832: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Jan  3 01:55:01.835: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/03/23 01:55:01.835
    Jan  3 01:55:01.835: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Jan  3 01:55:03.837: INFO: Creating deployment "test-rollover-deployment"
    Jan  3 01:55:03.846: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Jan  3 01:55:05.851: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Jan  3 01:55:05.854: INFO: Ensure that both replica sets have 1 created replica
    Jan  3 01:55:05.857: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Jan  3 01:55:05.862: INFO: Updating deployment test-rollover-deployment
    Jan  3 01:55:05.862: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Jan  3 01:55:07.874: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Jan  3 01:55:07.877: INFO: Make sure deployment "test-rollover-deployment" is complete
    Jan  3 01:55:07.880: INFO: all replica sets need to contain the pod-template-hash label
    Jan  3 01:55:07.880: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 1, 55, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 1, 55, 3, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 1, 55, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 1, 55, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 01:55:09.885: INFO: all replica sets need to contain the pod-template-hash label
    Jan  3 01:55:09.885: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 1, 55, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 1, 55, 3, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 1, 55, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 1, 55, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 01:55:11.885: INFO: all replica sets need to contain the pod-template-hash label
    Jan  3 01:55:11.885: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 1, 55, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 1, 55, 3, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 1, 55, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 1, 55, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 01:55:13.886: INFO: all replica sets need to contain the pod-template-hash label
    Jan  3 01:55:13.886: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 1, 55, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 1, 55, 3, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 1, 55, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 1, 55, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 01:55:15.885: INFO: all replica sets need to contain the pod-template-hash label
    Jan  3 01:55:15.885: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 1, 55, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 1, 55, 3, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 1, 55, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 1, 55, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 01:55:17.886: INFO: 
    Jan  3 01:55:17.886: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan  3 01:55:17.891: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-3163  07988573-0c09-4e9f-82bd-4b72b28d2129 977742 2 2023-01-03 01:55:03 +0000 UTC <nil> <nil> map[name:rollover-pod] map[createdTime:2023-01-03T10:55:03.840418613+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:2 updatedTime:2023-01-03T10:55:03.840418613+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update apps/v1 2023-01-03 01:55:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-03 01:55:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0073d58c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-03 01:55:03 +0000 UTC,LastTransitionTime:2023-01-03 01:55:03 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-01-03 01:55:17 +0000 UTC,LastTransitionTime:2023-01-03 01:55:03 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan  3 01:55:17.893: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-3163  ada3916a-01f9-4580-a09e-9fe4d3eba317 977732 2 2023-01-03 01:55:05 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[createdTime:2023-01-03T10:55:03.840418613+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2 updatedTime:2023-01-03T10:55:03.840418613+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [{apps/v1 Deployment test-rollover-deployment 07988573-0c09-4e9f-82bd-4b72b28d2129 0xc0073d5f67 0xc0073d5f68}] [] [{kube-controller-manager Update apps/v1 2023-01-03 01:55:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:createdTime":{},"f:creator":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{},"f:updatedTime":{},"f:updater":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07988573-0c09-4e9f-82bd-4b72b28d2129\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-03 01:55:17 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0006f80d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan  3 01:55:17.893: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Jan  3 01:55:17.893: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-3163  945968a4-7e93-46c5-8fe5-37c6f0ec54c3 977741 2 2023-01-03 01:54:56 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[createdTime:2023-01-03T10:54:56.82581287+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 updatedTime:2023-01-03T10:54:56.82581287+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [{apps/v1 Deployment test-rollover-deployment 07988573-0c09-4e9f-82bd-4b72b28d2129 0xc0073d5ca7 0xc0073d5ca8}] [] [{e2e.test Update apps/v1 2023-01-03 01:54:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-03 01:55:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07988573-0c09-4e9f-82bd-4b72b28d2129\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-03 01:55:17 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0073d5d78 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan  3 01:55:17.893: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-3163  75e12ec4-2d29-4c63-ba72-ff89d37c40f5 977662 2 2023-01-03 01:55:03 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[createdTime:2023-01-03T10:55:03.840418613+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T10:55:03.840418613+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [{apps/v1 Deployment test-rollover-deployment 07988573-0c09-4e9f-82bd-4b72b28d2129 0xc0073d5e17 0xc0073d5e18}] [] [{kube-controller-manager Update apps/v1 2023-01-03 01:55:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:createdTime":{},"f:creator":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{},"f:updatedTime":{},"f:updater":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07988573-0c09-4e9f-82bd-4b72b28d2129\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-03 01:55:05 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0073d5ec8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan  3 01:55:17.895: INFO: Pod "test-rollover-deployment-6d45fd857b-vttpv" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-vttpv test-rollover-deployment-6d45fd857b- deployment-3163  55ad4881-726a-4018-aee9-bbe78ca601e4 977685 0 2023-01-03 01:55:05 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cni.projectcalico.org/containerID:e6dff4478eda85352adb2e4a9829659927b0335ba931e94a7756bf80f0714df1 cni.projectcalico.org/podIP:10.244.246.122/32 cni.projectcalico.org/podIPs:10.244.246.122/32 createdTime:2023-01-03T10:55:05.976375211+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T10:55:05.976375211+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b ada3916a-01f9-4580-a09e-9fe4d3eba317 0xc002817977 0xc002817978}] [] [{kube-controller-manager Update v1 2023-01-03 01:55:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ada3916a-01f9-4580-a09e-9fe4d3eba317\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-03 01:55:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-03 01:55:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.246.122\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wdx7b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wdx7b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-master,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:55:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:55:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:55:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 01:55:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.5,PodIP:10.244.246.122,StartTime:2023-01-03 01:55:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-03 01:55:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787,ContainerID:cri-o://0cc989823657b92f4bb2e3ef8cef214235e4fd4d20a4632e9a53ad56e7728fec,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.246.122,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan  3 01:55:17.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-3163" for this suite. 01/03/23 01:55:17.898
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:55:17.901
Jan  3 01:55:17.901: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename watch 01/03/23 01:55:17.902
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:55:17.91
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:55:17.912
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 01/03/23 01:55:17.914
STEP: modifying the configmap once 01/03/23 01:55:17.928
STEP: modifying the configmap a second time 01/03/23 01:55:17.935
STEP: deleting the configmap 01/03/23 01:55:17.945
STEP: creating a watch on configmaps from the resource version returned by the first update 01/03/23 01:55:17.962
STEP: Expecting to observe notifications for all changes to the configmap after the first update 01/03/23 01:55:17.964
Jan  3 01:55:17.964: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5044  362f9bb6-1785-4ef4-b47e-3faa9bfc17b8 977757 0 2023-01-03 01:55:17 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[createdTime:2023-01-03T10:55:17.915779768+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T10:55:17.915779768+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update v1 2023-01-03 01:55:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  3 01:55:17.964: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5044  362f9bb6-1785-4ef4-b47e-3faa9bfc17b8 977758 0 2023-01-03 01:55:17 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[createdTime:2023-01-03T10:55:17.915779768+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T10:55:17.915779768+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update v1 2023-01-03 01:55:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan  3 01:55:17.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5044" for this suite. 01/03/23 01:55:17.987
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","completed":32,"skipped":620,"failed":0}
------------------------------
• [0.088 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:55:17.901
    Jan  3 01:55:17.901: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename watch 01/03/23 01:55:17.902
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:55:17.91
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:55:17.912
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 01/03/23 01:55:17.914
    STEP: modifying the configmap once 01/03/23 01:55:17.928
    STEP: modifying the configmap a second time 01/03/23 01:55:17.935
    STEP: deleting the configmap 01/03/23 01:55:17.945
    STEP: creating a watch on configmaps from the resource version returned by the first update 01/03/23 01:55:17.962
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 01/03/23 01:55:17.964
    Jan  3 01:55:17.964: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5044  362f9bb6-1785-4ef4-b47e-3faa9bfc17b8 977757 0 2023-01-03 01:55:17 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[createdTime:2023-01-03T10:55:17.915779768+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T10:55:17.915779768+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update v1 2023-01-03 01:55:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  3 01:55:17.964: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5044  362f9bb6-1785-4ef4-b47e-3faa9bfc17b8 977758 0 2023-01-03 01:55:17 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[createdTime:2023-01-03T10:55:17.915779768+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T10:55:17.915779768+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update v1 2023-01-03 01:55:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan  3 01:55:17.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-5044" for this suite. 01/03/23 01:55:17.987
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:55:17.992
Jan  3 01:55:17.992: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename projected 01/03/23 01:55:17.993
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:55:18.002
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:55:18.004
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
STEP: Creating configMap with name cm-test-opt-del-c0ee17fa-c92c-4a34-9259-426833ddc049 01/03/23 01:55:18.02
STEP: Creating configMap with name cm-test-opt-upd-b1a7206d-b9d3-4e51-a1c9-3587d69f6ce3 01/03/23 01:55:18.023
STEP: Creating the pod 01/03/23 01:55:18.03
Jan  3 01:55:18.040: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3354e874-14d1-4c61-8dac-a9ad2ea20e5e" in namespace "projected-8137" to be "running and ready"
Jan  3 01:55:18.054: INFO: Pod "pod-projected-configmaps-3354e874-14d1-4c61-8dac-a9ad2ea20e5e": Phase="Pending", Reason="", readiness=false. Elapsed: 14.058989ms
Jan  3 01:55:18.054: INFO: The phase of Pod pod-projected-configmaps-3354e874-14d1-4c61-8dac-a9ad2ea20e5e is Pending, waiting for it to be Running (with Ready = true)
Jan  3 01:55:20.056: INFO: Pod "pod-projected-configmaps-3354e874-14d1-4c61-8dac-a9ad2ea20e5e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016864154s
Jan  3 01:55:20.056: INFO: The phase of Pod pod-projected-configmaps-3354e874-14d1-4c61-8dac-a9ad2ea20e5e is Pending, waiting for it to be Running (with Ready = true)
Jan  3 01:55:22.057: INFO: Pod "pod-projected-configmaps-3354e874-14d1-4c61-8dac-a9ad2ea20e5e": Phase="Running", Reason="", readiness=true. Elapsed: 4.01718061s
Jan  3 01:55:22.057: INFO: The phase of Pod pod-projected-configmaps-3354e874-14d1-4c61-8dac-a9ad2ea20e5e is Running (Ready = true)
Jan  3 01:55:22.057: INFO: Pod "pod-projected-configmaps-3354e874-14d1-4c61-8dac-a9ad2ea20e5e" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-c0ee17fa-c92c-4a34-9259-426833ddc049 01/03/23 01:55:22.074
STEP: Updating configmap cm-test-opt-upd-b1a7206d-b9d3-4e51-a1c9-3587d69f6ce3 01/03/23 01:55:22.077
STEP: Creating configMap with name cm-test-opt-create-aa001812-e0fe-45c5-a142-48482c09c912 01/03/23 01:55:22.105
STEP: waiting to observe update in volume 01/03/23 01:55:22.111
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan  3 01:56:28.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8137" for this suite. 01/03/23 01:56:28.376
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":33,"skipped":682,"failed":0}
------------------------------
• [SLOW TEST] [70.386 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:55:17.992
    Jan  3 01:55:17.992: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename projected 01/03/23 01:55:17.993
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:55:18.002
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:55:18.004
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:173
    STEP: Creating configMap with name cm-test-opt-del-c0ee17fa-c92c-4a34-9259-426833ddc049 01/03/23 01:55:18.02
    STEP: Creating configMap with name cm-test-opt-upd-b1a7206d-b9d3-4e51-a1c9-3587d69f6ce3 01/03/23 01:55:18.023
    STEP: Creating the pod 01/03/23 01:55:18.03
    Jan  3 01:55:18.040: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3354e874-14d1-4c61-8dac-a9ad2ea20e5e" in namespace "projected-8137" to be "running and ready"
    Jan  3 01:55:18.054: INFO: Pod "pod-projected-configmaps-3354e874-14d1-4c61-8dac-a9ad2ea20e5e": Phase="Pending", Reason="", readiness=false. Elapsed: 14.058989ms
    Jan  3 01:55:18.054: INFO: The phase of Pod pod-projected-configmaps-3354e874-14d1-4c61-8dac-a9ad2ea20e5e is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 01:55:20.056: INFO: Pod "pod-projected-configmaps-3354e874-14d1-4c61-8dac-a9ad2ea20e5e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016864154s
    Jan  3 01:55:20.056: INFO: The phase of Pod pod-projected-configmaps-3354e874-14d1-4c61-8dac-a9ad2ea20e5e is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 01:55:22.057: INFO: Pod "pod-projected-configmaps-3354e874-14d1-4c61-8dac-a9ad2ea20e5e": Phase="Running", Reason="", readiness=true. Elapsed: 4.01718061s
    Jan  3 01:55:22.057: INFO: The phase of Pod pod-projected-configmaps-3354e874-14d1-4c61-8dac-a9ad2ea20e5e is Running (Ready = true)
    Jan  3 01:55:22.057: INFO: Pod "pod-projected-configmaps-3354e874-14d1-4c61-8dac-a9ad2ea20e5e" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-c0ee17fa-c92c-4a34-9259-426833ddc049 01/03/23 01:55:22.074
    STEP: Updating configmap cm-test-opt-upd-b1a7206d-b9d3-4e51-a1c9-3587d69f6ce3 01/03/23 01:55:22.077
    STEP: Creating configMap with name cm-test-opt-create-aa001812-e0fe-45c5-a142-48482c09c912 01/03/23 01:55:22.105
    STEP: waiting to observe update in volume 01/03/23 01:55:22.111
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan  3 01:56:28.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8137" for this suite. 01/03/23 01:56:28.376
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:56:28.38
Jan  3 01:56:28.380: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename configmap 01/03/23 01:56:28.381
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:56:28.392
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:56:28.393
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
STEP: Creating configMap with name configmap-test-volume-112be4a7-81eb-49d1-840b-b9d487f90547 01/03/23 01:56:28.395
STEP: Creating a pod to test consume configMaps 01/03/23 01:56:28.41
Jan  3 01:56:28.417: INFO: Waiting up to 5m0s for pod "pod-configmaps-5315374f-e404-434a-ae7d-4783af18a5d0" in namespace "configmap-5005" to be "Succeeded or Failed"
Jan  3 01:56:28.432: INFO: Pod "pod-configmaps-5315374f-e404-434a-ae7d-4783af18a5d0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.212108ms
Jan  3 01:56:30.434: INFO: Pod "pod-configmaps-5315374f-e404-434a-ae7d-4783af18a5d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016803399s
Jan  3 01:56:32.442: INFO: Pod "pod-configmaps-5315374f-e404-434a-ae7d-4783af18a5d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024418359s
STEP: Saw pod success 01/03/23 01:56:32.442
Jan  3 01:56:32.442: INFO: Pod "pod-configmaps-5315374f-e404-434a-ae7d-4783af18a5d0" satisfied condition "Succeeded or Failed"
Jan  3 01:56:32.443: INFO: Trying to get logs from node cncf-node1 pod pod-configmaps-5315374f-e404-434a-ae7d-4783af18a5d0 container agnhost-container: <nil>
STEP: delete the pod 01/03/23 01:56:32.455
Jan  3 01:56:32.478: INFO: Waiting for pod pod-configmaps-5315374f-e404-434a-ae7d-4783af18a5d0 to disappear
Jan  3 01:56:32.483: INFO: Pod pod-configmaps-5315374f-e404-434a-ae7d-4783af18a5d0 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan  3 01:56:32.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5005" for this suite. 01/03/23 01:56:32.486
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":34,"skipped":711,"failed":0}
------------------------------
• [4.109 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:56:28.38
    Jan  3 01:56:28.380: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename configmap 01/03/23 01:56:28.381
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:56:28.392
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:56:28.393
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:56
    STEP: Creating configMap with name configmap-test-volume-112be4a7-81eb-49d1-840b-b9d487f90547 01/03/23 01:56:28.395
    STEP: Creating a pod to test consume configMaps 01/03/23 01:56:28.41
    Jan  3 01:56:28.417: INFO: Waiting up to 5m0s for pod "pod-configmaps-5315374f-e404-434a-ae7d-4783af18a5d0" in namespace "configmap-5005" to be "Succeeded or Failed"
    Jan  3 01:56:28.432: INFO: Pod "pod-configmaps-5315374f-e404-434a-ae7d-4783af18a5d0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.212108ms
    Jan  3 01:56:30.434: INFO: Pod "pod-configmaps-5315374f-e404-434a-ae7d-4783af18a5d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016803399s
    Jan  3 01:56:32.442: INFO: Pod "pod-configmaps-5315374f-e404-434a-ae7d-4783af18a5d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024418359s
    STEP: Saw pod success 01/03/23 01:56:32.442
    Jan  3 01:56:32.442: INFO: Pod "pod-configmaps-5315374f-e404-434a-ae7d-4783af18a5d0" satisfied condition "Succeeded or Failed"
    Jan  3 01:56:32.443: INFO: Trying to get logs from node cncf-node1 pod pod-configmaps-5315374f-e404-434a-ae7d-4783af18a5d0 container agnhost-container: <nil>
    STEP: delete the pod 01/03/23 01:56:32.455
    Jan  3 01:56:32.478: INFO: Waiting for pod pod-configmaps-5315374f-e404-434a-ae7d-4783af18a5d0 to disappear
    Jan  3 01:56:32.483: INFO: Pod pod-configmaps-5315374f-e404-434a-ae7d-4783af18a5d0 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan  3 01:56:32.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-5005" for this suite. 01/03/23 01:56:32.486
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:56:32.49
Jan  3 01:56:32.490: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename svcaccounts 01/03/23 01:56:32.491
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:56:32.514
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:56:32.516
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
Jan  3 01:56:32.565: INFO: created pod pod-service-account-defaultsa
Jan  3 01:56:32.565: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jan  3 01:56:32.570: INFO: created pod pod-service-account-mountsa
Jan  3 01:56:32.570: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jan  3 01:56:32.588: INFO: created pod pod-service-account-nomountsa
Jan  3 01:56:32.588: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jan  3 01:56:32.600: INFO: created pod pod-service-account-defaultsa-mountspec
Jan  3 01:56:32.600: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jan  3 01:56:32.611: INFO: created pod pod-service-account-mountsa-mountspec
Jan  3 01:56:32.611: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jan  3 01:56:32.623: INFO: created pod pod-service-account-nomountsa-mountspec
Jan  3 01:56:32.623: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jan  3 01:56:32.634: INFO: created pod pod-service-account-defaultsa-nomountspec
Jan  3 01:56:32.634: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jan  3 01:56:32.646: INFO: created pod pod-service-account-mountsa-nomountspec
Jan  3 01:56:32.646: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jan  3 01:56:32.657: INFO: created pod pod-service-account-nomountsa-nomountspec
Jan  3 01:56:32.657: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan  3 01:56:32.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9631" for this suite. 01/03/23 01:56:32.71
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","completed":35,"skipped":747,"failed":0}
------------------------------
• [0.236 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:56:32.49
    Jan  3 01:56:32.490: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename svcaccounts 01/03/23 01:56:32.491
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:56:32.514
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:56:32.516
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:158
    Jan  3 01:56:32.565: INFO: created pod pod-service-account-defaultsa
    Jan  3 01:56:32.565: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Jan  3 01:56:32.570: INFO: created pod pod-service-account-mountsa
    Jan  3 01:56:32.570: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Jan  3 01:56:32.588: INFO: created pod pod-service-account-nomountsa
    Jan  3 01:56:32.588: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Jan  3 01:56:32.600: INFO: created pod pod-service-account-defaultsa-mountspec
    Jan  3 01:56:32.600: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Jan  3 01:56:32.611: INFO: created pod pod-service-account-mountsa-mountspec
    Jan  3 01:56:32.611: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Jan  3 01:56:32.623: INFO: created pod pod-service-account-nomountsa-mountspec
    Jan  3 01:56:32.623: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Jan  3 01:56:32.634: INFO: created pod pod-service-account-defaultsa-nomountspec
    Jan  3 01:56:32.634: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Jan  3 01:56:32.646: INFO: created pod pod-service-account-mountsa-nomountspec
    Jan  3 01:56:32.646: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Jan  3 01:56:32.657: INFO: created pod pod-service-account-nomountsa-nomountspec
    Jan  3 01:56:32.657: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan  3 01:56:32.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-9631" for this suite. 01/03/23 01:56:32.71
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:56:32.727
Jan  3 01:56:32.727: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename secrets 01/03/23 01:56:32.728
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:56:32.76
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:56:32.762
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
STEP: Creating secret with name secret-test-069e3b2c-bac7-4e40-a6e3-6490e9195d7f 01/03/23 01:56:32.764
STEP: Creating a pod to test consume secrets 01/03/23 01:56:32.77
Jan  3 01:56:32.779: INFO: Waiting up to 5m0s for pod "pod-secrets-7cc135e2-6e14-45b3-a7c1-199ac1fb9ca8" in namespace "secrets-9840" to be "Succeeded or Failed"
Jan  3 01:56:32.801: INFO: Pod "pod-secrets-7cc135e2-6e14-45b3-a7c1-199ac1fb9ca8": Phase="Pending", Reason="", readiness=false. Elapsed: 22.004033ms
Jan  3 01:56:34.803: INFO: Pod "pod-secrets-7cc135e2-6e14-45b3-a7c1-199ac1fb9ca8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024330266s
Jan  3 01:56:36.803: INFO: Pod "pod-secrets-7cc135e2-6e14-45b3-a7c1-199ac1fb9ca8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024189642s
Jan  3 01:56:38.803: INFO: Pod "pod-secrets-7cc135e2-6e14-45b3-a7c1-199ac1fb9ca8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024238411s
STEP: Saw pod success 01/03/23 01:56:38.803
Jan  3 01:56:38.804: INFO: Pod "pod-secrets-7cc135e2-6e14-45b3-a7c1-199ac1fb9ca8" satisfied condition "Succeeded or Failed"
Jan  3 01:56:38.819: INFO: Trying to get logs from node cncf-master pod pod-secrets-7cc135e2-6e14-45b3-a7c1-199ac1fb9ca8 container secret-env-test: <nil>
STEP: delete the pod 01/03/23 01:56:38.836
Jan  3 01:56:38.886: INFO: Waiting for pod pod-secrets-7cc135e2-6e14-45b3-a7c1-199ac1fb9ca8 to disappear
Jan  3 01:56:38.905: INFO: Pod pod-secrets-7cc135e2-6e14-45b3-a7c1-199ac1fb9ca8 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jan  3 01:56:38.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9840" for this suite. 01/03/23 01:56:38.908
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","completed":36,"skipped":751,"failed":0}
------------------------------
• [SLOW TEST] [6.183 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:56:32.727
    Jan  3 01:56:32.727: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename secrets 01/03/23 01:56:32.728
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:56:32.76
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:56:32.762
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:45
    STEP: Creating secret with name secret-test-069e3b2c-bac7-4e40-a6e3-6490e9195d7f 01/03/23 01:56:32.764
    STEP: Creating a pod to test consume secrets 01/03/23 01:56:32.77
    Jan  3 01:56:32.779: INFO: Waiting up to 5m0s for pod "pod-secrets-7cc135e2-6e14-45b3-a7c1-199ac1fb9ca8" in namespace "secrets-9840" to be "Succeeded or Failed"
    Jan  3 01:56:32.801: INFO: Pod "pod-secrets-7cc135e2-6e14-45b3-a7c1-199ac1fb9ca8": Phase="Pending", Reason="", readiness=false. Elapsed: 22.004033ms
    Jan  3 01:56:34.803: INFO: Pod "pod-secrets-7cc135e2-6e14-45b3-a7c1-199ac1fb9ca8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024330266s
    Jan  3 01:56:36.803: INFO: Pod "pod-secrets-7cc135e2-6e14-45b3-a7c1-199ac1fb9ca8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024189642s
    Jan  3 01:56:38.803: INFO: Pod "pod-secrets-7cc135e2-6e14-45b3-a7c1-199ac1fb9ca8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024238411s
    STEP: Saw pod success 01/03/23 01:56:38.803
    Jan  3 01:56:38.804: INFO: Pod "pod-secrets-7cc135e2-6e14-45b3-a7c1-199ac1fb9ca8" satisfied condition "Succeeded or Failed"
    Jan  3 01:56:38.819: INFO: Trying to get logs from node cncf-master pod pod-secrets-7cc135e2-6e14-45b3-a7c1-199ac1fb9ca8 container secret-env-test: <nil>
    STEP: delete the pod 01/03/23 01:56:38.836
    Jan  3 01:56:38.886: INFO: Waiting for pod pod-secrets-7cc135e2-6e14-45b3-a7c1-199ac1fb9ca8 to disappear
    Jan  3 01:56:38.905: INFO: Pod pod-secrets-7cc135e2-6e14-45b3-a7c1-199ac1fb9ca8 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jan  3 01:56:38.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-9840" for this suite. 01/03/23 01:56:38.908
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:56:38.911
Jan  3 01:56:38.911: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename projected 01/03/23 01:56:38.912
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:56:38.934
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:56:38.936
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
STEP: Creating configMap with name projected-configmap-test-volume-map-653a2b5f-4e70-4151-8a30-6d314c14b564 01/03/23 01:56:38.938
STEP: Creating a pod to test consume configMaps 01/03/23 01:56:38.95
Jan  3 01:56:38.958: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5ca623a8-f491-4378-8866-edfdb3dbf4cb" in namespace "projected-7351" to be "Succeeded or Failed"
Jan  3 01:56:38.996: INFO: Pod "pod-projected-configmaps-5ca623a8-f491-4378-8866-edfdb3dbf4cb": Phase="Pending", Reason="", readiness=false. Elapsed: 38.520754ms
Jan  3 01:56:40.999: INFO: Pod "pod-projected-configmaps-5ca623a8-f491-4378-8866-edfdb3dbf4cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041386516s
Jan  3 01:56:42.999: INFO: Pod "pod-projected-configmaps-5ca623a8-f491-4378-8866-edfdb3dbf4cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041635731s
STEP: Saw pod success 01/03/23 01:56:42.999
Jan  3 01:56:42.999: INFO: Pod "pod-projected-configmaps-5ca623a8-f491-4378-8866-edfdb3dbf4cb" satisfied condition "Succeeded or Failed"
Jan  3 01:56:43.001: INFO: Trying to get logs from node cncf-master pod pod-projected-configmaps-5ca623a8-f491-4378-8866-edfdb3dbf4cb container agnhost-container: <nil>
STEP: delete the pod 01/03/23 01:56:43.004
Jan  3 01:56:43.024: INFO: Waiting for pod pod-projected-configmaps-5ca623a8-f491-4378-8866-edfdb3dbf4cb to disappear
Jan  3 01:56:43.029: INFO: Pod pod-projected-configmaps-5ca623a8-f491-4378-8866-edfdb3dbf4cb no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan  3 01:56:43.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7351" for this suite. 01/03/23 01:56:43.032
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":37,"skipped":755,"failed":0}
------------------------------
• [4.123 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:56:38.911
    Jan  3 01:56:38.911: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename projected 01/03/23 01:56:38.912
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:56:38.934
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:56:38.936
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:108
    STEP: Creating configMap with name projected-configmap-test-volume-map-653a2b5f-4e70-4151-8a30-6d314c14b564 01/03/23 01:56:38.938
    STEP: Creating a pod to test consume configMaps 01/03/23 01:56:38.95
    Jan  3 01:56:38.958: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5ca623a8-f491-4378-8866-edfdb3dbf4cb" in namespace "projected-7351" to be "Succeeded or Failed"
    Jan  3 01:56:38.996: INFO: Pod "pod-projected-configmaps-5ca623a8-f491-4378-8866-edfdb3dbf4cb": Phase="Pending", Reason="", readiness=false. Elapsed: 38.520754ms
    Jan  3 01:56:40.999: INFO: Pod "pod-projected-configmaps-5ca623a8-f491-4378-8866-edfdb3dbf4cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041386516s
    Jan  3 01:56:42.999: INFO: Pod "pod-projected-configmaps-5ca623a8-f491-4378-8866-edfdb3dbf4cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041635731s
    STEP: Saw pod success 01/03/23 01:56:42.999
    Jan  3 01:56:42.999: INFO: Pod "pod-projected-configmaps-5ca623a8-f491-4378-8866-edfdb3dbf4cb" satisfied condition "Succeeded or Failed"
    Jan  3 01:56:43.001: INFO: Trying to get logs from node cncf-master pod pod-projected-configmaps-5ca623a8-f491-4378-8866-edfdb3dbf4cb container agnhost-container: <nil>
    STEP: delete the pod 01/03/23 01:56:43.004
    Jan  3 01:56:43.024: INFO: Waiting for pod pod-projected-configmaps-5ca623a8-f491-4378-8866-edfdb3dbf4cb to disappear
    Jan  3 01:56:43.029: INFO: Pod pod-projected-configmaps-5ca623a8-f491-4378-8866-edfdb3dbf4cb no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan  3 01:56:43.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7351" for this suite. 01/03/23 01:56:43.032
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:56:43.034
Jan  3 01:56:43.035: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename downward-api 01/03/23 01:56:43.035
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:56:43.042
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:56:43.044
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
STEP: Creating a pod to test downward API volume plugin 01/03/23 01:56:43.047
Jan  3 01:56:43.102: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3d6b8b45-3059-44d6-b724-fd5532f2c14f" in namespace "downward-api-5355" to be "Succeeded or Failed"
Jan  3 01:56:43.109: INFO: Pod "downwardapi-volume-3d6b8b45-3059-44d6-b724-fd5532f2c14f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.064627ms
Jan  3 01:56:45.113: INFO: Pod "downwardapi-volume-3d6b8b45-3059-44d6-b724-fd5532f2c14f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01013886s
Jan  3 01:56:47.112: INFO: Pod "downwardapi-volume-3d6b8b45-3059-44d6-b724-fd5532f2c14f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009959155s
STEP: Saw pod success 01/03/23 01:56:47.112
Jan  3 01:56:47.113: INFO: Pod "downwardapi-volume-3d6b8b45-3059-44d6-b724-fd5532f2c14f" satisfied condition "Succeeded or Failed"
Jan  3 01:56:47.114: INFO: Trying to get logs from node cncf-master pod downwardapi-volume-3d6b8b45-3059-44d6-b724-fd5532f2c14f container client-container: <nil>
STEP: delete the pod 01/03/23 01:56:47.117
Jan  3 01:56:47.133: INFO: Waiting for pod downwardapi-volume-3d6b8b45-3059-44d6-b724-fd5532f2c14f to disappear
Jan  3 01:56:47.138: INFO: Pod downwardapi-volume-3d6b8b45-3059-44d6-b724-fd5532f2c14f no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan  3 01:56:47.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5355" for this suite. 01/03/23 01:56:47.141
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","completed":38,"skipped":758,"failed":0}
------------------------------
• [4.109 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:56:43.034
    Jan  3 01:56:43.035: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename downward-api 01/03/23 01:56:43.035
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:56:43.042
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:56:43.044
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:52
    STEP: Creating a pod to test downward API volume plugin 01/03/23 01:56:43.047
    Jan  3 01:56:43.102: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3d6b8b45-3059-44d6-b724-fd5532f2c14f" in namespace "downward-api-5355" to be "Succeeded or Failed"
    Jan  3 01:56:43.109: INFO: Pod "downwardapi-volume-3d6b8b45-3059-44d6-b724-fd5532f2c14f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.064627ms
    Jan  3 01:56:45.113: INFO: Pod "downwardapi-volume-3d6b8b45-3059-44d6-b724-fd5532f2c14f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01013886s
    Jan  3 01:56:47.112: INFO: Pod "downwardapi-volume-3d6b8b45-3059-44d6-b724-fd5532f2c14f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009959155s
    STEP: Saw pod success 01/03/23 01:56:47.112
    Jan  3 01:56:47.113: INFO: Pod "downwardapi-volume-3d6b8b45-3059-44d6-b724-fd5532f2c14f" satisfied condition "Succeeded or Failed"
    Jan  3 01:56:47.114: INFO: Trying to get logs from node cncf-master pod downwardapi-volume-3d6b8b45-3059-44d6-b724-fd5532f2c14f container client-container: <nil>
    STEP: delete the pod 01/03/23 01:56:47.117
    Jan  3 01:56:47.133: INFO: Waiting for pod downwardapi-volume-3d6b8b45-3059-44d6-b724-fd5532f2c14f to disappear
    Jan  3 01:56:47.138: INFO: Pod downwardapi-volume-3d6b8b45-3059-44d6-b724-fd5532f2c14f no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan  3 01:56:47.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5355" for this suite. 01/03/23 01:56:47.141
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:56:47.144
Jan  3 01:56:47.144: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename daemonsets 01/03/23 01:56:47.144
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:56:47.161
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:56:47.163
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
STEP: Creating simple DaemonSet "daemon-set" 01/03/23 01:56:47.216
STEP: Check that daemon pods launch on every node of the cluster. 01/03/23 01:56:47.22
Jan  3 01:56:47.242: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  3 01:56:47.242: INFO: Node cncf-master is running 0 daemon pod, expected 1
Jan  3 01:56:48.282: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  3 01:56:48.282: INFO: Node cncf-master is running 0 daemon pod, expected 1
Jan  3 01:56:49.248: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan  3 01:56:49.248: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status 01/03/23 01:56:49.25
Jan  3 01:56:49.252: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 01/03/23 01:56:49.252
Jan  3 01:56:49.256: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 01/03/23 01:56:49.256
Jan  3 01:56:49.258: INFO: Observed &DaemonSet event: ADDED
Jan  3 01:56:49.258: INFO: Observed &DaemonSet event: MODIFIED
Jan  3 01:56:49.258: INFO: Observed &DaemonSet event: MODIFIED
Jan  3 01:56:49.258: INFO: Observed &DaemonSet event: MODIFIED
Jan  3 01:56:49.258: INFO: Observed &DaemonSet event: MODIFIED
Jan  3 01:56:49.258: INFO: Observed &DaemonSet event: MODIFIED
Jan  3 01:56:49.258: INFO: Found daemon set daemon-set in namespace daemonsets-4216 with labels: map[daemonset-name:daemon-set] annotations: map[createdTime:2023-01-03T10:56:47.218204568+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deprecated.daemonset.template.generation:1 updatedTime:2023-01-03T10:56:47.218204568+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan  3 01:56:49.258: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 01/03/23 01:56:49.258
STEP: watching for the daemon set status to be patched 01/03/23 01:56:49.282
Jan  3 01:56:49.283: INFO: Observed &DaemonSet event: ADDED
Jan  3 01:56:49.283: INFO: Observed &DaemonSet event: MODIFIED
Jan  3 01:56:49.284: INFO: Observed &DaemonSet event: MODIFIED
Jan  3 01:56:49.284: INFO: Observed &DaemonSet event: MODIFIED
Jan  3 01:56:49.284: INFO: Observed &DaemonSet event: MODIFIED
Jan  3 01:56:49.284: INFO: Observed &DaemonSet event: MODIFIED
Jan  3 01:56:49.284: INFO: Observed daemon set daemon-set in namespace daemonsets-4216 with annotations: map[createdTime:2023-01-03T10:56:47.218204568+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deprecated.daemonset.template.generation:1 updatedTime:2023-01-03T10:56:47.218204568+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan  3 01:56:49.284: INFO: Observed &DaemonSet event: MODIFIED
Jan  3 01:56:49.284: INFO: Found daemon set daemon-set in namespace daemonsets-4216 with labels: map[daemonset-name:daemon-set] annotations: map[createdTime:2023-01-03T10:56:47.218204568+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deprecated.daemonset.template.generation:1 updatedTime:2023-01-03T10:56:47.218204568+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Jan  3 01:56:49.284: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/03/23 01:56:49.314
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4216, will wait for the garbage collector to delete the pods 01/03/23 01:56:49.314
Jan  3 01:56:49.369: INFO: Deleting DaemonSet.extensions daemon-set took: 2.764962ms
Jan  3 01:56:49.470: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.972594ms
Jan  3 01:56:51.973: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  3 01:56:51.973: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan  3 01:56:51.974: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"978507"},"items":null}

Jan  3 01:56:51.976: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"978507"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan  3 01:56:51.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4216" for this suite. 01/03/23 01:56:51.986
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","completed":39,"skipped":761,"failed":0}
------------------------------
• [4.845 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:56:47.144
    Jan  3 01:56:47.144: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename daemonsets 01/03/23 01:56:47.144
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:56:47.161
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:56:47.163
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:861
    STEP: Creating simple DaemonSet "daemon-set" 01/03/23 01:56:47.216
    STEP: Check that daemon pods launch on every node of the cluster. 01/03/23 01:56:47.22
    Jan  3 01:56:47.242: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  3 01:56:47.242: INFO: Node cncf-master is running 0 daemon pod, expected 1
    Jan  3 01:56:48.282: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  3 01:56:48.282: INFO: Node cncf-master is running 0 daemon pod, expected 1
    Jan  3 01:56:49.248: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan  3 01:56:49.248: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Getting /status 01/03/23 01:56:49.25
    Jan  3 01:56:49.252: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 01/03/23 01:56:49.252
    Jan  3 01:56:49.256: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 01/03/23 01:56:49.256
    Jan  3 01:56:49.258: INFO: Observed &DaemonSet event: ADDED
    Jan  3 01:56:49.258: INFO: Observed &DaemonSet event: MODIFIED
    Jan  3 01:56:49.258: INFO: Observed &DaemonSet event: MODIFIED
    Jan  3 01:56:49.258: INFO: Observed &DaemonSet event: MODIFIED
    Jan  3 01:56:49.258: INFO: Observed &DaemonSet event: MODIFIED
    Jan  3 01:56:49.258: INFO: Observed &DaemonSet event: MODIFIED
    Jan  3 01:56:49.258: INFO: Found daemon set daemon-set in namespace daemonsets-4216 with labels: map[daemonset-name:daemon-set] annotations: map[createdTime:2023-01-03T10:56:47.218204568+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deprecated.daemonset.template.generation:1 updatedTime:2023-01-03T10:56:47.218204568+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan  3 01:56:49.258: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 01/03/23 01:56:49.258
    STEP: watching for the daemon set status to be patched 01/03/23 01:56:49.282
    Jan  3 01:56:49.283: INFO: Observed &DaemonSet event: ADDED
    Jan  3 01:56:49.283: INFO: Observed &DaemonSet event: MODIFIED
    Jan  3 01:56:49.284: INFO: Observed &DaemonSet event: MODIFIED
    Jan  3 01:56:49.284: INFO: Observed &DaemonSet event: MODIFIED
    Jan  3 01:56:49.284: INFO: Observed &DaemonSet event: MODIFIED
    Jan  3 01:56:49.284: INFO: Observed &DaemonSet event: MODIFIED
    Jan  3 01:56:49.284: INFO: Observed daemon set daemon-set in namespace daemonsets-4216 with annotations: map[createdTime:2023-01-03T10:56:47.218204568+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deprecated.daemonset.template.generation:1 updatedTime:2023-01-03T10:56:47.218204568+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan  3 01:56:49.284: INFO: Observed &DaemonSet event: MODIFIED
    Jan  3 01:56:49.284: INFO: Found daemon set daemon-set in namespace daemonsets-4216 with labels: map[daemonset-name:daemon-set] annotations: map[createdTime:2023-01-03T10:56:47.218204568+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deprecated.daemonset.template.generation:1 updatedTime:2023-01-03T10:56:47.218204568+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Jan  3 01:56:49.284: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/03/23 01:56:49.314
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4216, will wait for the garbage collector to delete the pods 01/03/23 01:56:49.314
    Jan  3 01:56:49.369: INFO: Deleting DaemonSet.extensions daemon-set took: 2.764962ms
    Jan  3 01:56:49.470: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.972594ms
    Jan  3 01:56:51.973: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  3 01:56:51.973: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan  3 01:56:51.974: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"978507"},"items":null}

    Jan  3 01:56:51.976: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"978507"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan  3 01:56:51.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-4216" for this suite. 01/03/23 01:56:51.986
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:56:51.989
Jan  3 01:56:51.990: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename gc 01/03/23 01:56:51.99
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:56:51.998
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:56:51.999
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 01/03/23 01:56:52.021
STEP: delete the rc 01/03/23 01:56:57.064
STEP: wait for the rc to be deleted 01/03/23 01:56:57.09
Jan  3 01:56:58.227: INFO: 80 pods remaining
Jan  3 01:56:58.227: INFO: 80 pods has nil DeletionTimestamp
Jan  3 01:56:58.227: INFO: 
Jan  3 01:56:59.147: INFO: 72 pods remaining
Jan  3 01:56:59.147: INFO: 70 pods has nil DeletionTimestamp
Jan  3 01:56:59.147: INFO: 
Jan  3 01:57:00.141: INFO: 60 pods remaining
Jan  3 01:57:00.141: INFO: 60 pods has nil DeletionTimestamp
Jan  3 01:57:00.141: INFO: 
Jan  3 01:57:01.097: INFO: 40 pods remaining
Jan  3 01:57:01.097: INFO: 40 pods has nil DeletionTimestamp
Jan  3 01:57:01.097: INFO: 
Jan  3 01:57:02.129: INFO: 31 pods remaining
Jan  3 01:57:02.129: INFO: 30 pods has nil DeletionTimestamp
Jan  3 01:57:02.129: INFO: 
Jan  3 01:57:03.119: INFO: 20 pods remaining
Jan  3 01:57:03.119: INFO: 20 pods has nil DeletionTimestamp
Jan  3 01:57:03.119: INFO: 
STEP: Gathering metrics 01/03/23 01:57:04.096
Jan  3 01:57:04.110: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cncf-master" in namespace "kube-system" to be "running and ready"
Jan  3 01:57:04.114: INFO: Pod "kube-controller-manager-cncf-master": Phase="Running", Reason="", readiness=true. Elapsed: 4.029298ms
Jan  3 01:57:04.114: INFO: The phase of Pod kube-controller-manager-cncf-master is Running (Ready = true)
Jan  3 01:57:04.114: INFO: Pod "kube-controller-manager-cncf-master" satisfied condition "running and ready"
Jan  3 01:57:04.546: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan  3 01:57:04.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4459" for this suite. 01/03/23 01:57:04.557
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","completed":40,"skipped":786,"failed":0}
------------------------------
• [SLOW TEST] [12.572 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:56:51.989
    Jan  3 01:56:51.990: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename gc 01/03/23 01:56:51.99
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:56:51.998
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:56:51.999
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 01/03/23 01:56:52.021
    STEP: delete the rc 01/03/23 01:56:57.064
    STEP: wait for the rc to be deleted 01/03/23 01:56:57.09
    Jan  3 01:56:58.227: INFO: 80 pods remaining
    Jan  3 01:56:58.227: INFO: 80 pods has nil DeletionTimestamp
    Jan  3 01:56:58.227: INFO: 
    Jan  3 01:56:59.147: INFO: 72 pods remaining
    Jan  3 01:56:59.147: INFO: 70 pods has nil DeletionTimestamp
    Jan  3 01:56:59.147: INFO: 
    Jan  3 01:57:00.141: INFO: 60 pods remaining
    Jan  3 01:57:00.141: INFO: 60 pods has nil DeletionTimestamp
    Jan  3 01:57:00.141: INFO: 
    Jan  3 01:57:01.097: INFO: 40 pods remaining
    Jan  3 01:57:01.097: INFO: 40 pods has nil DeletionTimestamp
    Jan  3 01:57:01.097: INFO: 
    Jan  3 01:57:02.129: INFO: 31 pods remaining
    Jan  3 01:57:02.129: INFO: 30 pods has nil DeletionTimestamp
    Jan  3 01:57:02.129: INFO: 
    Jan  3 01:57:03.119: INFO: 20 pods remaining
    Jan  3 01:57:03.119: INFO: 20 pods has nil DeletionTimestamp
    Jan  3 01:57:03.119: INFO: 
    STEP: Gathering metrics 01/03/23 01:57:04.096
    Jan  3 01:57:04.110: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cncf-master" in namespace "kube-system" to be "running and ready"
    Jan  3 01:57:04.114: INFO: Pod "kube-controller-manager-cncf-master": Phase="Running", Reason="", readiness=true. Elapsed: 4.029298ms
    Jan  3 01:57:04.114: INFO: The phase of Pod kube-controller-manager-cncf-master is Running (Ready = true)
    Jan  3 01:57:04.114: INFO: Pod "kube-controller-manager-cncf-master" satisfied condition "running and ready"
    Jan  3 01:57:04.546: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan  3 01:57:04.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-4459" for this suite. 01/03/23 01:57:04.557
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:57:04.564
Jan  3 01:57:04.564: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename kubelet-test 01/03/23 01:57:04.565
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:57:04.582
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:57:04.584
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan  3 01:57:12.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4731" for this suite. 01/03/23 01:57:12.614
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","completed":41,"skipped":845,"failed":0}
------------------------------
• [SLOW TEST] [8.053 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:57:04.564
    Jan  3 01:57:04.564: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename kubelet-test 01/03/23 01:57:04.565
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:57:04.582
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:57:04.584
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan  3 01:57:12.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-4731" for this suite. 01/03/23 01:57:12.614
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:57:12.617
Jan  3 01:57:12.617: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename pods 01/03/23 01:57:12.618
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:57:12.637
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:57:12.639
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
STEP: Create a pod 01/03/23 01:57:12.641
Jan  3 01:57:12.675: INFO: Waiting up to 5m0s for pod "pod-pfdl2" in namespace "pods-7242" to be "running"
Jan  3 01:57:12.724: INFO: Pod "pod-pfdl2": Phase="Pending", Reason="", readiness=false. Elapsed: 48.859773ms
Jan  3 01:57:14.726: INFO: Pod "pod-pfdl2": Phase="Running", Reason="", readiness=true. Elapsed: 2.051368129s
Jan  3 01:57:14.726: INFO: Pod "pod-pfdl2" satisfied condition "running"
STEP: patching /status 01/03/23 01:57:14.726
Jan  3 01:57:14.731: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan  3 01:57:14.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7242" for this suite. 01/03/23 01:57:14.75
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Pods should patch a pod status [Conformance]","completed":42,"skipped":852,"failed":0}
------------------------------
• [2.135 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:57:12.617
    Jan  3 01:57:12.617: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename pods 01/03/23 01:57:12.618
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:57:12.637
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:57:12.639
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1082
    STEP: Create a pod 01/03/23 01:57:12.641
    Jan  3 01:57:12.675: INFO: Waiting up to 5m0s for pod "pod-pfdl2" in namespace "pods-7242" to be "running"
    Jan  3 01:57:12.724: INFO: Pod "pod-pfdl2": Phase="Pending", Reason="", readiness=false. Elapsed: 48.859773ms
    Jan  3 01:57:14.726: INFO: Pod "pod-pfdl2": Phase="Running", Reason="", readiness=true. Elapsed: 2.051368129s
    Jan  3 01:57:14.726: INFO: Pod "pod-pfdl2" satisfied condition "running"
    STEP: patching /status 01/03/23 01:57:14.726
    Jan  3 01:57:14.731: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan  3 01:57:14.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-7242" for this suite. 01/03/23 01:57:14.75
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:57:14.754
Jan  3 01:57:14.754: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename kubectl 01/03/23 01:57:14.755
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:57:14.775
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:57:14.776
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
STEP: starting the proxy server 01/03/23 01:57:14.778
Jan  3 01:57:14.778: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-4287 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 01/03/23 01:57:14.819
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan  3 01:57:14.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4287" for this suite. 01/03/23 01:57:14.83
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","completed":43,"skipped":876,"failed":0}
------------------------------
• [0.079 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1785

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:57:14.754
    Jan  3 01:57:14.754: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename kubectl 01/03/23 01:57:14.755
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:57:14.775
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:57:14.776
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1785
    STEP: starting the proxy server 01/03/23 01:57:14.778
    Jan  3 01:57:14.778: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-4287 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 01/03/23 01:57:14.819
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan  3 01:57:14.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4287" for this suite. 01/03/23 01:57:14.83
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:57:14.833
Jan  3 01:57:14.833: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename resourcequota 01/03/23 01:57:14.834
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:57:14.843
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:57:14.845
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
STEP: Creating a ResourceQuota 01/03/23 01:57:14.847
STEP: Getting a ResourceQuota 01/03/23 01:57:14.857
STEP: Listing all ResourceQuotas with LabelSelector 01/03/23 01:57:14.86
STEP: Patching the ResourceQuota 01/03/23 01:57:14.866
STEP: Deleting a Collection of ResourceQuotas 01/03/23 01:57:14.87
STEP: Verifying the deleted ResourceQuota 01/03/23 01:57:14.878
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan  3 01:57:14.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9376" for this suite. 01/03/23 01:57:14.886
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]","completed":44,"skipped":885,"failed":0}
------------------------------
• [0.055 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:57:14.833
    Jan  3 01:57:14.833: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename resourcequota 01/03/23 01:57:14.834
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:57:14.843
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:57:14.845
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:933
    STEP: Creating a ResourceQuota 01/03/23 01:57:14.847
    STEP: Getting a ResourceQuota 01/03/23 01:57:14.857
    STEP: Listing all ResourceQuotas with LabelSelector 01/03/23 01:57:14.86
    STEP: Patching the ResourceQuota 01/03/23 01:57:14.866
    STEP: Deleting a Collection of ResourceQuotas 01/03/23 01:57:14.87
    STEP: Verifying the deleted ResourceQuota 01/03/23 01:57:14.878
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan  3 01:57:14.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-9376" for this suite. 01/03/23 01:57:14.886
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:57:14.888
Jan  3 01:57:14.888: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename custom-resource-definition 01/03/23 01:57:14.889
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:57:14.927
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:57:14.928
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Jan  3 01:57:14.932: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  3 01:57:15.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8494" for this suite. 01/03/23 01:57:15.49
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","completed":45,"skipped":892,"failed":0}
------------------------------
• [0.617 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:57:14.888
    Jan  3 01:57:14.888: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename custom-resource-definition 01/03/23 01:57:14.889
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:57:14.927
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:57:14.928
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Jan  3 01:57:14.932: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  3 01:57:15.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-8494" for this suite. 01/03/23 01:57:15.49
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:57:15.506
Jan  3 01:57:15.506: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename replicaset 01/03/23 01:57:15.506
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:57:15.535
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:57:15.537
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Jan  3 01:57:15.564: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan  3 01:57:20.567: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/03/23 01:57:20.567
STEP: Scaling up "test-rs" replicaset  01/03/23 01:57:20.567
Jan  3 01:57:20.571: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 01/03/23 01:57:20.571
W0103 01:57:20.576641      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan  3 01:57:20.577: INFO: observed ReplicaSet test-rs in namespace replicaset-5653 with ReadyReplicas 1, AvailableReplicas 1
Jan  3 01:57:20.613: INFO: observed ReplicaSet test-rs in namespace replicaset-5653 with ReadyReplicas 1, AvailableReplicas 1
Jan  3 01:57:20.666: INFO: observed ReplicaSet test-rs in namespace replicaset-5653 with ReadyReplicas 1, AvailableReplicas 1
Jan  3 01:57:20.682: INFO: observed ReplicaSet test-rs in namespace replicaset-5653 with ReadyReplicas 1, AvailableReplicas 1
Jan  3 01:57:21.659: INFO: observed ReplicaSet test-rs in namespace replicaset-5653 with ReadyReplicas 2, AvailableReplicas 2
Jan  3 01:57:23.052: INFO: observed Replicaset test-rs in namespace replicaset-5653 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan  3 01:57:23.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5653" for this suite. 01/03/23 01:57:23.058
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","completed":46,"skipped":904,"failed":0}
------------------------------
• [SLOW TEST] [7.554 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:57:15.506
    Jan  3 01:57:15.506: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename replicaset 01/03/23 01:57:15.506
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:57:15.535
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:57:15.537
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Jan  3 01:57:15.564: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan  3 01:57:20.567: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/03/23 01:57:20.567
    STEP: Scaling up "test-rs" replicaset  01/03/23 01:57:20.567
    Jan  3 01:57:20.571: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 01/03/23 01:57:20.571
    W0103 01:57:20.576641      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan  3 01:57:20.577: INFO: observed ReplicaSet test-rs in namespace replicaset-5653 with ReadyReplicas 1, AvailableReplicas 1
    Jan  3 01:57:20.613: INFO: observed ReplicaSet test-rs in namespace replicaset-5653 with ReadyReplicas 1, AvailableReplicas 1
    Jan  3 01:57:20.666: INFO: observed ReplicaSet test-rs in namespace replicaset-5653 with ReadyReplicas 1, AvailableReplicas 1
    Jan  3 01:57:20.682: INFO: observed ReplicaSet test-rs in namespace replicaset-5653 with ReadyReplicas 1, AvailableReplicas 1
    Jan  3 01:57:21.659: INFO: observed ReplicaSet test-rs in namespace replicaset-5653 with ReadyReplicas 2, AvailableReplicas 2
    Jan  3 01:57:23.052: INFO: observed Replicaset test-rs in namespace replicaset-5653 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan  3 01:57:23.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-5653" for this suite. 01/03/23 01:57:23.058
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:57:23.061
Jan  3 01:57:23.061: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename kubelet-test 01/03/23 01:57:23.061
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:57:23.08
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:57:23.081
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan  3 01:57:23.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4513" for this suite. 01/03/23 01:57:23.156
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","completed":47,"skipped":909,"failed":0}
------------------------------
• [0.112 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:57:23.061
    Jan  3 01:57:23.061: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename kubelet-test 01/03/23 01:57:23.061
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:57:23.08
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:57:23.081
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan  3 01:57:23.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-4513" for this suite. 01/03/23 01:57:23.156
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:57:23.173
Jan  3 01:57:23.173: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename gc 01/03/23 01:57:23.174
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:57:23.188
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:57:23.19
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Jan  3 01:57:23.235: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"542a9cd4-f974-448c-ac49-12c71d41a392", Controller:(*bool)(0xc007a439b2), BlockOwnerDeletion:(*bool)(0xc007a439b3)}}
Jan  3 01:57:23.298: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"6f951e0e-221a-43f8-bca6-83268fef8e50", Controller:(*bool)(0xc007a43c5a), BlockOwnerDeletion:(*bool)(0xc007a43c5b)}}
Jan  3 01:57:23.327: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"98c43db1-feb8-4fb7-8c1b-f35be7c2e660", Controller:(*bool)(0xc007a43ef2), BlockOwnerDeletion:(*bool)(0xc007a43ef3)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan  3 01:57:28.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2468" for this suite. 01/03/23 01:57:28.39
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","completed":48,"skipped":912,"failed":0}
------------------------------
• [SLOW TEST] [5.228 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:57:23.173
    Jan  3 01:57:23.173: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename gc 01/03/23 01:57:23.174
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:57:23.188
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:57:23.19
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Jan  3 01:57:23.235: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"542a9cd4-f974-448c-ac49-12c71d41a392", Controller:(*bool)(0xc007a439b2), BlockOwnerDeletion:(*bool)(0xc007a439b3)}}
    Jan  3 01:57:23.298: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"6f951e0e-221a-43f8-bca6-83268fef8e50", Controller:(*bool)(0xc007a43c5a), BlockOwnerDeletion:(*bool)(0xc007a43c5b)}}
    Jan  3 01:57:23.327: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"98c43db1-feb8-4fb7-8c1b-f35be7c2e660", Controller:(*bool)(0xc007a43ef2), BlockOwnerDeletion:(*bool)(0xc007a43ef3)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan  3 01:57:28.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-2468" for this suite. 01/03/23 01:57:28.39
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:57:28.402
Jan  3 01:57:28.402: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename events 01/03/23 01:57:28.403
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:57:28.452
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:57:28.454
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 01/03/23 01:57:28.456
STEP: listing events in all namespaces 01/03/23 01:57:28.512
STEP: listing events in test namespace 01/03/23 01:57:28.534
STEP: listing events with field selection filtering on source 01/03/23 01:57:28.566
STEP: listing events with field selection filtering on reportingController 01/03/23 01:57:28.572
STEP: getting the test event 01/03/23 01:57:28.617
STEP: patching the test event 01/03/23 01:57:28.657
STEP: getting the test event 01/03/23 01:57:28.742
STEP: updating the test event 01/03/23 01:57:28.749
STEP: getting the test event 01/03/23 01:57:28.784
STEP: deleting the test event 01/03/23 01:57:28.807
STEP: listing events in all namespaces 01/03/23 01:57:28.858
STEP: listing events in test namespace 01/03/23 01:57:28.873
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Jan  3 01:57:28.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7938" for this suite. 01/03/23 01:57:28.883
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","completed":49,"skipped":912,"failed":0}
------------------------------
• [0.725 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:57:28.402
    Jan  3 01:57:28.402: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename events 01/03/23 01:57:28.403
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:57:28.452
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:57:28.454
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 01/03/23 01:57:28.456
    STEP: listing events in all namespaces 01/03/23 01:57:28.512
    STEP: listing events in test namespace 01/03/23 01:57:28.534
    STEP: listing events with field selection filtering on source 01/03/23 01:57:28.566
    STEP: listing events with field selection filtering on reportingController 01/03/23 01:57:28.572
    STEP: getting the test event 01/03/23 01:57:28.617
    STEP: patching the test event 01/03/23 01:57:28.657
    STEP: getting the test event 01/03/23 01:57:28.742
    STEP: updating the test event 01/03/23 01:57:28.749
    STEP: getting the test event 01/03/23 01:57:28.784
    STEP: deleting the test event 01/03/23 01:57:28.807
    STEP: listing events in all namespaces 01/03/23 01:57:28.858
    STEP: listing events in test namespace 01/03/23 01:57:28.873
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Jan  3 01:57:28.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-7938" for this suite. 01/03/23 01:57:28.883
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:57:29.127
Jan  3 01:57:29.127: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename job 01/03/23 01:57:29.128
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:57:29.163
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:57:29.165
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
STEP: Creating a job 01/03/23 01:57:29.167
STEP: Ensuring job reaches completions 01/03/23 01:57:29.172
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan  3 01:57:41.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1797" for this suite. 01/03/23 01:57:41.178
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","completed":50,"skipped":916,"failed":0}
------------------------------
• [SLOW TEST] [12.053 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:57:29.127
    Jan  3 01:57:29.127: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename job 01/03/23 01:57:29.128
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:57:29.163
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:57:29.165
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:254
    STEP: Creating a job 01/03/23 01:57:29.167
    STEP: Ensuring job reaches completions 01/03/23 01:57:29.172
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan  3 01:57:41.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-1797" for this suite. 01/03/23 01:57:41.178
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:57:41.181
Jan  3 01:57:41.181: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename services 01/03/23 01:57:41.182
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:57:41.192
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:57:41.194
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
STEP: creating service in namespace services-6589 01/03/23 01:57:41.196
STEP: creating service affinity-clusterip in namespace services-6589 01/03/23 01:57:41.196
STEP: creating replication controller affinity-clusterip in namespace services-6589 01/03/23 01:57:41.211
I0103 01:57:41.229095      23 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-6589, replica count: 3
I0103 01:57:44.280526      23 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  3 01:57:44.283: INFO: Creating new exec pod
Jan  3 01:57:44.287: INFO: Waiting up to 5m0s for pod "execpod-affinityqs86n" in namespace "services-6589" to be "running"
Jan  3 01:57:44.296: INFO: Pod "execpod-affinityqs86n": Phase="Pending", Reason="", readiness=false. Elapsed: 8.312881ms
Jan  3 01:57:46.302: INFO: Pod "execpod-affinityqs86n": Phase="Running", Reason="", readiness=true. Elapsed: 2.014139035s
Jan  3 01:57:46.302: INFO: Pod "execpod-affinityqs86n" satisfied condition "running"
Jan  3 01:57:47.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-6589 exec execpod-affinityqs86n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Jan  3 01:57:47.448: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jan  3 01:57:47.448: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  3 01:57:47.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-6589 exec execpod-affinityqs86n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.178.252 80'
Jan  3 01:57:47.587: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.178.252 80\nConnection to 10.96.178.252 80 port [tcp/http] succeeded!\n"
Jan  3 01:57:47.587: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  3 01:57:47.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-6589 exec execpod-affinityqs86n -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.178.252:80/ ; done'
Jan  3 01:57:47.777: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.178.252:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.178.252:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.178.252:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.178.252:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.178.252:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.178.252:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.178.252:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.178.252:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.178.252:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.178.252:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.178.252:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.178.252:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.178.252:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.178.252:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.178.252:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.178.252:80/\n"
Jan  3 01:57:47.777: INFO: stdout: "\naffinity-clusterip-dclpz\naffinity-clusterip-dclpz\naffinity-clusterip-dclpz\naffinity-clusterip-dclpz\naffinity-clusterip-dclpz\naffinity-clusterip-dclpz\naffinity-clusterip-dclpz\naffinity-clusterip-dclpz\naffinity-clusterip-dclpz\naffinity-clusterip-dclpz\naffinity-clusterip-dclpz\naffinity-clusterip-dclpz\naffinity-clusterip-dclpz\naffinity-clusterip-dclpz\naffinity-clusterip-dclpz\naffinity-clusterip-dclpz"
Jan  3 01:57:47.777: INFO: Received response from host: affinity-clusterip-dclpz
Jan  3 01:57:47.777: INFO: Received response from host: affinity-clusterip-dclpz
Jan  3 01:57:47.777: INFO: Received response from host: affinity-clusterip-dclpz
Jan  3 01:57:47.777: INFO: Received response from host: affinity-clusterip-dclpz
Jan  3 01:57:47.777: INFO: Received response from host: affinity-clusterip-dclpz
Jan  3 01:57:47.777: INFO: Received response from host: affinity-clusterip-dclpz
Jan  3 01:57:47.777: INFO: Received response from host: affinity-clusterip-dclpz
Jan  3 01:57:47.777: INFO: Received response from host: affinity-clusterip-dclpz
Jan  3 01:57:47.777: INFO: Received response from host: affinity-clusterip-dclpz
Jan  3 01:57:47.777: INFO: Received response from host: affinity-clusterip-dclpz
Jan  3 01:57:47.777: INFO: Received response from host: affinity-clusterip-dclpz
Jan  3 01:57:47.777: INFO: Received response from host: affinity-clusterip-dclpz
Jan  3 01:57:47.777: INFO: Received response from host: affinity-clusterip-dclpz
Jan  3 01:57:47.777: INFO: Received response from host: affinity-clusterip-dclpz
Jan  3 01:57:47.777: INFO: Received response from host: affinity-clusterip-dclpz
Jan  3 01:57:47.777: INFO: Received response from host: affinity-clusterip-dclpz
Jan  3 01:57:47.777: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-6589, will wait for the garbage collector to delete the pods 01/03/23 01:57:47.819
Jan  3 01:57:47.887: INFO: Deleting ReplicationController affinity-clusterip took: 2.731925ms
Jan  3 01:57:47.987: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.397677ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  3 01:57:50.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6589" for this suite. 01/03/23 01:57:50.235
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","completed":51,"skipped":940,"failed":0}
------------------------------
• [SLOW TEST] [9.056 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:57:41.181
    Jan  3 01:57:41.181: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename services 01/03/23 01:57:41.182
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:57:41.192
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:57:41.194
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2157
    STEP: creating service in namespace services-6589 01/03/23 01:57:41.196
    STEP: creating service affinity-clusterip in namespace services-6589 01/03/23 01:57:41.196
    STEP: creating replication controller affinity-clusterip in namespace services-6589 01/03/23 01:57:41.211
    I0103 01:57:41.229095      23 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-6589, replica count: 3
    I0103 01:57:44.280526      23 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  3 01:57:44.283: INFO: Creating new exec pod
    Jan  3 01:57:44.287: INFO: Waiting up to 5m0s for pod "execpod-affinityqs86n" in namespace "services-6589" to be "running"
    Jan  3 01:57:44.296: INFO: Pod "execpod-affinityqs86n": Phase="Pending", Reason="", readiness=false. Elapsed: 8.312881ms
    Jan  3 01:57:46.302: INFO: Pod "execpod-affinityqs86n": Phase="Running", Reason="", readiness=true. Elapsed: 2.014139035s
    Jan  3 01:57:46.302: INFO: Pod "execpod-affinityqs86n" satisfied condition "running"
    Jan  3 01:57:47.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-6589 exec execpod-affinityqs86n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
    Jan  3 01:57:47.448: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Jan  3 01:57:47.448: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  3 01:57:47.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-6589 exec execpod-affinityqs86n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.178.252 80'
    Jan  3 01:57:47.587: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.178.252 80\nConnection to 10.96.178.252 80 port [tcp/http] succeeded!\n"
    Jan  3 01:57:47.587: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  3 01:57:47.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-6589 exec execpod-affinityqs86n -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.178.252:80/ ; done'
    Jan  3 01:57:47.777: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.178.252:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.178.252:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.178.252:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.178.252:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.178.252:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.178.252:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.178.252:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.178.252:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.178.252:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.178.252:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.178.252:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.178.252:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.178.252:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.178.252:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.178.252:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.178.252:80/\n"
    Jan  3 01:57:47.777: INFO: stdout: "\naffinity-clusterip-dclpz\naffinity-clusterip-dclpz\naffinity-clusterip-dclpz\naffinity-clusterip-dclpz\naffinity-clusterip-dclpz\naffinity-clusterip-dclpz\naffinity-clusterip-dclpz\naffinity-clusterip-dclpz\naffinity-clusterip-dclpz\naffinity-clusterip-dclpz\naffinity-clusterip-dclpz\naffinity-clusterip-dclpz\naffinity-clusterip-dclpz\naffinity-clusterip-dclpz\naffinity-clusterip-dclpz\naffinity-clusterip-dclpz"
    Jan  3 01:57:47.777: INFO: Received response from host: affinity-clusterip-dclpz
    Jan  3 01:57:47.777: INFO: Received response from host: affinity-clusterip-dclpz
    Jan  3 01:57:47.777: INFO: Received response from host: affinity-clusterip-dclpz
    Jan  3 01:57:47.777: INFO: Received response from host: affinity-clusterip-dclpz
    Jan  3 01:57:47.777: INFO: Received response from host: affinity-clusterip-dclpz
    Jan  3 01:57:47.777: INFO: Received response from host: affinity-clusterip-dclpz
    Jan  3 01:57:47.777: INFO: Received response from host: affinity-clusterip-dclpz
    Jan  3 01:57:47.777: INFO: Received response from host: affinity-clusterip-dclpz
    Jan  3 01:57:47.777: INFO: Received response from host: affinity-clusterip-dclpz
    Jan  3 01:57:47.777: INFO: Received response from host: affinity-clusterip-dclpz
    Jan  3 01:57:47.777: INFO: Received response from host: affinity-clusterip-dclpz
    Jan  3 01:57:47.777: INFO: Received response from host: affinity-clusterip-dclpz
    Jan  3 01:57:47.777: INFO: Received response from host: affinity-clusterip-dclpz
    Jan  3 01:57:47.777: INFO: Received response from host: affinity-clusterip-dclpz
    Jan  3 01:57:47.777: INFO: Received response from host: affinity-clusterip-dclpz
    Jan  3 01:57:47.777: INFO: Received response from host: affinity-clusterip-dclpz
    Jan  3 01:57:47.777: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-6589, will wait for the garbage collector to delete the pods 01/03/23 01:57:47.819
    Jan  3 01:57:47.887: INFO: Deleting ReplicationController affinity-clusterip took: 2.731925ms
    Jan  3 01:57:47.987: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.397677ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  3 01:57:50.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6589" for this suite. 01/03/23 01:57:50.235
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:57:50.239
Jan  3 01:57:50.239: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename statefulset 01/03/23 01:57:50.24
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:57:50.268
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:57:50.27
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-7680 01/03/23 01:57:50.272
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
STEP: Creating a new StatefulSet 01/03/23 01:57:50.285
Jan  3 01:57:50.314: INFO: Found 0 stateful pods, waiting for 3
Jan  3 01:58:00.317: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 01:58:00.317: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 01:58:00.317: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 01:58:00.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=statefulset-7680 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  3 01:58:00.462: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  3 01:58:00.462: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  3 01:58:00.462: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 01/03/23 01:58:10.489
Jan  3 01:58:10.503: INFO: Updating stateful set ss2
STEP: Creating a new revision 01/03/23 01:58:10.503
STEP: Updating Pods in reverse ordinal order 01/03/23 01:58:20.538
Jan  3 01:58:20.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=statefulset-7680 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  3 01:58:20.669: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan  3 01:58:20.669: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  3 01:58:20.669: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  3 01:58:30.681: INFO: Waiting for StatefulSet statefulset-7680/ss2 to complete update
STEP: Rolling back to a previous revision 01/03/23 01:58:40.69
Jan  3 01:58:40.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=statefulset-7680 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  3 01:58:40.846: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  3 01:58:40.846: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  3 01:58:40.847: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  3 01:58:50.869: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 01/03/23 01:59:00.923
Jan  3 01:59:00.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=statefulset-7680 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  3 01:59:01.068: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan  3 01:59:01.068: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  3 01:59:01.068: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan  3 01:59:11.079: INFO: Deleting all statefulset in ns statefulset-7680
Jan  3 01:59:11.081: INFO: Scaling statefulset ss2 to 0
Jan  3 01:59:21.100: INFO: Waiting for statefulset status.replicas updated to 0
Jan  3 01:59:21.102: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan  3 01:59:21.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7680" for this suite. 01/03/23 01:59:21.115
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","completed":52,"skipped":984,"failed":0}
------------------------------
• [SLOW TEST] [90.879 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:304

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:57:50.239
    Jan  3 01:57:50.239: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename statefulset 01/03/23 01:57:50.24
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:57:50.268
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:57:50.27
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-7680 01/03/23 01:57:50.272
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:304
    STEP: Creating a new StatefulSet 01/03/23 01:57:50.285
    Jan  3 01:57:50.314: INFO: Found 0 stateful pods, waiting for 3
    Jan  3 01:58:00.317: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan  3 01:58:00.317: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan  3 01:58:00.317: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Jan  3 01:58:00.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=statefulset-7680 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  3 01:58:00.462: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  3 01:58:00.462: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  3 01:58:00.462: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 01/03/23 01:58:10.489
    Jan  3 01:58:10.503: INFO: Updating stateful set ss2
    STEP: Creating a new revision 01/03/23 01:58:10.503
    STEP: Updating Pods in reverse ordinal order 01/03/23 01:58:20.538
    Jan  3 01:58:20.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=statefulset-7680 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  3 01:58:20.669: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan  3 01:58:20.669: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan  3 01:58:20.669: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan  3 01:58:30.681: INFO: Waiting for StatefulSet statefulset-7680/ss2 to complete update
    STEP: Rolling back to a previous revision 01/03/23 01:58:40.69
    Jan  3 01:58:40.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=statefulset-7680 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  3 01:58:40.846: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  3 01:58:40.846: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  3 01:58:40.847: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  3 01:58:50.869: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 01/03/23 01:59:00.923
    Jan  3 01:59:00.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=statefulset-7680 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  3 01:59:01.068: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan  3 01:59:01.068: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan  3 01:59:01.068: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan  3 01:59:11.079: INFO: Deleting all statefulset in ns statefulset-7680
    Jan  3 01:59:11.081: INFO: Scaling statefulset ss2 to 0
    Jan  3 01:59:21.100: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  3 01:59:21.102: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan  3 01:59:21.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-7680" for this suite. 01/03/23 01:59:21.115
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:59:21.119
Jan  3 01:59:21.119: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename sched-pred 01/03/23 01:59:21.12
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:59:21.174
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:59:21.176
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan  3 01:59:21.178: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan  3 01:59:21.183: INFO: Waiting for terminating namespaces to be deleted...
Jan  3 01:59:21.185: INFO: 
Logging pods the apiserver thinks is on node cncf-master before test
Jan  3 01:59:21.197: INFO: postgresql-c6b9bb88b-lqpz6 from hyperauth started at 2023-01-03 01:35:41 +0000 UTC (1 container statuses recorded)
Jan  3 01:59:21.197: INFO: 	Container postgresql ready: true, restart count 0
Jan  3 01:59:21.197: INFO: calico-kube-controllers-58dbc876ff-rc9xm from kube-system started at 2022-12-28 00:23:10 +0000 UTC (1 container statuses recorded)
Jan  3 01:59:21.197: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan  3 01:59:21.197: INFO: calico-node-zxb7d from kube-system started at 2022-12-28 00:23:10 +0000 UTC (1 container statuses recorded)
Jan  3 01:59:21.197: INFO: 	Container calico-node ready: true, restart count 0
Jan  3 01:59:21.197: INFO: coredns-565d847f94-lwkgz from kube-system started at 2022-12-28 00:23:10 +0000 UTC (1 container statuses recorded)
Jan  3 01:59:21.197: INFO: 	Container coredns ready: true, restart count 0
Jan  3 01:59:21.197: INFO: coredns-565d847f94-tsqd7 from kube-system started at 2022-12-28 00:23:10 +0000 UTC (1 container statuses recorded)
Jan  3 01:59:21.197: INFO: 	Container coredns ready: true, restart count 0
Jan  3 01:59:21.197: INFO: etcd-cncf-master from kube-system started at 2022-12-28 00:22:57 +0000 UTC (1 container statuses recorded)
Jan  3 01:59:21.197: INFO: 	Container etcd ready: true, restart count 0
Jan  3 01:59:21.197: INFO: kube-apiserver-cncf-master from kube-system started at 2023-01-03 01:37:53 +0000 UTC (1 container statuses recorded)
Jan  3 01:59:21.197: INFO: 	Container kube-apiserver ready: true, restart count 0
Jan  3 01:59:21.197: INFO: kube-controller-manager-cncf-master from kube-system started at 2022-12-28 00:22:57 +0000 UTC (1 container statuses recorded)
Jan  3 01:59:21.197: INFO: 	Container kube-controller-manager ready: true, restart count 2
Jan  3 01:59:21.197: INFO: kube-proxy-qvggp from kube-system started at 2022-12-28 00:23:10 +0000 UTC (1 container statuses recorded)
Jan  3 01:59:21.197: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  3 01:59:21.197: INFO: kube-scheduler-cncf-master from kube-system started at 2022-12-28 00:22:56 +0000 UTC (1 container statuses recorded)
Jan  3 01:59:21.197: INFO: 	Container kube-scheduler ready: true, restart count 2
Jan  3 01:59:21.197: INFO: bin-falsedf72387d-7f6b-4c37-b8f2-4e9c34712960 from kubelet-test-4513 started at 2023-01-03 01:57:23 +0000 UTC (1 container statuses recorded)
Jan  3 01:59:21.197: INFO: 	Container bin-falsedf72387d-7f6b-4c37-b8f2-4e9c34712960 ready: false, restart count 0
Jan  3 01:59:21.197: INFO: csi-cephfsplugin-g2vdb from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (2 container statuses recorded)
Jan  3 01:59:21.197: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Jan  3 01:59:21.197: INFO: 	Container driver-registrar ready: true, restart count 0
Jan  3 01:59:21.197: INFO: csi-rbdplugin-krkxg from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (2 container statuses recorded)
Jan  3 01:59:21.197: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jan  3 01:59:21.197: INFO: 	Container driver-registrar ready: true, restart count 0
Jan  3 01:59:21.197: INFO: csi-rbdplugin-provisioner-6f689965cc-56vzc from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (5 container statuses recorded)
Jan  3 01:59:21.197: INFO: 	Container csi-attacher ready: true, restart count 0
Jan  3 01:59:21.197: INFO: 	Container csi-provisioner ready: true, restart count 0
Jan  3 01:59:21.197: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jan  3 01:59:21.197: INFO: 	Container csi-resizer ready: true, restart count 0
Jan  3 01:59:21.197: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jan  3 01:59:21.197: INFO: rook-ceph-crashcollector-cncf-master-65b6767fc8-xjlw5 from rook-ceph started at 2023-01-02 01:35:37 +0000 UTC (1 container statuses recorded)
Jan  3 01:59:21.197: INFO: 	Container ceph-crash ready: true, restart count 0
Jan  3 01:59:21.197: INFO: rook-ceph-mds-myfs-b-65b48b8c9d-2w82f from rook-ceph started at 2023-01-02 01:35:37 +0000 UTC (1 container statuses recorded)
Jan  3 01:59:21.197: INFO: 	Container mds ready: true, restart count 0
Jan  3 01:59:21.197: INFO: rook-ceph-mgr-b-856c746f6-wkx97 from rook-ceph started at 2022-12-29 08:25:47 +0000 UTC (2 container statuses recorded)
Jan  3 01:59:21.197: INFO: 	Container mgr ready: true, restart count 0
Jan  3 01:59:21.197: INFO: 	Container watch-active ready: true, restart count 0
Jan  3 01:59:21.197: INFO: rook-ceph-mon-b-66ffc4d4d9-sm6t5 from rook-ceph started at 2022-12-29 08:25:22 +0000 UTC (1 container statuses recorded)
Jan  3 01:59:21.197: INFO: 	Container mon ready: true, restart count 0
Jan  3 01:59:21.197: INFO: rook-ceph-osd-1-58d74d96db-9ww4l from rook-ceph started at 2023-01-02 01:35:03 +0000 UTC (1 container statuses recorded)
Jan  3 01:59:21.197: INFO: 	Container osd ready: true, restart count 0
Jan  3 01:59:21.197: INFO: rook-ceph-osd-prepare-cncf-master-j6dcd from rook-ceph started at 2023-01-03 01:40:53 +0000 UTC (1 container statuses recorded)
Jan  3 01:59:21.197: INFO: 	Container provision ready: false, restart count 0
Jan  3 01:59:21.197: INFO: sonobuoy-systemd-logs-daemon-set-58995b31e0564399-fxv6h from sonobuoy started at 2023-01-03 01:46:49 +0000 UTC (2 container statuses recorded)
Jan  3 01:59:21.197: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  3 01:59:21.197: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  3 01:59:21.197: INFO: 
Logging pods the apiserver thinks is on node cncf-node1 before test
Jan  3 01:59:21.209: INFO: traefik-7c456f74d8-b7b6n from api-gateway-system started at 2023-01-03 01:41:21 +0000 UTC (1 container statuses recorded)
Jan  3 01:59:21.209: INFO: 	Container traefik ready: true, restart count 0
Jan  3 01:59:21.209: INFO: cert-manager-754886db57-z7pm7 from cert-manager started at 2023-01-03 01:35:12 +0000 UTC (1 container statuses recorded)
Jan  3 01:59:21.209: INFO: 	Container cert-manager ready: true, restart count 1
Jan  3 01:59:21.209: INFO: cert-manager-cainjector-66b4c7c78d-vc87n from cert-manager started at 2023-01-03 01:35:12 +0000 UTC (1 container statuses recorded)
Jan  3 01:59:21.209: INFO: 	Container cert-manager ready: true, restart count 4
Jan  3 01:59:21.209: INFO: cert-manager-webhook-78cdb5fdf-frlxh from cert-manager started at 2023-01-03 01:35:12 +0000 UTC (1 container statuses recorded)
Jan  3 01:59:21.209: INFO: 	Container cert-manager ready: true, restart count 0
Jan  3 01:59:21.209: INFO: console-59c484c98b-mcqpt from console-system started at 2023-01-03 01:41:24 +0000 UTC (1 container statuses recorded)
Jan  3 01:59:21.210: INFO: 	Container console ready: true, restart count 0
Jan  3 01:59:21.210: INFO: hyperauth-5cbb4d58b8-qpc2d from hyperauth started at 2023-01-03 01:37:01 +0000 UTC (1 container statuses recorded)
Jan  3 01:59:21.210: INFO: 	Container hyperauth ready: true, restart count 0
Jan  3 01:59:21.210: INFO: hypercloud5-api-server-8c944885b-mk6vb from hypercloud5-system started at 2023-01-03 01:38:50 +0000 UTC (1 container statuses recorded)
Jan  3 01:59:21.210: INFO: 	Container hypercloud5-api-server ready: true, restart count 4
Jan  3 01:59:21.210: INFO: timescaledb-0 from hypercloud5-system started at 2023-01-03 01:38:50 +0000 UTC (1 container statuses recorded)
Jan  3 01:59:21.210: INFO: 	Container timescaledb ready: true, restart count 0
Jan  3 01:59:21.210: INFO: calico-node-9p9ff from kube-system started at 2022-12-28 00:54:44 +0000 UTC (1 container statuses recorded)
Jan  3 01:59:21.210: INFO: 	Container calico-node ready: true, restart count 0
Jan  3 01:59:21.210: INFO: kube-proxy-jplgg from kube-system started at 2022-12-28 00:54:44 +0000 UTC (1 container statuses recorded)
Jan  3 01:59:21.210: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  3 01:59:21.210: INFO: snapshot-controller-545d7f8994-vg2ld from kube-system started at 2022-12-29 08:24:10 +0000 UTC (1 container statuses recorded)
Jan  3 01:59:21.210: INFO: 	Container snapshot-controller ready: true, restart count 3
Jan  3 01:59:21.210: INFO: csi-cephfsplugin-provisioner-69f54fb566-jxcpx from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (5 container statuses recorded)
Jan  3 01:59:21.210: INFO: 	Container csi-attacher ready: true, restart count 0
Jan  3 01:59:21.210: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Jan  3 01:59:21.210: INFO: 	Container csi-provisioner ready: true, restart count 0
Jan  3 01:59:21.210: INFO: 	Container csi-resizer ready: true, restart count 0
Jan  3 01:59:21.210: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jan  3 01:59:21.210: INFO: csi-cephfsplugin-stnqm from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (2 container statuses recorded)
Jan  3 01:59:21.210: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Jan  3 01:59:21.210: INFO: 	Container driver-registrar ready: true, restart count 0
Jan  3 01:59:21.210: INFO: csi-rbdplugin-provisioner-6f689965cc-wl454 from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (5 container statuses recorded)
Jan  3 01:59:21.210: INFO: 	Container csi-attacher ready: true, restart count 0
Jan  3 01:59:21.210: INFO: 	Container csi-provisioner ready: true, restart count 0
Jan  3 01:59:21.210: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jan  3 01:59:21.210: INFO: 	Container csi-resizer ready: true, restart count 0
Jan  3 01:59:21.210: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jan  3 01:59:21.210: INFO: csi-rbdplugin-t2hxd from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (2 container statuses recorded)
Jan  3 01:59:21.210: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jan  3 01:59:21.210: INFO: 	Container driver-registrar ready: true, restart count 0
Jan  3 01:59:21.210: INFO: rook-ceph-crashcollector-cncf-node1-846d9dbb97-scfg5 from rook-ceph started at 2023-01-02 01:35:36 +0000 UTC (1 container statuses recorded)
Jan  3 01:59:21.210: INFO: 	Container ceph-crash ready: true, restart count 0
Jan  3 01:59:21.210: INFO: rook-ceph-mds-myfs-a-58f7884986-6th9v from rook-ceph started at 2023-01-02 01:35:36 +0000 UTC (1 container statuses recorded)
Jan  3 01:59:21.210: INFO: 	Container mds ready: true, restart count 0
Jan  3 01:59:21.210: INFO: rook-ceph-mgr-a-7949c65bd-kjzgc from rook-ceph started at 2022-12-29 08:25:47 +0000 UTC (2 container statuses recorded)
Jan  3 01:59:21.210: INFO: 	Container mgr ready: true, restart count 0
Jan  3 01:59:21.210: INFO: 	Container watch-active ready: true, restart count 0
Jan  3 01:59:21.210: INFO: rook-ceph-mon-a-bdd44bcfb-s5hjx from rook-ceph started at 2022-12-29 08:24:58 +0000 UTC (1 container statuses recorded)
Jan  3 01:59:21.210: INFO: 	Container mon ready: true, restart count 0
Jan  3 01:59:21.210: INFO: rook-ceph-operator-7f56d78d46-z2j6r from rook-ceph started at 2023-01-02 01:29:14 +0000 UTC (1 container statuses recorded)
Jan  3 01:59:21.210: INFO: 	Container rook-ceph-operator ready: true, restart count 0
Jan  3 01:59:21.210: INFO: rook-ceph-osd-2-5d8bc49787-jd84z from rook-ceph started at 2023-01-02 01:35:03 +0000 UTC (1 container statuses recorded)
Jan  3 01:59:21.210: INFO: 	Container osd ready: true, restart count 0
Jan  3 01:59:21.210: INFO: rook-ceph-osd-prepare-cncf-node1-89n8c from rook-ceph started at 2023-01-03 01:40:56 +0000 UTC (1 container statuses recorded)
Jan  3 01:59:21.210: INFO: 	Container provision ready: false, restart count 0
Jan  3 01:59:21.210: INFO: rook-ceph-tools-75fb64866c-bcxp7 from rook-ceph started at 2022-12-30 02:06:28 +0000 UTC (1 container statuses recorded)
Jan  3 01:59:21.210: INFO: 	Container rook-ceph-tools ready: true, restart count 0
Jan  3 01:59:21.210: INFO: sonobuoy from sonobuoy started at 2023-01-03 01:46:49 +0000 UTC (1 container statuses recorded)
Jan  3 01:59:21.210: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan  3 01:59:21.210: INFO: sonobuoy-e2e-job-a15b13ec9aa249f2 from sonobuoy started at 2023-01-03 01:46:49 +0000 UTC (2 container statuses recorded)
Jan  3 01:59:21.210: INFO: 	Container e2e ready: true, restart count 0
Jan  3 01:59:21.210: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  3 01:59:21.210: INFO: sonobuoy-systemd-logs-daemon-set-58995b31e0564399-gndlm from sonobuoy started at 2023-01-03 01:46:49 +0000 UTC (2 container statuses recorded)
Jan  3 01:59:21.210: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  3 01:59:21.210: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  3 01:59:21.210: INFO: 
Logging pods the apiserver thinks is on node cncf-node2 before test
Jan  3 01:59:21.218: INFO: hyperauth-5cbb4d58b8-pqhc5 from hyperauth started at 2023-01-03 01:37:01 +0000 UTC (1 container statuses recorded)
Jan  3 01:59:21.218: INFO: 	Container hyperauth ready: true, restart count 0
Jan  3 01:59:21.218: INFO: calico-node-5f5qs from kube-system started at 2022-12-29 05:30:03 +0000 UTC (1 container statuses recorded)
Jan  3 01:59:21.218: INFO: 	Container calico-node ready: true, restart count 0
Jan  3 01:59:21.218: INFO: kube-proxy-ddlsm from kube-system started at 2022-12-29 05:30:03 +0000 UTC (1 container statuses recorded)
Jan  3 01:59:21.218: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  3 01:59:21.218: INFO: snapshot-controller-545d7f8994-qgm6l from kube-system started at 2022-12-29 08:24:10 +0000 UTC (1 container statuses recorded)
Jan  3 01:59:21.218: INFO: 	Container snapshot-controller ready: true, restart count 0
Jan  3 01:59:21.218: INFO: csi-cephfsplugin-hxtjf from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (2 container statuses recorded)
Jan  3 01:59:21.218: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Jan  3 01:59:21.218: INFO: 	Container driver-registrar ready: true, restart count 0
Jan  3 01:59:21.218: INFO: csi-cephfsplugin-provisioner-69f54fb566-xhc7t from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (5 container statuses recorded)
Jan  3 01:59:21.218: INFO: 	Container csi-attacher ready: true, restart count 0
Jan  3 01:59:21.218: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Jan  3 01:59:21.218: INFO: 	Container csi-provisioner ready: true, restart count 0
Jan  3 01:59:21.218: INFO: 	Container csi-resizer ready: true, restart count 0
Jan  3 01:59:21.218: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jan  3 01:59:21.218: INFO: csi-rbdplugin-qk6jk from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (2 container statuses recorded)
Jan  3 01:59:21.218: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jan  3 01:59:21.218: INFO: 	Container driver-registrar ready: true, restart count 0
Jan  3 01:59:21.218: INFO: rook-ceph-crashcollector-cncf-node2-898ff4558-twdv7 from rook-ceph started at 2022-12-29 08:26:03 +0000 UTC (1 container statuses recorded)
Jan  3 01:59:21.218: INFO: 	Container ceph-crash ready: true, restart count 0
Jan  3 01:59:21.218: INFO: rook-ceph-mon-c-6c468d6b45-m7rqj from rook-ceph started at 2022-12-29 08:25:33 +0000 UTC (1 container statuses recorded)
Jan  3 01:59:21.218: INFO: 	Container mon ready: true, restart count 0
Jan  3 01:59:21.218: INFO: rook-ceph-osd-0-7d877c8d99-hq5fw from rook-ceph started at 2023-01-02 01:35:02 +0000 UTC (1 container statuses recorded)
Jan  3 01:59:21.218: INFO: 	Container osd ready: true, restart count 0
Jan  3 01:59:21.218: INFO: rook-ceph-osd-prepare-cncf-node2-rnwb6 from rook-ceph started at 2023-01-03 01:40:59 +0000 UTC (1 container statuses recorded)
Jan  3 01:59:21.218: INFO: 	Container provision ready: false, restart count 0
Jan  3 01:59:21.218: INFO: sonobuoy-systemd-logs-daemon-set-58995b31e0564399-wjpkt from sonobuoy started at 2023-01-03 01:46:49 +0000 UTC (2 container statuses recorded)
Jan  3 01:59:21.218: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  3 01:59:21.218: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
STEP: Trying to schedule Pod with nonempty NodeSelector. 01/03/23 01:59:21.218
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1736a9f051d211b0], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling.] 01/03/23 01:59:21.267
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jan  3 01:59:22.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2977" for this suite. 01/03/23 01:59:22.273
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","completed":53,"skipped":1010,"failed":0}
------------------------------
• [1.156 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:59:21.119
    Jan  3 01:59:21.119: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename sched-pred 01/03/23 01:59:21.12
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:59:21.174
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:59:21.176
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jan  3 01:59:21.178: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan  3 01:59:21.183: INFO: Waiting for terminating namespaces to be deleted...
    Jan  3 01:59:21.185: INFO: 
    Logging pods the apiserver thinks is on node cncf-master before test
    Jan  3 01:59:21.197: INFO: postgresql-c6b9bb88b-lqpz6 from hyperauth started at 2023-01-03 01:35:41 +0000 UTC (1 container statuses recorded)
    Jan  3 01:59:21.197: INFO: 	Container postgresql ready: true, restart count 0
    Jan  3 01:59:21.197: INFO: calico-kube-controllers-58dbc876ff-rc9xm from kube-system started at 2022-12-28 00:23:10 +0000 UTC (1 container statuses recorded)
    Jan  3 01:59:21.197: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Jan  3 01:59:21.197: INFO: calico-node-zxb7d from kube-system started at 2022-12-28 00:23:10 +0000 UTC (1 container statuses recorded)
    Jan  3 01:59:21.197: INFO: 	Container calico-node ready: true, restart count 0
    Jan  3 01:59:21.197: INFO: coredns-565d847f94-lwkgz from kube-system started at 2022-12-28 00:23:10 +0000 UTC (1 container statuses recorded)
    Jan  3 01:59:21.197: INFO: 	Container coredns ready: true, restart count 0
    Jan  3 01:59:21.197: INFO: coredns-565d847f94-tsqd7 from kube-system started at 2022-12-28 00:23:10 +0000 UTC (1 container statuses recorded)
    Jan  3 01:59:21.197: INFO: 	Container coredns ready: true, restart count 0
    Jan  3 01:59:21.197: INFO: etcd-cncf-master from kube-system started at 2022-12-28 00:22:57 +0000 UTC (1 container statuses recorded)
    Jan  3 01:59:21.197: INFO: 	Container etcd ready: true, restart count 0
    Jan  3 01:59:21.197: INFO: kube-apiserver-cncf-master from kube-system started at 2023-01-03 01:37:53 +0000 UTC (1 container statuses recorded)
    Jan  3 01:59:21.197: INFO: 	Container kube-apiserver ready: true, restart count 0
    Jan  3 01:59:21.197: INFO: kube-controller-manager-cncf-master from kube-system started at 2022-12-28 00:22:57 +0000 UTC (1 container statuses recorded)
    Jan  3 01:59:21.197: INFO: 	Container kube-controller-manager ready: true, restart count 2
    Jan  3 01:59:21.197: INFO: kube-proxy-qvggp from kube-system started at 2022-12-28 00:23:10 +0000 UTC (1 container statuses recorded)
    Jan  3 01:59:21.197: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  3 01:59:21.197: INFO: kube-scheduler-cncf-master from kube-system started at 2022-12-28 00:22:56 +0000 UTC (1 container statuses recorded)
    Jan  3 01:59:21.197: INFO: 	Container kube-scheduler ready: true, restart count 2
    Jan  3 01:59:21.197: INFO: bin-falsedf72387d-7f6b-4c37-b8f2-4e9c34712960 from kubelet-test-4513 started at 2023-01-03 01:57:23 +0000 UTC (1 container statuses recorded)
    Jan  3 01:59:21.197: INFO: 	Container bin-falsedf72387d-7f6b-4c37-b8f2-4e9c34712960 ready: false, restart count 0
    Jan  3 01:59:21.197: INFO: csi-cephfsplugin-g2vdb from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (2 container statuses recorded)
    Jan  3 01:59:21.197: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
    Jan  3 01:59:21.197: INFO: 	Container driver-registrar ready: true, restart count 0
    Jan  3 01:59:21.197: INFO: csi-rbdplugin-krkxg from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (2 container statuses recorded)
    Jan  3 01:59:21.197: INFO: 	Container csi-rbdplugin ready: true, restart count 0
    Jan  3 01:59:21.197: INFO: 	Container driver-registrar ready: true, restart count 0
    Jan  3 01:59:21.197: INFO: csi-rbdplugin-provisioner-6f689965cc-56vzc from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (5 container statuses recorded)
    Jan  3 01:59:21.197: INFO: 	Container csi-attacher ready: true, restart count 0
    Jan  3 01:59:21.197: INFO: 	Container csi-provisioner ready: true, restart count 0
    Jan  3 01:59:21.197: INFO: 	Container csi-rbdplugin ready: true, restart count 0
    Jan  3 01:59:21.197: INFO: 	Container csi-resizer ready: true, restart count 0
    Jan  3 01:59:21.197: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Jan  3 01:59:21.197: INFO: rook-ceph-crashcollector-cncf-master-65b6767fc8-xjlw5 from rook-ceph started at 2023-01-02 01:35:37 +0000 UTC (1 container statuses recorded)
    Jan  3 01:59:21.197: INFO: 	Container ceph-crash ready: true, restart count 0
    Jan  3 01:59:21.197: INFO: rook-ceph-mds-myfs-b-65b48b8c9d-2w82f from rook-ceph started at 2023-01-02 01:35:37 +0000 UTC (1 container statuses recorded)
    Jan  3 01:59:21.197: INFO: 	Container mds ready: true, restart count 0
    Jan  3 01:59:21.197: INFO: rook-ceph-mgr-b-856c746f6-wkx97 from rook-ceph started at 2022-12-29 08:25:47 +0000 UTC (2 container statuses recorded)
    Jan  3 01:59:21.197: INFO: 	Container mgr ready: true, restart count 0
    Jan  3 01:59:21.197: INFO: 	Container watch-active ready: true, restart count 0
    Jan  3 01:59:21.197: INFO: rook-ceph-mon-b-66ffc4d4d9-sm6t5 from rook-ceph started at 2022-12-29 08:25:22 +0000 UTC (1 container statuses recorded)
    Jan  3 01:59:21.197: INFO: 	Container mon ready: true, restart count 0
    Jan  3 01:59:21.197: INFO: rook-ceph-osd-1-58d74d96db-9ww4l from rook-ceph started at 2023-01-02 01:35:03 +0000 UTC (1 container statuses recorded)
    Jan  3 01:59:21.197: INFO: 	Container osd ready: true, restart count 0
    Jan  3 01:59:21.197: INFO: rook-ceph-osd-prepare-cncf-master-j6dcd from rook-ceph started at 2023-01-03 01:40:53 +0000 UTC (1 container statuses recorded)
    Jan  3 01:59:21.197: INFO: 	Container provision ready: false, restart count 0
    Jan  3 01:59:21.197: INFO: sonobuoy-systemd-logs-daemon-set-58995b31e0564399-fxv6h from sonobuoy started at 2023-01-03 01:46:49 +0000 UTC (2 container statuses recorded)
    Jan  3 01:59:21.197: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  3 01:59:21.197: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  3 01:59:21.197: INFO: 
    Logging pods the apiserver thinks is on node cncf-node1 before test
    Jan  3 01:59:21.209: INFO: traefik-7c456f74d8-b7b6n from api-gateway-system started at 2023-01-03 01:41:21 +0000 UTC (1 container statuses recorded)
    Jan  3 01:59:21.209: INFO: 	Container traefik ready: true, restart count 0
    Jan  3 01:59:21.209: INFO: cert-manager-754886db57-z7pm7 from cert-manager started at 2023-01-03 01:35:12 +0000 UTC (1 container statuses recorded)
    Jan  3 01:59:21.209: INFO: 	Container cert-manager ready: true, restart count 1
    Jan  3 01:59:21.209: INFO: cert-manager-cainjector-66b4c7c78d-vc87n from cert-manager started at 2023-01-03 01:35:12 +0000 UTC (1 container statuses recorded)
    Jan  3 01:59:21.209: INFO: 	Container cert-manager ready: true, restart count 4
    Jan  3 01:59:21.209: INFO: cert-manager-webhook-78cdb5fdf-frlxh from cert-manager started at 2023-01-03 01:35:12 +0000 UTC (1 container statuses recorded)
    Jan  3 01:59:21.209: INFO: 	Container cert-manager ready: true, restart count 0
    Jan  3 01:59:21.209: INFO: console-59c484c98b-mcqpt from console-system started at 2023-01-03 01:41:24 +0000 UTC (1 container statuses recorded)
    Jan  3 01:59:21.210: INFO: 	Container console ready: true, restart count 0
    Jan  3 01:59:21.210: INFO: hyperauth-5cbb4d58b8-qpc2d from hyperauth started at 2023-01-03 01:37:01 +0000 UTC (1 container statuses recorded)
    Jan  3 01:59:21.210: INFO: 	Container hyperauth ready: true, restart count 0
    Jan  3 01:59:21.210: INFO: hypercloud5-api-server-8c944885b-mk6vb from hypercloud5-system started at 2023-01-03 01:38:50 +0000 UTC (1 container statuses recorded)
    Jan  3 01:59:21.210: INFO: 	Container hypercloud5-api-server ready: true, restart count 4
    Jan  3 01:59:21.210: INFO: timescaledb-0 from hypercloud5-system started at 2023-01-03 01:38:50 +0000 UTC (1 container statuses recorded)
    Jan  3 01:59:21.210: INFO: 	Container timescaledb ready: true, restart count 0
    Jan  3 01:59:21.210: INFO: calico-node-9p9ff from kube-system started at 2022-12-28 00:54:44 +0000 UTC (1 container statuses recorded)
    Jan  3 01:59:21.210: INFO: 	Container calico-node ready: true, restart count 0
    Jan  3 01:59:21.210: INFO: kube-proxy-jplgg from kube-system started at 2022-12-28 00:54:44 +0000 UTC (1 container statuses recorded)
    Jan  3 01:59:21.210: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  3 01:59:21.210: INFO: snapshot-controller-545d7f8994-vg2ld from kube-system started at 2022-12-29 08:24:10 +0000 UTC (1 container statuses recorded)
    Jan  3 01:59:21.210: INFO: 	Container snapshot-controller ready: true, restart count 3
    Jan  3 01:59:21.210: INFO: csi-cephfsplugin-provisioner-69f54fb566-jxcpx from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (5 container statuses recorded)
    Jan  3 01:59:21.210: INFO: 	Container csi-attacher ready: true, restart count 0
    Jan  3 01:59:21.210: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
    Jan  3 01:59:21.210: INFO: 	Container csi-provisioner ready: true, restart count 0
    Jan  3 01:59:21.210: INFO: 	Container csi-resizer ready: true, restart count 0
    Jan  3 01:59:21.210: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Jan  3 01:59:21.210: INFO: csi-cephfsplugin-stnqm from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (2 container statuses recorded)
    Jan  3 01:59:21.210: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
    Jan  3 01:59:21.210: INFO: 	Container driver-registrar ready: true, restart count 0
    Jan  3 01:59:21.210: INFO: csi-rbdplugin-provisioner-6f689965cc-wl454 from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (5 container statuses recorded)
    Jan  3 01:59:21.210: INFO: 	Container csi-attacher ready: true, restart count 0
    Jan  3 01:59:21.210: INFO: 	Container csi-provisioner ready: true, restart count 0
    Jan  3 01:59:21.210: INFO: 	Container csi-rbdplugin ready: true, restart count 0
    Jan  3 01:59:21.210: INFO: 	Container csi-resizer ready: true, restart count 0
    Jan  3 01:59:21.210: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Jan  3 01:59:21.210: INFO: csi-rbdplugin-t2hxd from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (2 container statuses recorded)
    Jan  3 01:59:21.210: INFO: 	Container csi-rbdplugin ready: true, restart count 0
    Jan  3 01:59:21.210: INFO: 	Container driver-registrar ready: true, restart count 0
    Jan  3 01:59:21.210: INFO: rook-ceph-crashcollector-cncf-node1-846d9dbb97-scfg5 from rook-ceph started at 2023-01-02 01:35:36 +0000 UTC (1 container statuses recorded)
    Jan  3 01:59:21.210: INFO: 	Container ceph-crash ready: true, restart count 0
    Jan  3 01:59:21.210: INFO: rook-ceph-mds-myfs-a-58f7884986-6th9v from rook-ceph started at 2023-01-02 01:35:36 +0000 UTC (1 container statuses recorded)
    Jan  3 01:59:21.210: INFO: 	Container mds ready: true, restart count 0
    Jan  3 01:59:21.210: INFO: rook-ceph-mgr-a-7949c65bd-kjzgc from rook-ceph started at 2022-12-29 08:25:47 +0000 UTC (2 container statuses recorded)
    Jan  3 01:59:21.210: INFO: 	Container mgr ready: true, restart count 0
    Jan  3 01:59:21.210: INFO: 	Container watch-active ready: true, restart count 0
    Jan  3 01:59:21.210: INFO: rook-ceph-mon-a-bdd44bcfb-s5hjx from rook-ceph started at 2022-12-29 08:24:58 +0000 UTC (1 container statuses recorded)
    Jan  3 01:59:21.210: INFO: 	Container mon ready: true, restart count 0
    Jan  3 01:59:21.210: INFO: rook-ceph-operator-7f56d78d46-z2j6r from rook-ceph started at 2023-01-02 01:29:14 +0000 UTC (1 container statuses recorded)
    Jan  3 01:59:21.210: INFO: 	Container rook-ceph-operator ready: true, restart count 0
    Jan  3 01:59:21.210: INFO: rook-ceph-osd-2-5d8bc49787-jd84z from rook-ceph started at 2023-01-02 01:35:03 +0000 UTC (1 container statuses recorded)
    Jan  3 01:59:21.210: INFO: 	Container osd ready: true, restart count 0
    Jan  3 01:59:21.210: INFO: rook-ceph-osd-prepare-cncf-node1-89n8c from rook-ceph started at 2023-01-03 01:40:56 +0000 UTC (1 container statuses recorded)
    Jan  3 01:59:21.210: INFO: 	Container provision ready: false, restart count 0
    Jan  3 01:59:21.210: INFO: rook-ceph-tools-75fb64866c-bcxp7 from rook-ceph started at 2022-12-30 02:06:28 +0000 UTC (1 container statuses recorded)
    Jan  3 01:59:21.210: INFO: 	Container rook-ceph-tools ready: true, restart count 0
    Jan  3 01:59:21.210: INFO: sonobuoy from sonobuoy started at 2023-01-03 01:46:49 +0000 UTC (1 container statuses recorded)
    Jan  3 01:59:21.210: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan  3 01:59:21.210: INFO: sonobuoy-e2e-job-a15b13ec9aa249f2 from sonobuoy started at 2023-01-03 01:46:49 +0000 UTC (2 container statuses recorded)
    Jan  3 01:59:21.210: INFO: 	Container e2e ready: true, restart count 0
    Jan  3 01:59:21.210: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  3 01:59:21.210: INFO: sonobuoy-systemd-logs-daemon-set-58995b31e0564399-gndlm from sonobuoy started at 2023-01-03 01:46:49 +0000 UTC (2 container statuses recorded)
    Jan  3 01:59:21.210: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  3 01:59:21.210: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  3 01:59:21.210: INFO: 
    Logging pods the apiserver thinks is on node cncf-node2 before test
    Jan  3 01:59:21.218: INFO: hyperauth-5cbb4d58b8-pqhc5 from hyperauth started at 2023-01-03 01:37:01 +0000 UTC (1 container statuses recorded)
    Jan  3 01:59:21.218: INFO: 	Container hyperauth ready: true, restart count 0
    Jan  3 01:59:21.218: INFO: calico-node-5f5qs from kube-system started at 2022-12-29 05:30:03 +0000 UTC (1 container statuses recorded)
    Jan  3 01:59:21.218: INFO: 	Container calico-node ready: true, restart count 0
    Jan  3 01:59:21.218: INFO: kube-proxy-ddlsm from kube-system started at 2022-12-29 05:30:03 +0000 UTC (1 container statuses recorded)
    Jan  3 01:59:21.218: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  3 01:59:21.218: INFO: snapshot-controller-545d7f8994-qgm6l from kube-system started at 2022-12-29 08:24:10 +0000 UTC (1 container statuses recorded)
    Jan  3 01:59:21.218: INFO: 	Container snapshot-controller ready: true, restart count 0
    Jan  3 01:59:21.218: INFO: csi-cephfsplugin-hxtjf from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (2 container statuses recorded)
    Jan  3 01:59:21.218: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
    Jan  3 01:59:21.218: INFO: 	Container driver-registrar ready: true, restart count 0
    Jan  3 01:59:21.218: INFO: csi-cephfsplugin-provisioner-69f54fb566-xhc7t from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (5 container statuses recorded)
    Jan  3 01:59:21.218: INFO: 	Container csi-attacher ready: true, restart count 0
    Jan  3 01:59:21.218: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
    Jan  3 01:59:21.218: INFO: 	Container csi-provisioner ready: true, restart count 0
    Jan  3 01:59:21.218: INFO: 	Container csi-resizer ready: true, restart count 0
    Jan  3 01:59:21.218: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Jan  3 01:59:21.218: INFO: csi-rbdplugin-qk6jk from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (2 container statuses recorded)
    Jan  3 01:59:21.218: INFO: 	Container csi-rbdplugin ready: true, restart count 0
    Jan  3 01:59:21.218: INFO: 	Container driver-registrar ready: true, restart count 0
    Jan  3 01:59:21.218: INFO: rook-ceph-crashcollector-cncf-node2-898ff4558-twdv7 from rook-ceph started at 2022-12-29 08:26:03 +0000 UTC (1 container statuses recorded)
    Jan  3 01:59:21.218: INFO: 	Container ceph-crash ready: true, restart count 0
    Jan  3 01:59:21.218: INFO: rook-ceph-mon-c-6c468d6b45-m7rqj from rook-ceph started at 2022-12-29 08:25:33 +0000 UTC (1 container statuses recorded)
    Jan  3 01:59:21.218: INFO: 	Container mon ready: true, restart count 0
    Jan  3 01:59:21.218: INFO: rook-ceph-osd-0-7d877c8d99-hq5fw from rook-ceph started at 2023-01-02 01:35:02 +0000 UTC (1 container statuses recorded)
    Jan  3 01:59:21.218: INFO: 	Container osd ready: true, restart count 0
    Jan  3 01:59:21.218: INFO: rook-ceph-osd-prepare-cncf-node2-rnwb6 from rook-ceph started at 2023-01-03 01:40:59 +0000 UTC (1 container statuses recorded)
    Jan  3 01:59:21.218: INFO: 	Container provision ready: false, restart count 0
    Jan  3 01:59:21.218: INFO: sonobuoy-systemd-logs-daemon-set-58995b31e0564399-wjpkt from sonobuoy started at 2023-01-03 01:46:49 +0000 UTC (2 container statuses recorded)
    Jan  3 01:59:21.218: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  3 01:59:21.218: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:438
    STEP: Trying to schedule Pod with nonempty NodeSelector. 01/03/23 01:59:21.218
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.1736a9f051d211b0], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling.] 01/03/23 01:59:21.267
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jan  3 01:59:22.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-2977" for this suite. 01/03/23 01:59:22.273
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:59:22.276
Jan  3 01:59:22.276: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename webhook 01/03/23 01:59:22.277
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:59:22.287
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:59:22.289
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/03/23 01:59:22.317
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/23 01:59:22.667
STEP: Deploying the webhook pod 01/03/23 01:59:22.673
STEP: Wait for the deployment to be ready 01/03/23 01:59:22.735
Jan  3 01:59:22.744: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/03/23 01:59:24.751
STEP: Verifying the service has paired with the endpoint 01/03/23 01:59:24.766
Jan  3 01:59:25.766: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 01/03/23 01:59:25.768
STEP: create a namespace for the webhook 01/03/23 01:59:25.781
STEP: create a configmap should be unconditionally rejected by the webhook 01/03/23 01:59:25.801
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  3 01:59:25.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-96" for this suite. 01/03/23 01:59:25.849
STEP: Destroying namespace "webhook-96-markers" for this suite. 01/03/23 01:59:25.851
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","completed":54,"skipped":1012,"failed":0}
------------------------------
• [3.652 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:59:22.276
    Jan  3 01:59:22.276: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename webhook 01/03/23 01:59:22.277
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:59:22.287
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:59:22.289
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/03/23 01:59:22.317
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/23 01:59:22.667
    STEP: Deploying the webhook pod 01/03/23 01:59:22.673
    STEP: Wait for the deployment to be ready 01/03/23 01:59:22.735
    Jan  3 01:59:22.744: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/03/23 01:59:24.751
    STEP: Verifying the service has paired with the endpoint 01/03/23 01:59:24.766
    Jan  3 01:59:25.766: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:238
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 01/03/23 01:59:25.768
    STEP: create a namespace for the webhook 01/03/23 01:59:25.781
    STEP: create a configmap should be unconditionally rejected by the webhook 01/03/23 01:59:25.801
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  3 01:59:25.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-96" for this suite. 01/03/23 01:59:25.849
    STEP: Destroying namespace "webhook-96-markers" for this suite. 01/03/23 01:59:25.851
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 01:59:25.929
Jan  3 01:59:25.929: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename cronjob 01/03/23 01:59:25.93
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:59:25.979
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:59:25.981
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 01/03/23 01:59:25.983
STEP: Ensuring more than one job is running at a time 01/03/23 01:59:25.989
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 01/03/23 02:01:01.992
STEP: Removing cronjob 01/03/23 02:01:01.994
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan  3 02:01:01.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-467" for this suite. 01/03/23 02:01:02.022
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","completed":55,"skipped":1031,"failed":0}
------------------------------
• [SLOW TEST] [96.103 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 01:59:25.929
    Jan  3 01:59:25.929: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename cronjob 01/03/23 01:59:25.93
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 01:59:25.979
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 01:59:25.981
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 01/03/23 01:59:25.983
    STEP: Ensuring more than one job is running at a time 01/03/23 01:59:25.989
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 01/03/23 02:01:01.992
    STEP: Removing cronjob 01/03/23 02:01:01.994
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan  3 02:01:01.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-467" for this suite. 01/03/23 02:01:02.022
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:01:02.033
Jan  3 02:01:02.033: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename emptydir 01/03/23 02:01:02.034
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:01:02.102
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:01:02.104
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
STEP: Creating a pod to test emptydir 0666 on tmpfs 01/03/23 02:01:02.106
Jan  3 02:01:02.116: INFO: Waiting up to 5m0s for pod "pod-5c036d50-f763-48c9-bbb3-417e9c670278" in namespace "emptydir-5115" to be "Succeeded or Failed"
Jan  3 02:01:02.123: INFO: Pod "pod-5c036d50-f763-48c9-bbb3-417e9c670278": Phase="Pending", Reason="", readiness=false. Elapsed: 7.087866ms
Jan  3 02:01:04.126: INFO: Pod "pod-5c036d50-f763-48c9-bbb3-417e9c670278": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009868105s
Jan  3 02:01:06.126: INFO: Pod "pod-5c036d50-f763-48c9-bbb3-417e9c670278": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009854721s
Jan  3 02:01:08.126: INFO: Pod "pod-5c036d50-f763-48c9-bbb3-417e9c670278": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009758705s
STEP: Saw pod success 01/03/23 02:01:08.126
Jan  3 02:01:08.126: INFO: Pod "pod-5c036d50-f763-48c9-bbb3-417e9c670278" satisfied condition "Succeeded or Failed"
Jan  3 02:01:08.128: INFO: Trying to get logs from node cncf-master pod pod-5c036d50-f763-48c9-bbb3-417e9c670278 container test-container: <nil>
STEP: delete the pod 01/03/23 02:01:08.141
Jan  3 02:01:08.159: INFO: Waiting for pod pod-5c036d50-f763-48c9-bbb3-417e9c670278 to disappear
Jan  3 02:01:08.164: INFO: Pod pod-5c036d50-f763-48c9-bbb3-417e9c670278 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan  3 02:01:08.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5115" for this suite. 01/03/23 02:01:08.167
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":56,"skipped":1039,"failed":0}
------------------------------
• [SLOW TEST] [6.137 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:01:02.033
    Jan  3 02:01:02.033: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename emptydir 01/03/23 02:01:02.034
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:01:02.102
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:01:02.104
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:106
    STEP: Creating a pod to test emptydir 0666 on tmpfs 01/03/23 02:01:02.106
    Jan  3 02:01:02.116: INFO: Waiting up to 5m0s for pod "pod-5c036d50-f763-48c9-bbb3-417e9c670278" in namespace "emptydir-5115" to be "Succeeded or Failed"
    Jan  3 02:01:02.123: INFO: Pod "pod-5c036d50-f763-48c9-bbb3-417e9c670278": Phase="Pending", Reason="", readiness=false. Elapsed: 7.087866ms
    Jan  3 02:01:04.126: INFO: Pod "pod-5c036d50-f763-48c9-bbb3-417e9c670278": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009868105s
    Jan  3 02:01:06.126: INFO: Pod "pod-5c036d50-f763-48c9-bbb3-417e9c670278": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009854721s
    Jan  3 02:01:08.126: INFO: Pod "pod-5c036d50-f763-48c9-bbb3-417e9c670278": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009758705s
    STEP: Saw pod success 01/03/23 02:01:08.126
    Jan  3 02:01:08.126: INFO: Pod "pod-5c036d50-f763-48c9-bbb3-417e9c670278" satisfied condition "Succeeded or Failed"
    Jan  3 02:01:08.128: INFO: Trying to get logs from node cncf-master pod pod-5c036d50-f763-48c9-bbb3-417e9c670278 container test-container: <nil>
    STEP: delete the pod 01/03/23 02:01:08.141
    Jan  3 02:01:08.159: INFO: Waiting for pod pod-5c036d50-f763-48c9-bbb3-417e9c670278 to disappear
    Jan  3 02:01:08.164: INFO: Pod pod-5c036d50-f763-48c9-bbb3-417e9c670278 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan  3 02:01:08.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5115" for this suite. 01/03/23 02:01:08.167
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:01:08.17
Jan  3 02:01:08.170: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename webhook 01/03/23 02:01:08.171
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:01:08.198
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:01:08.2
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/03/23 02:01:08.245
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/23 02:01:08.913
STEP: Deploying the webhook pod 01/03/23 02:01:08.937
STEP: Wait for the deployment to be ready 01/03/23 02:01:08.957
Jan  3 02:01:09.003: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/03/23 02:01:11.009
STEP: Verifying the service has paired with the endpoint 01/03/23 02:01:11.048
Jan  3 02:01:12.048: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
Jan  3 02:01:12.051: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Registering the custom resource webhook via the AdmissionRegistration API 01/03/23 02:01:12.562
STEP: Creating a custom resource that should be denied by the webhook 01/03/23 02:01:12.575
STEP: Creating a custom resource whose deletion would be denied by the webhook 01/03/23 02:01:14.584
STEP: Updating the custom resource with disallowed data should be denied 01/03/23 02:01:14.588
STEP: Deleting the custom resource should be denied 01/03/23 02:01:14.62
STEP: Remove the offending key and value from the custom resource data 01/03/23 02:01:14.624
STEP: Deleting the updated custom resource should be successful 01/03/23 02:01:14.629
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  3 02:01:15.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1789" for this suite. 01/03/23 02:01:15.163
STEP: Destroying namespace "webhook-1789-markers" for this suite. 01/03/23 02:01:15.165
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","completed":57,"skipped":1044,"failed":0}
------------------------------
• [SLOW TEST] [7.064 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:01:08.17
    Jan  3 02:01:08.170: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename webhook 01/03/23 02:01:08.171
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:01:08.198
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:01:08.2
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/03/23 02:01:08.245
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/23 02:01:08.913
    STEP: Deploying the webhook pod 01/03/23 02:01:08.937
    STEP: Wait for the deployment to be ready 01/03/23 02:01:08.957
    Jan  3 02:01:09.003: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/03/23 02:01:11.009
    STEP: Verifying the service has paired with the endpoint 01/03/23 02:01:11.048
    Jan  3 02:01:12.048: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:220
    Jan  3 02:01:12.051: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 01/03/23 02:01:12.562
    STEP: Creating a custom resource that should be denied by the webhook 01/03/23 02:01:12.575
    STEP: Creating a custom resource whose deletion would be denied by the webhook 01/03/23 02:01:14.584
    STEP: Updating the custom resource with disallowed data should be denied 01/03/23 02:01:14.588
    STEP: Deleting the custom resource should be denied 01/03/23 02:01:14.62
    STEP: Remove the offending key and value from the custom resource data 01/03/23 02:01:14.624
    STEP: Deleting the updated custom resource should be successful 01/03/23 02:01:14.629
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  3 02:01:15.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1789" for this suite. 01/03/23 02:01:15.163
    STEP: Destroying namespace "webhook-1789-markers" for this suite. 01/03/23 02:01:15.165
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:01:15.236
Jan  3 02:01:15.236: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename pod-network-test 01/03/23 02:01:15.236
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:01:15.274
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:01:15.276
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-5852 01/03/23 02:01:15.278
STEP: creating a selector 01/03/23 02:01:15.278
STEP: Creating the service pods in kubernetes 01/03/23 02:01:15.278
Jan  3 02:01:15.278: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan  3 02:01:15.327: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5852" to be "running and ready"
Jan  3 02:01:15.338: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.242709ms
Jan  3 02:01:15.338: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 02:01:17.341: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.014305626s
Jan  3 02:01:17.342: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 02:01:19.344: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.016339708s
Jan  3 02:01:19.344: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 02:01:21.341: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.013970792s
Jan  3 02:01:21.341: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 02:01:23.342: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.015072197s
Jan  3 02:01:23.342: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 02:01:25.341: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.013902591s
Jan  3 02:01:25.341: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 02:01:27.341: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.014258374s
Jan  3 02:01:27.341: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 02:01:29.342: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.015034361s
Jan  3 02:01:29.342: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 02:01:31.340: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.013125714s
Jan  3 02:01:31.340: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 02:01:33.367: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.040048013s
Jan  3 02:01:33.367: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 02:01:35.342: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.015116445s
Jan  3 02:01:35.342: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 02:01:37.342: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.014969809s
Jan  3 02:01:37.342: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan  3 02:01:37.342: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan  3 02:01:37.344: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5852" to be "running and ready"
Jan  3 02:01:37.345: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.612456ms
Jan  3 02:01:37.345: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan  3 02:01:37.345: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jan  3 02:01:37.347: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5852" to be "running and ready"
Jan  3 02:01:37.348: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 1.492147ms
Jan  3 02:01:37.348: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jan  3 02:01:37.348: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 01/03/23 02:01:37.35
Jan  3 02:01:37.355: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5852" to be "running"
Jan  3 02:01:37.361: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.16315ms
Jan  3 02:01:39.364: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009162836s
Jan  3 02:01:39.364: INFO: Pod "test-container-pod" satisfied condition "running"
Jan  3 02:01:39.365: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jan  3 02:01:39.365: INFO: Breadth first check of 10.244.246.120 on host 172.21.7.5...
Jan  3 02:01:39.367: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.246.122:9080/dial?request=hostname&protocol=http&host=10.244.246.120&port=8083&tries=1'] Namespace:pod-network-test-5852 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 02:01:39.367: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
Jan  3 02:01:39.367: INFO: ExecWithOptions: Clientset creation
Jan  3 02:01:39.368: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5852/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.246.122%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.246.120%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan  3 02:01:39.439: INFO: Waiting for responses: map[]
Jan  3 02:01:39.439: INFO: reached 10.244.246.120 after 0/1 tries
Jan  3 02:01:39.439: INFO: Breadth first check of 10.244.232.54 on host 172.21.7.7...
Jan  3 02:01:39.441: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.246.122:9080/dial?request=hostname&protocol=http&host=10.244.232.54&port=8083&tries=1'] Namespace:pod-network-test-5852 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 02:01:39.441: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
Jan  3 02:01:39.441: INFO: ExecWithOptions: Clientset creation
Jan  3 02:01:39.441: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5852/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.246.122%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.232.54%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan  3 02:01:39.528: INFO: Waiting for responses: map[]
Jan  3 02:01:39.528: INFO: reached 10.244.232.54 after 0/1 tries
Jan  3 02:01:39.528: INFO: Breadth first check of 10.244.89.84 on host 172.21.7.13...
Jan  3 02:01:39.540: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.246.122:9080/dial?request=hostname&protocol=http&host=10.244.89.84&port=8083&tries=1'] Namespace:pod-network-test-5852 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 02:01:39.540: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
Jan  3 02:01:39.540: INFO: ExecWithOptions: Clientset creation
Jan  3 02:01:39.540: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5852/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.246.122%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.89.84%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan  3 02:01:39.612: INFO: Waiting for responses: map[]
Jan  3 02:01:39.612: INFO: reached 10.244.89.84 after 0/1 tries
Jan  3 02:01:39.612: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jan  3 02:01:39.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5852" for this suite. 01/03/23 02:01:39.615
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","completed":58,"skipped":1073,"failed":0}
------------------------------
• [SLOW TEST] [24.381 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:01:15.236
    Jan  3 02:01:15.236: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename pod-network-test 01/03/23 02:01:15.236
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:01:15.274
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:01:15.276
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-5852 01/03/23 02:01:15.278
    STEP: creating a selector 01/03/23 02:01:15.278
    STEP: Creating the service pods in kubernetes 01/03/23 02:01:15.278
    Jan  3 02:01:15.278: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan  3 02:01:15.327: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5852" to be "running and ready"
    Jan  3 02:01:15.338: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.242709ms
    Jan  3 02:01:15.338: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 02:01:17.341: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.014305626s
    Jan  3 02:01:17.342: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 02:01:19.344: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.016339708s
    Jan  3 02:01:19.344: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 02:01:21.341: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.013970792s
    Jan  3 02:01:21.341: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 02:01:23.342: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.015072197s
    Jan  3 02:01:23.342: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 02:01:25.341: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.013902591s
    Jan  3 02:01:25.341: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 02:01:27.341: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.014258374s
    Jan  3 02:01:27.341: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 02:01:29.342: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.015034361s
    Jan  3 02:01:29.342: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 02:01:31.340: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.013125714s
    Jan  3 02:01:31.340: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 02:01:33.367: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.040048013s
    Jan  3 02:01:33.367: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 02:01:35.342: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.015116445s
    Jan  3 02:01:35.342: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 02:01:37.342: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.014969809s
    Jan  3 02:01:37.342: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan  3 02:01:37.342: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan  3 02:01:37.344: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5852" to be "running and ready"
    Jan  3 02:01:37.345: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.612456ms
    Jan  3 02:01:37.345: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan  3 02:01:37.345: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jan  3 02:01:37.347: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5852" to be "running and ready"
    Jan  3 02:01:37.348: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 1.492147ms
    Jan  3 02:01:37.348: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jan  3 02:01:37.348: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 01/03/23 02:01:37.35
    Jan  3 02:01:37.355: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5852" to be "running"
    Jan  3 02:01:37.361: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.16315ms
    Jan  3 02:01:39.364: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009162836s
    Jan  3 02:01:39.364: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan  3 02:01:39.365: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Jan  3 02:01:39.365: INFO: Breadth first check of 10.244.246.120 on host 172.21.7.5...
    Jan  3 02:01:39.367: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.246.122:9080/dial?request=hostname&protocol=http&host=10.244.246.120&port=8083&tries=1'] Namespace:pod-network-test-5852 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 02:01:39.367: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    Jan  3 02:01:39.367: INFO: ExecWithOptions: Clientset creation
    Jan  3 02:01:39.368: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5852/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.246.122%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.246.120%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan  3 02:01:39.439: INFO: Waiting for responses: map[]
    Jan  3 02:01:39.439: INFO: reached 10.244.246.120 after 0/1 tries
    Jan  3 02:01:39.439: INFO: Breadth first check of 10.244.232.54 on host 172.21.7.7...
    Jan  3 02:01:39.441: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.246.122:9080/dial?request=hostname&protocol=http&host=10.244.232.54&port=8083&tries=1'] Namespace:pod-network-test-5852 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 02:01:39.441: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    Jan  3 02:01:39.441: INFO: ExecWithOptions: Clientset creation
    Jan  3 02:01:39.441: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5852/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.246.122%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.232.54%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan  3 02:01:39.528: INFO: Waiting for responses: map[]
    Jan  3 02:01:39.528: INFO: reached 10.244.232.54 after 0/1 tries
    Jan  3 02:01:39.528: INFO: Breadth first check of 10.244.89.84 on host 172.21.7.13...
    Jan  3 02:01:39.540: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.246.122:9080/dial?request=hostname&protocol=http&host=10.244.89.84&port=8083&tries=1'] Namespace:pod-network-test-5852 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 02:01:39.540: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    Jan  3 02:01:39.540: INFO: ExecWithOptions: Clientset creation
    Jan  3 02:01:39.540: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5852/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.246.122%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.89.84%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan  3 02:01:39.612: INFO: Waiting for responses: map[]
    Jan  3 02:01:39.612: INFO: reached 10.244.89.84 after 0/1 tries
    Jan  3 02:01:39.612: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jan  3 02:01:39.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-5852" for this suite. 01/03/23 02:01:39.615
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:01:39.618
Jan  3 02:01:39.618: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename svcaccounts 01/03/23 02:01:39.619
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:01:39.629
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:01:39.631
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
Jan  3 02:01:39.655: INFO: created pod
Jan  3 02:01:39.655: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-1903" to be "Succeeded or Failed"
Jan  3 02:01:39.658: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.015711ms
Jan  3 02:01:41.660: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005112326s
Jan  3 02:01:43.663: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007366017s
STEP: Saw pod success 01/03/23 02:01:43.663
Jan  3 02:01:43.663: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Jan  3 02:02:13.664: INFO: polling logs
Jan  3 02:02:13.682: INFO: Pod logs: 
I0103 02:01:40.410242       1 log.go:195] OK: Got token
I0103 02:01:40.410269       1 log.go:195] validating with in-cluster discovery
I0103 02:01:40.410522       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
I0103 02:01:40.410563       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-1903:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1672711899, NotBefore:1672711299, IssuedAt:1672711299, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1903", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"c984d795-cf05-4f9a-9ee6-58b0cc78b35d"}}}
I0103 02:01:40.417608       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0103 02:01:40.422536       1 log.go:195] OK: Validated signature on JWT
I0103 02:01:40.422619       1 log.go:195] OK: Got valid claims from token!
I0103 02:01:40.422643       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-1903:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1672711899, NotBefore:1672711299, IssuedAt:1672711299, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1903", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"c984d795-cf05-4f9a-9ee6-58b0cc78b35d"}}}

Jan  3 02:02:13.682: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan  3 02:02:13.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1903" for this suite. 01/03/23 02:02:13.689
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","completed":59,"skipped":1084,"failed":0}
------------------------------
• [SLOW TEST] [34.074 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:01:39.618
    Jan  3 02:01:39.618: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename svcaccounts 01/03/23 02:01:39.619
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:01:39.629
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:01:39.631
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:528
    Jan  3 02:01:39.655: INFO: created pod
    Jan  3 02:01:39.655: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-1903" to be "Succeeded or Failed"
    Jan  3 02:01:39.658: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.015711ms
    Jan  3 02:01:41.660: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005112326s
    Jan  3 02:01:43.663: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007366017s
    STEP: Saw pod success 01/03/23 02:01:43.663
    Jan  3 02:01:43.663: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Jan  3 02:02:13.664: INFO: polling logs
    Jan  3 02:02:13.682: INFO: Pod logs: 
    I0103 02:01:40.410242       1 log.go:195] OK: Got token
    I0103 02:01:40.410269       1 log.go:195] validating with in-cluster discovery
    I0103 02:01:40.410522       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0103 02:01:40.410563       1 log.go:195] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-1903:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1672711899, NotBefore:1672711299, IssuedAt:1672711299, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1903", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"c984d795-cf05-4f9a-9ee6-58b0cc78b35d"}}}
    I0103 02:01:40.417608       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0103 02:01:40.422536       1 log.go:195] OK: Validated signature on JWT
    I0103 02:01:40.422619       1 log.go:195] OK: Got valid claims from token!
    I0103 02:01:40.422643       1 log.go:195] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-1903:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1672711899, NotBefore:1672711299, IssuedAt:1672711299, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1903", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"c984d795-cf05-4f9a-9ee6-58b0cc78b35d"}}}

    Jan  3 02:02:13.682: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan  3 02:02:13.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-1903" for this suite. 01/03/23 02:02:13.689
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:02:13.692
Jan  3 02:02:13.693: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename replicaset 01/03/23 02:02:13.694
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:02:13.705
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:02:13.707
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 01/03/23 02:02:13.709
Jan  3 02:02:13.728: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan  3 02:02:18.734: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/03/23 02:02:18.734
STEP: getting scale subresource 01/03/23 02:02:18.734
STEP: updating a scale subresource 01/03/23 02:02:18.74
STEP: verifying the replicaset Spec.Replicas was modified 01/03/23 02:02:18.745
STEP: Patch a scale subresource 01/03/23 02:02:18.768
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan  3 02:02:18.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-89" for this suite. 01/03/23 02:02:18.857
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","completed":60,"skipped":1096,"failed":0}
------------------------------
• [SLOW TEST] [5.198 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:02:13.692
    Jan  3 02:02:13.693: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename replicaset 01/03/23 02:02:13.694
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:02:13.705
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:02:13.707
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 01/03/23 02:02:13.709
    Jan  3 02:02:13.728: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan  3 02:02:18.734: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/03/23 02:02:18.734
    STEP: getting scale subresource 01/03/23 02:02:18.734
    STEP: updating a scale subresource 01/03/23 02:02:18.74
    STEP: verifying the replicaset Spec.Replicas was modified 01/03/23 02:02:18.745
    STEP: Patch a scale subresource 01/03/23 02:02:18.768
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan  3 02:02:18.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-89" for this suite. 01/03/23 02:02:18.857
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:02:18.891
Jan  3 02:02:18.891: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename container-probe 01/03/23 02:02:18.892
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:02:18.923
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:02:18.925
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
STEP: Creating pod liveness-22839089-0cbd-4211-a6f0-c1caad7e464f in namespace container-probe-48 01/03/23 02:02:18.927
Jan  3 02:02:18.946: INFO: Waiting up to 5m0s for pod "liveness-22839089-0cbd-4211-a6f0-c1caad7e464f" in namespace "container-probe-48" to be "not pending"
Jan  3 02:02:18.980: INFO: Pod "liveness-22839089-0cbd-4211-a6f0-c1caad7e464f": Phase="Pending", Reason="", readiness=false. Elapsed: 34.195971ms
Jan  3 02:02:20.983: INFO: Pod "liveness-22839089-0cbd-4211-a6f0-c1caad7e464f": Phase="Running", Reason="", readiness=true. Elapsed: 2.036862951s
Jan  3 02:02:20.983: INFO: Pod "liveness-22839089-0cbd-4211-a6f0-c1caad7e464f" satisfied condition "not pending"
Jan  3 02:02:20.983: INFO: Started pod liveness-22839089-0cbd-4211-a6f0-c1caad7e464f in namespace container-probe-48
STEP: checking the pod's current state and verifying that restartCount is present 01/03/23 02:02:20.983
Jan  3 02:02:20.985: INFO: Initial restart count of pod liveness-22839089-0cbd-4211-a6f0-c1caad7e464f is 0
STEP: deleting the pod 01/03/23 02:06:21.366
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan  3 02:06:21.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-48" for this suite. 01/03/23 02:06:21.462
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","completed":61,"skipped":1110,"failed":0}
------------------------------
• [SLOW TEST] [242.574 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:02:18.891
    Jan  3 02:02:18.891: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename container-probe 01/03/23 02:02:18.892
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:02:18.923
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:02:18.925
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:180
    STEP: Creating pod liveness-22839089-0cbd-4211-a6f0-c1caad7e464f in namespace container-probe-48 01/03/23 02:02:18.927
    Jan  3 02:02:18.946: INFO: Waiting up to 5m0s for pod "liveness-22839089-0cbd-4211-a6f0-c1caad7e464f" in namespace "container-probe-48" to be "not pending"
    Jan  3 02:02:18.980: INFO: Pod "liveness-22839089-0cbd-4211-a6f0-c1caad7e464f": Phase="Pending", Reason="", readiness=false. Elapsed: 34.195971ms
    Jan  3 02:02:20.983: INFO: Pod "liveness-22839089-0cbd-4211-a6f0-c1caad7e464f": Phase="Running", Reason="", readiness=true. Elapsed: 2.036862951s
    Jan  3 02:02:20.983: INFO: Pod "liveness-22839089-0cbd-4211-a6f0-c1caad7e464f" satisfied condition "not pending"
    Jan  3 02:02:20.983: INFO: Started pod liveness-22839089-0cbd-4211-a6f0-c1caad7e464f in namespace container-probe-48
    STEP: checking the pod's current state and verifying that restartCount is present 01/03/23 02:02:20.983
    Jan  3 02:02:20.985: INFO: Initial restart count of pod liveness-22839089-0cbd-4211-a6f0-c1caad7e464f is 0
    STEP: deleting the pod 01/03/23 02:06:21.366
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan  3 02:06:21.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-48" for this suite. 01/03/23 02:06:21.462
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:06:21.466
Jan  3 02:06:21.466: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename deployment 01/03/23 02:06:21.466
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:06:21.491
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:06:21.493
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 01/03/23 02:06:21.5
Jan  3 02:06:21.500: INFO: Creating simple deployment test-deployment-qd4zt
Jan  3 02:06:21.525: INFO: deployment "test-deployment-qd4zt" doesn't have the required revision set
STEP: Getting /status 01/03/23 02:06:23.532
Jan  3 02:06:23.534: INFO: Deployment test-deployment-qd4zt has Conditions: [{Available True 2023-01-03 02:06:22 +0000 UTC 2023-01-03 02:06:22 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-01-03 02:06:22 +0000 UTC 2023-01-03 02:06:21 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-qd4zt-777898ffcc" has successfully progressed.}]
STEP: updating Deployment Status 01/03/23 02:06:23.534
Jan  3 02:06:23.539: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 2, 6, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 2, 6, 22, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 2, 6, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 2, 6, 21, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-qd4zt-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 01/03/23 02:06:23.539
Jan  3 02:06:23.540: INFO: Observed &Deployment event: ADDED
Jan  3 02:06:23.540: INFO: Observed Deployment test-deployment-qd4zt in namespace deployment-2822 with annotations: map[createdTime:2023-01-03T11:06:21.502196631+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:06:21.502196631+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Progressing True 2023-01-03 02:06:21 +0000 UTC 2023-01-03 02:06:21 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-qd4zt-777898ffcc"}
Jan  3 02:06:23.540: INFO: Observed &Deployment event: MODIFIED
Jan  3 02:06:23.540: INFO: Observed Deployment test-deployment-qd4zt in namespace deployment-2822 with annotations: map[createdTime:2023-01-03T11:06:21.502196631+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:06:21.502196631+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Progressing True 2023-01-03 02:06:21 +0000 UTC 2023-01-03 02:06:21 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-qd4zt-777898ffcc"}
Jan  3 02:06:23.540: INFO: Observed Deployment test-deployment-qd4zt in namespace deployment-2822 with annotations: map[createdTime:2023-01-03T11:06:21.502196631+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:06:21.502196631+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Available False 2023-01-03 02:06:21 +0000 UTC 2023-01-03 02:06:21 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan  3 02:06:23.540: INFO: Observed &Deployment event: MODIFIED
Jan  3 02:06:23.540: INFO: Observed Deployment test-deployment-qd4zt in namespace deployment-2822 with annotations: map[createdTime:2023-01-03T11:06:21.502196631+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:06:21.502196631+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Available False 2023-01-03 02:06:21 +0000 UTC 2023-01-03 02:06:21 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan  3 02:06:23.540: INFO: Observed Deployment test-deployment-qd4zt in namespace deployment-2822 with annotations: map[createdTime:2023-01-03T11:06:21.502196631+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:06:21.502196631+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Progressing True 2023-01-03 02:06:21 +0000 UTC 2023-01-03 02:06:21 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-qd4zt-777898ffcc" is progressing.}
Jan  3 02:06:23.540: INFO: Observed &Deployment event: MODIFIED
Jan  3 02:06:23.540: INFO: Observed Deployment test-deployment-qd4zt in namespace deployment-2822 with annotations: map[createdTime:2023-01-03T11:06:21.502196631+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:06:21.502196631+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Available True 2023-01-03 02:06:22 +0000 UTC 2023-01-03 02:06:22 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan  3 02:06:23.540: INFO: Observed Deployment test-deployment-qd4zt in namespace deployment-2822 with annotations: map[createdTime:2023-01-03T11:06:21.502196631+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:06:21.502196631+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Progressing True 2023-01-03 02:06:22 +0000 UTC 2023-01-03 02:06:21 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-qd4zt-777898ffcc" has successfully progressed.}
Jan  3 02:06:23.541: INFO: Observed &Deployment event: MODIFIED
Jan  3 02:06:23.541: INFO: Observed Deployment test-deployment-qd4zt in namespace deployment-2822 with annotations: map[createdTime:2023-01-03T11:06:21.502196631+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:06:21.502196631+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Available True 2023-01-03 02:06:22 +0000 UTC 2023-01-03 02:06:22 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan  3 02:06:23.541: INFO: Observed Deployment test-deployment-qd4zt in namespace deployment-2822 with annotations: map[createdTime:2023-01-03T11:06:21.502196631+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:06:21.502196631+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Progressing True 2023-01-03 02:06:22 +0000 UTC 2023-01-03 02:06:21 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-qd4zt-777898ffcc" has successfully progressed.}
Jan  3 02:06:23.541: INFO: Found Deployment test-deployment-qd4zt in namespace deployment-2822 with labels: map[e2e:testing name:httpd] annotations: map[createdTime:2023-01-03T11:06:21.502196631+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:06:21.502196631+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan  3 02:06:23.541: INFO: Deployment test-deployment-qd4zt has an updated status
STEP: patching the Statefulset Status 01/03/23 02:06:23.541
Jan  3 02:06:23.541: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan  3 02:06:23.557: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 01/03/23 02:06:23.557
Jan  3 02:06:23.558: INFO: Observed &Deployment event: ADDED
Jan  3 02:06:23.558: INFO: Observed deployment test-deployment-qd4zt in namespace deployment-2822 with annotations: map[createdTime:2023-01-03T11:06:21.502196631+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:06:21.502196631+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Progressing True 2023-01-03 02:06:21 +0000 UTC 2023-01-03 02:06:21 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-qd4zt-777898ffcc"}
Jan  3 02:06:23.558: INFO: Observed &Deployment event: MODIFIED
Jan  3 02:06:23.558: INFO: Observed deployment test-deployment-qd4zt in namespace deployment-2822 with annotations: map[createdTime:2023-01-03T11:06:21.502196631+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:06:21.502196631+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Progressing True 2023-01-03 02:06:21 +0000 UTC 2023-01-03 02:06:21 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-qd4zt-777898ffcc"}
Jan  3 02:06:23.559: INFO: Observed deployment test-deployment-qd4zt in namespace deployment-2822 with annotations: map[createdTime:2023-01-03T11:06:21.502196631+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:06:21.502196631+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Available False 2023-01-03 02:06:21 +0000 UTC 2023-01-03 02:06:21 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan  3 02:06:23.559: INFO: Observed &Deployment event: MODIFIED
Jan  3 02:06:23.559: INFO: Observed deployment test-deployment-qd4zt in namespace deployment-2822 with annotations: map[createdTime:2023-01-03T11:06:21.502196631+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:06:21.502196631+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Available False 2023-01-03 02:06:21 +0000 UTC 2023-01-03 02:06:21 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan  3 02:06:23.559: INFO: Observed deployment test-deployment-qd4zt in namespace deployment-2822 with annotations: map[createdTime:2023-01-03T11:06:21.502196631+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:06:21.502196631+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Progressing True 2023-01-03 02:06:21 +0000 UTC 2023-01-03 02:06:21 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-qd4zt-777898ffcc" is progressing.}
Jan  3 02:06:23.559: INFO: Observed &Deployment event: MODIFIED
Jan  3 02:06:23.559: INFO: Observed deployment test-deployment-qd4zt in namespace deployment-2822 with annotations: map[createdTime:2023-01-03T11:06:21.502196631+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:06:21.502196631+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Available True 2023-01-03 02:06:22 +0000 UTC 2023-01-03 02:06:22 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan  3 02:06:23.559: INFO: Observed deployment test-deployment-qd4zt in namespace deployment-2822 with annotations: map[createdTime:2023-01-03T11:06:21.502196631+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:06:21.502196631+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Progressing True 2023-01-03 02:06:22 +0000 UTC 2023-01-03 02:06:21 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-qd4zt-777898ffcc" has successfully progressed.}
Jan  3 02:06:23.559: INFO: Observed &Deployment event: MODIFIED
Jan  3 02:06:23.559: INFO: Observed deployment test-deployment-qd4zt in namespace deployment-2822 with annotations: map[createdTime:2023-01-03T11:06:21.502196631+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:06:21.502196631+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Available True 2023-01-03 02:06:22 +0000 UTC 2023-01-03 02:06:22 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan  3 02:06:23.559: INFO: Observed deployment test-deployment-qd4zt in namespace deployment-2822 with annotations: map[createdTime:2023-01-03T11:06:21.502196631+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:06:21.502196631+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Progressing True 2023-01-03 02:06:22 +0000 UTC 2023-01-03 02:06:21 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-qd4zt-777898ffcc" has successfully progressed.}
Jan  3 02:06:23.559: INFO: Observed deployment test-deployment-qd4zt in namespace deployment-2822 with annotations: map[createdTime:2023-01-03T11:06:21.502196631+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:06:21.502196631+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan  3 02:06:23.559: INFO: Observed &Deployment event: MODIFIED
Jan  3 02:06:23.559: INFO: Found deployment test-deployment-qd4zt in namespace deployment-2822 with labels: map[e2e:testing name:httpd] annotations: map[createdTime:2023-01-03T11:06:21.502196631+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:06:21.502196631+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Jan  3 02:06:23.559: INFO: Deployment test-deployment-qd4zt has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan  3 02:06:23.563: INFO: Deployment "test-deployment-qd4zt":
&Deployment{ObjectMeta:{test-deployment-qd4zt  deployment-2822  8c713859-c143-4c3a-9c99-48413868e880 983815 1 2023-01-03 02:06:21 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[createdTime:2023-01-03T11:06:21.502196631+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:06:21.502196631+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update apps/v1 2023-01-03 02:06:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-01-03 02:06:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-01-03 02:06:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0073b2aa8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-qd4zt-777898ffcc",LastUpdateTime:2023-01-03 02:06:23 +0000 UTC,LastTransitionTime:2023-01-03 02:06:23 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan  3 02:06:23.565: INFO: New ReplicaSet "test-deployment-qd4zt-777898ffcc" of Deployment "test-deployment-qd4zt":
&ReplicaSet{ObjectMeta:{test-deployment-qd4zt-777898ffcc  deployment-2822  091d128e-6a86-407e-b455-e8db9125806c 983811 1 2023-01-03 02:06:21 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[createdTime:2023-01-03T11:06:21.502196631+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:06:21.502196631+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [{apps/v1 Deployment test-deployment-qd4zt 8c713859-c143-4c3a-9c99-48413868e880 0xc00392988e 0xc00392988f}] [] [{kube-controller-manager Update apps/v1 2023-01-03 02:06:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:createdTime":{},"f:creator":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{},"f:updatedTime":{},"f:updater":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8c713859-c143-4c3a-9c99-48413868e880\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-03 02:06:22 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003929948 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan  3 02:06:23.568: INFO: Pod "test-deployment-qd4zt-777898ffcc-bbctc" is available:
&Pod{ObjectMeta:{test-deployment-qd4zt-777898ffcc-bbctc test-deployment-qd4zt-777898ffcc- deployment-2822  0713e19a-4b2f-4adb-a5b3-198c6445e468 983810 0 2023-01-03 02:06:21 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cni.projectcalico.org/containerID:4dfd120868caf1bca25b2737d0b0eb0f6dcde389f27cd3cfe2682d0bdec0e573 cni.projectcalico.org/podIP:10.244.246.126/32 cni.projectcalico.org/podIPs:10.244.246.126/32 createdTime:2023-01-03T11:06:21.543674539+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T11:06:21.543674539+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet test-deployment-qd4zt-777898ffcc 091d128e-6a86-407e-b455-e8db9125806c 0xc007ae64de 0xc007ae64df}] [] [{kube-controller-manager Update v1 2023-01-03 02:06:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"091d128e-6a86-407e-b455-e8db9125806c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-03 02:06:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-03 02:06:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.246.126\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wdt48,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wdt48,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-master,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:06:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:06:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:06:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:06:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.5,PodIP:10.244.246.126,StartTime:2023-01-03 02:06:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-03 02:06:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://4d533f3d792959edb9be999c2006bc23da0da200d864680cc542e5002e3b4f7e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.246.126,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan  3 02:06:23.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2822" for this suite. 01/03/23 02:06:23.587
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","completed":62,"skipped":1128,"failed":0}
------------------------------
• [2.154 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:06:21.466
    Jan  3 02:06:21.466: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename deployment 01/03/23 02:06:21.466
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:06:21.491
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:06:21.493
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 01/03/23 02:06:21.5
    Jan  3 02:06:21.500: INFO: Creating simple deployment test-deployment-qd4zt
    Jan  3 02:06:21.525: INFO: deployment "test-deployment-qd4zt" doesn't have the required revision set
    STEP: Getting /status 01/03/23 02:06:23.532
    Jan  3 02:06:23.534: INFO: Deployment test-deployment-qd4zt has Conditions: [{Available True 2023-01-03 02:06:22 +0000 UTC 2023-01-03 02:06:22 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-01-03 02:06:22 +0000 UTC 2023-01-03 02:06:21 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-qd4zt-777898ffcc" has successfully progressed.}]
    STEP: updating Deployment Status 01/03/23 02:06:23.534
    Jan  3 02:06:23.539: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 2, 6, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 2, 6, 22, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 2, 6, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 2, 6, 21, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-qd4zt-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 01/03/23 02:06:23.539
    Jan  3 02:06:23.540: INFO: Observed &Deployment event: ADDED
    Jan  3 02:06:23.540: INFO: Observed Deployment test-deployment-qd4zt in namespace deployment-2822 with annotations: map[createdTime:2023-01-03T11:06:21.502196631+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:06:21.502196631+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Progressing True 2023-01-03 02:06:21 +0000 UTC 2023-01-03 02:06:21 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-qd4zt-777898ffcc"}
    Jan  3 02:06:23.540: INFO: Observed &Deployment event: MODIFIED
    Jan  3 02:06:23.540: INFO: Observed Deployment test-deployment-qd4zt in namespace deployment-2822 with annotations: map[createdTime:2023-01-03T11:06:21.502196631+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:06:21.502196631+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Progressing True 2023-01-03 02:06:21 +0000 UTC 2023-01-03 02:06:21 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-qd4zt-777898ffcc"}
    Jan  3 02:06:23.540: INFO: Observed Deployment test-deployment-qd4zt in namespace deployment-2822 with annotations: map[createdTime:2023-01-03T11:06:21.502196631+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:06:21.502196631+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Available False 2023-01-03 02:06:21 +0000 UTC 2023-01-03 02:06:21 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan  3 02:06:23.540: INFO: Observed &Deployment event: MODIFIED
    Jan  3 02:06:23.540: INFO: Observed Deployment test-deployment-qd4zt in namespace deployment-2822 with annotations: map[createdTime:2023-01-03T11:06:21.502196631+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:06:21.502196631+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Available False 2023-01-03 02:06:21 +0000 UTC 2023-01-03 02:06:21 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan  3 02:06:23.540: INFO: Observed Deployment test-deployment-qd4zt in namespace deployment-2822 with annotations: map[createdTime:2023-01-03T11:06:21.502196631+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:06:21.502196631+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Progressing True 2023-01-03 02:06:21 +0000 UTC 2023-01-03 02:06:21 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-qd4zt-777898ffcc" is progressing.}
    Jan  3 02:06:23.540: INFO: Observed &Deployment event: MODIFIED
    Jan  3 02:06:23.540: INFO: Observed Deployment test-deployment-qd4zt in namespace deployment-2822 with annotations: map[createdTime:2023-01-03T11:06:21.502196631+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:06:21.502196631+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Available True 2023-01-03 02:06:22 +0000 UTC 2023-01-03 02:06:22 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan  3 02:06:23.540: INFO: Observed Deployment test-deployment-qd4zt in namespace deployment-2822 with annotations: map[createdTime:2023-01-03T11:06:21.502196631+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:06:21.502196631+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Progressing True 2023-01-03 02:06:22 +0000 UTC 2023-01-03 02:06:21 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-qd4zt-777898ffcc" has successfully progressed.}
    Jan  3 02:06:23.541: INFO: Observed &Deployment event: MODIFIED
    Jan  3 02:06:23.541: INFO: Observed Deployment test-deployment-qd4zt in namespace deployment-2822 with annotations: map[createdTime:2023-01-03T11:06:21.502196631+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:06:21.502196631+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Available True 2023-01-03 02:06:22 +0000 UTC 2023-01-03 02:06:22 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan  3 02:06:23.541: INFO: Observed Deployment test-deployment-qd4zt in namespace deployment-2822 with annotations: map[createdTime:2023-01-03T11:06:21.502196631+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:06:21.502196631+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Progressing True 2023-01-03 02:06:22 +0000 UTC 2023-01-03 02:06:21 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-qd4zt-777898ffcc" has successfully progressed.}
    Jan  3 02:06:23.541: INFO: Found Deployment test-deployment-qd4zt in namespace deployment-2822 with labels: map[e2e:testing name:httpd] annotations: map[createdTime:2023-01-03T11:06:21.502196631+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:06:21.502196631+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan  3 02:06:23.541: INFO: Deployment test-deployment-qd4zt has an updated status
    STEP: patching the Statefulset Status 01/03/23 02:06:23.541
    Jan  3 02:06:23.541: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan  3 02:06:23.557: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 01/03/23 02:06:23.557
    Jan  3 02:06:23.558: INFO: Observed &Deployment event: ADDED
    Jan  3 02:06:23.558: INFO: Observed deployment test-deployment-qd4zt in namespace deployment-2822 with annotations: map[createdTime:2023-01-03T11:06:21.502196631+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:06:21.502196631+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Progressing True 2023-01-03 02:06:21 +0000 UTC 2023-01-03 02:06:21 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-qd4zt-777898ffcc"}
    Jan  3 02:06:23.558: INFO: Observed &Deployment event: MODIFIED
    Jan  3 02:06:23.558: INFO: Observed deployment test-deployment-qd4zt in namespace deployment-2822 with annotations: map[createdTime:2023-01-03T11:06:21.502196631+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:06:21.502196631+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Progressing True 2023-01-03 02:06:21 +0000 UTC 2023-01-03 02:06:21 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-qd4zt-777898ffcc"}
    Jan  3 02:06:23.559: INFO: Observed deployment test-deployment-qd4zt in namespace deployment-2822 with annotations: map[createdTime:2023-01-03T11:06:21.502196631+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:06:21.502196631+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Available False 2023-01-03 02:06:21 +0000 UTC 2023-01-03 02:06:21 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan  3 02:06:23.559: INFO: Observed &Deployment event: MODIFIED
    Jan  3 02:06:23.559: INFO: Observed deployment test-deployment-qd4zt in namespace deployment-2822 with annotations: map[createdTime:2023-01-03T11:06:21.502196631+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:06:21.502196631+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Available False 2023-01-03 02:06:21 +0000 UTC 2023-01-03 02:06:21 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan  3 02:06:23.559: INFO: Observed deployment test-deployment-qd4zt in namespace deployment-2822 with annotations: map[createdTime:2023-01-03T11:06:21.502196631+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:06:21.502196631+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Progressing True 2023-01-03 02:06:21 +0000 UTC 2023-01-03 02:06:21 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-qd4zt-777898ffcc" is progressing.}
    Jan  3 02:06:23.559: INFO: Observed &Deployment event: MODIFIED
    Jan  3 02:06:23.559: INFO: Observed deployment test-deployment-qd4zt in namespace deployment-2822 with annotations: map[createdTime:2023-01-03T11:06:21.502196631+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:06:21.502196631+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Available True 2023-01-03 02:06:22 +0000 UTC 2023-01-03 02:06:22 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan  3 02:06:23.559: INFO: Observed deployment test-deployment-qd4zt in namespace deployment-2822 with annotations: map[createdTime:2023-01-03T11:06:21.502196631+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:06:21.502196631+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Progressing True 2023-01-03 02:06:22 +0000 UTC 2023-01-03 02:06:21 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-qd4zt-777898ffcc" has successfully progressed.}
    Jan  3 02:06:23.559: INFO: Observed &Deployment event: MODIFIED
    Jan  3 02:06:23.559: INFO: Observed deployment test-deployment-qd4zt in namespace deployment-2822 with annotations: map[createdTime:2023-01-03T11:06:21.502196631+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:06:21.502196631+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Available True 2023-01-03 02:06:22 +0000 UTC 2023-01-03 02:06:22 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan  3 02:06:23.559: INFO: Observed deployment test-deployment-qd4zt in namespace deployment-2822 with annotations: map[createdTime:2023-01-03T11:06:21.502196631+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:06:21.502196631+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Progressing True 2023-01-03 02:06:22 +0000 UTC 2023-01-03 02:06:21 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-qd4zt-777898ffcc" has successfully progressed.}
    Jan  3 02:06:23.559: INFO: Observed deployment test-deployment-qd4zt in namespace deployment-2822 with annotations: map[createdTime:2023-01-03T11:06:21.502196631+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:06:21.502196631+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan  3 02:06:23.559: INFO: Observed &Deployment event: MODIFIED
    Jan  3 02:06:23.559: INFO: Found deployment test-deployment-qd4zt in namespace deployment-2822 with labels: map[e2e:testing name:httpd] annotations: map[createdTime:2023-01-03T11:06:21.502196631+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:06:21.502196631+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Jan  3 02:06:23.559: INFO: Deployment test-deployment-qd4zt has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan  3 02:06:23.563: INFO: Deployment "test-deployment-qd4zt":
    &Deployment{ObjectMeta:{test-deployment-qd4zt  deployment-2822  8c713859-c143-4c3a-9c99-48413868e880 983815 1 2023-01-03 02:06:21 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[createdTime:2023-01-03T11:06:21.502196631+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:06:21.502196631+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update apps/v1 2023-01-03 02:06:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-01-03 02:06:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-01-03 02:06:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0073b2aa8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-qd4zt-777898ffcc",LastUpdateTime:2023-01-03 02:06:23 +0000 UTC,LastTransitionTime:2023-01-03 02:06:23 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan  3 02:06:23.565: INFO: New ReplicaSet "test-deployment-qd4zt-777898ffcc" of Deployment "test-deployment-qd4zt":
    &ReplicaSet{ObjectMeta:{test-deployment-qd4zt-777898ffcc  deployment-2822  091d128e-6a86-407e-b455-e8db9125806c 983811 1 2023-01-03 02:06:21 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[createdTime:2023-01-03T11:06:21.502196631+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:06:21.502196631+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [{apps/v1 Deployment test-deployment-qd4zt 8c713859-c143-4c3a-9c99-48413868e880 0xc00392988e 0xc00392988f}] [] [{kube-controller-manager Update apps/v1 2023-01-03 02:06:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:createdTime":{},"f:creator":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{},"f:updatedTime":{},"f:updater":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8c713859-c143-4c3a-9c99-48413868e880\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-03 02:06:22 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003929948 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan  3 02:06:23.568: INFO: Pod "test-deployment-qd4zt-777898ffcc-bbctc" is available:
    &Pod{ObjectMeta:{test-deployment-qd4zt-777898ffcc-bbctc test-deployment-qd4zt-777898ffcc- deployment-2822  0713e19a-4b2f-4adb-a5b3-198c6445e468 983810 0 2023-01-03 02:06:21 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cni.projectcalico.org/containerID:4dfd120868caf1bca25b2737d0b0eb0f6dcde389f27cd3cfe2682d0bdec0e573 cni.projectcalico.org/podIP:10.244.246.126/32 cni.projectcalico.org/podIPs:10.244.246.126/32 createdTime:2023-01-03T11:06:21.543674539+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T11:06:21.543674539+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet test-deployment-qd4zt-777898ffcc 091d128e-6a86-407e-b455-e8db9125806c 0xc007ae64de 0xc007ae64df}] [] [{kube-controller-manager Update v1 2023-01-03 02:06:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"091d128e-6a86-407e-b455-e8db9125806c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-03 02:06:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-03 02:06:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.246.126\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wdt48,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wdt48,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-master,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:06:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:06:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:06:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:06:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.5,PodIP:10.244.246.126,StartTime:2023-01-03 02:06:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-03 02:06:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://4d533f3d792959edb9be999c2006bc23da0da200d864680cc542e5002e3b4f7e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.246.126,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan  3 02:06:23.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-2822" for this suite. 01/03/23 02:06:23.587
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:06:23.62
Jan  3 02:06:23.620: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename resourcequota 01/03/23 02:06:23.621
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:06:23.628
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:06:23.63
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
STEP: Counting existing ResourceQuota 01/03/23 02:06:23.632
STEP: Creating a ResourceQuota 01/03/23 02:06:28.634
STEP: Ensuring resource quota status is calculated 01/03/23 02:06:28.637
STEP: Creating a Service 01/03/23 02:06:30.647
STEP: Creating a NodePort Service 01/03/23 02:06:30.673
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 01/03/23 02:06:30.719
STEP: Ensuring resource quota status captures service creation 01/03/23 02:06:30.782
STEP: Deleting Services 01/03/23 02:06:32.786
STEP: Ensuring resource quota status released usage 01/03/23 02:06:32.864
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan  3 02:06:34.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1500" for this suite. 01/03/23 02:06:34.869
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","completed":63,"skipped":1137,"failed":0}
------------------------------
• [SLOW TEST] [11.252 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:06:23.62
    Jan  3 02:06:23.620: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename resourcequota 01/03/23 02:06:23.621
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:06:23.628
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:06:23.63
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:90
    STEP: Counting existing ResourceQuota 01/03/23 02:06:23.632
    STEP: Creating a ResourceQuota 01/03/23 02:06:28.634
    STEP: Ensuring resource quota status is calculated 01/03/23 02:06:28.637
    STEP: Creating a Service 01/03/23 02:06:30.647
    STEP: Creating a NodePort Service 01/03/23 02:06:30.673
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 01/03/23 02:06:30.719
    STEP: Ensuring resource quota status captures service creation 01/03/23 02:06:30.782
    STEP: Deleting Services 01/03/23 02:06:32.786
    STEP: Ensuring resource quota status released usage 01/03/23 02:06:32.864
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan  3 02:06:34.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-1500" for this suite. 01/03/23 02:06:34.869
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:06:34.872
Jan  3 02:06:34.872: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename downward-api 01/03/23 02:06:34.873
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:06:34.892
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:06:34.894
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
STEP: Creating a pod to test downward API volume plugin 01/03/23 02:06:34.896
Jan  3 02:06:34.901: INFO: Waiting up to 5m0s for pod "downwardapi-volume-72517444-1617-4ea7-a37b-0fe4c03f86d7" in namespace "downward-api-551" to be "Succeeded or Failed"
Jan  3 02:06:34.915: INFO: Pod "downwardapi-volume-72517444-1617-4ea7-a37b-0fe4c03f86d7": Phase="Pending", Reason="", readiness=false. Elapsed: 13.878642ms
Jan  3 02:06:36.918: INFO: Pod "downwardapi-volume-72517444-1617-4ea7-a37b-0fe4c03f86d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017231042s
Jan  3 02:06:38.918: INFO: Pod "downwardapi-volume-72517444-1617-4ea7-a37b-0fe4c03f86d7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017157388s
Jan  3 02:06:40.918: INFO: Pod "downwardapi-volume-72517444-1617-4ea7-a37b-0fe4c03f86d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016283384s
STEP: Saw pod success 01/03/23 02:06:40.918
Jan  3 02:06:40.918: INFO: Pod "downwardapi-volume-72517444-1617-4ea7-a37b-0fe4c03f86d7" satisfied condition "Succeeded or Failed"
Jan  3 02:06:40.919: INFO: Trying to get logs from node cncf-master pod downwardapi-volume-72517444-1617-4ea7-a37b-0fe4c03f86d7 container client-container: <nil>
STEP: delete the pod 01/03/23 02:06:40.93
Jan  3 02:06:40.961: INFO: Waiting for pod downwardapi-volume-72517444-1617-4ea7-a37b-0fe4c03f86d7 to disappear
Jan  3 02:06:40.966: INFO: Pod downwardapi-volume-72517444-1617-4ea7-a37b-0fe4c03f86d7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan  3 02:06:40.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-551" for this suite. 01/03/23 02:06:40.969
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":64,"skipped":1143,"failed":0}
------------------------------
• [SLOW TEST] [6.100 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:06:34.872
    Jan  3 02:06:34.872: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename downward-api 01/03/23 02:06:34.873
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:06:34.892
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:06:34.894
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:83
    STEP: Creating a pod to test downward API volume plugin 01/03/23 02:06:34.896
    Jan  3 02:06:34.901: INFO: Waiting up to 5m0s for pod "downwardapi-volume-72517444-1617-4ea7-a37b-0fe4c03f86d7" in namespace "downward-api-551" to be "Succeeded or Failed"
    Jan  3 02:06:34.915: INFO: Pod "downwardapi-volume-72517444-1617-4ea7-a37b-0fe4c03f86d7": Phase="Pending", Reason="", readiness=false. Elapsed: 13.878642ms
    Jan  3 02:06:36.918: INFO: Pod "downwardapi-volume-72517444-1617-4ea7-a37b-0fe4c03f86d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017231042s
    Jan  3 02:06:38.918: INFO: Pod "downwardapi-volume-72517444-1617-4ea7-a37b-0fe4c03f86d7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017157388s
    Jan  3 02:06:40.918: INFO: Pod "downwardapi-volume-72517444-1617-4ea7-a37b-0fe4c03f86d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016283384s
    STEP: Saw pod success 01/03/23 02:06:40.918
    Jan  3 02:06:40.918: INFO: Pod "downwardapi-volume-72517444-1617-4ea7-a37b-0fe4c03f86d7" satisfied condition "Succeeded or Failed"
    Jan  3 02:06:40.919: INFO: Trying to get logs from node cncf-master pod downwardapi-volume-72517444-1617-4ea7-a37b-0fe4c03f86d7 container client-container: <nil>
    STEP: delete the pod 01/03/23 02:06:40.93
    Jan  3 02:06:40.961: INFO: Waiting for pod downwardapi-volume-72517444-1617-4ea7-a37b-0fe4c03f86d7 to disappear
    Jan  3 02:06:40.966: INFO: Pod downwardapi-volume-72517444-1617-4ea7-a37b-0fe4c03f86d7 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan  3 02:06:40.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-551" for this suite. 01/03/23 02:06:40.969
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:06:40.972
Jan  3 02:06:40.972: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename namespaces 01/03/23 02:06:40.973
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:06:40.992
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:06:40.993
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
STEP: Read namespace status 01/03/23 02:06:40.995
Jan  3 02:06:40.997: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 01/03/23 02:06:40.997
Jan  3 02:06:41.000: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 01/03/23 02:06:41
Jan  3 02:06:41.011: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jan  3 02:06:41.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9723" for this suite. 01/03/23 02:06:41.019
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]","completed":65,"skipped":1143,"failed":0}
------------------------------
• [0.050 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:06:40.972
    Jan  3 02:06:40.972: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename namespaces 01/03/23 02:06:40.973
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:06:40.992
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:06:40.993
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:298
    STEP: Read namespace status 01/03/23 02:06:40.995
    Jan  3 02:06:40.997: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 01/03/23 02:06:40.997
    Jan  3 02:06:41.000: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 01/03/23 02:06:41
    Jan  3 02:06:41.011: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jan  3 02:06:41.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-9723" for this suite. 01/03/23 02:06:41.019
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:06:41.022
Jan  3 02:06:41.022: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename taint-single-pod 01/03/23 02:06:41.023
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:06:41.037
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:06:41.039
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:166
Jan  3 02:06:41.041: INFO: Waiting up to 1m0s for all nodes to be ready
Jan  3 02:07:41.091: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
Jan  3 02:07:41.093: INFO: Starting informer...
STEP: Starting pod... 01/03/23 02:07:41.093
Jan  3 02:07:41.301: INFO: Pod is running on cncf-master. Tainting Node
STEP: Trying to apply a taint on the Node 01/03/23 02:07:41.301
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/03/23 02:07:41.311
STEP: Waiting short time to make sure Pod is queued for deletion 01/03/23 02:07:41.334
Jan  3 02:07:41.334: INFO: Pod wasn't evicted. Proceeding
Jan  3 02:07:41.334: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/03/23 02:07:41.353
STEP: Waiting some time to make sure that toleration time passed. 01/03/23 02:07:41.377
Jan  3 02:08:56.377: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:187
Jan  3 02:08:56.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-4915" for this suite. 01/03/23 02:08:56.381
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","completed":66,"skipped":1156,"failed":0}
------------------------------
• [SLOW TEST] [135.362 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:06:41.022
    Jan  3 02:06:41.022: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename taint-single-pod 01/03/23 02:06:41.023
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:06:41.037
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:06:41.039
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:166
    Jan  3 02:06:41.041: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan  3 02:07:41.091: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:289
    Jan  3 02:07:41.093: INFO: Starting informer...
    STEP: Starting pod... 01/03/23 02:07:41.093
    Jan  3 02:07:41.301: INFO: Pod is running on cncf-master. Tainting Node
    STEP: Trying to apply a taint on the Node 01/03/23 02:07:41.301
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/03/23 02:07:41.311
    STEP: Waiting short time to make sure Pod is queued for deletion 01/03/23 02:07:41.334
    Jan  3 02:07:41.334: INFO: Pod wasn't evicted. Proceeding
    Jan  3 02:07:41.334: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/03/23 02:07:41.353
    STEP: Waiting some time to make sure that toleration time passed. 01/03/23 02:07:41.377
    Jan  3 02:08:56.377: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:187
    Jan  3 02:08:56.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-single-pod-4915" for this suite. 01/03/23 02:08:56.381
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:08:56.385
Jan  3 02:08:56.385: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename projected 01/03/23 02:08:56.386
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:08:56.418
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:08:56.42
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
STEP: Creating a pod to test downward API volume plugin 01/03/23 02:08:56.422
Jan  3 02:08:56.439: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dac590a5-b106-41aa-85a5-ed2b8e5582d4" in namespace "projected-7845" to be "Succeeded or Failed"
Jan  3 02:08:56.447: INFO: Pod "downwardapi-volume-dac590a5-b106-41aa-85a5-ed2b8e5582d4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.031783ms
Jan  3 02:08:58.449: INFO: Pod "downwardapi-volume-dac590a5-b106-41aa-85a5-ed2b8e5582d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010851197s
Jan  3 02:09:00.449: INFO: Pod "downwardapi-volume-dac590a5-b106-41aa-85a5-ed2b8e5582d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010329399s
STEP: Saw pod success 01/03/23 02:09:00.449
Jan  3 02:09:00.449: INFO: Pod "downwardapi-volume-dac590a5-b106-41aa-85a5-ed2b8e5582d4" satisfied condition "Succeeded or Failed"
Jan  3 02:09:00.451: INFO: Trying to get logs from node cncf-master pod downwardapi-volume-dac590a5-b106-41aa-85a5-ed2b8e5582d4 container client-container: <nil>
STEP: delete the pod 01/03/23 02:09:00.46
Jan  3 02:09:00.475: INFO: Waiting for pod downwardapi-volume-dac590a5-b106-41aa-85a5-ed2b8e5582d4 to disappear
Jan  3 02:09:00.481: INFO: Pod downwardapi-volume-dac590a5-b106-41aa-85a5-ed2b8e5582d4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan  3 02:09:00.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7845" for this suite. 01/03/23 02:09:00.484
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","completed":67,"skipped":1174,"failed":0}
------------------------------
• [4.101 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:08:56.385
    Jan  3 02:08:56.385: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename projected 01/03/23 02:08:56.386
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:08:56.418
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:08:56.42
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:220
    STEP: Creating a pod to test downward API volume plugin 01/03/23 02:08:56.422
    Jan  3 02:08:56.439: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dac590a5-b106-41aa-85a5-ed2b8e5582d4" in namespace "projected-7845" to be "Succeeded or Failed"
    Jan  3 02:08:56.447: INFO: Pod "downwardapi-volume-dac590a5-b106-41aa-85a5-ed2b8e5582d4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.031783ms
    Jan  3 02:08:58.449: INFO: Pod "downwardapi-volume-dac590a5-b106-41aa-85a5-ed2b8e5582d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010851197s
    Jan  3 02:09:00.449: INFO: Pod "downwardapi-volume-dac590a5-b106-41aa-85a5-ed2b8e5582d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010329399s
    STEP: Saw pod success 01/03/23 02:09:00.449
    Jan  3 02:09:00.449: INFO: Pod "downwardapi-volume-dac590a5-b106-41aa-85a5-ed2b8e5582d4" satisfied condition "Succeeded or Failed"
    Jan  3 02:09:00.451: INFO: Trying to get logs from node cncf-master pod downwardapi-volume-dac590a5-b106-41aa-85a5-ed2b8e5582d4 container client-container: <nil>
    STEP: delete the pod 01/03/23 02:09:00.46
    Jan  3 02:09:00.475: INFO: Waiting for pod downwardapi-volume-dac590a5-b106-41aa-85a5-ed2b8e5582d4 to disappear
    Jan  3 02:09:00.481: INFO: Pod downwardapi-volume-dac590a5-b106-41aa-85a5-ed2b8e5582d4 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan  3 02:09:00.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7845" for this suite. 01/03/23 02:09:00.484
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:09:00.487
Jan  3 02:09:00.487: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename resourcequota 01/03/23 02:09:00.488
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:09:00.498
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:09:00.5
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
STEP: Counting existing ResourceQuota 01/03/23 02:09:00.502
STEP: Creating a ResourceQuota 01/03/23 02:09:05.514
STEP: Ensuring resource quota status is calculated 01/03/23 02:09:05.518
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan  3 02:09:07.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3852" for this suite. 01/03/23 02:09:07.523
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","completed":68,"skipped":1189,"failed":0}
------------------------------
• [SLOW TEST] [7.039 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:09:00.487
    Jan  3 02:09:00.487: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename resourcequota 01/03/23 02:09:00.488
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:09:00.498
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:09:00.5
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:65
    STEP: Counting existing ResourceQuota 01/03/23 02:09:00.502
    STEP: Creating a ResourceQuota 01/03/23 02:09:05.514
    STEP: Ensuring resource quota status is calculated 01/03/23 02:09:05.518
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan  3 02:09:07.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-3852" for this suite. 01/03/23 02:09:07.523
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:09:07.527
Jan  3 02:09:07.528: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename projected 01/03/23 02:09:07.529
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:09:07.562
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:09:07.564
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
STEP: Creating configMap with name projected-configmap-test-volume-0d60d7f9-4956-4d2c-9383-1f7cc0aa3464 01/03/23 02:09:07.566
STEP: Creating a pod to test consume configMaps 01/03/23 02:09:07.592
Jan  3 02:09:07.597: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bd44db5c-da3f-4b5b-b879-048155891c64" in namespace "projected-9526" to be "Succeeded or Failed"
Jan  3 02:09:07.630: INFO: Pod "pod-projected-configmaps-bd44db5c-da3f-4b5b-b879-048155891c64": Phase="Pending", Reason="", readiness=false. Elapsed: 32.879534ms
Jan  3 02:09:09.633: INFO: Pod "pod-projected-configmaps-bd44db5c-da3f-4b5b-b879-048155891c64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036048244s
Jan  3 02:09:11.633: INFO: Pod "pod-projected-configmaps-bd44db5c-da3f-4b5b-b879-048155891c64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035833541s
STEP: Saw pod success 01/03/23 02:09:11.633
Jan  3 02:09:11.633: INFO: Pod "pod-projected-configmaps-bd44db5c-da3f-4b5b-b879-048155891c64" satisfied condition "Succeeded or Failed"
Jan  3 02:09:11.635: INFO: Trying to get logs from node cncf-master pod pod-projected-configmaps-bd44db5c-da3f-4b5b-b879-048155891c64 container agnhost-container: <nil>
STEP: delete the pod 01/03/23 02:09:11.638
Jan  3 02:09:11.676: INFO: Waiting for pod pod-projected-configmaps-bd44db5c-da3f-4b5b-b879-048155891c64 to disappear
Jan  3 02:09:11.687: INFO: Pod pod-projected-configmaps-bd44db5c-da3f-4b5b-b879-048155891c64 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan  3 02:09:11.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9526" for this suite. 01/03/23 02:09:11.69
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":69,"skipped":1222,"failed":0}
------------------------------
• [4.166 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:09:07.527
    Jan  3 02:09:07.528: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename projected 01/03/23 02:09:07.529
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:09:07.562
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:09:07.564
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:56
    STEP: Creating configMap with name projected-configmap-test-volume-0d60d7f9-4956-4d2c-9383-1f7cc0aa3464 01/03/23 02:09:07.566
    STEP: Creating a pod to test consume configMaps 01/03/23 02:09:07.592
    Jan  3 02:09:07.597: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bd44db5c-da3f-4b5b-b879-048155891c64" in namespace "projected-9526" to be "Succeeded or Failed"
    Jan  3 02:09:07.630: INFO: Pod "pod-projected-configmaps-bd44db5c-da3f-4b5b-b879-048155891c64": Phase="Pending", Reason="", readiness=false. Elapsed: 32.879534ms
    Jan  3 02:09:09.633: INFO: Pod "pod-projected-configmaps-bd44db5c-da3f-4b5b-b879-048155891c64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036048244s
    Jan  3 02:09:11.633: INFO: Pod "pod-projected-configmaps-bd44db5c-da3f-4b5b-b879-048155891c64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035833541s
    STEP: Saw pod success 01/03/23 02:09:11.633
    Jan  3 02:09:11.633: INFO: Pod "pod-projected-configmaps-bd44db5c-da3f-4b5b-b879-048155891c64" satisfied condition "Succeeded or Failed"
    Jan  3 02:09:11.635: INFO: Trying to get logs from node cncf-master pod pod-projected-configmaps-bd44db5c-da3f-4b5b-b879-048155891c64 container agnhost-container: <nil>
    STEP: delete the pod 01/03/23 02:09:11.638
    Jan  3 02:09:11.676: INFO: Waiting for pod pod-projected-configmaps-bd44db5c-da3f-4b5b-b879-048155891c64 to disappear
    Jan  3 02:09:11.687: INFO: Pod pod-projected-configmaps-bd44db5c-da3f-4b5b-b879-048155891c64 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan  3 02:09:11.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9526" for this suite. 01/03/23 02:09:11.69
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:09:11.694
Jan  3 02:09:11.694: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename var-expansion 01/03/23 02:09:11.695
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:09:11.704
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:09:11.706
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
STEP: Creating a pod to test env composition 01/03/23 02:09:11.708
Jan  3 02:09:11.749: INFO: Waiting up to 5m0s for pod "var-expansion-60581f4d-4a37-49c0-a99e-1d25d5288614" in namespace "var-expansion-4203" to be "Succeeded or Failed"
Jan  3 02:09:11.756: INFO: Pod "var-expansion-60581f4d-4a37-49c0-a99e-1d25d5288614": Phase="Pending", Reason="", readiness=false. Elapsed: 7.043198ms
Jan  3 02:09:13.759: INFO: Pod "var-expansion-60581f4d-4a37-49c0-a99e-1d25d5288614": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009819217s
Jan  3 02:09:15.760: INFO: Pod "var-expansion-60581f4d-4a37-49c0-a99e-1d25d5288614": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010939077s
STEP: Saw pod success 01/03/23 02:09:15.76
Jan  3 02:09:15.760: INFO: Pod "var-expansion-60581f4d-4a37-49c0-a99e-1d25d5288614" satisfied condition "Succeeded or Failed"
Jan  3 02:09:15.762: INFO: Trying to get logs from node cncf-master pod var-expansion-60581f4d-4a37-49c0-a99e-1d25d5288614 container dapi-container: <nil>
STEP: delete the pod 01/03/23 02:09:15.765
Jan  3 02:09:15.780: INFO: Waiting for pod var-expansion-60581f4d-4a37-49c0-a99e-1d25d5288614 to disappear
Jan  3 02:09:15.793: INFO: Pod var-expansion-60581f4d-4a37-49c0-a99e-1d25d5288614 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan  3 02:09:15.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4203" for this suite. 01/03/23 02:09:15.795
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","completed":70,"skipped":1232,"failed":0}
------------------------------
• [4.104 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:09:11.694
    Jan  3 02:09:11.694: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename var-expansion 01/03/23 02:09:11.695
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:09:11.704
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:09:11.706
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:43
    STEP: Creating a pod to test env composition 01/03/23 02:09:11.708
    Jan  3 02:09:11.749: INFO: Waiting up to 5m0s for pod "var-expansion-60581f4d-4a37-49c0-a99e-1d25d5288614" in namespace "var-expansion-4203" to be "Succeeded or Failed"
    Jan  3 02:09:11.756: INFO: Pod "var-expansion-60581f4d-4a37-49c0-a99e-1d25d5288614": Phase="Pending", Reason="", readiness=false. Elapsed: 7.043198ms
    Jan  3 02:09:13.759: INFO: Pod "var-expansion-60581f4d-4a37-49c0-a99e-1d25d5288614": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009819217s
    Jan  3 02:09:15.760: INFO: Pod "var-expansion-60581f4d-4a37-49c0-a99e-1d25d5288614": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010939077s
    STEP: Saw pod success 01/03/23 02:09:15.76
    Jan  3 02:09:15.760: INFO: Pod "var-expansion-60581f4d-4a37-49c0-a99e-1d25d5288614" satisfied condition "Succeeded or Failed"
    Jan  3 02:09:15.762: INFO: Trying to get logs from node cncf-master pod var-expansion-60581f4d-4a37-49c0-a99e-1d25d5288614 container dapi-container: <nil>
    STEP: delete the pod 01/03/23 02:09:15.765
    Jan  3 02:09:15.780: INFO: Waiting for pod var-expansion-60581f4d-4a37-49c0-a99e-1d25d5288614 to disappear
    Jan  3 02:09:15.793: INFO: Pod var-expansion-60581f4d-4a37-49c0-a99e-1d25d5288614 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan  3 02:09:15.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-4203" for this suite. 01/03/23 02:09:15.795
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:09:15.798
Jan  3 02:09:15.798: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename kubectl 01/03/23 02:09:15.799
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:09:15.808
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:09:15.81
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
STEP: validating cluster-info 01/03/23 02:09:15.811
Jan  3 02:09:15.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-4583 cluster-info'
Jan  3 02:09:15.868: INFO: stderr: ""
Jan  3 02:09:15.868: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan  3 02:09:15.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4583" for this suite. 01/03/23 02:09:15.871
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","completed":71,"skipped":1241,"failed":0}
------------------------------
• [0.075 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1242
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:09:15.798
    Jan  3 02:09:15.798: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename kubectl 01/03/23 02:09:15.799
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:09:15.808
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:09:15.81
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1248
    STEP: validating cluster-info 01/03/23 02:09:15.811
    Jan  3 02:09:15.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-4583 cluster-info'
    Jan  3 02:09:15.868: INFO: stderr: ""
    Jan  3 02:09:15.868: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan  3 02:09:15.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4583" for this suite. 01/03/23 02:09:15.871
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:09:15.874
Jan  3 02:09:15.874: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename sysctl 01/03/23 02:09:15.875
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:09:15.882
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:09:15.884
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 01/03/23 02:09:15.886
STEP: Watching for error events or started pod 01/03/23 02:09:15.899
STEP: Waiting for pod completion 01/03/23 02:09:17.902
Jan  3 02:09:17.902: INFO: Waiting up to 3m0s for pod "sysctl-f22c8ab2-1d8a-4837-8610-45fff7623024" in namespace "sysctl-2758" to be "completed"
Jan  3 02:09:17.903: INFO: Pod "sysctl-f22c8ab2-1d8a-4837-8610-45fff7623024": Phase="Pending", Reason="", readiness=false. Elapsed: 1.610466ms
Jan  3 02:09:19.907: INFO: Pod "sysctl-f22c8ab2-1d8a-4837-8610-45fff7623024": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004745177s
Jan  3 02:09:19.907: INFO: Pod "sysctl-f22c8ab2-1d8a-4837-8610-45fff7623024" satisfied condition "completed"
STEP: Checking that the pod succeeded 01/03/23 02:09:19.908
STEP: Getting logs from the pod 01/03/23 02:09:19.908
STEP: Checking that the sysctl is actually updated 01/03/23 02:09:19.911
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Jan  3 02:09:19.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-2758" for this suite. 01/03/23 02:09:19.914
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":72,"skipped":1245,"failed":0}
------------------------------
• [4.043 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:09:15.874
    Jan  3 02:09:15.874: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename sysctl 01/03/23 02:09:15.875
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:09:15.882
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:09:15.884
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 01/03/23 02:09:15.886
    STEP: Watching for error events or started pod 01/03/23 02:09:15.899
    STEP: Waiting for pod completion 01/03/23 02:09:17.902
    Jan  3 02:09:17.902: INFO: Waiting up to 3m0s for pod "sysctl-f22c8ab2-1d8a-4837-8610-45fff7623024" in namespace "sysctl-2758" to be "completed"
    Jan  3 02:09:17.903: INFO: Pod "sysctl-f22c8ab2-1d8a-4837-8610-45fff7623024": Phase="Pending", Reason="", readiness=false. Elapsed: 1.610466ms
    Jan  3 02:09:19.907: INFO: Pod "sysctl-f22c8ab2-1d8a-4837-8610-45fff7623024": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004745177s
    Jan  3 02:09:19.907: INFO: Pod "sysctl-f22c8ab2-1d8a-4837-8610-45fff7623024" satisfied condition "completed"
    STEP: Checking that the pod succeeded 01/03/23 02:09:19.908
    STEP: Getting logs from the pod 01/03/23 02:09:19.908
    STEP: Checking that the sysctl is actually updated 01/03/23 02:09:19.911
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan  3 02:09:19.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-2758" for this suite. 01/03/23 02:09:19.914
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:09:19.918
Jan  3 02:09:19.918: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename dns 01/03/23 02:09:19.918
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:09:19.957
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:09:19.959
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 01/03/23 02:09:19.961
Jan  3 02:09:19.966: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-7549  4710601d-9e0a-462c-8488-a54eb1c99750 985286 0 2023-01-03 02:09:19 +0000 UTC <nil> <nil> map[] map[createdTime:2023-01-03T11:09:19.964321266+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T11:09:19.964321266+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update v1 2023-01-03 02:09:19 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fz5q4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fz5q4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 02:09:19.966: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-7549" to be "running and ready"
Jan  3 02:09:19.979: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 12.080874ms
Jan  3 02:09:19.979: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jan  3 02:09:21.981: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.014220265s
Jan  3 02:09:21.981: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Jan  3 02:09:21.981: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 01/03/23 02:09:21.981
Jan  3 02:09:21.981: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-7549 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 02:09:21.981: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
Jan  3 02:09:21.982: INFO: ExecWithOptions: Clientset creation
Jan  3 02:09:21.982: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-7549/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 01/03/23 02:09:22.08
Jan  3 02:09:22.080: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-7549 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 02:09:22.080: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
Jan  3 02:09:22.081: INFO: ExecWithOptions: Clientset creation
Jan  3 02:09:22.081: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-7549/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan  3 02:09:22.170: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan  3 02:09:22.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7549" for this suite. 01/03/23 02:09:22.193
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","completed":73,"skipped":1265,"failed":0}
------------------------------
• [2.278 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:09:19.918
    Jan  3 02:09:19.918: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename dns 01/03/23 02:09:19.918
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:09:19.957
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:09:19.959
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 01/03/23 02:09:19.961
    Jan  3 02:09:19.966: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-7549  4710601d-9e0a-462c-8488-a54eb1c99750 985286 0 2023-01-03 02:09:19 +0000 UTC <nil> <nil> map[] map[createdTime:2023-01-03T11:09:19.964321266+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T11:09:19.964321266+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update v1 2023-01-03 02:09:19 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fz5q4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fz5q4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 02:09:19.966: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-7549" to be "running and ready"
    Jan  3 02:09:19.979: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 12.080874ms
    Jan  3 02:09:19.979: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 02:09:21.981: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.014220265s
    Jan  3 02:09:21.981: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Jan  3 02:09:21.981: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 01/03/23 02:09:21.981
    Jan  3 02:09:21.981: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-7549 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 02:09:21.981: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    Jan  3 02:09:21.982: INFO: ExecWithOptions: Clientset creation
    Jan  3 02:09:21.982: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-7549/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 01/03/23 02:09:22.08
    Jan  3 02:09:22.080: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-7549 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 02:09:22.080: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    Jan  3 02:09:22.081: INFO: ExecWithOptions: Clientset creation
    Jan  3 02:09:22.081: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-7549/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan  3 02:09:22.170: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan  3 02:09:22.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-7549" for this suite. 01/03/23 02:09:22.193
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:09:22.196
Jan  3 02:09:22.196: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename dns 01/03/23 02:09:22.197
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:09:22.22
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:09:22.222
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 01/03/23 02:09:22.224
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 01/03/23 02:09:22.224
STEP: creating a pod to probe DNS 01/03/23 02:09:22.224
STEP: submitting the pod to kubernetes 01/03/23 02:09:22.224
Jan  3 02:09:22.265: INFO: Waiting up to 15m0s for pod "dns-test-5b187e24-e156-4d2c-b149-5ee40efec0e7" in namespace "dns-1371" to be "running"
Jan  3 02:09:22.278: INFO: Pod "dns-test-5b187e24-e156-4d2c-b149-5ee40efec0e7": Phase="Pending", Reason="", readiness=false. Elapsed: 13.044895ms
Jan  3 02:09:24.280: INFO: Pod "dns-test-5b187e24-e156-4d2c-b149-5ee40efec0e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015154168s
Jan  3 02:09:26.280: INFO: Pod "dns-test-5b187e24-e156-4d2c-b149-5ee40efec0e7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015598238s
Jan  3 02:09:28.280: INFO: Pod "dns-test-5b187e24-e156-4d2c-b149-5ee40efec0e7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015471161s
Jan  3 02:09:30.281: INFO: Pod "dns-test-5b187e24-e156-4d2c-b149-5ee40efec0e7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.015854567s
Jan  3 02:09:32.288: INFO: Pod "dns-test-5b187e24-e156-4d2c-b149-5ee40efec0e7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.022954892s
Jan  3 02:09:34.369: INFO: Pod "dns-test-5b187e24-e156-4d2c-b149-5ee40efec0e7": Phase="Pending", Reason="", readiness=false. Elapsed: 12.104133799s
Jan  3 02:09:36.281: INFO: Pod "dns-test-5b187e24-e156-4d2c-b149-5ee40efec0e7": Phase="Running", Reason="", readiness=true. Elapsed: 14.016344935s
Jan  3 02:09:36.281: INFO: Pod "dns-test-5b187e24-e156-4d2c-b149-5ee40efec0e7" satisfied condition "running"
STEP: retrieving the pod 01/03/23 02:09:36.281
STEP: looking for the results for each expected name from probers 01/03/23 02:09:36.283
Jan  3 02:09:36.291: INFO: DNS probes using dns-1371/dns-test-5b187e24-e156-4d2c-b149-5ee40efec0e7 succeeded

STEP: deleting the pod 01/03/23 02:09:36.291
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan  3 02:09:36.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1371" for this suite. 01/03/23 02:09:36.342
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","completed":74,"skipped":1276,"failed":0}
------------------------------
• [SLOW TEST] [14.148 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:09:22.196
    Jan  3 02:09:22.196: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename dns 01/03/23 02:09:22.197
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:09:22.22
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:09:22.222
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     01/03/23 02:09:22.224
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     01/03/23 02:09:22.224
    STEP: creating a pod to probe DNS 01/03/23 02:09:22.224
    STEP: submitting the pod to kubernetes 01/03/23 02:09:22.224
    Jan  3 02:09:22.265: INFO: Waiting up to 15m0s for pod "dns-test-5b187e24-e156-4d2c-b149-5ee40efec0e7" in namespace "dns-1371" to be "running"
    Jan  3 02:09:22.278: INFO: Pod "dns-test-5b187e24-e156-4d2c-b149-5ee40efec0e7": Phase="Pending", Reason="", readiness=false. Elapsed: 13.044895ms
    Jan  3 02:09:24.280: INFO: Pod "dns-test-5b187e24-e156-4d2c-b149-5ee40efec0e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015154168s
    Jan  3 02:09:26.280: INFO: Pod "dns-test-5b187e24-e156-4d2c-b149-5ee40efec0e7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015598238s
    Jan  3 02:09:28.280: INFO: Pod "dns-test-5b187e24-e156-4d2c-b149-5ee40efec0e7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015471161s
    Jan  3 02:09:30.281: INFO: Pod "dns-test-5b187e24-e156-4d2c-b149-5ee40efec0e7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.015854567s
    Jan  3 02:09:32.288: INFO: Pod "dns-test-5b187e24-e156-4d2c-b149-5ee40efec0e7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.022954892s
    Jan  3 02:09:34.369: INFO: Pod "dns-test-5b187e24-e156-4d2c-b149-5ee40efec0e7": Phase="Pending", Reason="", readiness=false. Elapsed: 12.104133799s
    Jan  3 02:09:36.281: INFO: Pod "dns-test-5b187e24-e156-4d2c-b149-5ee40efec0e7": Phase="Running", Reason="", readiness=true. Elapsed: 14.016344935s
    Jan  3 02:09:36.281: INFO: Pod "dns-test-5b187e24-e156-4d2c-b149-5ee40efec0e7" satisfied condition "running"
    STEP: retrieving the pod 01/03/23 02:09:36.281
    STEP: looking for the results for each expected name from probers 01/03/23 02:09:36.283
    Jan  3 02:09:36.291: INFO: DNS probes using dns-1371/dns-test-5b187e24-e156-4d2c-b149-5ee40efec0e7 succeeded

    STEP: deleting the pod 01/03/23 02:09:36.291
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan  3 02:09:36.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-1371" for this suite. 01/03/23 02:09:36.342
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:09:36.345
Jan  3 02:09:36.345: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename secrets 01/03/23 02:09:36.346
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:09:36.353
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:09:36.355
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
STEP: Creating secret with name secret-test-a0fe739e-d702-40c7-941d-8b26f6d1b6de 01/03/23 02:09:36.408
STEP: Creating a pod to test consume secrets 01/03/23 02:09:36.427
Jan  3 02:09:36.470: INFO: Waiting up to 5m0s for pod "pod-secrets-97d175c1-56a6-498d-9e42-efd508f97e5b" in namespace "secrets-6637" to be "Succeeded or Failed"
Jan  3 02:09:36.477: INFO: Pod "pod-secrets-97d175c1-56a6-498d-9e42-efd508f97e5b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.247627ms
Jan  3 02:09:38.523: INFO: Pod "pod-secrets-97d175c1-56a6-498d-9e42-efd508f97e5b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053379173s
Jan  3 02:09:40.480: INFO: Pod "pod-secrets-97d175c1-56a6-498d-9e42-efd508f97e5b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010327677s
Jan  3 02:09:42.480: INFO: Pod "pod-secrets-97d175c1-56a6-498d-9e42-efd508f97e5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010491117s
STEP: Saw pod success 01/03/23 02:09:42.48
Jan  3 02:09:42.480: INFO: Pod "pod-secrets-97d175c1-56a6-498d-9e42-efd508f97e5b" satisfied condition "Succeeded or Failed"
Jan  3 02:09:42.482: INFO: Trying to get logs from node cncf-master pod pod-secrets-97d175c1-56a6-498d-9e42-efd508f97e5b container secret-volume-test: <nil>
STEP: delete the pod 01/03/23 02:09:42.485
Jan  3 02:09:42.542: INFO: Waiting for pod pod-secrets-97d175c1-56a6-498d-9e42-efd508f97e5b to disappear
Jan  3 02:09:42.547: INFO: Pod pod-secrets-97d175c1-56a6-498d-9e42-efd508f97e5b no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan  3 02:09:42.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6637" for this suite. 01/03/23 02:09:42.55
STEP: Destroying namespace "secret-namespace-2645" for this suite. 01/03/23 02:09:42.552
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","completed":75,"skipped":1289,"failed":0}
------------------------------
• [SLOW TEST] [6.214 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:09:36.345
    Jan  3 02:09:36.345: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename secrets 01/03/23 02:09:36.346
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:09:36.353
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:09:36.355
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:98
    STEP: Creating secret with name secret-test-a0fe739e-d702-40c7-941d-8b26f6d1b6de 01/03/23 02:09:36.408
    STEP: Creating a pod to test consume secrets 01/03/23 02:09:36.427
    Jan  3 02:09:36.470: INFO: Waiting up to 5m0s for pod "pod-secrets-97d175c1-56a6-498d-9e42-efd508f97e5b" in namespace "secrets-6637" to be "Succeeded or Failed"
    Jan  3 02:09:36.477: INFO: Pod "pod-secrets-97d175c1-56a6-498d-9e42-efd508f97e5b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.247627ms
    Jan  3 02:09:38.523: INFO: Pod "pod-secrets-97d175c1-56a6-498d-9e42-efd508f97e5b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053379173s
    Jan  3 02:09:40.480: INFO: Pod "pod-secrets-97d175c1-56a6-498d-9e42-efd508f97e5b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010327677s
    Jan  3 02:09:42.480: INFO: Pod "pod-secrets-97d175c1-56a6-498d-9e42-efd508f97e5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010491117s
    STEP: Saw pod success 01/03/23 02:09:42.48
    Jan  3 02:09:42.480: INFO: Pod "pod-secrets-97d175c1-56a6-498d-9e42-efd508f97e5b" satisfied condition "Succeeded or Failed"
    Jan  3 02:09:42.482: INFO: Trying to get logs from node cncf-master pod pod-secrets-97d175c1-56a6-498d-9e42-efd508f97e5b container secret-volume-test: <nil>
    STEP: delete the pod 01/03/23 02:09:42.485
    Jan  3 02:09:42.542: INFO: Waiting for pod pod-secrets-97d175c1-56a6-498d-9e42-efd508f97e5b to disappear
    Jan  3 02:09:42.547: INFO: Pod pod-secrets-97d175c1-56a6-498d-9e42-efd508f97e5b no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan  3 02:09:42.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-6637" for this suite. 01/03/23 02:09:42.55
    STEP: Destroying namespace "secret-namespace-2645" for this suite. 01/03/23 02:09:42.552
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:09:42.56
Jan  3 02:09:42.560: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename downward-api 01/03/23 02:09:42.561
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:09:42.593
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:09:42.595
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
STEP: Creating the pod 01/03/23 02:09:42.597
Jan  3 02:09:42.608: INFO: Waiting up to 5m0s for pod "labelsupdate7dd69a32-efb3-40bf-bbce-3dee41e8fbd1" in namespace "downward-api-2426" to be "running and ready"
Jan  3 02:09:42.616: INFO: Pod "labelsupdate7dd69a32-efb3-40bf-bbce-3dee41e8fbd1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.94739ms
Jan  3 02:09:42.616: INFO: The phase of Pod labelsupdate7dd69a32-efb3-40bf-bbce-3dee41e8fbd1 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 02:09:44.619: INFO: Pod "labelsupdate7dd69a32-efb3-40bf-bbce-3dee41e8fbd1": Phase="Running", Reason="", readiness=true. Elapsed: 2.011238409s
Jan  3 02:09:44.619: INFO: The phase of Pod labelsupdate7dd69a32-efb3-40bf-bbce-3dee41e8fbd1 is Running (Ready = true)
Jan  3 02:09:44.619: INFO: Pod "labelsupdate7dd69a32-efb3-40bf-bbce-3dee41e8fbd1" satisfied condition "running and ready"
Jan  3 02:09:45.131: INFO: Successfully updated pod "labelsupdate7dd69a32-efb3-40bf-bbce-3dee41e8fbd1"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan  3 02:09:49.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2426" for this suite. 01/03/23 02:09:49.157
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","completed":76,"skipped":1297,"failed":0}
------------------------------
• [SLOW TEST] [6.600 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:09:42.56
    Jan  3 02:09:42.560: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename downward-api 01/03/23 02:09:42.561
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:09:42.593
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:09:42.595
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:129
    STEP: Creating the pod 01/03/23 02:09:42.597
    Jan  3 02:09:42.608: INFO: Waiting up to 5m0s for pod "labelsupdate7dd69a32-efb3-40bf-bbce-3dee41e8fbd1" in namespace "downward-api-2426" to be "running and ready"
    Jan  3 02:09:42.616: INFO: Pod "labelsupdate7dd69a32-efb3-40bf-bbce-3dee41e8fbd1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.94739ms
    Jan  3 02:09:42.616: INFO: The phase of Pod labelsupdate7dd69a32-efb3-40bf-bbce-3dee41e8fbd1 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 02:09:44.619: INFO: Pod "labelsupdate7dd69a32-efb3-40bf-bbce-3dee41e8fbd1": Phase="Running", Reason="", readiness=true. Elapsed: 2.011238409s
    Jan  3 02:09:44.619: INFO: The phase of Pod labelsupdate7dd69a32-efb3-40bf-bbce-3dee41e8fbd1 is Running (Ready = true)
    Jan  3 02:09:44.619: INFO: Pod "labelsupdate7dd69a32-efb3-40bf-bbce-3dee41e8fbd1" satisfied condition "running and ready"
    Jan  3 02:09:45.131: INFO: Successfully updated pod "labelsupdate7dd69a32-efb3-40bf-bbce-3dee41e8fbd1"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan  3 02:09:49.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2426" for this suite. 01/03/23 02:09:49.157
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:09:49.16
Jan  3 02:09:49.160: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename pods 01/03/23 02:09:49.161
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:09:49.17
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:09:49.172
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
STEP: creating the pod 01/03/23 02:09:49.174
STEP: submitting the pod to kubernetes 01/03/23 02:09:49.174
Jan  3 02:09:49.194: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-857d532f-5b1d-49ab-a7d7-26649f7ea62a" in namespace "pods-8680" to be "running and ready"
Jan  3 02:09:49.199: INFO: Pod "pod-update-activedeadlineseconds-857d532f-5b1d-49ab-a7d7-26649f7ea62a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.522474ms
Jan  3 02:09:49.199: INFO: The phase of Pod pod-update-activedeadlineseconds-857d532f-5b1d-49ab-a7d7-26649f7ea62a is Pending, waiting for it to be Running (with Ready = true)
Jan  3 02:09:51.239: INFO: Pod "pod-update-activedeadlineseconds-857d532f-5b1d-49ab-a7d7-26649f7ea62a": Phase="Running", Reason="", readiness=true. Elapsed: 2.045703657s
Jan  3 02:09:51.239: INFO: The phase of Pod pod-update-activedeadlineseconds-857d532f-5b1d-49ab-a7d7-26649f7ea62a is Running (Ready = true)
Jan  3 02:09:51.239: INFO: Pod "pod-update-activedeadlineseconds-857d532f-5b1d-49ab-a7d7-26649f7ea62a" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 01/03/23 02:09:51.241
STEP: updating the pod 01/03/23 02:09:51.243
Jan  3 02:09:51.749: INFO: Successfully updated pod "pod-update-activedeadlineseconds-857d532f-5b1d-49ab-a7d7-26649f7ea62a"
Jan  3 02:09:51.749: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-857d532f-5b1d-49ab-a7d7-26649f7ea62a" in namespace "pods-8680" to be "terminated with reason DeadlineExceeded"
Jan  3 02:09:51.759: INFO: Pod "pod-update-activedeadlineseconds-857d532f-5b1d-49ab-a7d7-26649f7ea62a": Phase="Running", Reason="", readiness=true. Elapsed: 9.320601ms
Jan  3 02:09:53.762: INFO: Pod "pod-update-activedeadlineseconds-857d532f-5b1d-49ab-a7d7-26649f7ea62a": Phase="Running", Reason="", readiness=true. Elapsed: 2.012187634s
Jan  3 02:09:55.762: INFO: Pod "pod-update-activedeadlineseconds-857d532f-5b1d-49ab-a7d7-26649f7ea62a": Phase="Running", Reason="", readiness=false. Elapsed: 4.012939859s
Jan  3 02:09:57.762: INFO: Pod "pod-update-activedeadlineseconds-857d532f-5b1d-49ab-a7d7-26649f7ea62a": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.013040624s
Jan  3 02:09:57.763: INFO: Pod "pod-update-activedeadlineseconds-857d532f-5b1d-49ab-a7d7-26649f7ea62a" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan  3 02:09:57.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8680" for this suite. 01/03/23 02:09:57.765
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","completed":77,"skipped":1297,"failed":0}
------------------------------
• [SLOW TEST] [8.608 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:09:49.16
    Jan  3 02:09:49.160: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename pods 01/03/23 02:09:49.161
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:09:49.17
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:09:49.172
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:397
    STEP: creating the pod 01/03/23 02:09:49.174
    STEP: submitting the pod to kubernetes 01/03/23 02:09:49.174
    Jan  3 02:09:49.194: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-857d532f-5b1d-49ab-a7d7-26649f7ea62a" in namespace "pods-8680" to be "running and ready"
    Jan  3 02:09:49.199: INFO: Pod "pod-update-activedeadlineseconds-857d532f-5b1d-49ab-a7d7-26649f7ea62a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.522474ms
    Jan  3 02:09:49.199: INFO: The phase of Pod pod-update-activedeadlineseconds-857d532f-5b1d-49ab-a7d7-26649f7ea62a is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 02:09:51.239: INFO: Pod "pod-update-activedeadlineseconds-857d532f-5b1d-49ab-a7d7-26649f7ea62a": Phase="Running", Reason="", readiness=true. Elapsed: 2.045703657s
    Jan  3 02:09:51.239: INFO: The phase of Pod pod-update-activedeadlineseconds-857d532f-5b1d-49ab-a7d7-26649f7ea62a is Running (Ready = true)
    Jan  3 02:09:51.239: INFO: Pod "pod-update-activedeadlineseconds-857d532f-5b1d-49ab-a7d7-26649f7ea62a" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 01/03/23 02:09:51.241
    STEP: updating the pod 01/03/23 02:09:51.243
    Jan  3 02:09:51.749: INFO: Successfully updated pod "pod-update-activedeadlineseconds-857d532f-5b1d-49ab-a7d7-26649f7ea62a"
    Jan  3 02:09:51.749: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-857d532f-5b1d-49ab-a7d7-26649f7ea62a" in namespace "pods-8680" to be "terminated with reason DeadlineExceeded"
    Jan  3 02:09:51.759: INFO: Pod "pod-update-activedeadlineseconds-857d532f-5b1d-49ab-a7d7-26649f7ea62a": Phase="Running", Reason="", readiness=true. Elapsed: 9.320601ms
    Jan  3 02:09:53.762: INFO: Pod "pod-update-activedeadlineseconds-857d532f-5b1d-49ab-a7d7-26649f7ea62a": Phase="Running", Reason="", readiness=true. Elapsed: 2.012187634s
    Jan  3 02:09:55.762: INFO: Pod "pod-update-activedeadlineseconds-857d532f-5b1d-49ab-a7d7-26649f7ea62a": Phase="Running", Reason="", readiness=false. Elapsed: 4.012939859s
    Jan  3 02:09:57.762: INFO: Pod "pod-update-activedeadlineseconds-857d532f-5b1d-49ab-a7d7-26649f7ea62a": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.013040624s
    Jan  3 02:09:57.763: INFO: Pod "pod-update-activedeadlineseconds-857d532f-5b1d-49ab-a7d7-26649f7ea62a" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan  3 02:09:57.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-8680" for this suite. 01/03/23 02:09:57.765
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:09:57.769
Jan  3 02:09:57.769: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename container-lifecycle-hook 01/03/23 02:09:57.77
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:09:57.785
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:09:57.787
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 01/03/23 02:09:57.794
Jan  3 02:09:57.799: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-3925" to be "running and ready"
Jan  3 02:09:57.808: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 9.213548ms
Jan  3 02:09:57.808: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan  3 02:10:00.478: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.678798802s
Jan  3 02:10:00.478: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan  3 02:10:00.478: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
STEP: create the pod with lifecycle hook 01/03/23 02:10:00.479
Jan  3 02:10:00.975: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-3925" to be "running and ready"
Jan  3 02:10:00.976: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.511639ms
Jan  3 02:10:00.977: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan  3 02:10:02.979: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0041429s
Jan  3 02:10:02.979: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan  3 02:10:04.980: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.005277307s
Jan  3 02:10:04.980: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Jan  3 02:10:04.980: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 01/03/23 02:10:04.982
Jan  3 02:10:04.985: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan  3 02:10:04.996: INFO: Pod pod-with-prestop-exec-hook still exists
Jan  3 02:10:06.997: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan  3 02:10:07.019: INFO: Pod pod-with-prestop-exec-hook still exists
Jan  3 02:10:08.997: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan  3 02:10:08.999: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 01/03/23 02:10:08.999
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jan  3 02:10:09.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3925" for this suite. 01/03/23 02:10:09.006
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","completed":78,"skipped":1312,"failed":0}
------------------------------
• [SLOW TEST] [11.240 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:114

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:09:57.769
    Jan  3 02:09:57.769: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/03/23 02:09:57.77
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:09:57.785
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:09:57.787
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 01/03/23 02:09:57.794
    Jan  3 02:09:57.799: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-3925" to be "running and ready"
    Jan  3 02:09:57.808: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 9.213548ms
    Jan  3 02:09:57.808: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 02:10:00.478: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.678798802s
    Jan  3 02:10:00.478: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan  3 02:10:00.478: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:114
    STEP: create the pod with lifecycle hook 01/03/23 02:10:00.479
    Jan  3 02:10:00.975: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-3925" to be "running and ready"
    Jan  3 02:10:00.976: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.511639ms
    Jan  3 02:10:00.977: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 02:10:02.979: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0041429s
    Jan  3 02:10:02.979: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 02:10:04.980: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.005277307s
    Jan  3 02:10:04.980: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Jan  3 02:10:04.980: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 01/03/23 02:10:04.982
    Jan  3 02:10:04.985: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan  3 02:10:04.996: INFO: Pod pod-with-prestop-exec-hook still exists
    Jan  3 02:10:06.997: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan  3 02:10:07.019: INFO: Pod pod-with-prestop-exec-hook still exists
    Jan  3 02:10:08.997: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan  3 02:10:08.999: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 01/03/23 02:10:08.999
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jan  3 02:10:09.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-3925" for this suite. 01/03/23 02:10:09.006
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:10:09.009
Jan  3 02:10:09.009: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename secrets 01/03/23 02:10:09.01
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:10:09.037
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:10:09.039
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
STEP: creating a secret 01/03/23 02:10:09.041
STEP: listing secrets in all namespaces to ensure that there are more than zero 01/03/23 02:10:09.044
STEP: patching the secret 01/03/23 02:10:09.047
STEP: deleting the secret using a LabelSelector 01/03/23 02:10:09.078
STEP: listing secrets in all namespaces, searching for label name and value in patch 01/03/23 02:10:09.081
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jan  3 02:10:09.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9627" for this suite. 01/03/23 02:10:09.09
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","completed":79,"skipped":1316,"failed":0}
------------------------------
• [0.083 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:10:09.009
    Jan  3 02:10:09.009: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename secrets 01/03/23 02:10:09.01
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:10:09.037
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:10:09.039
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:153
    STEP: creating a secret 01/03/23 02:10:09.041
    STEP: listing secrets in all namespaces to ensure that there are more than zero 01/03/23 02:10:09.044
    STEP: patching the secret 01/03/23 02:10:09.047
    STEP: deleting the secret using a LabelSelector 01/03/23 02:10:09.078
    STEP: listing secrets in all namespaces, searching for label name and value in patch 01/03/23 02:10:09.081
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jan  3 02:10:09.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-9627" for this suite. 01/03/23 02:10:09.09
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:10:09.093
Jan  3 02:10:09.093: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename kubelet-test 01/03/23 02:10:09.094
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:10:09.104
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:10:09.105
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Jan  3 02:10:09.120: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsa6f3b191-9c7f-41c6-9d97-ad918c01a4f0" in namespace "kubelet-test-9055" to be "running and ready"
Jan  3 02:10:09.126: INFO: Pod "busybox-readonly-fsa6f3b191-9c7f-41c6-9d97-ad918c01a4f0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.508056ms
Jan  3 02:10:09.126: INFO: The phase of Pod busybox-readonly-fsa6f3b191-9c7f-41c6-9d97-ad918c01a4f0 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 02:10:11.129: INFO: Pod "busybox-readonly-fsa6f3b191-9c7f-41c6-9d97-ad918c01a4f0": Phase="Running", Reason="", readiness=true. Elapsed: 2.00864865s
Jan  3 02:10:11.129: INFO: The phase of Pod busybox-readonly-fsa6f3b191-9c7f-41c6-9d97-ad918c01a4f0 is Running (Ready = true)
Jan  3 02:10:11.129: INFO: Pod "busybox-readonly-fsa6f3b191-9c7f-41c6-9d97-ad918c01a4f0" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan  3 02:10:11.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9055" for this suite. 01/03/23 02:10:11.136
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","completed":80,"skipped":1330,"failed":0}
------------------------------
• [2.046 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:10:09.093
    Jan  3 02:10:09.093: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename kubelet-test 01/03/23 02:10:09.094
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:10:09.104
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:10:09.105
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Jan  3 02:10:09.120: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsa6f3b191-9c7f-41c6-9d97-ad918c01a4f0" in namespace "kubelet-test-9055" to be "running and ready"
    Jan  3 02:10:09.126: INFO: Pod "busybox-readonly-fsa6f3b191-9c7f-41c6-9d97-ad918c01a4f0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.508056ms
    Jan  3 02:10:09.126: INFO: The phase of Pod busybox-readonly-fsa6f3b191-9c7f-41c6-9d97-ad918c01a4f0 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 02:10:11.129: INFO: Pod "busybox-readonly-fsa6f3b191-9c7f-41c6-9d97-ad918c01a4f0": Phase="Running", Reason="", readiness=true. Elapsed: 2.00864865s
    Jan  3 02:10:11.129: INFO: The phase of Pod busybox-readonly-fsa6f3b191-9c7f-41c6-9d97-ad918c01a4f0 is Running (Ready = true)
    Jan  3 02:10:11.129: INFO: Pod "busybox-readonly-fsa6f3b191-9c7f-41c6-9d97-ad918c01a4f0" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan  3 02:10:11.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-9055" for this suite. 01/03/23 02:10:11.136
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:10:11.139
Jan  3 02:10:11.139: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename replicaset 01/03/23 02:10:11.14
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:10:11.149
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:10:11.152
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 01/03/23 02:10:11.154
STEP: Verify that the required pods have come up 01/03/23 02:10:11.165
Jan  3 02:10:11.166: INFO: Pod name sample-pod: Found 0 pods out of 3
Jan  3 02:10:16.169: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 01/03/23 02:10:16.169
Jan  3 02:10:16.170: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 01/03/23 02:10:16.17
STEP: DeleteCollection of the ReplicaSets 01/03/23 02:10:16.178
STEP: After DeleteCollection verify that ReplicaSets have been deleted 01/03/23 02:10:16.181
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan  3 02:10:16.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4025" for this suite. 01/03/23 02:10:16.191
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","completed":81,"skipped":1331,"failed":0}
------------------------------
• [SLOW TEST] [5.080 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:10:11.139
    Jan  3 02:10:11.139: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename replicaset 01/03/23 02:10:11.14
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:10:11.149
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:10:11.152
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 01/03/23 02:10:11.154
    STEP: Verify that the required pods have come up 01/03/23 02:10:11.165
    Jan  3 02:10:11.166: INFO: Pod name sample-pod: Found 0 pods out of 3
    Jan  3 02:10:16.169: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 01/03/23 02:10:16.169
    Jan  3 02:10:16.170: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 01/03/23 02:10:16.17
    STEP: DeleteCollection of the ReplicaSets 01/03/23 02:10:16.178
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 01/03/23 02:10:16.181
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan  3 02:10:16.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-4025" for this suite. 01/03/23 02:10:16.191
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Services
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:10:16.22
Jan  3 02:10:16.220: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename services 01/03/23 02:10:16.221
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:10:16.282
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:10:16.284
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
STEP: creating service in namespace services-117 01/03/23 02:10:16.286
Jan  3 02:10:16.291: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-117" to be "running and ready"
Jan  3 02:10:16.311: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 19.858654ms
Jan  3 02:10:16.311: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jan  3 02:10:18.316: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.025566829s
Jan  3 02:10:18.316: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Jan  3 02:10:18.316: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Jan  3 02:10:18.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-117 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jan  3 02:10:18.457: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jan  3 02:10:18.457: INFO: stdout: "iptables"
Jan  3 02:10:18.457: INFO: proxyMode: iptables
Jan  3 02:10:18.477: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jan  3 02:10:18.482: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-117 01/03/23 02:10:18.482
STEP: creating replication controller affinity-clusterip-timeout in namespace services-117 01/03/23 02:10:18.49
I0103 02:10:18.507587      23 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-117, replica count: 3
I0103 02:10:21.558747      23 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  3 02:10:21.561: INFO: Creating new exec pod
Jan  3 02:10:21.565: INFO: Waiting up to 5m0s for pod "execpod-affinitysbwh5" in namespace "services-117" to be "running"
Jan  3 02:10:21.574: INFO: Pod "execpod-affinitysbwh5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.121772ms
Jan  3 02:10:23.585: INFO: Pod "execpod-affinitysbwh5": Phase="Running", Reason="", readiness=true. Elapsed: 2.019601477s
Jan  3 02:10:23.585: INFO: Pod "execpod-affinitysbwh5" satisfied condition "running"
Jan  3 02:10:24.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-117 exec execpod-affinitysbwh5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Jan  3 02:10:24.711: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Jan  3 02:10:24.711: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  3 02:10:24.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-117 exec execpod-affinitysbwh5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.130.213 80'
Jan  3 02:10:24.843: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.130.213 80\nConnection to 10.96.130.213 80 port [tcp/http] succeeded!\n"
Jan  3 02:10:24.843: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  3 02:10:24.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-117 exec execpod-affinitysbwh5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.130.213:80/ ; done'
Jan  3 02:10:25.036: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.130.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.130.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.130.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.130.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.130.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.130.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.130.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.130.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.130.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.130.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.130.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.130.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.130.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.130.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.130.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.130.213:80/\n"
Jan  3 02:10:25.036: INFO: stdout: "\naffinity-clusterip-timeout-zsq6j\naffinity-clusterip-timeout-zsq6j\naffinity-clusterip-timeout-zsq6j\naffinity-clusterip-timeout-zsq6j\naffinity-clusterip-timeout-zsq6j\naffinity-clusterip-timeout-zsq6j\naffinity-clusterip-timeout-zsq6j\naffinity-clusterip-timeout-zsq6j\naffinity-clusterip-timeout-zsq6j\naffinity-clusterip-timeout-zsq6j\naffinity-clusterip-timeout-zsq6j\naffinity-clusterip-timeout-zsq6j\naffinity-clusterip-timeout-zsq6j\naffinity-clusterip-timeout-zsq6j\naffinity-clusterip-timeout-zsq6j\naffinity-clusterip-timeout-zsq6j"
Jan  3 02:10:25.036: INFO: Received response from host: affinity-clusterip-timeout-zsq6j
Jan  3 02:10:25.036: INFO: Received response from host: affinity-clusterip-timeout-zsq6j
Jan  3 02:10:25.036: INFO: Received response from host: affinity-clusterip-timeout-zsq6j
Jan  3 02:10:25.036: INFO: Received response from host: affinity-clusterip-timeout-zsq6j
Jan  3 02:10:25.036: INFO: Received response from host: affinity-clusterip-timeout-zsq6j
Jan  3 02:10:25.036: INFO: Received response from host: affinity-clusterip-timeout-zsq6j
Jan  3 02:10:25.036: INFO: Received response from host: affinity-clusterip-timeout-zsq6j
Jan  3 02:10:25.036: INFO: Received response from host: affinity-clusterip-timeout-zsq6j
Jan  3 02:10:25.036: INFO: Received response from host: affinity-clusterip-timeout-zsq6j
Jan  3 02:10:25.036: INFO: Received response from host: affinity-clusterip-timeout-zsq6j
Jan  3 02:10:25.036: INFO: Received response from host: affinity-clusterip-timeout-zsq6j
Jan  3 02:10:25.036: INFO: Received response from host: affinity-clusterip-timeout-zsq6j
Jan  3 02:10:25.036: INFO: Received response from host: affinity-clusterip-timeout-zsq6j
Jan  3 02:10:25.036: INFO: Received response from host: affinity-clusterip-timeout-zsq6j
Jan  3 02:10:25.036: INFO: Received response from host: affinity-clusterip-timeout-zsq6j
Jan  3 02:10:25.036: INFO: Received response from host: affinity-clusterip-timeout-zsq6j
Jan  3 02:10:25.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-117 exec execpod-affinitysbwh5 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.96.130.213:80/'
Jan  3 02:10:25.171: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.96.130.213:80/\n"
Jan  3 02:10:25.171: INFO: stdout: "affinity-clusterip-timeout-zsq6j"
Jan  3 02:10:45.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-117 exec execpod-affinitysbwh5 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.96.130.213:80/'
Jan  3 02:10:45.326: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.96.130.213:80/\n"
Jan  3 02:10:45.326: INFO: stdout: "affinity-clusterip-timeout-6j7zr"
Jan  3 02:10:45.326: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-117, will wait for the garbage collector to delete the pods 01/03/23 02:10:45.347
Jan  3 02:10:45.405: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 2.527254ms
Jan  3 02:10:45.506: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 101.066932ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  3 02:10:47.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-117" for this suite. 01/03/23 02:10:47.966
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","completed":82,"skipped":1339,"failed":0}
------------------------------
• [SLOW TEST] [31.748 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:10:16.22
    Jan  3 02:10:16.220: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename services 01/03/23 02:10:16.221
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:10:16.282
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:10:16.284
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2173
    STEP: creating service in namespace services-117 01/03/23 02:10:16.286
    Jan  3 02:10:16.291: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-117" to be "running and ready"
    Jan  3 02:10:16.311: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 19.858654ms
    Jan  3 02:10:16.311: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 02:10:18.316: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.025566829s
    Jan  3 02:10:18.316: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Jan  3 02:10:18.316: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Jan  3 02:10:18.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-117 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Jan  3 02:10:18.457: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    Jan  3 02:10:18.457: INFO: stdout: "iptables"
    Jan  3 02:10:18.457: INFO: proxyMode: iptables
    Jan  3 02:10:18.477: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Jan  3 02:10:18.482: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-clusterip-timeout in namespace services-117 01/03/23 02:10:18.482
    STEP: creating replication controller affinity-clusterip-timeout in namespace services-117 01/03/23 02:10:18.49
    I0103 02:10:18.507587      23 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-117, replica count: 3
    I0103 02:10:21.558747      23 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  3 02:10:21.561: INFO: Creating new exec pod
    Jan  3 02:10:21.565: INFO: Waiting up to 5m0s for pod "execpod-affinitysbwh5" in namespace "services-117" to be "running"
    Jan  3 02:10:21.574: INFO: Pod "execpod-affinitysbwh5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.121772ms
    Jan  3 02:10:23.585: INFO: Pod "execpod-affinitysbwh5": Phase="Running", Reason="", readiness=true. Elapsed: 2.019601477s
    Jan  3 02:10:23.585: INFO: Pod "execpod-affinitysbwh5" satisfied condition "running"
    Jan  3 02:10:24.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-117 exec execpod-affinitysbwh5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
    Jan  3 02:10:24.711: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
    Jan  3 02:10:24.711: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  3 02:10:24.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-117 exec execpod-affinitysbwh5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.130.213 80'
    Jan  3 02:10:24.843: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.130.213 80\nConnection to 10.96.130.213 80 port [tcp/http] succeeded!\n"
    Jan  3 02:10:24.843: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  3 02:10:24.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-117 exec execpod-affinitysbwh5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.130.213:80/ ; done'
    Jan  3 02:10:25.036: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.130.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.130.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.130.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.130.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.130.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.130.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.130.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.130.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.130.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.130.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.130.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.130.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.130.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.130.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.130.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.130.213:80/\n"
    Jan  3 02:10:25.036: INFO: stdout: "\naffinity-clusterip-timeout-zsq6j\naffinity-clusterip-timeout-zsq6j\naffinity-clusterip-timeout-zsq6j\naffinity-clusterip-timeout-zsq6j\naffinity-clusterip-timeout-zsq6j\naffinity-clusterip-timeout-zsq6j\naffinity-clusterip-timeout-zsq6j\naffinity-clusterip-timeout-zsq6j\naffinity-clusterip-timeout-zsq6j\naffinity-clusterip-timeout-zsq6j\naffinity-clusterip-timeout-zsq6j\naffinity-clusterip-timeout-zsq6j\naffinity-clusterip-timeout-zsq6j\naffinity-clusterip-timeout-zsq6j\naffinity-clusterip-timeout-zsq6j\naffinity-clusterip-timeout-zsq6j"
    Jan  3 02:10:25.036: INFO: Received response from host: affinity-clusterip-timeout-zsq6j
    Jan  3 02:10:25.036: INFO: Received response from host: affinity-clusterip-timeout-zsq6j
    Jan  3 02:10:25.036: INFO: Received response from host: affinity-clusterip-timeout-zsq6j
    Jan  3 02:10:25.036: INFO: Received response from host: affinity-clusterip-timeout-zsq6j
    Jan  3 02:10:25.036: INFO: Received response from host: affinity-clusterip-timeout-zsq6j
    Jan  3 02:10:25.036: INFO: Received response from host: affinity-clusterip-timeout-zsq6j
    Jan  3 02:10:25.036: INFO: Received response from host: affinity-clusterip-timeout-zsq6j
    Jan  3 02:10:25.036: INFO: Received response from host: affinity-clusterip-timeout-zsq6j
    Jan  3 02:10:25.036: INFO: Received response from host: affinity-clusterip-timeout-zsq6j
    Jan  3 02:10:25.036: INFO: Received response from host: affinity-clusterip-timeout-zsq6j
    Jan  3 02:10:25.036: INFO: Received response from host: affinity-clusterip-timeout-zsq6j
    Jan  3 02:10:25.036: INFO: Received response from host: affinity-clusterip-timeout-zsq6j
    Jan  3 02:10:25.036: INFO: Received response from host: affinity-clusterip-timeout-zsq6j
    Jan  3 02:10:25.036: INFO: Received response from host: affinity-clusterip-timeout-zsq6j
    Jan  3 02:10:25.036: INFO: Received response from host: affinity-clusterip-timeout-zsq6j
    Jan  3 02:10:25.036: INFO: Received response from host: affinity-clusterip-timeout-zsq6j
    Jan  3 02:10:25.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-117 exec execpod-affinitysbwh5 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.96.130.213:80/'
    Jan  3 02:10:25.171: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.96.130.213:80/\n"
    Jan  3 02:10:25.171: INFO: stdout: "affinity-clusterip-timeout-zsq6j"
    Jan  3 02:10:45.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-117 exec execpod-affinitysbwh5 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.96.130.213:80/'
    Jan  3 02:10:45.326: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.96.130.213:80/\n"
    Jan  3 02:10:45.326: INFO: stdout: "affinity-clusterip-timeout-6j7zr"
    Jan  3 02:10:45.326: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-117, will wait for the garbage collector to delete the pods 01/03/23 02:10:45.347
    Jan  3 02:10:45.405: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 2.527254ms
    Jan  3 02:10:45.506: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 101.066932ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  3 02:10:47.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-117" for this suite. 01/03/23 02:10:47.966
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:10:47.969
Jan  3 02:10:47.969: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename sched-preemption 01/03/23 02:10:47.969
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:10:48.024
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:10:48.026
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan  3 02:10:48.044: INFO: Waiting up to 1m0s for all nodes to be ready
Jan  3 02:11:48.100: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:11:48.102
Jan  3 02:11:48.102: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename sched-preemption-path 01/03/23 02:11:48.103
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:11:48.114
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:11:48.116
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:690
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
Jan  3 02:11:48.142: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Jan  3 02:11:48.144: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/framework.go:187
Jan  3 02:11:48.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-2763" for this suite. 01/03/23 02:11:48.163
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:706
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jan  3 02:11:48.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7549" for this suite. 01/03/23 02:11:48.194
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","completed":83,"skipped":1350,"failed":0}
------------------------------
• [SLOW TEST] [60.290 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:683
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:733

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:10:47.969
    Jan  3 02:10:47.969: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename sched-preemption 01/03/23 02:10:47.969
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:10:48.024
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:10:48.026
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jan  3 02:10:48.044: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan  3 02:11:48.100: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:11:48.102
    Jan  3 02:11:48.102: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename sched-preemption-path 01/03/23 02:11:48.103
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:11:48.114
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:11:48.116
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:690
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:733
    Jan  3 02:11:48.142: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Jan  3 02:11:48.144: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/framework.go:187
    Jan  3 02:11:48.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-2763" for this suite. 01/03/23 02:11:48.163
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:706
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jan  3 02:11:48.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-7549" for this suite. 01/03/23 02:11:48.194
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:11:48.259
Jan  3 02:11:48.259: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename downward-api 01/03/23 02:11:48.259
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:11:48.274
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:11:48.275
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
STEP: Creating a pod to test downward API volume plugin 01/03/23 02:11:48.277
Jan  3 02:11:48.300: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c19d15fb-55ac-432c-88d4-909067cd11df" in namespace "downward-api-3214" to be "Succeeded or Failed"
Jan  3 02:11:48.349: INFO: Pod "downwardapi-volume-c19d15fb-55ac-432c-88d4-909067cd11df": Phase="Pending", Reason="", readiness=false. Elapsed: 49.438881ms
Jan  3 02:11:50.353: INFO: Pod "downwardapi-volume-c19d15fb-55ac-432c-88d4-909067cd11df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052911572s
Jan  3 02:11:52.352: INFO: Pod "downwardapi-volume-c19d15fb-55ac-432c-88d4-909067cd11df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051977022s
STEP: Saw pod success 01/03/23 02:11:52.352
Jan  3 02:11:52.352: INFO: Pod "downwardapi-volume-c19d15fb-55ac-432c-88d4-909067cd11df" satisfied condition "Succeeded or Failed"
Jan  3 02:11:52.354: INFO: Trying to get logs from node cncf-master pod downwardapi-volume-c19d15fb-55ac-432c-88d4-909067cd11df container client-container: <nil>
STEP: delete the pod 01/03/23 02:11:52.364
Jan  3 02:11:52.377: INFO: Waiting for pod downwardapi-volume-c19d15fb-55ac-432c-88d4-909067cd11df to disappear
Jan  3 02:11:52.383: INFO: Pod downwardapi-volume-c19d15fb-55ac-432c-88d4-909067cd11df no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan  3 02:11:52.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3214" for this suite. 01/03/23 02:11:52.386
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":84,"skipped":1356,"failed":0}
------------------------------
• [4.129 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:11:48.259
    Jan  3 02:11:48.259: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename downward-api 01/03/23 02:11:48.259
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:11:48.274
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:11:48.275
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:67
    STEP: Creating a pod to test downward API volume plugin 01/03/23 02:11:48.277
    Jan  3 02:11:48.300: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c19d15fb-55ac-432c-88d4-909067cd11df" in namespace "downward-api-3214" to be "Succeeded or Failed"
    Jan  3 02:11:48.349: INFO: Pod "downwardapi-volume-c19d15fb-55ac-432c-88d4-909067cd11df": Phase="Pending", Reason="", readiness=false. Elapsed: 49.438881ms
    Jan  3 02:11:50.353: INFO: Pod "downwardapi-volume-c19d15fb-55ac-432c-88d4-909067cd11df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052911572s
    Jan  3 02:11:52.352: INFO: Pod "downwardapi-volume-c19d15fb-55ac-432c-88d4-909067cd11df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051977022s
    STEP: Saw pod success 01/03/23 02:11:52.352
    Jan  3 02:11:52.352: INFO: Pod "downwardapi-volume-c19d15fb-55ac-432c-88d4-909067cd11df" satisfied condition "Succeeded or Failed"
    Jan  3 02:11:52.354: INFO: Trying to get logs from node cncf-master pod downwardapi-volume-c19d15fb-55ac-432c-88d4-909067cd11df container client-container: <nil>
    STEP: delete the pod 01/03/23 02:11:52.364
    Jan  3 02:11:52.377: INFO: Waiting for pod downwardapi-volume-c19d15fb-55ac-432c-88d4-909067cd11df to disappear
    Jan  3 02:11:52.383: INFO: Pod downwardapi-volume-c19d15fb-55ac-432c-88d4-909067cd11df no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan  3 02:11:52.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-3214" for this suite. 01/03/23 02:11:52.386
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:11:52.389
Jan  3 02:11:52.389: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename cronjob 01/03/23 02:11:52.39
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:11:52.408
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:11:52.41
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 01/03/23 02:11:52.412
STEP: Ensuring a job is scheduled 01/03/23 02:11:52.416
STEP: Ensuring exactly one is scheduled 01/03/23 02:12:00.44
STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/03/23 02:12:00.442
STEP: Ensuring no more jobs are scheduled 01/03/23 02:12:00.444
STEP: Removing cronjob 01/03/23 02:17:00.449
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan  3 02:17:00.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-8231" for this suite. 01/03/23 02:17:00.477
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","completed":85,"skipped":1381,"failed":0}
------------------------------
• [SLOW TEST] [308.170 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:11:52.389
    Jan  3 02:11:52.389: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename cronjob 01/03/23 02:11:52.39
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:11:52.408
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:11:52.41
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 01/03/23 02:11:52.412
    STEP: Ensuring a job is scheduled 01/03/23 02:11:52.416
    STEP: Ensuring exactly one is scheduled 01/03/23 02:12:00.44
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/03/23 02:12:00.442
    STEP: Ensuring no more jobs are scheduled 01/03/23 02:12:00.444
    STEP: Removing cronjob 01/03/23 02:17:00.449
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan  3 02:17:00.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-8231" for this suite. 01/03/23 02:17:00.477
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:17:00.561
Jan  3 02:17:00.561: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename emptydir-wrapper 01/03/23 02:17:00.562
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:17:00.586
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:17:00.588
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 01/03/23 02:17:00.59
STEP: Creating RC which spawns configmap-volume pods 01/03/23 02:17:01.128
Jan  3 02:17:01.152: INFO: Pod name wrapped-volume-race-8f64c39e-13cf-4017-8d7e-1d8004c79276: Found 0 pods out of 5
Jan  3 02:17:06.157: INFO: Pod name wrapped-volume-race-8f64c39e-13cf-4017-8d7e-1d8004c79276: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/03/23 02:17:06.157
Jan  3 02:17:06.157: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8f64c39e-13cf-4017-8d7e-1d8004c79276-4jwnw" in namespace "emptydir-wrapper-9520" to be "running"
Jan  3 02:17:06.159: INFO: Pod "wrapped-volume-race-8f64c39e-13cf-4017-8d7e-1d8004c79276-4jwnw": Phase="Pending", Reason="", readiness=false. Elapsed: 1.873434ms
Jan  3 02:17:08.162: INFO: Pod "wrapped-volume-race-8f64c39e-13cf-4017-8d7e-1d8004c79276-4jwnw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005677078s
Jan  3 02:17:10.163: INFO: Pod "wrapped-volume-race-8f64c39e-13cf-4017-8d7e-1d8004c79276-4jwnw": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005857991s
Jan  3 02:17:12.162: INFO: Pod "wrapped-volume-race-8f64c39e-13cf-4017-8d7e-1d8004c79276-4jwnw": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005378939s
Jan  3 02:17:14.162: INFO: Pod "wrapped-volume-race-8f64c39e-13cf-4017-8d7e-1d8004c79276-4jwnw": Phase="Pending", Reason="", readiness=false. Elapsed: 8.005070499s
Jan  3 02:17:16.162: INFO: Pod "wrapped-volume-race-8f64c39e-13cf-4017-8d7e-1d8004c79276-4jwnw": Phase="Pending", Reason="", readiness=false. Elapsed: 10.004840233s
Jan  3 02:17:18.163: INFO: Pod "wrapped-volume-race-8f64c39e-13cf-4017-8d7e-1d8004c79276-4jwnw": Phase="Running", Reason="", readiness=true. Elapsed: 12.005915762s
Jan  3 02:17:18.163: INFO: Pod "wrapped-volume-race-8f64c39e-13cf-4017-8d7e-1d8004c79276-4jwnw" satisfied condition "running"
Jan  3 02:17:18.163: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8f64c39e-13cf-4017-8d7e-1d8004c79276-5bf6j" in namespace "emptydir-wrapper-9520" to be "running"
Jan  3 02:17:18.165: INFO: Pod "wrapped-volume-race-8f64c39e-13cf-4017-8d7e-1d8004c79276-5bf6j": Phase="Running", Reason="", readiness=true. Elapsed: 2.022177ms
Jan  3 02:17:18.165: INFO: Pod "wrapped-volume-race-8f64c39e-13cf-4017-8d7e-1d8004c79276-5bf6j" satisfied condition "running"
Jan  3 02:17:18.165: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8f64c39e-13cf-4017-8d7e-1d8004c79276-975w6" in namespace "emptydir-wrapper-9520" to be "running"
Jan  3 02:17:18.167: INFO: Pod "wrapped-volume-race-8f64c39e-13cf-4017-8d7e-1d8004c79276-975w6": Phase="Running", Reason="", readiness=true. Elapsed: 1.959289ms
Jan  3 02:17:18.167: INFO: Pod "wrapped-volume-race-8f64c39e-13cf-4017-8d7e-1d8004c79276-975w6" satisfied condition "running"
Jan  3 02:17:18.167: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8f64c39e-13cf-4017-8d7e-1d8004c79276-9l2dc" in namespace "emptydir-wrapper-9520" to be "running"
Jan  3 02:17:18.169: INFO: Pod "wrapped-volume-race-8f64c39e-13cf-4017-8d7e-1d8004c79276-9l2dc": Phase="Running", Reason="", readiness=true. Elapsed: 1.97523ms
Jan  3 02:17:18.169: INFO: Pod "wrapped-volume-race-8f64c39e-13cf-4017-8d7e-1d8004c79276-9l2dc" satisfied condition "running"
Jan  3 02:17:18.169: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8f64c39e-13cf-4017-8d7e-1d8004c79276-fc2fq" in namespace "emptydir-wrapper-9520" to be "running"
Jan  3 02:17:18.171: INFO: Pod "wrapped-volume-race-8f64c39e-13cf-4017-8d7e-1d8004c79276-fc2fq": Phase="Running", Reason="", readiness=true. Elapsed: 1.944431ms
Jan  3 02:17:18.171: INFO: Pod "wrapped-volume-race-8f64c39e-13cf-4017-8d7e-1d8004c79276-fc2fq" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-8f64c39e-13cf-4017-8d7e-1d8004c79276 in namespace emptydir-wrapper-9520, will wait for the garbage collector to delete the pods 01/03/23 02:17:18.171
Jan  3 02:17:18.240: INFO: Deleting ReplicationController wrapped-volume-race-8f64c39e-13cf-4017-8d7e-1d8004c79276 took: 16.493759ms
Jan  3 02:17:18.341: INFO: Terminating ReplicationController wrapped-volume-race-8f64c39e-13cf-4017-8d7e-1d8004c79276 pods took: 100.315351ms
STEP: Creating RC which spawns configmap-volume pods 01/03/23 02:17:23.463
Jan  3 02:17:23.499: INFO: Pod name wrapped-volume-race-2a86528f-2c75-44f7-a5a3-8abb838a809a: Found 0 pods out of 5
Jan  3 02:17:28.506: INFO: Pod name wrapped-volume-race-2a86528f-2c75-44f7-a5a3-8abb838a809a: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/03/23 02:17:28.506
Jan  3 02:17:28.506: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2a86528f-2c75-44f7-a5a3-8abb838a809a-dcft2" in namespace "emptydir-wrapper-9520" to be "running"
Jan  3 02:17:28.509: INFO: Pod "wrapped-volume-race-2a86528f-2c75-44f7-a5a3-8abb838a809a-dcft2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.448754ms
Jan  3 02:17:30.513: INFO: Pod "wrapped-volume-race-2a86528f-2c75-44f7-a5a3-8abb838a809a-dcft2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006309731s
Jan  3 02:17:32.513: INFO: Pod "wrapped-volume-race-2a86528f-2c75-44f7-a5a3-8abb838a809a-dcft2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006219868s
Jan  3 02:17:34.525: INFO: Pod "wrapped-volume-race-2a86528f-2c75-44f7-a5a3-8abb838a809a-dcft2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018931257s
Jan  3 02:17:36.518: INFO: Pod "wrapped-volume-race-2a86528f-2c75-44f7-a5a3-8abb838a809a-dcft2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.01198559s
Jan  3 02:17:38.514: INFO: Pod "wrapped-volume-race-2a86528f-2c75-44f7-a5a3-8abb838a809a-dcft2": Phase="Running", Reason="", readiness=true. Elapsed: 10.007582429s
Jan  3 02:17:38.514: INFO: Pod "wrapped-volume-race-2a86528f-2c75-44f7-a5a3-8abb838a809a-dcft2" satisfied condition "running"
Jan  3 02:17:38.514: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2a86528f-2c75-44f7-a5a3-8abb838a809a-f9bp7" in namespace "emptydir-wrapper-9520" to be "running"
Jan  3 02:17:38.516: INFO: Pod "wrapped-volume-race-2a86528f-2c75-44f7-a5a3-8abb838a809a-f9bp7": Phase="Running", Reason="", readiness=true. Elapsed: 2.221971ms
Jan  3 02:17:38.516: INFO: Pod "wrapped-volume-race-2a86528f-2c75-44f7-a5a3-8abb838a809a-f9bp7" satisfied condition "running"
Jan  3 02:17:38.516: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2a86528f-2c75-44f7-a5a3-8abb838a809a-m5hr8" in namespace "emptydir-wrapper-9520" to be "running"
Jan  3 02:17:38.518: INFO: Pod "wrapped-volume-race-2a86528f-2c75-44f7-a5a3-8abb838a809a-m5hr8": Phase="Running", Reason="", readiness=true. Elapsed: 1.923611ms
Jan  3 02:17:38.518: INFO: Pod "wrapped-volume-race-2a86528f-2c75-44f7-a5a3-8abb838a809a-m5hr8" satisfied condition "running"
Jan  3 02:17:38.518: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2a86528f-2c75-44f7-a5a3-8abb838a809a-vjhtl" in namespace "emptydir-wrapper-9520" to be "running"
Jan  3 02:17:38.520: INFO: Pod "wrapped-volume-race-2a86528f-2c75-44f7-a5a3-8abb838a809a-vjhtl": Phase="Running", Reason="", readiness=true. Elapsed: 1.953114ms
Jan  3 02:17:38.520: INFO: Pod "wrapped-volume-race-2a86528f-2c75-44f7-a5a3-8abb838a809a-vjhtl" satisfied condition "running"
Jan  3 02:17:38.520: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2a86528f-2c75-44f7-a5a3-8abb838a809a-w7cds" in namespace "emptydir-wrapper-9520" to be "running"
Jan  3 02:17:38.522: INFO: Pod "wrapped-volume-race-2a86528f-2c75-44f7-a5a3-8abb838a809a-w7cds": Phase="Running", Reason="", readiness=true. Elapsed: 2.080744ms
Jan  3 02:17:38.522: INFO: Pod "wrapped-volume-race-2a86528f-2c75-44f7-a5a3-8abb838a809a-w7cds" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-2a86528f-2c75-44f7-a5a3-8abb838a809a in namespace emptydir-wrapper-9520, will wait for the garbage collector to delete the pods 01/03/23 02:17:38.522
Jan  3 02:17:38.578: INFO: Deleting ReplicationController wrapped-volume-race-2a86528f-2c75-44f7-a5a3-8abb838a809a took: 3.213098ms
Jan  3 02:17:38.679: INFO: Terminating ReplicationController wrapped-volume-race-2a86528f-2c75-44f7-a5a3-8abb838a809a pods took: 101.020802ms
STEP: Creating RC which spawns configmap-volume pods 01/03/23 02:17:51.485
Jan  3 02:17:51.508: INFO: Pod name wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305: Found 0 pods out of 5
Jan  3 02:17:56.513: INFO: Pod name wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/03/23 02:17:56.513
Jan  3 02:17:56.513: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305-7k6pm" in namespace "emptydir-wrapper-9520" to be "running"
Jan  3 02:17:56.515: INFO: Pod "wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305-7k6pm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.373251ms
Jan  3 02:17:58.531: INFO: Pod "wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305-7k6pm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017459081s
Jan  3 02:18:00.520: INFO: Pod "wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305-7k6pm": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006944452s
Jan  3 02:18:02.519: INFO: Pod "wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305-7k6pm": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00583084s
Jan  3 02:18:04.519: INFO: Pod "wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305-7k6pm": Phase="Pending", Reason="", readiness=false. Elapsed: 8.005953704s
Jan  3 02:18:06.520: INFO: Pod "wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305-7k6pm": Phase="Pending", Reason="", readiness=false. Elapsed: 10.00735991s
Jan  3 02:18:08.520: INFO: Pod "wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305-7k6pm": Phase="Pending", Reason="", readiness=false. Elapsed: 12.00669884s
Jan  3 02:18:10.519: INFO: Pod "wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305-7k6pm": Phase="Running", Reason="", readiness=true. Elapsed: 14.005926503s
Jan  3 02:18:10.519: INFO: Pod "wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305-7k6pm" satisfied condition "running"
Jan  3 02:18:10.519: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305-hswf8" in namespace "emptydir-wrapper-9520" to be "running"
Jan  3 02:18:10.521: INFO: Pod "wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305-hswf8": Phase="Running", Reason="", readiness=true. Elapsed: 2.054181ms
Jan  3 02:18:10.521: INFO: Pod "wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305-hswf8" satisfied condition "running"
Jan  3 02:18:10.521: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305-mxhcq" in namespace "emptydir-wrapper-9520" to be "running"
Jan  3 02:18:10.523: INFO: Pod "wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305-mxhcq": Phase="Running", Reason="", readiness=true. Elapsed: 1.967336ms
Jan  3 02:18:10.523: INFO: Pod "wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305-mxhcq" satisfied condition "running"
Jan  3 02:18:10.523: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305-nl4gm" in namespace "emptydir-wrapper-9520" to be "running"
Jan  3 02:18:10.525: INFO: Pod "wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305-nl4gm": Phase="Running", Reason="", readiness=true. Elapsed: 1.890071ms
Jan  3 02:18:10.525: INFO: Pod "wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305-nl4gm" satisfied condition "running"
Jan  3 02:18:10.525: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305-xz425" in namespace "emptydir-wrapper-9520" to be "running"
Jan  3 02:18:10.527: INFO: Pod "wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305-xz425": Phase="Running", Reason="", readiness=true. Elapsed: 1.95382ms
Jan  3 02:18:10.527: INFO: Pod "wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305-xz425" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305 in namespace emptydir-wrapper-9520, will wait for the garbage collector to delete the pods 01/03/23 02:18:10.527
Jan  3 02:18:10.583: INFO: Deleting ReplicationController wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305 took: 3.080082ms
Jan  3 02:18:10.683: INFO: Terminating ReplicationController wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305 pods took: 100.080908ms
STEP: Cleaning up the configMaps 01/03/23 02:18:21.584
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Jan  3 02:18:21.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9520" for this suite. 01/03/23 02:18:22.003
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","completed":86,"skipped":1405,"failed":0}
------------------------------
• [SLOW TEST] [81.444 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:17:00.561
    Jan  3 02:17:00.561: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename emptydir-wrapper 01/03/23 02:17:00.562
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:17:00.586
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:17:00.588
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 01/03/23 02:17:00.59
    STEP: Creating RC which spawns configmap-volume pods 01/03/23 02:17:01.128
    Jan  3 02:17:01.152: INFO: Pod name wrapped-volume-race-8f64c39e-13cf-4017-8d7e-1d8004c79276: Found 0 pods out of 5
    Jan  3 02:17:06.157: INFO: Pod name wrapped-volume-race-8f64c39e-13cf-4017-8d7e-1d8004c79276: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/03/23 02:17:06.157
    Jan  3 02:17:06.157: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8f64c39e-13cf-4017-8d7e-1d8004c79276-4jwnw" in namespace "emptydir-wrapper-9520" to be "running"
    Jan  3 02:17:06.159: INFO: Pod "wrapped-volume-race-8f64c39e-13cf-4017-8d7e-1d8004c79276-4jwnw": Phase="Pending", Reason="", readiness=false. Elapsed: 1.873434ms
    Jan  3 02:17:08.162: INFO: Pod "wrapped-volume-race-8f64c39e-13cf-4017-8d7e-1d8004c79276-4jwnw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005677078s
    Jan  3 02:17:10.163: INFO: Pod "wrapped-volume-race-8f64c39e-13cf-4017-8d7e-1d8004c79276-4jwnw": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005857991s
    Jan  3 02:17:12.162: INFO: Pod "wrapped-volume-race-8f64c39e-13cf-4017-8d7e-1d8004c79276-4jwnw": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005378939s
    Jan  3 02:17:14.162: INFO: Pod "wrapped-volume-race-8f64c39e-13cf-4017-8d7e-1d8004c79276-4jwnw": Phase="Pending", Reason="", readiness=false. Elapsed: 8.005070499s
    Jan  3 02:17:16.162: INFO: Pod "wrapped-volume-race-8f64c39e-13cf-4017-8d7e-1d8004c79276-4jwnw": Phase="Pending", Reason="", readiness=false. Elapsed: 10.004840233s
    Jan  3 02:17:18.163: INFO: Pod "wrapped-volume-race-8f64c39e-13cf-4017-8d7e-1d8004c79276-4jwnw": Phase="Running", Reason="", readiness=true. Elapsed: 12.005915762s
    Jan  3 02:17:18.163: INFO: Pod "wrapped-volume-race-8f64c39e-13cf-4017-8d7e-1d8004c79276-4jwnw" satisfied condition "running"
    Jan  3 02:17:18.163: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8f64c39e-13cf-4017-8d7e-1d8004c79276-5bf6j" in namespace "emptydir-wrapper-9520" to be "running"
    Jan  3 02:17:18.165: INFO: Pod "wrapped-volume-race-8f64c39e-13cf-4017-8d7e-1d8004c79276-5bf6j": Phase="Running", Reason="", readiness=true. Elapsed: 2.022177ms
    Jan  3 02:17:18.165: INFO: Pod "wrapped-volume-race-8f64c39e-13cf-4017-8d7e-1d8004c79276-5bf6j" satisfied condition "running"
    Jan  3 02:17:18.165: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8f64c39e-13cf-4017-8d7e-1d8004c79276-975w6" in namespace "emptydir-wrapper-9520" to be "running"
    Jan  3 02:17:18.167: INFO: Pod "wrapped-volume-race-8f64c39e-13cf-4017-8d7e-1d8004c79276-975w6": Phase="Running", Reason="", readiness=true. Elapsed: 1.959289ms
    Jan  3 02:17:18.167: INFO: Pod "wrapped-volume-race-8f64c39e-13cf-4017-8d7e-1d8004c79276-975w6" satisfied condition "running"
    Jan  3 02:17:18.167: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8f64c39e-13cf-4017-8d7e-1d8004c79276-9l2dc" in namespace "emptydir-wrapper-9520" to be "running"
    Jan  3 02:17:18.169: INFO: Pod "wrapped-volume-race-8f64c39e-13cf-4017-8d7e-1d8004c79276-9l2dc": Phase="Running", Reason="", readiness=true. Elapsed: 1.97523ms
    Jan  3 02:17:18.169: INFO: Pod "wrapped-volume-race-8f64c39e-13cf-4017-8d7e-1d8004c79276-9l2dc" satisfied condition "running"
    Jan  3 02:17:18.169: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8f64c39e-13cf-4017-8d7e-1d8004c79276-fc2fq" in namespace "emptydir-wrapper-9520" to be "running"
    Jan  3 02:17:18.171: INFO: Pod "wrapped-volume-race-8f64c39e-13cf-4017-8d7e-1d8004c79276-fc2fq": Phase="Running", Reason="", readiness=true. Elapsed: 1.944431ms
    Jan  3 02:17:18.171: INFO: Pod "wrapped-volume-race-8f64c39e-13cf-4017-8d7e-1d8004c79276-fc2fq" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-8f64c39e-13cf-4017-8d7e-1d8004c79276 in namespace emptydir-wrapper-9520, will wait for the garbage collector to delete the pods 01/03/23 02:17:18.171
    Jan  3 02:17:18.240: INFO: Deleting ReplicationController wrapped-volume-race-8f64c39e-13cf-4017-8d7e-1d8004c79276 took: 16.493759ms
    Jan  3 02:17:18.341: INFO: Terminating ReplicationController wrapped-volume-race-8f64c39e-13cf-4017-8d7e-1d8004c79276 pods took: 100.315351ms
    STEP: Creating RC which spawns configmap-volume pods 01/03/23 02:17:23.463
    Jan  3 02:17:23.499: INFO: Pod name wrapped-volume-race-2a86528f-2c75-44f7-a5a3-8abb838a809a: Found 0 pods out of 5
    Jan  3 02:17:28.506: INFO: Pod name wrapped-volume-race-2a86528f-2c75-44f7-a5a3-8abb838a809a: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/03/23 02:17:28.506
    Jan  3 02:17:28.506: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2a86528f-2c75-44f7-a5a3-8abb838a809a-dcft2" in namespace "emptydir-wrapper-9520" to be "running"
    Jan  3 02:17:28.509: INFO: Pod "wrapped-volume-race-2a86528f-2c75-44f7-a5a3-8abb838a809a-dcft2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.448754ms
    Jan  3 02:17:30.513: INFO: Pod "wrapped-volume-race-2a86528f-2c75-44f7-a5a3-8abb838a809a-dcft2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006309731s
    Jan  3 02:17:32.513: INFO: Pod "wrapped-volume-race-2a86528f-2c75-44f7-a5a3-8abb838a809a-dcft2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006219868s
    Jan  3 02:17:34.525: INFO: Pod "wrapped-volume-race-2a86528f-2c75-44f7-a5a3-8abb838a809a-dcft2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018931257s
    Jan  3 02:17:36.518: INFO: Pod "wrapped-volume-race-2a86528f-2c75-44f7-a5a3-8abb838a809a-dcft2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.01198559s
    Jan  3 02:17:38.514: INFO: Pod "wrapped-volume-race-2a86528f-2c75-44f7-a5a3-8abb838a809a-dcft2": Phase="Running", Reason="", readiness=true. Elapsed: 10.007582429s
    Jan  3 02:17:38.514: INFO: Pod "wrapped-volume-race-2a86528f-2c75-44f7-a5a3-8abb838a809a-dcft2" satisfied condition "running"
    Jan  3 02:17:38.514: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2a86528f-2c75-44f7-a5a3-8abb838a809a-f9bp7" in namespace "emptydir-wrapper-9520" to be "running"
    Jan  3 02:17:38.516: INFO: Pod "wrapped-volume-race-2a86528f-2c75-44f7-a5a3-8abb838a809a-f9bp7": Phase="Running", Reason="", readiness=true. Elapsed: 2.221971ms
    Jan  3 02:17:38.516: INFO: Pod "wrapped-volume-race-2a86528f-2c75-44f7-a5a3-8abb838a809a-f9bp7" satisfied condition "running"
    Jan  3 02:17:38.516: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2a86528f-2c75-44f7-a5a3-8abb838a809a-m5hr8" in namespace "emptydir-wrapper-9520" to be "running"
    Jan  3 02:17:38.518: INFO: Pod "wrapped-volume-race-2a86528f-2c75-44f7-a5a3-8abb838a809a-m5hr8": Phase="Running", Reason="", readiness=true. Elapsed: 1.923611ms
    Jan  3 02:17:38.518: INFO: Pod "wrapped-volume-race-2a86528f-2c75-44f7-a5a3-8abb838a809a-m5hr8" satisfied condition "running"
    Jan  3 02:17:38.518: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2a86528f-2c75-44f7-a5a3-8abb838a809a-vjhtl" in namespace "emptydir-wrapper-9520" to be "running"
    Jan  3 02:17:38.520: INFO: Pod "wrapped-volume-race-2a86528f-2c75-44f7-a5a3-8abb838a809a-vjhtl": Phase="Running", Reason="", readiness=true. Elapsed: 1.953114ms
    Jan  3 02:17:38.520: INFO: Pod "wrapped-volume-race-2a86528f-2c75-44f7-a5a3-8abb838a809a-vjhtl" satisfied condition "running"
    Jan  3 02:17:38.520: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2a86528f-2c75-44f7-a5a3-8abb838a809a-w7cds" in namespace "emptydir-wrapper-9520" to be "running"
    Jan  3 02:17:38.522: INFO: Pod "wrapped-volume-race-2a86528f-2c75-44f7-a5a3-8abb838a809a-w7cds": Phase="Running", Reason="", readiness=true. Elapsed: 2.080744ms
    Jan  3 02:17:38.522: INFO: Pod "wrapped-volume-race-2a86528f-2c75-44f7-a5a3-8abb838a809a-w7cds" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-2a86528f-2c75-44f7-a5a3-8abb838a809a in namespace emptydir-wrapper-9520, will wait for the garbage collector to delete the pods 01/03/23 02:17:38.522
    Jan  3 02:17:38.578: INFO: Deleting ReplicationController wrapped-volume-race-2a86528f-2c75-44f7-a5a3-8abb838a809a took: 3.213098ms
    Jan  3 02:17:38.679: INFO: Terminating ReplicationController wrapped-volume-race-2a86528f-2c75-44f7-a5a3-8abb838a809a pods took: 101.020802ms
    STEP: Creating RC which spawns configmap-volume pods 01/03/23 02:17:51.485
    Jan  3 02:17:51.508: INFO: Pod name wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305: Found 0 pods out of 5
    Jan  3 02:17:56.513: INFO: Pod name wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/03/23 02:17:56.513
    Jan  3 02:17:56.513: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305-7k6pm" in namespace "emptydir-wrapper-9520" to be "running"
    Jan  3 02:17:56.515: INFO: Pod "wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305-7k6pm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.373251ms
    Jan  3 02:17:58.531: INFO: Pod "wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305-7k6pm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017459081s
    Jan  3 02:18:00.520: INFO: Pod "wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305-7k6pm": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006944452s
    Jan  3 02:18:02.519: INFO: Pod "wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305-7k6pm": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00583084s
    Jan  3 02:18:04.519: INFO: Pod "wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305-7k6pm": Phase="Pending", Reason="", readiness=false. Elapsed: 8.005953704s
    Jan  3 02:18:06.520: INFO: Pod "wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305-7k6pm": Phase="Pending", Reason="", readiness=false. Elapsed: 10.00735991s
    Jan  3 02:18:08.520: INFO: Pod "wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305-7k6pm": Phase="Pending", Reason="", readiness=false. Elapsed: 12.00669884s
    Jan  3 02:18:10.519: INFO: Pod "wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305-7k6pm": Phase="Running", Reason="", readiness=true. Elapsed: 14.005926503s
    Jan  3 02:18:10.519: INFO: Pod "wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305-7k6pm" satisfied condition "running"
    Jan  3 02:18:10.519: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305-hswf8" in namespace "emptydir-wrapper-9520" to be "running"
    Jan  3 02:18:10.521: INFO: Pod "wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305-hswf8": Phase="Running", Reason="", readiness=true. Elapsed: 2.054181ms
    Jan  3 02:18:10.521: INFO: Pod "wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305-hswf8" satisfied condition "running"
    Jan  3 02:18:10.521: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305-mxhcq" in namespace "emptydir-wrapper-9520" to be "running"
    Jan  3 02:18:10.523: INFO: Pod "wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305-mxhcq": Phase="Running", Reason="", readiness=true. Elapsed: 1.967336ms
    Jan  3 02:18:10.523: INFO: Pod "wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305-mxhcq" satisfied condition "running"
    Jan  3 02:18:10.523: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305-nl4gm" in namespace "emptydir-wrapper-9520" to be "running"
    Jan  3 02:18:10.525: INFO: Pod "wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305-nl4gm": Phase="Running", Reason="", readiness=true. Elapsed: 1.890071ms
    Jan  3 02:18:10.525: INFO: Pod "wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305-nl4gm" satisfied condition "running"
    Jan  3 02:18:10.525: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305-xz425" in namespace "emptydir-wrapper-9520" to be "running"
    Jan  3 02:18:10.527: INFO: Pod "wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305-xz425": Phase="Running", Reason="", readiness=true. Elapsed: 1.95382ms
    Jan  3 02:18:10.527: INFO: Pod "wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305-xz425" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305 in namespace emptydir-wrapper-9520, will wait for the garbage collector to delete the pods 01/03/23 02:18:10.527
    Jan  3 02:18:10.583: INFO: Deleting ReplicationController wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305 took: 3.080082ms
    Jan  3 02:18:10.683: INFO: Terminating ReplicationController wrapped-volume-race-a3d7de29-0d30-460e-9fe8-a28559e1f305 pods took: 100.080908ms
    STEP: Cleaning up the configMaps 01/03/23 02:18:21.584
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Jan  3 02:18:21.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-9520" for this suite. 01/03/23 02:18:22.003
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:18:22.006
Jan  3 02:18:22.006: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename services 01/03/23 02:18:22.007
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:18:22.024
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:18:22.026
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
STEP: fetching services 01/03/23 02:18:22.03
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  3 02:18:22.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2232" for this suite. 01/03/23 02:18:22.056
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","completed":87,"skipped":1412,"failed":0}
------------------------------
• [0.054 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:18:22.006
    Jan  3 02:18:22.006: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename services 01/03/23 02:18:22.007
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:18:22.024
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:18:22.026
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3206
    STEP: fetching services 01/03/23 02:18:22.03
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  3 02:18:22.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2232" for this suite. 01/03/23 02:18:22.056
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:18:22.06
Jan  3 02:18:22.060: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename resourcequota 01/03/23 02:18:22.061
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:18:22.07
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:18:22.072
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
STEP: Counting existing ResourceQuota 01/03/23 02:18:39.077
STEP: Creating a ResourceQuota 01/03/23 02:18:44.083
STEP: Ensuring resource quota status is calculated 01/03/23 02:18:44.087
STEP: Creating a ConfigMap 01/03/23 02:18:46.091
STEP: Ensuring resource quota status captures configMap creation 01/03/23 02:18:46.103
STEP: Deleting a ConfigMap 01/03/23 02:18:48.107
STEP: Ensuring resource quota status released usage 01/03/23 02:18:48.109
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan  3 02:18:50.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2730" for this suite. 01/03/23 02:18:50.116
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","completed":88,"skipped":1427,"failed":0}
------------------------------
• [SLOW TEST] [28.059 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:18:22.06
    Jan  3 02:18:22.060: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename resourcequota 01/03/23 02:18:22.061
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:18:22.07
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:18:22.072
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:316
    STEP: Counting existing ResourceQuota 01/03/23 02:18:39.077
    STEP: Creating a ResourceQuota 01/03/23 02:18:44.083
    STEP: Ensuring resource quota status is calculated 01/03/23 02:18:44.087
    STEP: Creating a ConfigMap 01/03/23 02:18:46.091
    STEP: Ensuring resource quota status captures configMap creation 01/03/23 02:18:46.103
    STEP: Deleting a ConfigMap 01/03/23 02:18:48.107
    STEP: Ensuring resource quota status released usage 01/03/23 02:18:48.109
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan  3 02:18:50.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-2730" for this suite. 01/03/23 02:18:50.116
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:18:50.12
Jan  3 02:18:50.120: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename downward-api 01/03/23 02:18:50.121
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:18:50.134
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:18:50.136
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
STEP: Creating a pod to test downward API volume plugin 01/03/23 02:18:50.138
Jan  3 02:18:50.160: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0e06fbee-d8cf-4363-bc5f-ca93969048cc" in namespace "downward-api-8982" to be "Succeeded or Failed"
Jan  3 02:18:50.172: INFO: Pod "downwardapi-volume-0e06fbee-d8cf-4363-bc5f-ca93969048cc": Phase="Pending", Reason="", readiness=false. Elapsed: 11.465913ms
Jan  3 02:18:52.175: INFO: Pod "downwardapi-volume-0e06fbee-d8cf-4363-bc5f-ca93969048cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014398973s
Jan  3 02:18:54.175: INFO: Pod "downwardapi-volume-0e06fbee-d8cf-4363-bc5f-ca93969048cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014864651s
STEP: Saw pod success 01/03/23 02:18:54.175
Jan  3 02:18:54.175: INFO: Pod "downwardapi-volume-0e06fbee-d8cf-4363-bc5f-ca93969048cc" satisfied condition "Succeeded or Failed"
Jan  3 02:18:54.177: INFO: Trying to get logs from node cncf-master pod downwardapi-volume-0e06fbee-d8cf-4363-bc5f-ca93969048cc container client-container: <nil>
STEP: delete the pod 01/03/23 02:18:54.188
Jan  3 02:18:54.209: INFO: Waiting for pod downwardapi-volume-0e06fbee-d8cf-4363-bc5f-ca93969048cc to disappear
Jan  3 02:18:54.215: INFO: Pod downwardapi-volume-0e06fbee-d8cf-4363-bc5f-ca93969048cc no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan  3 02:18:54.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8982" for this suite. 01/03/23 02:18:54.217
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","completed":89,"skipped":1453,"failed":0}
------------------------------
• [4.099 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:18:50.12
    Jan  3 02:18:50.120: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename downward-api 01/03/23 02:18:50.121
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:18:50.134
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:18:50.136
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:192
    STEP: Creating a pod to test downward API volume plugin 01/03/23 02:18:50.138
    Jan  3 02:18:50.160: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0e06fbee-d8cf-4363-bc5f-ca93969048cc" in namespace "downward-api-8982" to be "Succeeded or Failed"
    Jan  3 02:18:50.172: INFO: Pod "downwardapi-volume-0e06fbee-d8cf-4363-bc5f-ca93969048cc": Phase="Pending", Reason="", readiness=false. Elapsed: 11.465913ms
    Jan  3 02:18:52.175: INFO: Pod "downwardapi-volume-0e06fbee-d8cf-4363-bc5f-ca93969048cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014398973s
    Jan  3 02:18:54.175: INFO: Pod "downwardapi-volume-0e06fbee-d8cf-4363-bc5f-ca93969048cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014864651s
    STEP: Saw pod success 01/03/23 02:18:54.175
    Jan  3 02:18:54.175: INFO: Pod "downwardapi-volume-0e06fbee-d8cf-4363-bc5f-ca93969048cc" satisfied condition "Succeeded or Failed"
    Jan  3 02:18:54.177: INFO: Trying to get logs from node cncf-master pod downwardapi-volume-0e06fbee-d8cf-4363-bc5f-ca93969048cc container client-container: <nil>
    STEP: delete the pod 01/03/23 02:18:54.188
    Jan  3 02:18:54.209: INFO: Waiting for pod downwardapi-volume-0e06fbee-d8cf-4363-bc5f-ca93969048cc to disappear
    Jan  3 02:18:54.215: INFO: Pod downwardapi-volume-0e06fbee-d8cf-4363-bc5f-ca93969048cc no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan  3 02:18:54.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8982" for this suite. 01/03/23 02:18:54.217
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:18:54.221
Jan  3 02:18:54.221: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename replicaset 01/03/23 02:18:54.221
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:18:54.232
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:18:54.233
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 01/03/23 02:18:54.251
STEP: Verify that the required pods have come up. 01/03/23 02:18:54.255
Jan  3 02:18:54.260: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan  3 02:18:59.272: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/03/23 02:18:59.272
STEP: Getting /status 01/03/23 02:18:59.272
Jan  3 02:18:59.289: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 01/03/23 02:18:59.289
Jan  3 02:18:59.313: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 01/03/23 02:18:59.313
Jan  3 02:18:59.314: INFO: Observed &ReplicaSet event: ADDED
Jan  3 02:18:59.315: INFO: Observed &ReplicaSet event: MODIFIED
Jan  3 02:18:59.315: INFO: Observed &ReplicaSet event: MODIFIED
Jan  3 02:18:59.315: INFO: Observed &ReplicaSet event: MODIFIED
Jan  3 02:18:59.315: INFO: Found replicaset test-rs in namespace replicaset-1195 with labels: map[name:sample-pod pod:httpd] annotations: map[createdTime:2023-01-03T11:18:54.25322169+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T11:18:54.25322169+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan  3 02:18:59.315: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 01/03/23 02:18:59.315
Jan  3 02:18:59.315: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan  3 02:18:59.318: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 01/03/23 02:18:59.318
Jan  3 02:18:59.319: INFO: Observed &ReplicaSet event: ADDED
Jan  3 02:18:59.320: INFO: Observed &ReplicaSet event: MODIFIED
Jan  3 02:18:59.320: INFO: Observed &ReplicaSet event: MODIFIED
Jan  3 02:18:59.320: INFO: Observed &ReplicaSet event: MODIFIED
Jan  3 02:18:59.320: INFO: Observed replicaset test-rs in namespace replicaset-1195 with annotations: map[createdTime:2023-01-03T11:18:54.25322169+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T11:18:54.25322169+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan  3 02:18:59.320: INFO: Observed &ReplicaSet event: MODIFIED
Jan  3 02:18:59.320: INFO: Found replicaset test-rs in namespace replicaset-1195 with labels: map[name:sample-pod pod:httpd] annotations: map[createdTime:2023-01-03T11:18:54.25322169+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T11:18:54.25322169+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Jan  3 02:18:59.320: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan  3 02:18:59.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1195" for this suite. 01/03/23 02:18:59.337
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","completed":90,"skipped":1468,"failed":0}
------------------------------
• [SLOW TEST] [5.121 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:18:54.221
    Jan  3 02:18:54.221: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename replicaset 01/03/23 02:18:54.221
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:18:54.232
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:18:54.233
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 01/03/23 02:18:54.251
    STEP: Verify that the required pods have come up. 01/03/23 02:18:54.255
    Jan  3 02:18:54.260: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan  3 02:18:59.272: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/03/23 02:18:59.272
    STEP: Getting /status 01/03/23 02:18:59.272
    Jan  3 02:18:59.289: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 01/03/23 02:18:59.289
    Jan  3 02:18:59.313: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 01/03/23 02:18:59.313
    Jan  3 02:18:59.314: INFO: Observed &ReplicaSet event: ADDED
    Jan  3 02:18:59.315: INFO: Observed &ReplicaSet event: MODIFIED
    Jan  3 02:18:59.315: INFO: Observed &ReplicaSet event: MODIFIED
    Jan  3 02:18:59.315: INFO: Observed &ReplicaSet event: MODIFIED
    Jan  3 02:18:59.315: INFO: Found replicaset test-rs in namespace replicaset-1195 with labels: map[name:sample-pod pod:httpd] annotations: map[createdTime:2023-01-03T11:18:54.25322169+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T11:18:54.25322169+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan  3 02:18:59.315: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 01/03/23 02:18:59.315
    Jan  3 02:18:59.315: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan  3 02:18:59.318: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 01/03/23 02:18:59.318
    Jan  3 02:18:59.319: INFO: Observed &ReplicaSet event: ADDED
    Jan  3 02:18:59.320: INFO: Observed &ReplicaSet event: MODIFIED
    Jan  3 02:18:59.320: INFO: Observed &ReplicaSet event: MODIFIED
    Jan  3 02:18:59.320: INFO: Observed &ReplicaSet event: MODIFIED
    Jan  3 02:18:59.320: INFO: Observed replicaset test-rs in namespace replicaset-1195 with annotations: map[createdTime:2023-01-03T11:18:54.25322169+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T11:18:54.25322169+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan  3 02:18:59.320: INFO: Observed &ReplicaSet event: MODIFIED
    Jan  3 02:18:59.320: INFO: Found replicaset test-rs in namespace replicaset-1195 with labels: map[name:sample-pod pod:httpd] annotations: map[createdTime:2023-01-03T11:18:54.25322169+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T11:18:54.25322169+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Jan  3 02:18:59.320: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan  3 02:18:59.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-1195" for this suite. 01/03/23 02:18:59.337
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:18:59.342
Jan  3 02:18:59.342: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename dns 01/03/23 02:18:59.343
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:18:59.352
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:18:59.354
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 01/03/23 02:18:59.356
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1111.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1111.svc.cluster.local; sleep 1; done
 01/03/23 02:18:59.37
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1111.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1111.svc.cluster.local; sleep 1; done
 01/03/23 02:18:59.37
STEP: creating a pod to probe DNS 01/03/23 02:18:59.37
STEP: submitting the pod to kubernetes 01/03/23 02:18:59.37
Jan  3 02:18:59.379: INFO: Waiting up to 15m0s for pod "dns-test-0fc2ec17-9990-464b-bccd-d93ea15cbeda" in namespace "dns-1111" to be "running"
Jan  3 02:18:59.432: INFO: Pod "dns-test-0fc2ec17-9990-464b-bccd-d93ea15cbeda": Phase="Pending", Reason="", readiness=false. Elapsed: 52.987847ms
Jan  3 02:19:01.434: INFO: Pod "dns-test-0fc2ec17-9990-464b-bccd-d93ea15cbeda": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055676392s
Jan  3 02:19:03.436: INFO: Pod "dns-test-0fc2ec17-9990-464b-bccd-d93ea15cbeda": Phase="Running", Reason="", readiness=true. Elapsed: 4.056985033s
Jan  3 02:19:03.436: INFO: Pod "dns-test-0fc2ec17-9990-464b-bccd-d93ea15cbeda" satisfied condition "running"
STEP: retrieving the pod 01/03/23 02:19:03.436
STEP: looking for the results for each expected name from probers 01/03/23 02:19:03.437
Jan  3 02:19:03.442: INFO: DNS probes using dns-test-0fc2ec17-9990-464b-bccd-d93ea15cbeda succeeded

STEP: deleting the pod 01/03/23 02:19:03.442
STEP: changing the externalName to bar.example.com 01/03/23 02:19:03.462
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1111.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1111.svc.cluster.local; sleep 1; done
 01/03/23 02:19:03.469
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1111.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1111.svc.cluster.local; sleep 1; done
 01/03/23 02:19:03.469
STEP: creating a second pod to probe DNS 01/03/23 02:19:03.469
STEP: submitting the pod to kubernetes 01/03/23 02:19:03.469
Jan  3 02:19:03.500: INFO: Waiting up to 15m0s for pod "dns-test-f246e70c-fafd-43b6-b389-689f4accb9e6" in namespace "dns-1111" to be "running"
Jan  3 02:19:03.502: INFO: Pod "dns-test-f246e70c-fafd-43b6-b389-689f4accb9e6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.687729ms
Jan  3 02:19:05.505: INFO: Pod "dns-test-f246e70c-fafd-43b6-b389-689f4accb9e6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00496205s
Jan  3 02:19:07.505: INFO: Pod "dns-test-f246e70c-fafd-43b6-b389-689f4accb9e6": Phase="Running", Reason="", readiness=true. Elapsed: 4.005158601s
Jan  3 02:19:07.505: INFO: Pod "dns-test-f246e70c-fafd-43b6-b389-689f4accb9e6" satisfied condition "running"
STEP: retrieving the pod 01/03/23 02:19:07.505
STEP: looking for the results for each expected name from probers 01/03/23 02:19:07.507
Jan  3 02:19:07.509: INFO: File wheezy_udp@dns-test-service-3.dns-1111.svc.cluster.local from pod  dns-1111/dns-test-f246e70c-fafd-43b6-b389-689f4accb9e6 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan  3 02:19:07.511: INFO: File jessie_udp@dns-test-service-3.dns-1111.svc.cluster.local from pod  dns-1111/dns-test-f246e70c-fafd-43b6-b389-689f4accb9e6 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan  3 02:19:07.511: INFO: Lookups using dns-1111/dns-test-f246e70c-fafd-43b6-b389-689f4accb9e6 failed for: [wheezy_udp@dns-test-service-3.dns-1111.svc.cluster.local jessie_udp@dns-test-service-3.dns-1111.svc.cluster.local]

Jan  3 02:19:12.515: INFO: File wheezy_udp@dns-test-service-3.dns-1111.svc.cluster.local from pod  dns-1111/dns-test-f246e70c-fafd-43b6-b389-689f4accb9e6 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan  3 02:19:12.517: INFO: File jessie_udp@dns-test-service-3.dns-1111.svc.cluster.local from pod  dns-1111/dns-test-f246e70c-fafd-43b6-b389-689f4accb9e6 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan  3 02:19:12.517: INFO: Lookups using dns-1111/dns-test-f246e70c-fafd-43b6-b389-689f4accb9e6 failed for: [wheezy_udp@dns-test-service-3.dns-1111.svc.cluster.local jessie_udp@dns-test-service-3.dns-1111.svc.cluster.local]

Jan  3 02:19:17.516: INFO: File wheezy_udp@dns-test-service-3.dns-1111.svc.cluster.local from pod  dns-1111/dns-test-f246e70c-fafd-43b6-b389-689f4accb9e6 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan  3 02:19:17.518: INFO: File jessie_udp@dns-test-service-3.dns-1111.svc.cluster.local from pod  dns-1111/dns-test-f246e70c-fafd-43b6-b389-689f4accb9e6 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan  3 02:19:17.518: INFO: Lookups using dns-1111/dns-test-f246e70c-fafd-43b6-b389-689f4accb9e6 failed for: [wheezy_udp@dns-test-service-3.dns-1111.svc.cluster.local jessie_udp@dns-test-service-3.dns-1111.svc.cluster.local]

Jan  3 02:19:22.515: INFO: File wheezy_udp@dns-test-service-3.dns-1111.svc.cluster.local from pod  dns-1111/dns-test-f246e70c-fafd-43b6-b389-689f4accb9e6 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan  3 02:19:22.517: INFO: File jessie_udp@dns-test-service-3.dns-1111.svc.cluster.local from pod  dns-1111/dns-test-f246e70c-fafd-43b6-b389-689f4accb9e6 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan  3 02:19:22.517: INFO: Lookups using dns-1111/dns-test-f246e70c-fafd-43b6-b389-689f4accb9e6 failed for: [wheezy_udp@dns-test-service-3.dns-1111.svc.cluster.local jessie_udp@dns-test-service-3.dns-1111.svc.cluster.local]

Jan  3 02:19:27.516: INFO: File wheezy_udp@dns-test-service-3.dns-1111.svc.cluster.local from pod  dns-1111/dns-test-f246e70c-fafd-43b6-b389-689f4accb9e6 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan  3 02:19:27.518: INFO: File jessie_udp@dns-test-service-3.dns-1111.svc.cluster.local from pod  dns-1111/dns-test-f246e70c-fafd-43b6-b389-689f4accb9e6 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan  3 02:19:27.518: INFO: Lookups using dns-1111/dns-test-f246e70c-fafd-43b6-b389-689f4accb9e6 failed for: [wheezy_udp@dns-test-service-3.dns-1111.svc.cluster.local jessie_udp@dns-test-service-3.dns-1111.svc.cluster.local]

Jan  3 02:19:32.514: INFO: File wheezy_udp@dns-test-service-3.dns-1111.svc.cluster.local from pod  dns-1111/dns-test-f246e70c-fafd-43b6-b389-689f4accb9e6 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan  3 02:19:32.517: INFO: File jessie_udp@dns-test-service-3.dns-1111.svc.cluster.local from pod  dns-1111/dns-test-f246e70c-fafd-43b6-b389-689f4accb9e6 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan  3 02:19:32.517: INFO: Lookups using dns-1111/dns-test-f246e70c-fafd-43b6-b389-689f4accb9e6 failed for: [wheezy_udp@dns-test-service-3.dns-1111.svc.cluster.local jessie_udp@dns-test-service-3.dns-1111.svc.cluster.local]

Jan  3 02:19:37.517: INFO: DNS probes using dns-test-f246e70c-fafd-43b6-b389-689f4accb9e6 succeeded

STEP: deleting the pod 01/03/23 02:19:37.517
STEP: changing the service to type=ClusterIP 01/03/23 02:19:37.536
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1111.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-1111.svc.cluster.local; sleep 1; done
 01/03/23 02:19:37.559
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1111.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-1111.svc.cluster.local; sleep 1; done
 01/03/23 02:19:37.559
STEP: creating a third pod to probe DNS 01/03/23 02:19:37.559
STEP: submitting the pod to kubernetes 01/03/23 02:19:37.565
Jan  3 02:19:37.571: INFO: Waiting up to 15m0s for pod "dns-test-9f143b4e-9eff-4344-bae8-da678ffce410" in namespace "dns-1111" to be "running"
Jan  3 02:19:37.586: INFO: Pod "dns-test-9f143b4e-9eff-4344-bae8-da678ffce410": Phase="Pending", Reason="", readiness=false. Elapsed: 15.169902ms
Jan  3 02:19:39.590: INFO: Pod "dns-test-9f143b4e-9eff-4344-bae8-da678ffce410": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018528963s
Jan  3 02:19:41.590: INFO: Pod "dns-test-9f143b4e-9eff-4344-bae8-da678ffce410": Phase="Running", Reason="", readiness=true. Elapsed: 4.018466683s
Jan  3 02:19:41.590: INFO: Pod "dns-test-9f143b4e-9eff-4344-bae8-da678ffce410" satisfied condition "running"
STEP: retrieving the pod 01/03/23 02:19:41.59
STEP: looking for the results for each expected name from probers 01/03/23 02:19:41.591
Jan  3 02:19:41.596: INFO: DNS probes using dns-test-9f143b4e-9eff-4344-bae8-da678ffce410 succeeded

STEP: deleting the pod 01/03/23 02:19:41.596
STEP: deleting the test externalName service 01/03/23 02:19:41.617
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan  3 02:19:41.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1111" for this suite. 01/03/23 02:19:41.656
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","completed":91,"skipped":1468,"failed":0}
------------------------------
• [SLOW TEST] [42.317 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:18:59.342
    Jan  3 02:18:59.342: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename dns 01/03/23 02:18:59.343
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:18:59.352
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:18:59.354
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 01/03/23 02:18:59.356
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1111.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1111.svc.cluster.local; sleep 1; done
     01/03/23 02:18:59.37
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1111.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1111.svc.cluster.local; sleep 1; done
     01/03/23 02:18:59.37
    STEP: creating a pod to probe DNS 01/03/23 02:18:59.37
    STEP: submitting the pod to kubernetes 01/03/23 02:18:59.37
    Jan  3 02:18:59.379: INFO: Waiting up to 15m0s for pod "dns-test-0fc2ec17-9990-464b-bccd-d93ea15cbeda" in namespace "dns-1111" to be "running"
    Jan  3 02:18:59.432: INFO: Pod "dns-test-0fc2ec17-9990-464b-bccd-d93ea15cbeda": Phase="Pending", Reason="", readiness=false. Elapsed: 52.987847ms
    Jan  3 02:19:01.434: INFO: Pod "dns-test-0fc2ec17-9990-464b-bccd-d93ea15cbeda": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055676392s
    Jan  3 02:19:03.436: INFO: Pod "dns-test-0fc2ec17-9990-464b-bccd-d93ea15cbeda": Phase="Running", Reason="", readiness=true. Elapsed: 4.056985033s
    Jan  3 02:19:03.436: INFO: Pod "dns-test-0fc2ec17-9990-464b-bccd-d93ea15cbeda" satisfied condition "running"
    STEP: retrieving the pod 01/03/23 02:19:03.436
    STEP: looking for the results for each expected name from probers 01/03/23 02:19:03.437
    Jan  3 02:19:03.442: INFO: DNS probes using dns-test-0fc2ec17-9990-464b-bccd-d93ea15cbeda succeeded

    STEP: deleting the pod 01/03/23 02:19:03.442
    STEP: changing the externalName to bar.example.com 01/03/23 02:19:03.462
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1111.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1111.svc.cluster.local; sleep 1; done
     01/03/23 02:19:03.469
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1111.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1111.svc.cluster.local; sleep 1; done
     01/03/23 02:19:03.469
    STEP: creating a second pod to probe DNS 01/03/23 02:19:03.469
    STEP: submitting the pod to kubernetes 01/03/23 02:19:03.469
    Jan  3 02:19:03.500: INFO: Waiting up to 15m0s for pod "dns-test-f246e70c-fafd-43b6-b389-689f4accb9e6" in namespace "dns-1111" to be "running"
    Jan  3 02:19:03.502: INFO: Pod "dns-test-f246e70c-fafd-43b6-b389-689f4accb9e6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.687729ms
    Jan  3 02:19:05.505: INFO: Pod "dns-test-f246e70c-fafd-43b6-b389-689f4accb9e6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00496205s
    Jan  3 02:19:07.505: INFO: Pod "dns-test-f246e70c-fafd-43b6-b389-689f4accb9e6": Phase="Running", Reason="", readiness=true. Elapsed: 4.005158601s
    Jan  3 02:19:07.505: INFO: Pod "dns-test-f246e70c-fafd-43b6-b389-689f4accb9e6" satisfied condition "running"
    STEP: retrieving the pod 01/03/23 02:19:07.505
    STEP: looking for the results for each expected name from probers 01/03/23 02:19:07.507
    Jan  3 02:19:07.509: INFO: File wheezy_udp@dns-test-service-3.dns-1111.svc.cluster.local from pod  dns-1111/dns-test-f246e70c-fafd-43b6-b389-689f4accb9e6 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan  3 02:19:07.511: INFO: File jessie_udp@dns-test-service-3.dns-1111.svc.cluster.local from pod  dns-1111/dns-test-f246e70c-fafd-43b6-b389-689f4accb9e6 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan  3 02:19:07.511: INFO: Lookups using dns-1111/dns-test-f246e70c-fafd-43b6-b389-689f4accb9e6 failed for: [wheezy_udp@dns-test-service-3.dns-1111.svc.cluster.local jessie_udp@dns-test-service-3.dns-1111.svc.cluster.local]

    Jan  3 02:19:12.515: INFO: File wheezy_udp@dns-test-service-3.dns-1111.svc.cluster.local from pod  dns-1111/dns-test-f246e70c-fafd-43b6-b389-689f4accb9e6 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan  3 02:19:12.517: INFO: File jessie_udp@dns-test-service-3.dns-1111.svc.cluster.local from pod  dns-1111/dns-test-f246e70c-fafd-43b6-b389-689f4accb9e6 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan  3 02:19:12.517: INFO: Lookups using dns-1111/dns-test-f246e70c-fafd-43b6-b389-689f4accb9e6 failed for: [wheezy_udp@dns-test-service-3.dns-1111.svc.cluster.local jessie_udp@dns-test-service-3.dns-1111.svc.cluster.local]

    Jan  3 02:19:17.516: INFO: File wheezy_udp@dns-test-service-3.dns-1111.svc.cluster.local from pod  dns-1111/dns-test-f246e70c-fafd-43b6-b389-689f4accb9e6 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan  3 02:19:17.518: INFO: File jessie_udp@dns-test-service-3.dns-1111.svc.cluster.local from pod  dns-1111/dns-test-f246e70c-fafd-43b6-b389-689f4accb9e6 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan  3 02:19:17.518: INFO: Lookups using dns-1111/dns-test-f246e70c-fafd-43b6-b389-689f4accb9e6 failed for: [wheezy_udp@dns-test-service-3.dns-1111.svc.cluster.local jessie_udp@dns-test-service-3.dns-1111.svc.cluster.local]

    Jan  3 02:19:22.515: INFO: File wheezy_udp@dns-test-service-3.dns-1111.svc.cluster.local from pod  dns-1111/dns-test-f246e70c-fafd-43b6-b389-689f4accb9e6 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan  3 02:19:22.517: INFO: File jessie_udp@dns-test-service-3.dns-1111.svc.cluster.local from pod  dns-1111/dns-test-f246e70c-fafd-43b6-b389-689f4accb9e6 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan  3 02:19:22.517: INFO: Lookups using dns-1111/dns-test-f246e70c-fafd-43b6-b389-689f4accb9e6 failed for: [wheezy_udp@dns-test-service-3.dns-1111.svc.cluster.local jessie_udp@dns-test-service-3.dns-1111.svc.cluster.local]

    Jan  3 02:19:27.516: INFO: File wheezy_udp@dns-test-service-3.dns-1111.svc.cluster.local from pod  dns-1111/dns-test-f246e70c-fafd-43b6-b389-689f4accb9e6 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan  3 02:19:27.518: INFO: File jessie_udp@dns-test-service-3.dns-1111.svc.cluster.local from pod  dns-1111/dns-test-f246e70c-fafd-43b6-b389-689f4accb9e6 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan  3 02:19:27.518: INFO: Lookups using dns-1111/dns-test-f246e70c-fafd-43b6-b389-689f4accb9e6 failed for: [wheezy_udp@dns-test-service-3.dns-1111.svc.cluster.local jessie_udp@dns-test-service-3.dns-1111.svc.cluster.local]

    Jan  3 02:19:32.514: INFO: File wheezy_udp@dns-test-service-3.dns-1111.svc.cluster.local from pod  dns-1111/dns-test-f246e70c-fafd-43b6-b389-689f4accb9e6 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan  3 02:19:32.517: INFO: File jessie_udp@dns-test-service-3.dns-1111.svc.cluster.local from pod  dns-1111/dns-test-f246e70c-fafd-43b6-b389-689f4accb9e6 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan  3 02:19:32.517: INFO: Lookups using dns-1111/dns-test-f246e70c-fafd-43b6-b389-689f4accb9e6 failed for: [wheezy_udp@dns-test-service-3.dns-1111.svc.cluster.local jessie_udp@dns-test-service-3.dns-1111.svc.cluster.local]

    Jan  3 02:19:37.517: INFO: DNS probes using dns-test-f246e70c-fafd-43b6-b389-689f4accb9e6 succeeded

    STEP: deleting the pod 01/03/23 02:19:37.517
    STEP: changing the service to type=ClusterIP 01/03/23 02:19:37.536
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1111.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-1111.svc.cluster.local; sleep 1; done
     01/03/23 02:19:37.559
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1111.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-1111.svc.cluster.local; sleep 1; done
     01/03/23 02:19:37.559
    STEP: creating a third pod to probe DNS 01/03/23 02:19:37.559
    STEP: submitting the pod to kubernetes 01/03/23 02:19:37.565
    Jan  3 02:19:37.571: INFO: Waiting up to 15m0s for pod "dns-test-9f143b4e-9eff-4344-bae8-da678ffce410" in namespace "dns-1111" to be "running"
    Jan  3 02:19:37.586: INFO: Pod "dns-test-9f143b4e-9eff-4344-bae8-da678ffce410": Phase="Pending", Reason="", readiness=false. Elapsed: 15.169902ms
    Jan  3 02:19:39.590: INFO: Pod "dns-test-9f143b4e-9eff-4344-bae8-da678ffce410": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018528963s
    Jan  3 02:19:41.590: INFO: Pod "dns-test-9f143b4e-9eff-4344-bae8-da678ffce410": Phase="Running", Reason="", readiness=true. Elapsed: 4.018466683s
    Jan  3 02:19:41.590: INFO: Pod "dns-test-9f143b4e-9eff-4344-bae8-da678ffce410" satisfied condition "running"
    STEP: retrieving the pod 01/03/23 02:19:41.59
    STEP: looking for the results for each expected name from probers 01/03/23 02:19:41.591
    Jan  3 02:19:41.596: INFO: DNS probes using dns-test-9f143b4e-9eff-4344-bae8-da678ffce410 succeeded

    STEP: deleting the pod 01/03/23 02:19:41.596
    STEP: deleting the test externalName service 01/03/23 02:19:41.617
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan  3 02:19:41.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-1111" for this suite. 01/03/23 02:19:41.656
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:19:41.659
Jan  3 02:19:41.659: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename statefulset 01/03/23 02:19:41.66
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:19:41.668
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:19:41.67
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-8442 01/03/23 02:19:41.671
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
STEP: Creating a new StatefulSet 01/03/23 02:19:41.684
Jan  3 02:19:41.739: INFO: Found 0 stateful pods, waiting for 3
Jan  3 02:19:51.742: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 02:19:51.742: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 02:19:51.742: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 01/03/23 02:19:51.748
Jan  3 02:19:51.763: INFO: Updating stateful set ss2
STEP: Creating a new revision 01/03/23 02:19:51.763
STEP: Not applying an update when the partition is greater than the number of replicas 01/03/23 02:20:01.783
STEP: Performing a canary update 01/03/23 02:20:01.783
Jan  3 02:20:01.798: INFO: Updating stateful set ss2
Jan  3 02:20:01.822: INFO: Waiting for Pod statefulset-8442/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
STEP: Restoring Pods to the correct revision when they are deleted 01/03/23 02:20:11.826
Jan  3 02:20:11.910: INFO: Found 1 stateful pods, waiting for 3
Jan  3 02:20:21.913: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 02:20:21.913: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 02:20:21.913: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 01/03/23 02:20:21.917
Jan  3 02:20:21.932: INFO: Updating stateful set ss2
Jan  3 02:20:21.952: INFO: Waiting for Pod statefulset-8442/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Jan  3 02:20:31.972: INFO: Updating stateful set ss2
Jan  3 02:20:31.993: INFO: Waiting for StatefulSet statefulset-8442/ss2 to complete update
Jan  3 02:20:31.993: INFO: Waiting for Pod statefulset-8442/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan  3 02:20:41.998: INFO: Deleting all statefulset in ns statefulset-8442
Jan  3 02:20:42.000: INFO: Scaling statefulset ss2 to 0
Jan  3 02:20:52.017: INFO: Waiting for statefulset status.replicas updated to 0
Jan  3 02:20:52.041: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan  3 02:20:52.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8442" for this suite. 01/03/23 02:20:52.055
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","completed":92,"skipped":1470,"failed":0}
------------------------------
• [SLOW TEST] [70.405 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:315

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:19:41.659
    Jan  3 02:19:41.659: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename statefulset 01/03/23 02:19:41.66
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:19:41.668
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:19:41.67
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-8442 01/03/23 02:19:41.671
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:315
    STEP: Creating a new StatefulSet 01/03/23 02:19:41.684
    Jan  3 02:19:41.739: INFO: Found 0 stateful pods, waiting for 3
    Jan  3 02:19:51.742: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan  3 02:19:51.742: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan  3 02:19:51.742: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 01/03/23 02:19:51.748
    Jan  3 02:19:51.763: INFO: Updating stateful set ss2
    STEP: Creating a new revision 01/03/23 02:19:51.763
    STEP: Not applying an update when the partition is greater than the number of replicas 01/03/23 02:20:01.783
    STEP: Performing a canary update 01/03/23 02:20:01.783
    Jan  3 02:20:01.798: INFO: Updating stateful set ss2
    Jan  3 02:20:01.822: INFO: Waiting for Pod statefulset-8442/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    STEP: Restoring Pods to the correct revision when they are deleted 01/03/23 02:20:11.826
    Jan  3 02:20:11.910: INFO: Found 1 stateful pods, waiting for 3
    Jan  3 02:20:21.913: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan  3 02:20:21.913: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan  3 02:20:21.913: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 01/03/23 02:20:21.917
    Jan  3 02:20:21.932: INFO: Updating stateful set ss2
    Jan  3 02:20:21.952: INFO: Waiting for Pod statefulset-8442/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Jan  3 02:20:31.972: INFO: Updating stateful set ss2
    Jan  3 02:20:31.993: INFO: Waiting for StatefulSet statefulset-8442/ss2 to complete update
    Jan  3 02:20:31.993: INFO: Waiting for Pod statefulset-8442/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan  3 02:20:41.998: INFO: Deleting all statefulset in ns statefulset-8442
    Jan  3 02:20:42.000: INFO: Scaling statefulset ss2 to 0
    Jan  3 02:20:52.017: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  3 02:20:52.041: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan  3 02:20:52.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-8442" for this suite. 01/03/23 02:20:52.055
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:20:52.065
Jan  3 02:20:52.065: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename svcaccounts 01/03/23 02:20:52.066
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:20:52.074
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:20:52.076
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
STEP: Creating a pod to test service account token:  01/03/23 02:20:52.09
Jan  3 02:20:52.100: INFO: Waiting up to 5m0s for pod "test-pod-461e0436-e747-4bc2-a131-6f7374cfc180" in namespace "svcaccounts-6206" to be "Succeeded or Failed"
Jan  3 02:20:52.113: INFO: Pod "test-pod-461e0436-e747-4bc2-a131-6f7374cfc180": Phase="Pending", Reason="", readiness=false. Elapsed: 13.722241ms
Jan  3 02:20:54.116: INFO: Pod "test-pod-461e0436-e747-4bc2-a131-6f7374cfc180": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016714288s
Jan  3 02:20:56.116: INFO: Pod "test-pod-461e0436-e747-4bc2-a131-6f7374cfc180": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01625841s
STEP: Saw pod success 01/03/23 02:20:56.116
Jan  3 02:20:56.116: INFO: Pod "test-pod-461e0436-e747-4bc2-a131-6f7374cfc180" satisfied condition "Succeeded or Failed"
Jan  3 02:20:56.118: INFO: Trying to get logs from node cncf-master pod test-pod-461e0436-e747-4bc2-a131-6f7374cfc180 container agnhost-container: <nil>
STEP: delete the pod 01/03/23 02:20:56.127
Jan  3 02:20:56.223: INFO: Waiting for pod test-pod-461e0436-e747-4bc2-a131-6f7374cfc180 to disappear
Jan  3 02:20:56.225: INFO: Pod test-pod-461e0436-e747-4bc2-a131-6f7374cfc180 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan  3 02:20:56.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6206" for this suite. 01/03/23 02:20:56.228
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","completed":93,"skipped":1495,"failed":0}
------------------------------
• [4.165 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:20:52.065
    Jan  3 02:20:52.065: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename svcaccounts 01/03/23 02:20:52.066
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:20:52.074
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:20:52.076
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:272
    STEP: Creating a pod to test service account token:  01/03/23 02:20:52.09
    Jan  3 02:20:52.100: INFO: Waiting up to 5m0s for pod "test-pod-461e0436-e747-4bc2-a131-6f7374cfc180" in namespace "svcaccounts-6206" to be "Succeeded or Failed"
    Jan  3 02:20:52.113: INFO: Pod "test-pod-461e0436-e747-4bc2-a131-6f7374cfc180": Phase="Pending", Reason="", readiness=false. Elapsed: 13.722241ms
    Jan  3 02:20:54.116: INFO: Pod "test-pod-461e0436-e747-4bc2-a131-6f7374cfc180": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016714288s
    Jan  3 02:20:56.116: INFO: Pod "test-pod-461e0436-e747-4bc2-a131-6f7374cfc180": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01625841s
    STEP: Saw pod success 01/03/23 02:20:56.116
    Jan  3 02:20:56.116: INFO: Pod "test-pod-461e0436-e747-4bc2-a131-6f7374cfc180" satisfied condition "Succeeded or Failed"
    Jan  3 02:20:56.118: INFO: Trying to get logs from node cncf-master pod test-pod-461e0436-e747-4bc2-a131-6f7374cfc180 container agnhost-container: <nil>
    STEP: delete the pod 01/03/23 02:20:56.127
    Jan  3 02:20:56.223: INFO: Waiting for pod test-pod-461e0436-e747-4bc2-a131-6f7374cfc180 to disappear
    Jan  3 02:20:56.225: INFO: Pod test-pod-461e0436-e747-4bc2-a131-6f7374cfc180 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan  3 02:20:56.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-6206" for this suite. 01/03/23 02:20:56.228
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:20:56.231
Jan  3 02:20:56.231: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename security-context-test 01/03/23 02:20:56.231
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:20:56.239
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:20:56.241
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
Jan  3 02:20:56.261: INFO: Waiting up to 5m0s for pod "busybox-user-65534-dfc5a48e-2e39-4ddc-ab3f-4a793bc9f316" in namespace "security-context-test-5615" to be "Succeeded or Failed"
Jan  3 02:20:56.268: INFO: Pod "busybox-user-65534-dfc5a48e-2e39-4ddc-ab3f-4a793bc9f316": Phase="Pending", Reason="", readiness=false. Elapsed: 7.083444ms
Jan  3 02:20:58.271: INFO: Pod "busybox-user-65534-dfc5a48e-2e39-4ddc-ab3f-4a793bc9f316": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010140159s
Jan  3 02:21:00.272: INFO: Pod "busybox-user-65534-dfc5a48e-2e39-4ddc-ab3f-4a793bc9f316": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011119299s
Jan  3 02:21:00.272: INFO: Pod "busybox-user-65534-dfc5a48e-2e39-4ddc-ab3f-4a793bc9f316" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan  3 02:21:00.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5615" for this suite. 01/03/23 02:21:00.275
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","completed":94,"skipped":1510,"failed":0}
------------------------------
• [4.047 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:308
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:20:56.231
    Jan  3 02:20:56.231: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename security-context-test 01/03/23 02:20:56.231
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:20:56.239
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:20:56.241
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:346
    Jan  3 02:20:56.261: INFO: Waiting up to 5m0s for pod "busybox-user-65534-dfc5a48e-2e39-4ddc-ab3f-4a793bc9f316" in namespace "security-context-test-5615" to be "Succeeded or Failed"
    Jan  3 02:20:56.268: INFO: Pod "busybox-user-65534-dfc5a48e-2e39-4ddc-ab3f-4a793bc9f316": Phase="Pending", Reason="", readiness=false. Elapsed: 7.083444ms
    Jan  3 02:20:58.271: INFO: Pod "busybox-user-65534-dfc5a48e-2e39-4ddc-ab3f-4a793bc9f316": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010140159s
    Jan  3 02:21:00.272: INFO: Pod "busybox-user-65534-dfc5a48e-2e39-4ddc-ab3f-4a793bc9f316": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011119299s
    Jan  3 02:21:00.272: INFO: Pod "busybox-user-65534-dfc5a48e-2e39-4ddc-ab3f-4a793bc9f316" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan  3 02:21:00.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-5615" for this suite. 01/03/23 02:21:00.275
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:21:00.28
Jan  3 02:21:00.280: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename namespaces 01/03/23 02:21:00.281
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:21:00.293
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:21:00.295
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
STEP: creating a Namespace 01/03/23 02:21:00.297
STEP: patching the Namespace 01/03/23 02:21:00.315
STEP: get the Namespace and ensuring it has the label 01/03/23 02:21:00.334
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jan  3 02:21:00.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-383" for this suite. 01/03/23 02:21:00.341
STEP: Destroying namespace "nspatchtest-4aa52c8d-4627-4895-ac08-c04cb7c5d4c8-4952" for this suite. 01/03/23 02:21:00.343
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","completed":95,"skipped":1574,"failed":0}
------------------------------
• [0.072 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:21:00.28
    Jan  3 02:21:00.280: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename namespaces 01/03/23 02:21:00.281
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:21:00.293
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:21:00.295
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:267
    STEP: creating a Namespace 01/03/23 02:21:00.297
    STEP: patching the Namespace 01/03/23 02:21:00.315
    STEP: get the Namespace and ensuring it has the label 01/03/23 02:21:00.334
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jan  3 02:21:00.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-383" for this suite. 01/03/23 02:21:00.341
    STEP: Destroying namespace "nspatchtest-4aa52c8d-4627-4895-ac08-c04cb7c5d4c8-4952" for this suite. 01/03/23 02:21:00.343
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:21:00.355
Jan  3 02:21:00.355: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename projected 01/03/23 02:21:00.356
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:21:00.362
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:21:00.364
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
STEP: Creating configMap with name projected-configmap-test-volume-7aeaf01d-2155-4dba-88b9-04f42c40a94c 01/03/23 02:21:00.389
STEP: Creating a pod to test consume configMaps 01/03/23 02:21:00.403
Jan  3 02:21:00.410: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-380b9c73-ae40-4f52-ab94-05b583542377" in namespace "projected-4897" to be "Succeeded or Failed"
Jan  3 02:21:00.426: INFO: Pod "pod-projected-configmaps-380b9c73-ae40-4f52-ab94-05b583542377": Phase="Pending", Reason="", readiness=false. Elapsed: 15.70839ms
Jan  3 02:21:02.429: INFO: Pod "pod-projected-configmaps-380b9c73-ae40-4f52-ab94-05b583542377": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018347962s
Jan  3 02:21:04.429: INFO: Pod "pod-projected-configmaps-380b9c73-ae40-4f52-ab94-05b583542377": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0192232s
STEP: Saw pod success 01/03/23 02:21:04.429
Jan  3 02:21:04.430: INFO: Pod "pod-projected-configmaps-380b9c73-ae40-4f52-ab94-05b583542377" satisfied condition "Succeeded or Failed"
Jan  3 02:21:04.431: INFO: Trying to get logs from node cncf-master pod pod-projected-configmaps-380b9c73-ae40-4f52-ab94-05b583542377 container agnhost-container: <nil>
STEP: delete the pod 01/03/23 02:21:04.434
Jan  3 02:21:04.448: INFO: Waiting for pod pod-projected-configmaps-380b9c73-ae40-4f52-ab94-05b583542377 to disappear
Jan  3 02:21:04.453: INFO: Pod pod-projected-configmaps-380b9c73-ae40-4f52-ab94-05b583542377 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan  3 02:21:04.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4897" for this suite. 01/03/23 02:21:04.456
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":96,"skipped":1652,"failed":0}
------------------------------
• [4.103 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:21:00.355
    Jan  3 02:21:00.355: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename projected 01/03/23 02:21:00.356
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:21:00.362
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:21:00.364
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:73
    STEP: Creating configMap with name projected-configmap-test-volume-7aeaf01d-2155-4dba-88b9-04f42c40a94c 01/03/23 02:21:00.389
    STEP: Creating a pod to test consume configMaps 01/03/23 02:21:00.403
    Jan  3 02:21:00.410: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-380b9c73-ae40-4f52-ab94-05b583542377" in namespace "projected-4897" to be "Succeeded or Failed"
    Jan  3 02:21:00.426: INFO: Pod "pod-projected-configmaps-380b9c73-ae40-4f52-ab94-05b583542377": Phase="Pending", Reason="", readiness=false. Elapsed: 15.70839ms
    Jan  3 02:21:02.429: INFO: Pod "pod-projected-configmaps-380b9c73-ae40-4f52-ab94-05b583542377": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018347962s
    Jan  3 02:21:04.429: INFO: Pod "pod-projected-configmaps-380b9c73-ae40-4f52-ab94-05b583542377": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0192232s
    STEP: Saw pod success 01/03/23 02:21:04.429
    Jan  3 02:21:04.430: INFO: Pod "pod-projected-configmaps-380b9c73-ae40-4f52-ab94-05b583542377" satisfied condition "Succeeded or Failed"
    Jan  3 02:21:04.431: INFO: Trying to get logs from node cncf-master pod pod-projected-configmaps-380b9c73-ae40-4f52-ab94-05b583542377 container agnhost-container: <nil>
    STEP: delete the pod 01/03/23 02:21:04.434
    Jan  3 02:21:04.448: INFO: Waiting for pod pod-projected-configmaps-380b9c73-ae40-4f52-ab94-05b583542377 to disappear
    Jan  3 02:21:04.453: INFO: Pod pod-projected-configmaps-380b9c73-ae40-4f52-ab94-05b583542377 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan  3 02:21:04.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4897" for this suite. 01/03/23 02:21:04.456
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:21:04.458
Jan  3 02:21:04.458: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename services 01/03/23 02:21:04.459
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:21:04.47
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:21:04.472
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
STEP: creating an Endpoint 01/03/23 02:21:04.509
STEP: waiting for available Endpoint 01/03/23 02:21:04.512
STEP: listing all Endpoints 01/03/23 02:21:04.513
STEP: updating the Endpoint 01/03/23 02:21:04.517
STEP: fetching the Endpoint 01/03/23 02:21:04.519
STEP: patching the Endpoint 01/03/23 02:21:04.531
STEP: fetching the Endpoint 01/03/23 02:21:04.54
STEP: deleting the Endpoint by Collection 01/03/23 02:21:04.545
STEP: waiting for Endpoint deletion 01/03/23 02:21:04.548
STEP: fetching the Endpoint 01/03/23 02:21:04.548
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  3 02:21:04.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2533" for this suite. 01/03/23 02:21:04.553
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","completed":97,"skipped":1653,"failed":0}
------------------------------
• [0.097 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:21:04.458
    Jan  3 02:21:04.458: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename services 01/03/23 02:21:04.459
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:21:04.47
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:21:04.472
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3231
    STEP: creating an Endpoint 01/03/23 02:21:04.509
    STEP: waiting for available Endpoint 01/03/23 02:21:04.512
    STEP: listing all Endpoints 01/03/23 02:21:04.513
    STEP: updating the Endpoint 01/03/23 02:21:04.517
    STEP: fetching the Endpoint 01/03/23 02:21:04.519
    STEP: patching the Endpoint 01/03/23 02:21:04.531
    STEP: fetching the Endpoint 01/03/23 02:21:04.54
    STEP: deleting the Endpoint by Collection 01/03/23 02:21:04.545
    STEP: waiting for Endpoint deletion 01/03/23 02:21:04.548
    STEP: fetching the Endpoint 01/03/23 02:21:04.548
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  3 02:21:04.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2533" for this suite. 01/03/23 02:21:04.553
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:21:04.556
Jan  3 02:21:04.557: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename security-context-test 01/03/23 02:21:04.557
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:21:04.571
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:21:04.573
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
Jan  3 02:21:04.580: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-f8c3b41e-1e45-4cc6-85fb-40b3742873f8" in namespace "security-context-test-3295" to be "Succeeded or Failed"
Jan  3 02:21:04.594: INFO: Pod "alpine-nnp-false-f8c3b41e-1e45-4cc6-85fb-40b3742873f8": Phase="Pending", Reason="", readiness=false. Elapsed: 14.051192ms
Jan  3 02:21:06.596: INFO: Pod "alpine-nnp-false-f8c3b41e-1e45-4cc6-85fb-40b3742873f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016268943s
Jan  3 02:21:08.606: INFO: Pod "alpine-nnp-false-f8c3b41e-1e45-4cc6-85fb-40b3742873f8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025771847s
Jan  3 02:21:10.597: INFO: Pod "alpine-nnp-false-f8c3b41e-1e45-4cc6-85fb-40b3742873f8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017110673s
Jan  3 02:21:12.598: INFO: Pod "alpine-nnp-false-f8c3b41e-1e45-4cc6-85fb-40b3742873f8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.017447723s
Jan  3 02:21:14.596: INFO: Pod "alpine-nnp-false-f8c3b41e-1e45-4cc6-85fb-40b3742873f8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.016203135s
Jan  3 02:21:16.598: INFO: Pod "alpine-nnp-false-f8c3b41e-1e45-4cc6-85fb-40b3742873f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.017324307s
Jan  3 02:21:16.598: INFO: Pod "alpine-nnp-false-f8c3b41e-1e45-4cc6-85fb-40b3742873f8" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan  3 02:21:16.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3295" for this suite. 01/03/23 02:21:16.604
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","completed":98,"skipped":1668,"failed":0}
------------------------------
• [SLOW TEST] [12.051 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:554
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:608

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:21:04.556
    Jan  3 02:21:04.557: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename security-context-test 01/03/23 02:21:04.557
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:21:04.571
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:21:04.573
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:608
    Jan  3 02:21:04.580: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-f8c3b41e-1e45-4cc6-85fb-40b3742873f8" in namespace "security-context-test-3295" to be "Succeeded or Failed"
    Jan  3 02:21:04.594: INFO: Pod "alpine-nnp-false-f8c3b41e-1e45-4cc6-85fb-40b3742873f8": Phase="Pending", Reason="", readiness=false. Elapsed: 14.051192ms
    Jan  3 02:21:06.596: INFO: Pod "alpine-nnp-false-f8c3b41e-1e45-4cc6-85fb-40b3742873f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016268943s
    Jan  3 02:21:08.606: INFO: Pod "alpine-nnp-false-f8c3b41e-1e45-4cc6-85fb-40b3742873f8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025771847s
    Jan  3 02:21:10.597: INFO: Pod "alpine-nnp-false-f8c3b41e-1e45-4cc6-85fb-40b3742873f8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017110673s
    Jan  3 02:21:12.598: INFO: Pod "alpine-nnp-false-f8c3b41e-1e45-4cc6-85fb-40b3742873f8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.017447723s
    Jan  3 02:21:14.596: INFO: Pod "alpine-nnp-false-f8c3b41e-1e45-4cc6-85fb-40b3742873f8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.016203135s
    Jan  3 02:21:16.598: INFO: Pod "alpine-nnp-false-f8c3b41e-1e45-4cc6-85fb-40b3742873f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.017324307s
    Jan  3 02:21:16.598: INFO: Pod "alpine-nnp-false-f8c3b41e-1e45-4cc6-85fb-40b3742873f8" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan  3 02:21:16.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-3295" for this suite. 01/03/23 02:21:16.604
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:21:16.608
Jan  3 02:21:16.609: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename replication-controller 01/03/23 02:21:16.609
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:21:16.62
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:21:16.622
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
Jan  3 02:21:16.624: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 01/03/23 02:21:16.643
STEP: Checking rc "condition-test" has the desired failure condition set 01/03/23 02:21:16.649
STEP: Scaling down rc "condition-test" to satisfy pod quota 01/03/23 02:21:17.674
Jan  3 02:21:17.678: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 01/03/23 02:21:17.678
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan  3 02:21:18.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2344" for this suite. 01/03/23 02:21:18.688
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","completed":99,"skipped":1698,"failed":0}
------------------------------
• [2.101 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:21:16.608
    Jan  3 02:21:16.609: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename replication-controller 01/03/23 02:21:16.609
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:21:16.62
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:21:16.622
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:82
    Jan  3 02:21:16.624: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 01/03/23 02:21:16.643
    STEP: Checking rc "condition-test" has the desired failure condition set 01/03/23 02:21:16.649
    STEP: Scaling down rc "condition-test" to satisfy pod quota 01/03/23 02:21:17.674
    Jan  3 02:21:17.678: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 01/03/23 02:21:17.678
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan  3 02:21:18.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-2344" for this suite. 01/03/23 02:21:18.688
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:21:18.712
Jan  3 02:21:18.712: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename configmap 01/03/23 02:21:18.712
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:21:18.736
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:21:18.738
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
STEP: Creating configMap with name configmap-test-upd-163f5aa0-69fc-4877-9d9f-61060d9d7d53 01/03/23 02:21:18.743
STEP: Creating the pod 01/03/23 02:21:18.746
Jan  3 02:21:18.768: INFO: Waiting up to 5m0s for pod "pod-configmaps-ec32f275-6719-49a6-bf1a-fdc7e3dc95a5" in namespace "configmap-6691" to be "running"
Jan  3 02:21:18.775: INFO: Pod "pod-configmaps-ec32f275-6719-49a6-bf1a-fdc7e3dc95a5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.851786ms
Jan  3 02:21:20.777: INFO: Pod "pod-configmaps-ec32f275-6719-49a6-bf1a-fdc7e3dc95a5": Phase="Running", Reason="", readiness=false. Elapsed: 2.009331758s
Jan  3 02:21:20.777: INFO: Pod "pod-configmaps-ec32f275-6719-49a6-bf1a-fdc7e3dc95a5" satisfied condition "running"
STEP: Waiting for pod with text data 01/03/23 02:21:20.777
STEP: Waiting for pod with binary data 01/03/23 02:21:20.781
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan  3 02:21:20.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6691" for this suite. 01/03/23 02:21:20.789
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","completed":100,"skipped":1736,"failed":0}
------------------------------
• [2.080 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:21:18.712
    Jan  3 02:21:18.712: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename configmap 01/03/23 02:21:18.712
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:21:18.736
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:21:18.738
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:174
    STEP: Creating configMap with name configmap-test-upd-163f5aa0-69fc-4877-9d9f-61060d9d7d53 01/03/23 02:21:18.743
    STEP: Creating the pod 01/03/23 02:21:18.746
    Jan  3 02:21:18.768: INFO: Waiting up to 5m0s for pod "pod-configmaps-ec32f275-6719-49a6-bf1a-fdc7e3dc95a5" in namespace "configmap-6691" to be "running"
    Jan  3 02:21:18.775: INFO: Pod "pod-configmaps-ec32f275-6719-49a6-bf1a-fdc7e3dc95a5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.851786ms
    Jan  3 02:21:20.777: INFO: Pod "pod-configmaps-ec32f275-6719-49a6-bf1a-fdc7e3dc95a5": Phase="Running", Reason="", readiness=false. Elapsed: 2.009331758s
    Jan  3 02:21:20.777: INFO: Pod "pod-configmaps-ec32f275-6719-49a6-bf1a-fdc7e3dc95a5" satisfied condition "running"
    STEP: Waiting for pod with text data 01/03/23 02:21:20.777
    STEP: Waiting for pod with binary data 01/03/23 02:21:20.781
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan  3 02:21:20.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6691" for this suite. 01/03/23 02:21:20.789
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:21:20.794
Jan  3 02:21:20.794: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename dns 01/03/23 02:21:20.795
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:21:20.826
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:21:20.828
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 01/03/23 02:21:20.83
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6571.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-6571.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6571.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-6571.svc.cluster.local;sleep 1; done
 01/03/23 02:21:20.834
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6571.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-6571.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6571.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-6571.svc.cluster.local;sleep 1; done
 01/03/23 02:21:20.834
STEP: creating a pod to probe DNS 01/03/23 02:21:20.834
STEP: submitting the pod to kubernetes 01/03/23 02:21:20.834
Jan  3 02:21:20.855: INFO: Waiting up to 15m0s for pod "dns-test-6354bd26-0f75-4001-9161-9895736eca64" in namespace "dns-6571" to be "running"
Jan  3 02:21:20.872: INFO: Pod "dns-test-6354bd26-0f75-4001-9161-9895736eca64": Phase="Pending", Reason="", readiness=false. Elapsed: 16.708934ms
Jan  3 02:21:22.875: INFO: Pod "dns-test-6354bd26-0f75-4001-9161-9895736eca64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020073483s
Jan  3 02:21:24.875: INFO: Pod "dns-test-6354bd26-0f75-4001-9161-9895736eca64": Phase="Running", Reason="", readiness=true. Elapsed: 4.020131896s
Jan  3 02:21:24.875: INFO: Pod "dns-test-6354bd26-0f75-4001-9161-9895736eca64" satisfied condition "running"
STEP: retrieving the pod 01/03/23 02:21:24.875
STEP: looking for the results for each expected name from probers 01/03/23 02:21:24.877
Jan  3 02:21:24.879: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
Jan  3 02:21:24.881: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
Jan  3 02:21:24.883: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
Jan  3 02:21:24.885: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
Jan  3 02:21:24.886: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
Jan  3 02:21:24.888: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
Jan  3 02:21:24.890: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
Jan  3 02:21:24.891: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
Jan  3 02:21:24.891: INFO: Lookups using dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6571.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6571.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local jessie_udp@dns-test-service-2.dns-6571.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6571.svc.cluster.local]

Jan  3 02:21:29.894: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
Jan  3 02:21:29.896: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
Jan  3 02:21:29.898: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
Jan  3 02:21:29.899: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
Jan  3 02:21:29.901: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
Jan  3 02:21:29.903: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
Jan  3 02:21:29.905: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
Jan  3 02:21:29.906: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
Jan  3 02:21:29.906: INFO: Lookups using dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6571.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6571.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local jessie_udp@dns-test-service-2.dns-6571.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6571.svc.cluster.local]

Jan  3 02:21:34.897: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
Jan  3 02:21:34.901: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
Jan  3 02:21:34.903: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
Jan  3 02:21:34.905: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
Jan  3 02:21:34.906: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
Jan  3 02:21:34.908: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
Jan  3 02:21:34.910: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
Jan  3 02:21:34.912: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
Jan  3 02:21:34.912: INFO: Lookups using dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6571.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6571.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local jessie_udp@dns-test-service-2.dns-6571.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6571.svc.cluster.local]

Jan  3 02:21:39.897: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
Jan  3 02:21:39.899: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
Jan  3 02:21:39.900: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
Jan  3 02:21:39.902: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
Jan  3 02:21:39.904: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
Jan  3 02:21:39.906: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
Jan  3 02:21:39.908: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
Jan  3 02:21:39.909: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
Jan  3 02:21:39.909: INFO: Lookups using dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6571.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6571.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local jessie_udp@dns-test-service-2.dns-6571.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6571.svc.cluster.local]

Jan  3 02:21:44.897: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
Jan  3 02:21:44.898: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
Jan  3 02:21:44.900: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
Jan  3 02:21:44.902: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
Jan  3 02:21:44.904: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
Jan  3 02:21:44.906: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
Jan  3 02:21:44.907: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
Jan  3 02:21:44.909: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
Jan  3 02:21:44.909: INFO: Lookups using dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6571.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6571.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local jessie_udp@dns-test-service-2.dns-6571.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6571.svc.cluster.local]

Jan  3 02:21:49.897: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
Jan  3 02:21:49.899: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
Jan  3 02:21:49.901: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
Jan  3 02:21:49.903: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
Jan  3 02:21:49.904: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
Jan  3 02:21:49.906: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
Jan  3 02:21:49.908: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
Jan  3 02:21:49.909: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
Jan  3 02:21:49.909: INFO: Lookups using dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6571.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6571.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local jessie_udp@dns-test-service-2.dns-6571.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6571.svc.cluster.local]

Jan  3 02:21:54.906: INFO: DNS probes using dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64 succeeded

STEP: deleting the pod 01/03/23 02:21:54.906
STEP: deleting the test headless service 01/03/23 02:21:54.949
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan  3 02:21:54.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6571" for this suite. 01/03/23 02:21:54.978
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","completed":101,"skipped":1795,"failed":0}
------------------------------
• [SLOW TEST] [34.189 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:21:20.794
    Jan  3 02:21:20.794: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename dns 01/03/23 02:21:20.795
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:21:20.826
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:21:20.828
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 01/03/23 02:21:20.83
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6571.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-6571.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6571.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-6571.svc.cluster.local;sleep 1; done
     01/03/23 02:21:20.834
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6571.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-6571.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6571.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-6571.svc.cluster.local;sleep 1; done
     01/03/23 02:21:20.834
    STEP: creating a pod to probe DNS 01/03/23 02:21:20.834
    STEP: submitting the pod to kubernetes 01/03/23 02:21:20.834
    Jan  3 02:21:20.855: INFO: Waiting up to 15m0s for pod "dns-test-6354bd26-0f75-4001-9161-9895736eca64" in namespace "dns-6571" to be "running"
    Jan  3 02:21:20.872: INFO: Pod "dns-test-6354bd26-0f75-4001-9161-9895736eca64": Phase="Pending", Reason="", readiness=false. Elapsed: 16.708934ms
    Jan  3 02:21:22.875: INFO: Pod "dns-test-6354bd26-0f75-4001-9161-9895736eca64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020073483s
    Jan  3 02:21:24.875: INFO: Pod "dns-test-6354bd26-0f75-4001-9161-9895736eca64": Phase="Running", Reason="", readiness=true. Elapsed: 4.020131896s
    Jan  3 02:21:24.875: INFO: Pod "dns-test-6354bd26-0f75-4001-9161-9895736eca64" satisfied condition "running"
    STEP: retrieving the pod 01/03/23 02:21:24.875
    STEP: looking for the results for each expected name from probers 01/03/23 02:21:24.877
    Jan  3 02:21:24.879: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
    Jan  3 02:21:24.881: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
    Jan  3 02:21:24.883: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
    Jan  3 02:21:24.885: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
    Jan  3 02:21:24.886: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
    Jan  3 02:21:24.888: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
    Jan  3 02:21:24.890: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
    Jan  3 02:21:24.891: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
    Jan  3 02:21:24.891: INFO: Lookups using dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6571.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6571.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local jessie_udp@dns-test-service-2.dns-6571.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6571.svc.cluster.local]

    Jan  3 02:21:29.894: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
    Jan  3 02:21:29.896: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
    Jan  3 02:21:29.898: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
    Jan  3 02:21:29.899: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
    Jan  3 02:21:29.901: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
    Jan  3 02:21:29.903: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
    Jan  3 02:21:29.905: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
    Jan  3 02:21:29.906: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
    Jan  3 02:21:29.906: INFO: Lookups using dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6571.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6571.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local jessie_udp@dns-test-service-2.dns-6571.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6571.svc.cluster.local]

    Jan  3 02:21:34.897: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
    Jan  3 02:21:34.901: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
    Jan  3 02:21:34.903: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
    Jan  3 02:21:34.905: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
    Jan  3 02:21:34.906: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
    Jan  3 02:21:34.908: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
    Jan  3 02:21:34.910: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
    Jan  3 02:21:34.912: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
    Jan  3 02:21:34.912: INFO: Lookups using dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6571.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6571.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local jessie_udp@dns-test-service-2.dns-6571.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6571.svc.cluster.local]

    Jan  3 02:21:39.897: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
    Jan  3 02:21:39.899: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
    Jan  3 02:21:39.900: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
    Jan  3 02:21:39.902: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
    Jan  3 02:21:39.904: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
    Jan  3 02:21:39.906: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
    Jan  3 02:21:39.908: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
    Jan  3 02:21:39.909: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
    Jan  3 02:21:39.909: INFO: Lookups using dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6571.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6571.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local jessie_udp@dns-test-service-2.dns-6571.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6571.svc.cluster.local]

    Jan  3 02:21:44.897: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
    Jan  3 02:21:44.898: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
    Jan  3 02:21:44.900: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
    Jan  3 02:21:44.902: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
    Jan  3 02:21:44.904: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
    Jan  3 02:21:44.906: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
    Jan  3 02:21:44.907: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
    Jan  3 02:21:44.909: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
    Jan  3 02:21:44.909: INFO: Lookups using dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6571.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6571.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local jessie_udp@dns-test-service-2.dns-6571.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6571.svc.cluster.local]

    Jan  3 02:21:49.897: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
    Jan  3 02:21:49.899: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
    Jan  3 02:21:49.901: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
    Jan  3 02:21:49.903: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
    Jan  3 02:21:49.904: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
    Jan  3 02:21:49.906: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
    Jan  3 02:21:49.908: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
    Jan  3 02:21:49.909: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6571.svc.cluster.local from pod dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64: the server could not find the requested resource (get pods dns-test-6354bd26-0f75-4001-9161-9895736eca64)
    Jan  3 02:21:49.909: INFO: Lookups using dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6571.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6571.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6571.svc.cluster.local jessie_udp@dns-test-service-2.dns-6571.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6571.svc.cluster.local]

    Jan  3 02:21:54.906: INFO: DNS probes using dns-6571/dns-test-6354bd26-0f75-4001-9161-9895736eca64 succeeded

    STEP: deleting the pod 01/03/23 02:21:54.906
    STEP: deleting the test headless service 01/03/23 02:21:54.949
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan  3 02:21:54.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-6571" for this suite. 01/03/23 02:21:54.978
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:21:54.983
Jan  3 02:21:54.983: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename resourcequota 01/03/23 02:21:54.984
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:21:55.011
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:21:55.013
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
STEP: Counting existing ResourceQuota 01/03/23 02:21:55.015
STEP: Creating a ResourceQuota 01/03/23 02:22:00.017
STEP: Ensuring resource quota status is calculated 01/03/23 02:22:00.021
STEP: Creating a ReplicaSet 01/03/23 02:22:02.024
STEP: Ensuring resource quota status captures replicaset creation 01/03/23 02:22:02.037
STEP: Deleting a ReplicaSet 01/03/23 02:22:04.04
STEP: Ensuring resource quota status released usage 01/03/23 02:22:04.043
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan  3 02:22:06.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1604" for this suite. 01/03/23 02:22:06.049
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","completed":102,"skipped":1796,"failed":0}
------------------------------
• [SLOW TEST] [11.069 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:21:54.983
    Jan  3 02:21:54.983: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename resourcequota 01/03/23 02:21:54.984
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:21:55.011
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:21:55.013
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:438
    STEP: Counting existing ResourceQuota 01/03/23 02:21:55.015
    STEP: Creating a ResourceQuota 01/03/23 02:22:00.017
    STEP: Ensuring resource quota status is calculated 01/03/23 02:22:00.021
    STEP: Creating a ReplicaSet 01/03/23 02:22:02.024
    STEP: Ensuring resource quota status captures replicaset creation 01/03/23 02:22:02.037
    STEP: Deleting a ReplicaSet 01/03/23 02:22:04.04
    STEP: Ensuring resource quota status released usage 01/03/23 02:22:04.043
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan  3 02:22:06.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-1604" for this suite. 01/03/23 02:22:06.049
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:22:06.053
Jan  3 02:22:06.053: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename lease-test 01/03/23 02:22:06.054
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:22:06.108
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:22:06.11
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/framework.go:187
Jan  3 02:22:06.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-4432" for this suite. 01/03/23 02:22:06.181
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","completed":103,"skipped":1807,"failed":0}
------------------------------
• [0.132 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:22:06.053
    Jan  3 02:22:06.053: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename lease-test 01/03/23 02:22:06.054
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:22:06.108
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:22:06.11
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/framework.go:187
    Jan  3 02:22:06.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "lease-test-4432" for this suite. 01/03/23 02:22:06.181
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:22:06.185
Jan  3 02:22:06.185: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename crd-publish-openapi 01/03/23 02:22:06.186
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:22:06.234
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:22:06.236
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
STEP: set up a multi version CRD 01/03/23 02:22:06.238
Jan  3 02:22:06.239: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: mark a version not serverd 01/03/23 02:22:19.34
STEP: check the unserved version gets removed 01/03/23 02:22:19.351
STEP: check the other version is not changed 01/03/23 02:22:24.274
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  3 02:22:34.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4638" for this suite. 01/03/23 02:22:34.37
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","completed":104,"skipped":1825,"failed":0}
------------------------------
• [SLOW TEST] [28.188 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:22:06.185
    Jan  3 02:22:06.185: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename crd-publish-openapi 01/03/23 02:22:06.186
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:22:06.234
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:22:06.236
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:441
    STEP: set up a multi version CRD 01/03/23 02:22:06.238
    Jan  3 02:22:06.239: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: mark a version not serverd 01/03/23 02:22:19.34
    STEP: check the unserved version gets removed 01/03/23 02:22:19.351
    STEP: check the other version is not changed 01/03/23 02:22:24.274
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  3 02:22:34.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-4638" for this suite. 01/03/23 02:22:34.37
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:22:34.373
Jan  3 02:22:34.374: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename emptydir 01/03/23 02:22:34.374
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:22:34.383
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:22:34.385
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
STEP: Creating a pod to test emptydir volume type on tmpfs 01/03/23 02:22:34.387
Jan  3 02:22:34.403: INFO: Waiting up to 5m0s for pod "pod-6527242c-cd39-40f6-88da-948abfe51e1b" in namespace "emptydir-2931" to be "Succeeded or Failed"
Jan  3 02:22:34.406: INFO: Pod "pod-6527242c-cd39-40f6-88da-948abfe51e1b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.034704ms
Jan  3 02:22:36.409: INFO: Pod "pod-6527242c-cd39-40f6-88da-948abfe51e1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006261654s
Jan  3 02:22:38.410: INFO: Pod "pod-6527242c-cd39-40f6-88da-948abfe51e1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00641034s
STEP: Saw pod success 01/03/23 02:22:38.41
Jan  3 02:22:38.410: INFO: Pod "pod-6527242c-cd39-40f6-88da-948abfe51e1b" satisfied condition "Succeeded or Failed"
Jan  3 02:22:38.411: INFO: Trying to get logs from node cncf-master pod pod-6527242c-cd39-40f6-88da-948abfe51e1b container test-container: <nil>
STEP: delete the pod 01/03/23 02:22:38.415
Jan  3 02:22:38.464: INFO: Waiting for pod pod-6527242c-cd39-40f6-88da-948abfe51e1b to disappear
Jan  3 02:22:38.484: INFO: Pod pod-6527242c-cd39-40f6-88da-948abfe51e1b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan  3 02:22:38.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2931" for this suite. 01/03/23 02:22:38.486
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":105,"skipped":1832,"failed":0}
------------------------------
• [4.115 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:22:34.373
    Jan  3 02:22:34.374: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename emptydir 01/03/23 02:22:34.374
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:22:34.383
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:22:34.385
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:86
    STEP: Creating a pod to test emptydir volume type on tmpfs 01/03/23 02:22:34.387
    Jan  3 02:22:34.403: INFO: Waiting up to 5m0s for pod "pod-6527242c-cd39-40f6-88da-948abfe51e1b" in namespace "emptydir-2931" to be "Succeeded or Failed"
    Jan  3 02:22:34.406: INFO: Pod "pod-6527242c-cd39-40f6-88da-948abfe51e1b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.034704ms
    Jan  3 02:22:36.409: INFO: Pod "pod-6527242c-cd39-40f6-88da-948abfe51e1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006261654s
    Jan  3 02:22:38.410: INFO: Pod "pod-6527242c-cd39-40f6-88da-948abfe51e1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00641034s
    STEP: Saw pod success 01/03/23 02:22:38.41
    Jan  3 02:22:38.410: INFO: Pod "pod-6527242c-cd39-40f6-88da-948abfe51e1b" satisfied condition "Succeeded or Failed"
    Jan  3 02:22:38.411: INFO: Trying to get logs from node cncf-master pod pod-6527242c-cd39-40f6-88da-948abfe51e1b container test-container: <nil>
    STEP: delete the pod 01/03/23 02:22:38.415
    Jan  3 02:22:38.464: INFO: Waiting for pod pod-6527242c-cd39-40f6-88da-948abfe51e1b to disappear
    Jan  3 02:22:38.484: INFO: Pod pod-6527242c-cd39-40f6-88da-948abfe51e1b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan  3 02:22:38.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2931" for this suite. 01/03/23 02:22:38.486
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:22:38.49
Jan  3 02:22:38.490: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename crd-publish-openapi 01/03/23 02:22:38.49
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:22:38.503
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:22:38.505
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 01/03/23 02:22:38.507
Jan  3 02:22:38.508: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
Jan  3 02:22:43.853: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  3 02:23:04.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7136" for this suite. 01/03/23 02:23:04.657
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","completed":106,"skipped":1852,"failed":0}
------------------------------
• [SLOW TEST] [26.169 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:22:38.49
    Jan  3 02:22:38.490: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename crd-publish-openapi 01/03/23 02:22:38.49
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:22:38.503
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:22:38.505
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:356
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 01/03/23 02:22:38.507
    Jan  3 02:22:38.508: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    Jan  3 02:22:43.853: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  3 02:23:04.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-7136" for this suite. 01/03/23 02:23:04.657
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:23:04.659
Jan  3 02:23:04.659: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename subpath 01/03/23 02:23:04.66
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:23:04.67
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:23:04.672
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/03/23 02:23:04.674
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-7nwh 01/03/23 02:23:04.716
STEP: Creating a pod to test atomic-volume-subpath 01/03/23 02:23:04.716
Jan  3 02:23:04.725: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-7nwh" in namespace "subpath-7168" to be "Succeeded or Failed"
Jan  3 02:23:04.737: INFO: Pod "pod-subpath-test-downwardapi-7nwh": Phase="Pending", Reason="", readiness=false. Elapsed: 12.321933ms
Jan  3 02:23:06.739: INFO: Pod "pod-subpath-test-downwardapi-7nwh": Phase="Running", Reason="", readiness=true. Elapsed: 2.014719886s
Jan  3 02:23:08.740: INFO: Pod "pod-subpath-test-downwardapi-7nwh": Phase="Running", Reason="", readiness=true. Elapsed: 4.015296123s
Jan  3 02:23:10.741: INFO: Pod "pod-subpath-test-downwardapi-7nwh": Phase="Running", Reason="", readiness=true. Elapsed: 6.016564493s
Jan  3 02:23:12.740: INFO: Pod "pod-subpath-test-downwardapi-7nwh": Phase="Running", Reason="", readiness=true. Elapsed: 8.01581422s
Jan  3 02:23:14.740: INFO: Pod "pod-subpath-test-downwardapi-7nwh": Phase="Running", Reason="", readiness=true. Elapsed: 10.014976549s
Jan  3 02:23:16.740: INFO: Pod "pod-subpath-test-downwardapi-7nwh": Phase="Running", Reason="", readiness=true. Elapsed: 12.015517275s
Jan  3 02:23:18.740: INFO: Pod "pod-subpath-test-downwardapi-7nwh": Phase="Running", Reason="", readiness=true. Elapsed: 14.015888176s
Jan  3 02:23:20.741: INFO: Pod "pod-subpath-test-downwardapi-7nwh": Phase="Running", Reason="", readiness=true. Elapsed: 16.016089283s
Jan  3 02:23:22.740: INFO: Pod "pod-subpath-test-downwardapi-7nwh": Phase="Running", Reason="", readiness=true. Elapsed: 18.015448626s
Jan  3 02:23:24.740: INFO: Pod "pod-subpath-test-downwardapi-7nwh": Phase="Running", Reason="", readiness=true. Elapsed: 20.015391577s
Jan  3 02:23:26.740: INFO: Pod "pod-subpath-test-downwardapi-7nwh": Phase="Running", Reason="", readiness=false. Elapsed: 22.014997501s
Jan  3 02:23:28.741: INFO: Pod "pod-subpath-test-downwardapi-7nwh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.016060629s
STEP: Saw pod success 01/03/23 02:23:28.741
Jan  3 02:23:28.741: INFO: Pod "pod-subpath-test-downwardapi-7nwh" satisfied condition "Succeeded or Failed"
Jan  3 02:23:28.742: INFO: Trying to get logs from node cncf-master pod pod-subpath-test-downwardapi-7nwh container test-container-subpath-downwardapi-7nwh: <nil>
STEP: delete the pod 01/03/23 02:23:28.746
Jan  3 02:23:28.768: INFO: Waiting for pod pod-subpath-test-downwardapi-7nwh to disappear
Jan  3 02:23:28.773: INFO: Pod pod-subpath-test-downwardapi-7nwh no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-7nwh 01/03/23 02:23:28.773
Jan  3 02:23:28.773: INFO: Deleting pod "pod-subpath-test-downwardapi-7nwh" in namespace "subpath-7168"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan  3 02:23:28.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7168" for this suite. 01/03/23 02:23:28.778
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]","completed":107,"skipped":1855,"failed":0}
------------------------------
• [SLOW TEST] [24.121 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:23:04.659
    Jan  3 02:23:04.659: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename subpath 01/03/23 02:23:04.66
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:23:04.67
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:23:04.672
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/03/23 02:23:04.674
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-7nwh 01/03/23 02:23:04.716
    STEP: Creating a pod to test atomic-volume-subpath 01/03/23 02:23:04.716
    Jan  3 02:23:04.725: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-7nwh" in namespace "subpath-7168" to be "Succeeded or Failed"
    Jan  3 02:23:04.737: INFO: Pod "pod-subpath-test-downwardapi-7nwh": Phase="Pending", Reason="", readiness=false. Elapsed: 12.321933ms
    Jan  3 02:23:06.739: INFO: Pod "pod-subpath-test-downwardapi-7nwh": Phase="Running", Reason="", readiness=true. Elapsed: 2.014719886s
    Jan  3 02:23:08.740: INFO: Pod "pod-subpath-test-downwardapi-7nwh": Phase="Running", Reason="", readiness=true. Elapsed: 4.015296123s
    Jan  3 02:23:10.741: INFO: Pod "pod-subpath-test-downwardapi-7nwh": Phase="Running", Reason="", readiness=true. Elapsed: 6.016564493s
    Jan  3 02:23:12.740: INFO: Pod "pod-subpath-test-downwardapi-7nwh": Phase="Running", Reason="", readiness=true. Elapsed: 8.01581422s
    Jan  3 02:23:14.740: INFO: Pod "pod-subpath-test-downwardapi-7nwh": Phase="Running", Reason="", readiness=true. Elapsed: 10.014976549s
    Jan  3 02:23:16.740: INFO: Pod "pod-subpath-test-downwardapi-7nwh": Phase="Running", Reason="", readiness=true. Elapsed: 12.015517275s
    Jan  3 02:23:18.740: INFO: Pod "pod-subpath-test-downwardapi-7nwh": Phase="Running", Reason="", readiness=true. Elapsed: 14.015888176s
    Jan  3 02:23:20.741: INFO: Pod "pod-subpath-test-downwardapi-7nwh": Phase="Running", Reason="", readiness=true. Elapsed: 16.016089283s
    Jan  3 02:23:22.740: INFO: Pod "pod-subpath-test-downwardapi-7nwh": Phase="Running", Reason="", readiness=true. Elapsed: 18.015448626s
    Jan  3 02:23:24.740: INFO: Pod "pod-subpath-test-downwardapi-7nwh": Phase="Running", Reason="", readiness=true. Elapsed: 20.015391577s
    Jan  3 02:23:26.740: INFO: Pod "pod-subpath-test-downwardapi-7nwh": Phase="Running", Reason="", readiness=false. Elapsed: 22.014997501s
    Jan  3 02:23:28.741: INFO: Pod "pod-subpath-test-downwardapi-7nwh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.016060629s
    STEP: Saw pod success 01/03/23 02:23:28.741
    Jan  3 02:23:28.741: INFO: Pod "pod-subpath-test-downwardapi-7nwh" satisfied condition "Succeeded or Failed"
    Jan  3 02:23:28.742: INFO: Trying to get logs from node cncf-master pod pod-subpath-test-downwardapi-7nwh container test-container-subpath-downwardapi-7nwh: <nil>
    STEP: delete the pod 01/03/23 02:23:28.746
    Jan  3 02:23:28.768: INFO: Waiting for pod pod-subpath-test-downwardapi-7nwh to disappear
    Jan  3 02:23:28.773: INFO: Pod pod-subpath-test-downwardapi-7nwh no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-7nwh 01/03/23 02:23:28.773
    Jan  3 02:23:28.773: INFO: Deleting pod "pod-subpath-test-downwardapi-7nwh" in namespace "subpath-7168"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan  3 02:23:28.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-7168" for this suite. 01/03/23 02:23:28.778
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:23:28.781
Jan  3 02:23:28.781: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename pod-network-test 01/03/23 02:23:28.782
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:23:28.796
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:23:28.798
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-5707 01/03/23 02:23:28.8
STEP: creating a selector 01/03/23 02:23:28.8
STEP: Creating the service pods in kubernetes 01/03/23 02:23:28.8
Jan  3 02:23:28.800: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan  3 02:23:28.843: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5707" to be "running and ready"
Jan  3 02:23:28.883: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 39.832622ms
Jan  3 02:23:28.883: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 02:23:30.886: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.042590969s
Jan  3 02:23:30.886: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 02:23:32.887: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.043734789s
Jan  3 02:23:32.887: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 02:23:34.886: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.042859406s
Jan  3 02:23:34.886: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 02:23:36.886: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.042324601s
Jan  3 02:23:36.886: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 02:23:38.886: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.042681782s
Jan  3 02:23:38.886: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 02:23:40.886: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.04249976s
Jan  3 02:23:40.886: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan  3 02:23:40.886: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan  3 02:23:40.888: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5707" to be "running and ready"
Jan  3 02:23:40.890: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.219592ms
Jan  3 02:23:40.890: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan  3 02:23:40.890: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jan  3 02:23:40.892: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5707" to be "running and ready"
Jan  3 02:23:40.893: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 1.844919ms
Jan  3 02:23:40.893: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jan  3 02:23:40.893: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 01/03/23 02:23:40.895
Jan  3 02:23:40.906: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5707" to be "running"
Jan  3 02:23:40.922: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 16.544767ms
Jan  3 02:23:42.925: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.019608204s
Jan  3 02:23:42.925: INFO: Pod "test-container-pod" satisfied condition "running"
Jan  3 02:23:42.927: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-5707" to be "running"
Jan  3 02:23:42.929: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.729255ms
Jan  3 02:23:42.929: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jan  3 02:23:42.930: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jan  3 02:23:42.930: INFO: Going to poll 10.244.246.115 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jan  3 02:23:42.932: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.246.115 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5707 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 02:23:42.932: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
Jan  3 02:23:42.932: INFO: ExecWithOptions: Clientset creation
Jan  3 02:23:42.932: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5707/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.246.115+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan  3 02:23:44.004: INFO: Found all 1 expected endpoints: [netserver-0]
Jan  3 02:23:44.004: INFO: Going to poll 10.244.232.9 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jan  3 02:23:44.006: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.232.9 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5707 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 02:23:44.006: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
Jan  3 02:23:44.007: INFO: ExecWithOptions: Clientset creation
Jan  3 02:23:44.007: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5707/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.232.9+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan  3 02:23:45.086: INFO: Found all 1 expected endpoints: [netserver-1]
Jan  3 02:23:45.086: INFO: Going to poll 10.244.89.102 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jan  3 02:23:45.100: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.89.102 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5707 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 02:23:45.100: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
Jan  3 02:23:45.100: INFO: ExecWithOptions: Clientset creation
Jan  3 02:23:45.100: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5707/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.89.102+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan  3 02:23:46.186: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jan  3 02:23:46.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5707" for this suite. 01/03/23 02:23:46.189
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","completed":108,"skipped":1859,"failed":0}
------------------------------
• [SLOW TEST] [17.412 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:23:28.781
    Jan  3 02:23:28.781: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename pod-network-test 01/03/23 02:23:28.782
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:23:28.796
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:23:28.798
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-5707 01/03/23 02:23:28.8
    STEP: creating a selector 01/03/23 02:23:28.8
    STEP: Creating the service pods in kubernetes 01/03/23 02:23:28.8
    Jan  3 02:23:28.800: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan  3 02:23:28.843: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5707" to be "running and ready"
    Jan  3 02:23:28.883: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 39.832622ms
    Jan  3 02:23:28.883: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 02:23:30.886: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.042590969s
    Jan  3 02:23:30.886: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 02:23:32.887: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.043734789s
    Jan  3 02:23:32.887: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 02:23:34.886: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.042859406s
    Jan  3 02:23:34.886: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 02:23:36.886: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.042324601s
    Jan  3 02:23:36.886: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 02:23:38.886: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.042681782s
    Jan  3 02:23:38.886: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 02:23:40.886: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.04249976s
    Jan  3 02:23:40.886: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan  3 02:23:40.886: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan  3 02:23:40.888: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5707" to be "running and ready"
    Jan  3 02:23:40.890: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.219592ms
    Jan  3 02:23:40.890: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan  3 02:23:40.890: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jan  3 02:23:40.892: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5707" to be "running and ready"
    Jan  3 02:23:40.893: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 1.844919ms
    Jan  3 02:23:40.893: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jan  3 02:23:40.893: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 01/03/23 02:23:40.895
    Jan  3 02:23:40.906: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5707" to be "running"
    Jan  3 02:23:40.922: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 16.544767ms
    Jan  3 02:23:42.925: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.019608204s
    Jan  3 02:23:42.925: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan  3 02:23:42.927: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-5707" to be "running"
    Jan  3 02:23:42.929: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.729255ms
    Jan  3 02:23:42.929: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jan  3 02:23:42.930: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Jan  3 02:23:42.930: INFO: Going to poll 10.244.246.115 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Jan  3 02:23:42.932: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.246.115 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5707 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 02:23:42.932: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    Jan  3 02:23:42.932: INFO: ExecWithOptions: Clientset creation
    Jan  3 02:23:42.932: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5707/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.246.115+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan  3 02:23:44.004: INFO: Found all 1 expected endpoints: [netserver-0]
    Jan  3 02:23:44.004: INFO: Going to poll 10.244.232.9 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Jan  3 02:23:44.006: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.232.9 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5707 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 02:23:44.006: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    Jan  3 02:23:44.007: INFO: ExecWithOptions: Clientset creation
    Jan  3 02:23:44.007: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5707/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.232.9+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan  3 02:23:45.086: INFO: Found all 1 expected endpoints: [netserver-1]
    Jan  3 02:23:45.086: INFO: Going to poll 10.244.89.102 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Jan  3 02:23:45.100: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.89.102 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5707 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 02:23:45.100: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    Jan  3 02:23:45.100: INFO: ExecWithOptions: Clientset creation
    Jan  3 02:23:45.100: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5707/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.89.102+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan  3 02:23:46.186: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jan  3 02:23:46.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-5707" for this suite. 01/03/23 02:23:46.189
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:23:46.194
Jan  3 02:23:46.194: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename projected 01/03/23 02:23:46.195
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:23:46.203
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:23:46.205
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
STEP: Creating projection with secret that has name projected-secret-test-map-62588d6d-a070-4482-9cb8-953dc413c28f 01/03/23 02:23:46.217
STEP: Creating a pod to test consume secrets 01/03/23 02:23:46.22
Jan  3 02:23:46.229: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-416fd9c9-87b7-4374-8ec9-711bb0438f9d" in namespace "projected-7480" to be "Succeeded or Failed"
Jan  3 02:23:46.234: INFO: Pod "pod-projected-secrets-416fd9c9-87b7-4374-8ec9-711bb0438f9d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.89243ms
Jan  3 02:23:48.238: INFO: Pod "pod-projected-secrets-416fd9c9-87b7-4374-8ec9-711bb0438f9d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008154785s
Jan  3 02:23:50.237: INFO: Pod "pod-projected-secrets-416fd9c9-87b7-4374-8ec9-711bb0438f9d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007767174s
Jan  3 02:23:52.243: INFO: Pod "pod-projected-secrets-416fd9c9-87b7-4374-8ec9-711bb0438f9d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013912951s
STEP: Saw pod success 01/03/23 02:23:52.243
Jan  3 02:23:52.244: INFO: Pod "pod-projected-secrets-416fd9c9-87b7-4374-8ec9-711bb0438f9d" satisfied condition "Succeeded or Failed"
Jan  3 02:23:52.245: INFO: Trying to get logs from node cncf-master pod pod-projected-secrets-416fd9c9-87b7-4374-8ec9-711bb0438f9d container projected-secret-volume-test: <nil>
STEP: delete the pod 01/03/23 02:23:52.248
Jan  3 02:23:52.267: INFO: Waiting for pod pod-projected-secrets-416fd9c9-87b7-4374-8ec9-711bb0438f9d to disappear
Jan  3 02:23:52.273: INFO: Pod pod-projected-secrets-416fd9c9-87b7-4374-8ec9-711bb0438f9d no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan  3 02:23:52.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7480" for this suite. 01/03/23 02:23:52.275
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":109,"skipped":1903,"failed":0}
------------------------------
• [SLOW TEST] [6.084 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:23:46.194
    Jan  3 02:23:46.194: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename projected 01/03/23 02:23:46.195
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:23:46.203
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:23:46.205
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:77
    STEP: Creating projection with secret that has name projected-secret-test-map-62588d6d-a070-4482-9cb8-953dc413c28f 01/03/23 02:23:46.217
    STEP: Creating a pod to test consume secrets 01/03/23 02:23:46.22
    Jan  3 02:23:46.229: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-416fd9c9-87b7-4374-8ec9-711bb0438f9d" in namespace "projected-7480" to be "Succeeded or Failed"
    Jan  3 02:23:46.234: INFO: Pod "pod-projected-secrets-416fd9c9-87b7-4374-8ec9-711bb0438f9d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.89243ms
    Jan  3 02:23:48.238: INFO: Pod "pod-projected-secrets-416fd9c9-87b7-4374-8ec9-711bb0438f9d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008154785s
    Jan  3 02:23:50.237: INFO: Pod "pod-projected-secrets-416fd9c9-87b7-4374-8ec9-711bb0438f9d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007767174s
    Jan  3 02:23:52.243: INFO: Pod "pod-projected-secrets-416fd9c9-87b7-4374-8ec9-711bb0438f9d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013912951s
    STEP: Saw pod success 01/03/23 02:23:52.243
    Jan  3 02:23:52.244: INFO: Pod "pod-projected-secrets-416fd9c9-87b7-4374-8ec9-711bb0438f9d" satisfied condition "Succeeded or Failed"
    Jan  3 02:23:52.245: INFO: Trying to get logs from node cncf-master pod pod-projected-secrets-416fd9c9-87b7-4374-8ec9-711bb0438f9d container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/03/23 02:23:52.248
    Jan  3 02:23:52.267: INFO: Waiting for pod pod-projected-secrets-416fd9c9-87b7-4374-8ec9-711bb0438f9d to disappear
    Jan  3 02:23:52.273: INFO: Pod pod-projected-secrets-416fd9c9-87b7-4374-8ec9-711bb0438f9d no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan  3 02:23:52.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7480" for this suite. 01/03/23 02:23:52.275
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:23:52.278
Jan  3 02:23:52.278: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename var-expansion 01/03/23 02:23:52.279
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:23:52.32
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:23:52.321
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
STEP: creating the pod 01/03/23 02:23:52.346
STEP: waiting for pod running 01/03/23 02:23:52.373
Jan  3 02:23:52.373: INFO: Waiting up to 2m0s for pod "var-expansion-f8784cd1-9d88-46a8-acda-1a7f27186a77" in namespace "var-expansion-9810" to be "running"
Jan  3 02:23:52.404: INFO: Pod "var-expansion-f8784cd1-9d88-46a8-acda-1a7f27186a77": Phase="Pending", Reason="", readiness=false. Elapsed: 31.204687ms
Jan  3 02:23:54.406: INFO: Pod "var-expansion-f8784cd1-9d88-46a8-acda-1a7f27186a77": Phase="Running", Reason="", readiness=true. Elapsed: 2.033631581s
Jan  3 02:23:54.406: INFO: Pod "var-expansion-f8784cd1-9d88-46a8-acda-1a7f27186a77" satisfied condition "running"
STEP: creating a file in subpath 01/03/23 02:23:54.406
Jan  3 02:23:54.408: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-9810 PodName:var-expansion-f8784cd1-9d88-46a8-acda-1a7f27186a77 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 02:23:54.408: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
Jan  3 02:23:54.408: INFO: ExecWithOptions: Clientset creation
Jan  3 02:23:54.409: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-9810/pods/var-expansion-f8784cd1-9d88-46a8-acda-1a7f27186a77/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 01/03/23 02:23:54.484
Jan  3 02:23:54.486: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-9810 PodName:var-expansion-f8784cd1-9d88-46a8-acda-1a7f27186a77 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 02:23:54.486: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
Jan  3 02:23:54.486: INFO: ExecWithOptions: Clientset creation
Jan  3 02:23:54.486: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-9810/pods/var-expansion-f8784cd1-9d88-46a8-acda-1a7f27186a77/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 01/03/23 02:23:54.553
Jan  3 02:23:55.066: INFO: Successfully updated pod "var-expansion-f8784cd1-9d88-46a8-acda-1a7f27186a77"
STEP: waiting for annotated pod running 01/03/23 02:23:55.066
Jan  3 02:23:55.066: INFO: Waiting up to 2m0s for pod "var-expansion-f8784cd1-9d88-46a8-acda-1a7f27186a77" in namespace "var-expansion-9810" to be "running"
Jan  3 02:23:55.070: INFO: Pod "var-expansion-f8784cd1-9d88-46a8-acda-1a7f27186a77": Phase="Running", Reason="", readiness=true. Elapsed: 3.574379ms
Jan  3 02:23:55.070: INFO: Pod "var-expansion-f8784cd1-9d88-46a8-acda-1a7f27186a77" satisfied condition "running"
STEP: deleting the pod gracefully 01/03/23 02:23:55.07
Jan  3 02:23:55.070: INFO: Deleting pod "var-expansion-f8784cd1-9d88-46a8-acda-1a7f27186a77" in namespace "var-expansion-9810"
Jan  3 02:23:55.072: INFO: Wait up to 5m0s for pod "var-expansion-f8784cd1-9d88-46a8-acda-1a7f27186a77" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan  3 02:24:29.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9810" for this suite. 01/03/23 02:24:29.088
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","completed":110,"skipped":1912,"failed":0}
------------------------------
• [SLOW TEST] [36.812 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:23:52.278
    Jan  3 02:23:52.278: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename var-expansion 01/03/23 02:23:52.279
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:23:52.32
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:23:52.321
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:296
    STEP: creating the pod 01/03/23 02:23:52.346
    STEP: waiting for pod running 01/03/23 02:23:52.373
    Jan  3 02:23:52.373: INFO: Waiting up to 2m0s for pod "var-expansion-f8784cd1-9d88-46a8-acda-1a7f27186a77" in namespace "var-expansion-9810" to be "running"
    Jan  3 02:23:52.404: INFO: Pod "var-expansion-f8784cd1-9d88-46a8-acda-1a7f27186a77": Phase="Pending", Reason="", readiness=false. Elapsed: 31.204687ms
    Jan  3 02:23:54.406: INFO: Pod "var-expansion-f8784cd1-9d88-46a8-acda-1a7f27186a77": Phase="Running", Reason="", readiness=true. Elapsed: 2.033631581s
    Jan  3 02:23:54.406: INFO: Pod "var-expansion-f8784cd1-9d88-46a8-acda-1a7f27186a77" satisfied condition "running"
    STEP: creating a file in subpath 01/03/23 02:23:54.406
    Jan  3 02:23:54.408: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-9810 PodName:var-expansion-f8784cd1-9d88-46a8-acda-1a7f27186a77 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 02:23:54.408: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    Jan  3 02:23:54.408: INFO: ExecWithOptions: Clientset creation
    Jan  3 02:23:54.409: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-9810/pods/var-expansion-f8784cd1-9d88-46a8-acda-1a7f27186a77/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 01/03/23 02:23:54.484
    Jan  3 02:23:54.486: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-9810 PodName:var-expansion-f8784cd1-9d88-46a8-acda-1a7f27186a77 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 02:23:54.486: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    Jan  3 02:23:54.486: INFO: ExecWithOptions: Clientset creation
    Jan  3 02:23:54.486: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-9810/pods/var-expansion-f8784cd1-9d88-46a8-acda-1a7f27186a77/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 01/03/23 02:23:54.553
    Jan  3 02:23:55.066: INFO: Successfully updated pod "var-expansion-f8784cd1-9d88-46a8-acda-1a7f27186a77"
    STEP: waiting for annotated pod running 01/03/23 02:23:55.066
    Jan  3 02:23:55.066: INFO: Waiting up to 2m0s for pod "var-expansion-f8784cd1-9d88-46a8-acda-1a7f27186a77" in namespace "var-expansion-9810" to be "running"
    Jan  3 02:23:55.070: INFO: Pod "var-expansion-f8784cd1-9d88-46a8-acda-1a7f27186a77": Phase="Running", Reason="", readiness=true. Elapsed: 3.574379ms
    Jan  3 02:23:55.070: INFO: Pod "var-expansion-f8784cd1-9d88-46a8-acda-1a7f27186a77" satisfied condition "running"
    STEP: deleting the pod gracefully 01/03/23 02:23:55.07
    Jan  3 02:23:55.070: INFO: Deleting pod "var-expansion-f8784cd1-9d88-46a8-acda-1a7f27186a77" in namespace "var-expansion-9810"
    Jan  3 02:23:55.072: INFO: Wait up to 5m0s for pod "var-expansion-f8784cd1-9d88-46a8-acda-1a7f27186a77" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan  3 02:24:29.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-9810" for this suite. 01/03/23 02:24:29.088
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:24:29.091
Jan  3 02:24:29.091: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename emptydir 01/03/23 02:24:29.092
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:24:29.131
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:24:29.133
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
STEP: Creating a pod to test emptydir 0644 on node default medium 01/03/23 02:24:29.135
Jan  3 02:24:29.154: INFO: Waiting up to 5m0s for pod "pod-88d4d87e-786f-4472-aaa4-fc83366e6d7c" in namespace "emptydir-3325" to be "Succeeded or Failed"
Jan  3 02:24:29.160: INFO: Pod "pod-88d4d87e-786f-4472-aaa4-fc83366e6d7c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.594148ms
Jan  3 02:24:31.167: INFO: Pod "pod-88d4d87e-786f-4472-aaa4-fc83366e6d7c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01304585s
Jan  3 02:24:33.168: INFO: Pod "pod-88d4d87e-786f-4472-aaa4-fc83366e6d7c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013963199s
Jan  3 02:24:35.163: INFO: Pod "pod-88d4d87e-786f-4472-aaa4-fc83366e6d7c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008971987s
STEP: Saw pod success 01/03/23 02:24:35.163
Jan  3 02:24:35.163: INFO: Pod "pod-88d4d87e-786f-4472-aaa4-fc83366e6d7c" satisfied condition "Succeeded or Failed"
Jan  3 02:24:35.165: INFO: Trying to get logs from node cncf-master pod pod-88d4d87e-786f-4472-aaa4-fc83366e6d7c container test-container: <nil>
STEP: delete the pod 01/03/23 02:24:35.174
Jan  3 02:24:35.189: INFO: Waiting for pod pod-88d4d87e-786f-4472-aaa4-fc83366e6d7c to disappear
Jan  3 02:24:35.194: INFO: Pod pod-88d4d87e-786f-4472-aaa4-fc83366e6d7c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan  3 02:24:35.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3325" for this suite. 01/03/23 02:24:35.197
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":111,"skipped":1926,"failed":0}
------------------------------
• [SLOW TEST] [6.108 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:24:29.091
    Jan  3 02:24:29.091: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename emptydir 01/03/23 02:24:29.092
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:24:29.131
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:24:29.133
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:196
    STEP: Creating a pod to test emptydir 0644 on node default medium 01/03/23 02:24:29.135
    Jan  3 02:24:29.154: INFO: Waiting up to 5m0s for pod "pod-88d4d87e-786f-4472-aaa4-fc83366e6d7c" in namespace "emptydir-3325" to be "Succeeded or Failed"
    Jan  3 02:24:29.160: INFO: Pod "pod-88d4d87e-786f-4472-aaa4-fc83366e6d7c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.594148ms
    Jan  3 02:24:31.167: INFO: Pod "pod-88d4d87e-786f-4472-aaa4-fc83366e6d7c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01304585s
    Jan  3 02:24:33.168: INFO: Pod "pod-88d4d87e-786f-4472-aaa4-fc83366e6d7c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013963199s
    Jan  3 02:24:35.163: INFO: Pod "pod-88d4d87e-786f-4472-aaa4-fc83366e6d7c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008971987s
    STEP: Saw pod success 01/03/23 02:24:35.163
    Jan  3 02:24:35.163: INFO: Pod "pod-88d4d87e-786f-4472-aaa4-fc83366e6d7c" satisfied condition "Succeeded or Failed"
    Jan  3 02:24:35.165: INFO: Trying to get logs from node cncf-master pod pod-88d4d87e-786f-4472-aaa4-fc83366e6d7c container test-container: <nil>
    STEP: delete the pod 01/03/23 02:24:35.174
    Jan  3 02:24:35.189: INFO: Waiting for pod pod-88d4d87e-786f-4472-aaa4-fc83366e6d7c to disappear
    Jan  3 02:24:35.194: INFO: Pod pod-88d4d87e-786f-4472-aaa4-fc83366e6d7c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan  3 02:24:35.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-3325" for this suite. 01/03/23 02:24:35.197
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:24:35.201
Jan  3 02:24:35.201: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename downward-api 01/03/23 02:24:35.201
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:24:35.211
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:24:35.213
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
STEP: Creating a pod to test downward api env vars 01/03/23 02:24:35.215
Jan  3 02:24:35.228: INFO: Waiting up to 5m0s for pod "downward-api-20881476-85ba-4ad3-88a6-f8b1ceb291a0" in namespace "downward-api-2659" to be "Succeeded or Failed"
Jan  3 02:24:35.237: INFO: Pod "downward-api-20881476-85ba-4ad3-88a6-f8b1ceb291a0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.432852ms
Jan  3 02:24:37.240: INFO: Pod "downward-api-20881476-85ba-4ad3-88a6-f8b1ceb291a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011752124s
Jan  3 02:24:39.239: INFO: Pod "downward-api-20881476-85ba-4ad3-88a6-f8b1ceb291a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010942284s
STEP: Saw pod success 01/03/23 02:24:39.239
Jan  3 02:24:39.239: INFO: Pod "downward-api-20881476-85ba-4ad3-88a6-f8b1ceb291a0" satisfied condition "Succeeded or Failed"
Jan  3 02:24:39.241: INFO: Trying to get logs from node cncf-master pod downward-api-20881476-85ba-4ad3-88a6-f8b1ceb291a0 container dapi-container: <nil>
STEP: delete the pod 01/03/23 02:24:39.261
Jan  3 02:24:39.281: INFO: Waiting for pod downward-api-20881476-85ba-4ad3-88a6-f8b1ceb291a0 to disappear
Jan  3 02:24:39.286: INFO: Pod downward-api-20881476-85ba-4ad3-88a6-f8b1ceb291a0 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan  3 02:24:39.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2659" for this suite. 01/03/23 02:24:39.293
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","completed":112,"skipped":1957,"failed":0}
------------------------------
• [4.094 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:24:35.201
    Jan  3 02:24:35.201: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename downward-api 01/03/23 02:24:35.201
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:24:35.211
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:24:35.213
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:216
    STEP: Creating a pod to test downward api env vars 01/03/23 02:24:35.215
    Jan  3 02:24:35.228: INFO: Waiting up to 5m0s for pod "downward-api-20881476-85ba-4ad3-88a6-f8b1ceb291a0" in namespace "downward-api-2659" to be "Succeeded or Failed"
    Jan  3 02:24:35.237: INFO: Pod "downward-api-20881476-85ba-4ad3-88a6-f8b1ceb291a0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.432852ms
    Jan  3 02:24:37.240: INFO: Pod "downward-api-20881476-85ba-4ad3-88a6-f8b1ceb291a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011752124s
    Jan  3 02:24:39.239: INFO: Pod "downward-api-20881476-85ba-4ad3-88a6-f8b1ceb291a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010942284s
    STEP: Saw pod success 01/03/23 02:24:39.239
    Jan  3 02:24:39.239: INFO: Pod "downward-api-20881476-85ba-4ad3-88a6-f8b1ceb291a0" satisfied condition "Succeeded or Failed"
    Jan  3 02:24:39.241: INFO: Trying to get logs from node cncf-master pod downward-api-20881476-85ba-4ad3-88a6-f8b1ceb291a0 container dapi-container: <nil>
    STEP: delete the pod 01/03/23 02:24:39.261
    Jan  3 02:24:39.281: INFO: Waiting for pod downward-api-20881476-85ba-4ad3-88a6-f8b1ceb291a0 to disappear
    Jan  3 02:24:39.286: INFO: Pod downward-api-20881476-85ba-4ad3-88a6-f8b1ceb291a0 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan  3 02:24:39.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2659" for this suite. 01/03/23 02:24:39.293
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:24:39.295
Jan  3 02:24:39.295: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename pods 01/03/23 02:24:39.296
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:24:39.316
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:24:39.317
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 01/03/23 02:24:39.319
STEP: submitting the pod to kubernetes 01/03/23 02:24:39.319
STEP: verifying QOS class is set on the pod 01/03/23 02:24:39.325
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:187
Jan  3 02:24:39.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4509" for this suite. 01/03/23 02:24:39.338
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","completed":113,"skipped":1965,"failed":0}
------------------------------
• [0.061 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:24:39.295
    Jan  3 02:24:39.295: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename pods 01/03/23 02:24:39.296
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:24:39.316
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:24:39.317
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 01/03/23 02:24:39.319
    STEP: submitting the pod to kubernetes 01/03/23 02:24:39.319
    STEP: verifying QOS class is set on the pod 01/03/23 02:24:39.325
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:187
    Jan  3 02:24:39.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4509" for this suite. 01/03/23 02:24:39.338
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:24:39.358
Jan  3 02:24:39.358: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename services 01/03/23 02:24:39.358
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:24:39.412
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:24:39.413
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  3 02:24:39.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6778" for this suite. 01/03/23 02:24:39.432
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","completed":114,"skipped":1991,"failed":0}
------------------------------
• [0.077 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:24:39.358
    Jan  3 02:24:39.358: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename services 01/03/23 02:24:39.358
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:24:39.412
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:24:39.413
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:781
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  3 02:24:39.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6778" for this suite. 01/03/23 02:24:39.432
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:24:39.435
Jan  3 02:24:39.435: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename downward-api 01/03/23 02:24:39.436
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:24:39.446
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:24:39.448
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
STEP: Creating a pod to test downward API volume plugin 01/03/23 02:24:39.45
Jan  3 02:24:39.467: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7616370c-2737-43eb-a0b9-601860b12729" in namespace "downward-api-6959" to be "Succeeded or Failed"
Jan  3 02:24:39.475: INFO: Pod "downwardapi-volume-7616370c-2737-43eb-a0b9-601860b12729": Phase="Pending", Reason="", readiness=false. Elapsed: 7.562724ms
Jan  3 02:24:41.477: INFO: Pod "downwardapi-volume-7616370c-2737-43eb-a0b9-601860b12729": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009778672s
Jan  3 02:24:43.477: INFO: Pod "downwardapi-volume-7616370c-2737-43eb-a0b9-601860b12729": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009754793s
Jan  3 02:24:45.477: INFO: Pod "downwardapi-volume-7616370c-2737-43eb-a0b9-601860b12729": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009465114s
STEP: Saw pod success 01/03/23 02:24:45.477
Jan  3 02:24:45.477: INFO: Pod "downwardapi-volume-7616370c-2737-43eb-a0b9-601860b12729" satisfied condition "Succeeded or Failed"
Jan  3 02:24:45.479: INFO: Trying to get logs from node cncf-master pod downwardapi-volume-7616370c-2737-43eb-a0b9-601860b12729 container client-container: <nil>
STEP: delete the pod 01/03/23 02:24:45.482
Jan  3 02:24:45.516: INFO: Waiting for pod downwardapi-volume-7616370c-2737-43eb-a0b9-601860b12729 to disappear
Jan  3 02:24:45.521: INFO: Pod downwardapi-volume-7616370c-2737-43eb-a0b9-601860b12729 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan  3 02:24:45.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6959" for this suite. 01/03/23 02:24:45.523
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":115,"skipped":1992,"failed":0}
------------------------------
• [SLOW TEST] [6.091 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:24:39.435
    Jan  3 02:24:39.435: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename downward-api 01/03/23 02:24:39.436
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:24:39.446
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:24:39.448
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:260
    STEP: Creating a pod to test downward API volume plugin 01/03/23 02:24:39.45
    Jan  3 02:24:39.467: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7616370c-2737-43eb-a0b9-601860b12729" in namespace "downward-api-6959" to be "Succeeded or Failed"
    Jan  3 02:24:39.475: INFO: Pod "downwardapi-volume-7616370c-2737-43eb-a0b9-601860b12729": Phase="Pending", Reason="", readiness=false. Elapsed: 7.562724ms
    Jan  3 02:24:41.477: INFO: Pod "downwardapi-volume-7616370c-2737-43eb-a0b9-601860b12729": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009778672s
    Jan  3 02:24:43.477: INFO: Pod "downwardapi-volume-7616370c-2737-43eb-a0b9-601860b12729": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009754793s
    Jan  3 02:24:45.477: INFO: Pod "downwardapi-volume-7616370c-2737-43eb-a0b9-601860b12729": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009465114s
    STEP: Saw pod success 01/03/23 02:24:45.477
    Jan  3 02:24:45.477: INFO: Pod "downwardapi-volume-7616370c-2737-43eb-a0b9-601860b12729" satisfied condition "Succeeded or Failed"
    Jan  3 02:24:45.479: INFO: Trying to get logs from node cncf-master pod downwardapi-volume-7616370c-2737-43eb-a0b9-601860b12729 container client-container: <nil>
    STEP: delete the pod 01/03/23 02:24:45.482
    Jan  3 02:24:45.516: INFO: Waiting for pod downwardapi-volume-7616370c-2737-43eb-a0b9-601860b12729 to disappear
    Jan  3 02:24:45.521: INFO: Pod downwardapi-volume-7616370c-2737-43eb-a0b9-601860b12729 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan  3 02:24:45.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6959" for this suite. 01/03/23 02:24:45.523
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:24:45.528
Jan  3 02:24:45.528: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename cronjob 01/03/23 02:24:45.529
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:24:45.538
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:24:45.54
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 01/03/23 02:24:45.541
STEP: creating 01/03/23 02:24:45.542
STEP: getting 01/03/23 02:24:45.553
STEP: listing 01/03/23 02:24:45.555
STEP: watching 01/03/23 02:24:45.557
Jan  3 02:24:45.557: INFO: starting watch
STEP: cluster-wide listing 01/03/23 02:24:45.558
STEP: cluster-wide watching 01/03/23 02:24:45.559
Jan  3 02:24:45.559: INFO: starting watch
STEP: patching 01/03/23 02:24:45.56
STEP: updating 01/03/23 02:24:45.563
Jan  3 02:24:45.578: INFO: waiting for watch events with expected annotations
Jan  3 02:24:45.578: INFO: saw patched and updated annotations
STEP: patching /status 01/03/23 02:24:45.578
STEP: updating /status 01/03/23 02:24:45.584
STEP: get /status 01/03/23 02:24:45.601
STEP: deleting 01/03/23 02:24:45.603
STEP: deleting a collection 01/03/23 02:24:45.647
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan  3 02:24:45.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-9070" for this suite. 01/03/23 02:24:45.655
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","completed":116,"skipped":2049,"failed":0}
------------------------------
• [0.129 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:24:45.528
    Jan  3 02:24:45.528: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename cronjob 01/03/23 02:24:45.529
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:24:45.538
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:24:45.54
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 01/03/23 02:24:45.541
    STEP: creating 01/03/23 02:24:45.542
    STEP: getting 01/03/23 02:24:45.553
    STEP: listing 01/03/23 02:24:45.555
    STEP: watching 01/03/23 02:24:45.557
    Jan  3 02:24:45.557: INFO: starting watch
    STEP: cluster-wide listing 01/03/23 02:24:45.558
    STEP: cluster-wide watching 01/03/23 02:24:45.559
    Jan  3 02:24:45.559: INFO: starting watch
    STEP: patching 01/03/23 02:24:45.56
    STEP: updating 01/03/23 02:24:45.563
    Jan  3 02:24:45.578: INFO: waiting for watch events with expected annotations
    Jan  3 02:24:45.578: INFO: saw patched and updated annotations
    STEP: patching /status 01/03/23 02:24:45.578
    STEP: updating /status 01/03/23 02:24:45.584
    STEP: get /status 01/03/23 02:24:45.601
    STEP: deleting 01/03/23 02:24:45.603
    STEP: deleting a collection 01/03/23 02:24:45.647
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan  3 02:24:45.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-9070" for this suite. 01/03/23 02:24:45.655
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:24:45.658
Jan  3 02:24:45.658: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename pods 01/03/23 02:24:45.658
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:24:45.676
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:24:45.677
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
STEP: creating the pod 01/03/23 02:24:45.679
STEP: setting up watch 01/03/23 02:24:45.679
STEP: submitting the pod to kubernetes 01/03/23 02:24:45.782
STEP: verifying the pod is in kubernetes 01/03/23 02:24:45.787
STEP: verifying pod creation was observed 01/03/23 02:24:45.799
Jan  3 02:24:45.799: INFO: Waiting up to 5m0s for pod "pod-submit-remove-fca2e64a-f601-4ed2-9c65-c871ed27dfbd" in namespace "pods-6230" to be "running"
Jan  3 02:24:45.801: INFO: Pod "pod-submit-remove-fca2e64a-f601-4ed2-9c65-c871ed27dfbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.806755ms
Jan  3 02:24:47.804: INFO: Pod "pod-submit-remove-fca2e64a-f601-4ed2-9c65-c871ed27dfbd": Phase="Running", Reason="", readiness=true. Elapsed: 2.004585859s
Jan  3 02:24:47.804: INFO: Pod "pod-submit-remove-fca2e64a-f601-4ed2-9c65-c871ed27dfbd" satisfied condition "running"
STEP: deleting the pod gracefully 01/03/23 02:24:47.805
STEP: verifying pod deletion was observed 01/03/23 02:24:47.808
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan  3 02:24:50.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6230" for this suite. 01/03/23 02:24:50.57
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","completed":117,"skipped":2059,"failed":0}
------------------------------
• [4.914 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:24:45.658
    Jan  3 02:24:45.658: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename pods 01/03/23 02:24:45.658
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:24:45.676
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:24:45.677
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:225
    STEP: creating the pod 01/03/23 02:24:45.679
    STEP: setting up watch 01/03/23 02:24:45.679
    STEP: submitting the pod to kubernetes 01/03/23 02:24:45.782
    STEP: verifying the pod is in kubernetes 01/03/23 02:24:45.787
    STEP: verifying pod creation was observed 01/03/23 02:24:45.799
    Jan  3 02:24:45.799: INFO: Waiting up to 5m0s for pod "pod-submit-remove-fca2e64a-f601-4ed2-9c65-c871ed27dfbd" in namespace "pods-6230" to be "running"
    Jan  3 02:24:45.801: INFO: Pod "pod-submit-remove-fca2e64a-f601-4ed2-9c65-c871ed27dfbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.806755ms
    Jan  3 02:24:47.804: INFO: Pod "pod-submit-remove-fca2e64a-f601-4ed2-9c65-c871ed27dfbd": Phase="Running", Reason="", readiness=true. Elapsed: 2.004585859s
    Jan  3 02:24:47.804: INFO: Pod "pod-submit-remove-fca2e64a-f601-4ed2-9c65-c871ed27dfbd" satisfied condition "running"
    STEP: deleting the pod gracefully 01/03/23 02:24:47.805
    STEP: verifying pod deletion was observed 01/03/23 02:24:47.808
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan  3 02:24:50.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-6230" for this suite. 01/03/23 02:24:50.57
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:24:50.573
Jan  3 02:24:50.573: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename downward-api 01/03/23 02:24:50.573
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:24:50.607
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:24:50.609
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
STEP: Creating a pod to test downward api env vars 01/03/23 02:24:50.611
Jan  3 02:24:50.622: INFO: Waiting up to 5m0s for pod "downward-api-f16dd28b-9125-4dcd-b429-afc6e3d74479" in namespace "downward-api-9318" to be "Succeeded or Failed"
Jan  3 02:24:50.641: INFO: Pod "downward-api-f16dd28b-9125-4dcd-b429-afc6e3d74479": Phase="Pending", Reason="", readiness=false. Elapsed: 19.173866ms
Jan  3 02:24:52.644: INFO: Pod "downward-api-f16dd28b-9125-4dcd-b429-afc6e3d74479": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021704579s
Jan  3 02:24:54.645: INFO: Pod "downward-api-f16dd28b-9125-4dcd-b429-afc6e3d74479": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022980893s
STEP: Saw pod success 01/03/23 02:24:54.645
Jan  3 02:24:54.645: INFO: Pod "downward-api-f16dd28b-9125-4dcd-b429-afc6e3d74479" satisfied condition "Succeeded or Failed"
Jan  3 02:24:54.647: INFO: Trying to get logs from node cncf-master pod downward-api-f16dd28b-9125-4dcd-b429-afc6e3d74479 container dapi-container: <nil>
STEP: delete the pod 01/03/23 02:24:54.651
Jan  3 02:24:54.699: INFO: Waiting for pod downward-api-f16dd28b-9125-4dcd-b429-afc6e3d74479 to disappear
Jan  3 02:24:54.711: INFO: Pod downward-api-f16dd28b-9125-4dcd-b429-afc6e3d74479 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan  3 02:24:54.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9318" for this suite. 01/03/23 02:24:54.713
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","completed":118,"skipped":2072,"failed":0}
------------------------------
• [4.143 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:24:50.573
    Jan  3 02:24:50.573: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename downward-api 01/03/23 02:24:50.573
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:24:50.607
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:24:50.609
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:165
    STEP: Creating a pod to test downward api env vars 01/03/23 02:24:50.611
    Jan  3 02:24:50.622: INFO: Waiting up to 5m0s for pod "downward-api-f16dd28b-9125-4dcd-b429-afc6e3d74479" in namespace "downward-api-9318" to be "Succeeded or Failed"
    Jan  3 02:24:50.641: INFO: Pod "downward-api-f16dd28b-9125-4dcd-b429-afc6e3d74479": Phase="Pending", Reason="", readiness=false. Elapsed: 19.173866ms
    Jan  3 02:24:52.644: INFO: Pod "downward-api-f16dd28b-9125-4dcd-b429-afc6e3d74479": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021704579s
    Jan  3 02:24:54.645: INFO: Pod "downward-api-f16dd28b-9125-4dcd-b429-afc6e3d74479": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022980893s
    STEP: Saw pod success 01/03/23 02:24:54.645
    Jan  3 02:24:54.645: INFO: Pod "downward-api-f16dd28b-9125-4dcd-b429-afc6e3d74479" satisfied condition "Succeeded or Failed"
    Jan  3 02:24:54.647: INFO: Trying to get logs from node cncf-master pod downward-api-f16dd28b-9125-4dcd-b429-afc6e3d74479 container dapi-container: <nil>
    STEP: delete the pod 01/03/23 02:24:54.651
    Jan  3 02:24:54.699: INFO: Waiting for pod downward-api-f16dd28b-9125-4dcd-b429-afc6e3d74479 to disappear
    Jan  3 02:24:54.711: INFO: Pod downward-api-f16dd28b-9125-4dcd-b429-afc6e3d74479 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan  3 02:24:54.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9318" for this suite. 01/03/23 02:24:54.713
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:24:54.718
Jan  3 02:24:54.718: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename var-expansion 01/03/23 02:24:54.719
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:24:54.741
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:24:54.743
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
STEP: Creating a pod to test substitution in container's args 01/03/23 02:24:54.745
Jan  3 02:24:54.750: INFO: Waiting up to 5m0s for pod "var-expansion-503795e2-7315-4b9c-b768-81e597a3ceb5" in namespace "var-expansion-8898" to be "Succeeded or Failed"
Jan  3 02:24:54.762: INFO: Pod "var-expansion-503795e2-7315-4b9c-b768-81e597a3ceb5": Phase="Pending", Reason="", readiness=false. Elapsed: 11.766091ms
Jan  3 02:24:56.765: INFO: Pod "var-expansion-503795e2-7315-4b9c-b768-81e597a3ceb5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014277796s
Jan  3 02:24:58.766: INFO: Pod "var-expansion-503795e2-7315-4b9c-b768-81e597a3ceb5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015385056s
STEP: Saw pod success 01/03/23 02:24:58.766
Jan  3 02:24:58.766: INFO: Pod "var-expansion-503795e2-7315-4b9c-b768-81e597a3ceb5" satisfied condition "Succeeded or Failed"
Jan  3 02:24:58.767: INFO: Trying to get logs from node cncf-master pod var-expansion-503795e2-7315-4b9c-b768-81e597a3ceb5 container dapi-container: <nil>
STEP: delete the pod 01/03/23 02:24:58.77
Jan  3 02:24:58.780: INFO: Waiting for pod var-expansion-503795e2-7315-4b9c-b768-81e597a3ceb5 to disappear
Jan  3 02:24:58.785: INFO: Pod var-expansion-503795e2-7315-4b9c-b768-81e597a3ceb5 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan  3 02:24:58.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8898" for this suite. 01/03/23 02:24:58.788
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","completed":119,"skipped":2125,"failed":0}
------------------------------
• [4.073 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:24:54.718
    Jan  3 02:24:54.718: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename var-expansion 01/03/23 02:24:54.719
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:24:54.741
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:24:54.743
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:91
    STEP: Creating a pod to test substitution in container's args 01/03/23 02:24:54.745
    Jan  3 02:24:54.750: INFO: Waiting up to 5m0s for pod "var-expansion-503795e2-7315-4b9c-b768-81e597a3ceb5" in namespace "var-expansion-8898" to be "Succeeded or Failed"
    Jan  3 02:24:54.762: INFO: Pod "var-expansion-503795e2-7315-4b9c-b768-81e597a3ceb5": Phase="Pending", Reason="", readiness=false. Elapsed: 11.766091ms
    Jan  3 02:24:56.765: INFO: Pod "var-expansion-503795e2-7315-4b9c-b768-81e597a3ceb5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014277796s
    Jan  3 02:24:58.766: INFO: Pod "var-expansion-503795e2-7315-4b9c-b768-81e597a3ceb5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015385056s
    STEP: Saw pod success 01/03/23 02:24:58.766
    Jan  3 02:24:58.766: INFO: Pod "var-expansion-503795e2-7315-4b9c-b768-81e597a3ceb5" satisfied condition "Succeeded or Failed"
    Jan  3 02:24:58.767: INFO: Trying to get logs from node cncf-master pod var-expansion-503795e2-7315-4b9c-b768-81e597a3ceb5 container dapi-container: <nil>
    STEP: delete the pod 01/03/23 02:24:58.77
    Jan  3 02:24:58.780: INFO: Waiting for pod var-expansion-503795e2-7315-4b9c-b768-81e597a3ceb5 to disappear
    Jan  3 02:24:58.785: INFO: Pod var-expansion-503795e2-7315-4b9c-b768-81e597a3ceb5 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan  3 02:24:58.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-8898" for this suite. 01/03/23 02:24:58.788
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:24:58.793
Jan  3 02:24:58.793: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename kubectl 01/03/23 02:24:58.794
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:24:58.812
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:24:58.814
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
Jan  3 02:24:58.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-4298 version'
Jan  3 02:24:58.867: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Jan  3 02:24:58.867: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.0\", GitCommit:\"a866cbe2e5bbaa01cfd5e969aa3e033f3282a8a2\", GitTreeState:\"clean\", BuildDate:\"2022-08-23T17:44:59Z\", GoVersion:\"go1.19\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.0\", GitCommit:\"a866cbe2e5bbaa01cfd5e969aa3e033f3282a8a2\", GitTreeState:\"clean\", BuildDate:\"2022-08-23T17:38:15Z\", GoVersion:\"go1.19\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan  3 02:24:58.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4298" for this suite. 01/03/23 02:24:58.87
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","completed":120,"skipped":2193,"failed":0}
------------------------------
• [0.079 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1677
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1683

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:24:58.793
    Jan  3 02:24:58.793: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename kubectl 01/03/23 02:24:58.794
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:24:58.812
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:24:58.814
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1683
    Jan  3 02:24:58.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-4298 version'
    Jan  3 02:24:58.867: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Jan  3 02:24:58.867: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.0\", GitCommit:\"a866cbe2e5bbaa01cfd5e969aa3e033f3282a8a2\", GitTreeState:\"clean\", BuildDate:\"2022-08-23T17:44:59Z\", GoVersion:\"go1.19\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.0\", GitCommit:\"a866cbe2e5bbaa01cfd5e969aa3e033f3282a8a2\", GitTreeState:\"clean\", BuildDate:\"2022-08-23T17:38:15Z\", GoVersion:\"go1.19\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan  3 02:24:58.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4298" for this suite. 01/03/23 02:24:58.87
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:24:58.873
Jan  3 02:24:58.873: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename gc 01/03/23 02:24:58.874
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:24:58.883
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:24:58.884
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 01/03/23 02:24:58.886
STEP: Wait for the Deployment to create new ReplicaSet 01/03/23 02:24:58.898
STEP: delete the deployment 01/03/23 02:24:59.403
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 01/03/23 02:24:59.408
STEP: Gathering metrics 01/03/23 02:24:59.959
Jan  3 02:24:59.966: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cncf-master" in namespace "kube-system" to be "running and ready"
Jan  3 02:24:59.968: INFO: Pod "kube-controller-manager-cncf-master": Phase="Running", Reason="", readiness=true. Elapsed: 1.718687ms
Jan  3 02:24:59.968: INFO: The phase of Pod kube-controller-manager-cncf-master is Running (Ready = true)
Jan  3 02:24:59.968: INFO: Pod "kube-controller-manager-cncf-master" satisfied condition "running and ready"
Jan  3 02:25:00.027: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan  3 02:25:00.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3899" for this suite. 01/03/23 02:25:00.03
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","completed":121,"skipped":2209,"failed":0}
------------------------------
• [1.160 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:24:58.873
    Jan  3 02:24:58.873: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename gc 01/03/23 02:24:58.874
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:24:58.883
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:24:58.884
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 01/03/23 02:24:58.886
    STEP: Wait for the Deployment to create new ReplicaSet 01/03/23 02:24:58.898
    STEP: delete the deployment 01/03/23 02:24:59.403
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 01/03/23 02:24:59.408
    STEP: Gathering metrics 01/03/23 02:24:59.959
    Jan  3 02:24:59.966: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cncf-master" in namespace "kube-system" to be "running and ready"
    Jan  3 02:24:59.968: INFO: Pod "kube-controller-manager-cncf-master": Phase="Running", Reason="", readiness=true. Elapsed: 1.718687ms
    Jan  3 02:24:59.968: INFO: The phase of Pod kube-controller-manager-cncf-master is Running (Ready = true)
    Jan  3 02:24:59.968: INFO: Pod "kube-controller-manager-cncf-master" satisfied condition "running and ready"
    Jan  3 02:25:00.027: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan  3 02:25:00.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-3899" for this suite. 01/03/23 02:25:00.03
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:25:00.033
Jan  3 02:25:00.034: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename kubelet-test 01/03/23 02:25:00.035
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:25:00.055
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:25:00.057
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 01/03/23 02:25:00.086
Jan  3 02:25:00.087: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases39524e23-5b87-48bc-b14e-cc0b6db4db92" in namespace "kubelet-test-9357" to be "completed"
Jan  3 02:25:00.089: INFO: Pod "agnhost-host-aliases39524e23-5b87-48bc-b14e-cc0b6db4db92": Phase="Pending", Reason="", readiness=false. Elapsed: 2.286841ms
Jan  3 02:25:02.104: INFO: Pod "agnhost-host-aliases39524e23-5b87-48bc-b14e-cc0b6db4db92": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017242369s
Jan  3 02:25:04.092: INFO: Pod "agnhost-host-aliases39524e23-5b87-48bc-b14e-cc0b6db4db92": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005440992s
Jan  3 02:25:04.092: INFO: Pod "agnhost-host-aliases39524e23-5b87-48bc-b14e-cc0b6db4db92" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan  3 02:25:04.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9357" for this suite. 01/03/23 02:25:04.098
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]","completed":122,"skipped":2213,"failed":0}
------------------------------
• [4.067 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:25:00.033
    Jan  3 02:25:00.034: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename kubelet-test 01/03/23 02:25:00.035
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:25:00.055
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:25:00.057
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 01/03/23 02:25:00.086
    Jan  3 02:25:00.087: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases39524e23-5b87-48bc-b14e-cc0b6db4db92" in namespace "kubelet-test-9357" to be "completed"
    Jan  3 02:25:00.089: INFO: Pod "agnhost-host-aliases39524e23-5b87-48bc-b14e-cc0b6db4db92": Phase="Pending", Reason="", readiness=false. Elapsed: 2.286841ms
    Jan  3 02:25:02.104: INFO: Pod "agnhost-host-aliases39524e23-5b87-48bc-b14e-cc0b6db4db92": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017242369s
    Jan  3 02:25:04.092: INFO: Pod "agnhost-host-aliases39524e23-5b87-48bc-b14e-cc0b6db4db92": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005440992s
    Jan  3 02:25:04.092: INFO: Pod "agnhost-host-aliases39524e23-5b87-48bc-b14e-cc0b6db4db92" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan  3 02:25:04.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-9357" for this suite. 01/03/23 02:25:04.098
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:25:04.101
Jan  3 02:25:04.101: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename downward-api 01/03/23 02:25:04.102
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:25:04.124
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:25:04.126
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
STEP: Creating a pod to test downward api env vars 01/03/23 02:25:04.128
Jan  3 02:25:04.133: INFO: Waiting up to 5m0s for pod "downward-api-0f167abd-a62f-493a-b566-fc4b9120da8f" in namespace "downward-api-8007" to be "Succeeded or Failed"
Jan  3 02:25:04.152: INFO: Pod "downward-api-0f167abd-a62f-493a-b566-fc4b9120da8f": Phase="Pending", Reason="", readiness=false. Elapsed: 19.151605ms
Jan  3 02:25:06.155: INFO: Pod "downward-api-0f167abd-a62f-493a-b566-fc4b9120da8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021943951s
Jan  3 02:25:08.157: INFO: Pod "downward-api-0f167abd-a62f-493a-b566-fc4b9120da8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023560415s
STEP: Saw pod success 01/03/23 02:25:08.157
Jan  3 02:25:08.157: INFO: Pod "downward-api-0f167abd-a62f-493a-b566-fc4b9120da8f" satisfied condition "Succeeded or Failed"
Jan  3 02:25:08.171: INFO: Trying to get logs from node cncf-master pod downward-api-0f167abd-a62f-493a-b566-fc4b9120da8f container dapi-container: <nil>
STEP: delete the pod 01/03/23 02:25:08.178
Jan  3 02:25:08.236: INFO: Waiting for pod downward-api-0f167abd-a62f-493a-b566-fc4b9120da8f to disappear
Jan  3 02:25:08.241: INFO: Pod downward-api-0f167abd-a62f-493a-b566-fc4b9120da8f no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan  3 02:25:08.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8007" for this suite. 01/03/23 02:25:08.244
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","completed":123,"skipped":2229,"failed":0}
------------------------------
• [4.146 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:25:04.101
    Jan  3 02:25:04.101: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename downward-api 01/03/23 02:25:04.102
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:25:04.124
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:25:04.126
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:43
    STEP: Creating a pod to test downward api env vars 01/03/23 02:25:04.128
    Jan  3 02:25:04.133: INFO: Waiting up to 5m0s for pod "downward-api-0f167abd-a62f-493a-b566-fc4b9120da8f" in namespace "downward-api-8007" to be "Succeeded or Failed"
    Jan  3 02:25:04.152: INFO: Pod "downward-api-0f167abd-a62f-493a-b566-fc4b9120da8f": Phase="Pending", Reason="", readiness=false. Elapsed: 19.151605ms
    Jan  3 02:25:06.155: INFO: Pod "downward-api-0f167abd-a62f-493a-b566-fc4b9120da8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021943951s
    Jan  3 02:25:08.157: INFO: Pod "downward-api-0f167abd-a62f-493a-b566-fc4b9120da8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023560415s
    STEP: Saw pod success 01/03/23 02:25:08.157
    Jan  3 02:25:08.157: INFO: Pod "downward-api-0f167abd-a62f-493a-b566-fc4b9120da8f" satisfied condition "Succeeded or Failed"
    Jan  3 02:25:08.171: INFO: Trying to get logs from node cncf-master pod downward-api-0f167abd-a62f-493a-b566-fc4b9120da8f container dapi-container: <nil>
    STEP: delete the pod 01/03/23 02:25:08.178
    Jan  3 02:25:08.236: INFO: Waiting for pod downward-api-0f167abd-a62f-493a-b566-fc4b9120da8f to disappear
    Jan  3 02:25:08.241: INFO: Pod downward-api-0f167abd-a62f-493a-b566-fc4b9120da8f no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan  3 02:25:08.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8007" for this suite. 01/03/23 02:25:08.244
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:25:08.248
Jan  3 02:25:08.248: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename sched-pred 01/03/23 02:25:08.249
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:25:08.257
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:25:08.258
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan  3 02:25:08.262: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan  3 02:25:08.279: INFO: Waiting for terminating namespaces to be deleted...
Jan  3 02:25:08.280: INFO: 
Logging pods the apiserver thinks is on node cncf-master before test
Jan  3 02:25:08.289: INFO: calico-kube-controllers-58dbc876ff-xt8wh from kube-system started at 2023-01-03 02:07:41 +0000 UTC (1 container statuses recorded)
Jan  3 02:25:08.289: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan  3 02:25:08.289: INFO: calico-node-zxb7d from kube-system started at 2022-12-28 00:23:10 +0000 UTC (1 container statuses recorded)
Jan  3 02:25:08.289: INFO: 	Container calico-node ready: true, restart count 0
Jan  3 02:25:08.289: INFO: etcd-cncf-master from kube-system started at 2022-12-28 00:22:57 +0000 UTC (1 container statuses recorded)
Jan  3 02:25:08.289: INFO: 	Container etcd ready: true, restart count 0
Jan  3 02:25:08.289: INFO: kube-apiserver-cncf-master from kube-system started at 2023-01-03 01:37:53 +0000 UTC (1 container statuses recorded)
Jan  3 02:25:08.289: INFO: 	Container kube-apiserver ready: true, restart count 0
Jan  3 02:25:08.289: INFO: kube-controller-manager-cncf-master from kube-system started at 2022-12-28 00:22:57 +0000 UTC (1 container statuses recorded)
Jan  3 02:25:08.289: INFO: 	Container kube-controller-manager ready: true, restart count 2
Jan  3 02:25:08.289: INFO: kube-proxy-qvggp from kube-system started at 2022-12-28 00:23:10 +0000 UTC (1 container statuses recorded)
Jan  3 02:25:08.289: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  3 02:25:08.289: INFO: kube-scheduler-cncf-master from kube-system started at 2022-12-28 00:22:56 +0000 UTC (1 container statuses recorded)
Jan  3 02:25:08.289: INFO: 	Container kube-scheduler ready: true, restart count 2
Jan  3 02:25:08.289: INFO: agnhost-host-aliases39524e23-5b87-48bc-b14e-cc0b6db4db92 from kubelet-test-9357 started at 2023-01-03 02:25:00 +0000 UTC (1 container statuses recorded)
Jan  3 02:25:08.289: INFO: 	Container agnhost-container ready: false, restart count 0
Jan  3 02:25:08.289: INFO: pod-qos-class-5ddd4a92-8a6e-44e9-b8b2-502ac886b043 from pods-4509 started at 2023-01-03 02:24:39 +0000 UTC (1 container statuses recorded)
Jan  3 02:25:08.289: INFO: 	Container agnhost ready: false, restart count 0
Jan  3 02:25:08.289: INFO: csi-cephfsplugin-gj7gj from rook-ceph started at 2023-01-03 02:07:49 +0000 UTC (2 container statuses recorded)
Jan  3 02:25:08.289: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Jan  3 02:25:08.289: INFO: 	Container driver-registrar ready: true, restart count 0
Jan  3 02:25:08.289: INFO: csi-rbdplugin-vwr2v from rook-ceph started at 2023-01-03 02:07:47 +0000 UTC (2 container statuses recorded)
Jan  3 02:25:08.289: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jan  3 02:25:08.289: INFO: 	Container driver-registrar ready: true, restart count 0
Jan  3 02:25:08.289: INFO: rook-ceph-crashcollector-cncf-master-7569fb669b-slpjd from rook-ceph started at 2023-01-03 02:07:59 +0000 UTC (1 container statuses recorded)
Jan  3 02:25:08.289: INFO: 	Container ceph-crash ready: true, restart count 0
Jan  3 02:25:08.289: INFO: rook-ceph-mon-b-66ffc4d4d9-85fqv from rook-ceph started at 2023-01-03 02:07:41 +0000 UTC (1 container statuses recorded)
Jan  3 02:25:08.289: INFO: 	Container mon ready: true, restart count 0
Jan  3 02:25:08.289: INFO: rook-ceph-osd-1-58d74d96db-zg5pm from rook-ceph started at 2023-01-03 02:07:41 +0000 UTC (1 container statuses recorded)
Jan  3 02:25:08.289: INFO: 	Container osd ready: true, restart count 0
Jan  3 02:25:08.289: INFO: sonobuoy-systemd-logs-daemon-set-58995b31e0564399-fxv6h from sonobuoy started at 2023-01-03 01:46:49 +0000 UTC (2 container statuses recorded)
Jan  3 02:25:08.289: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  3 02:25:08.289: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  3 02:25:08.289: INFO: 
Logging pods the apiserver thinks is on node cncf-node1 before test
Jan  3 02:25:08.302: INFO: traefik-7c456f74d8-b7b6n from api-gateway-system started at 2023-01-03 01:41:21 +0000 UTC (1 container statuses recorded)
Jan  3 02:25:08.302: INFO: 	Container traefik ready: true, restart count 0
Jan  3 02:25:08.302: INFO: cert-manager-754886db57-z7pm7 from cert-manager started at 2023-01-03 01:35:12 +0000 UTC (1 container statuses recorded)
Jan  3 02:25:08.302: INFO: 	Container cert-manager ready: true, restart count 1
Jan  3 02:25:08.302: INFO: cert-manager-cainjector-66b4c7c78d-vc87n from cert-manager started at 2023-01-03 01:35:12 +0000 UTC (1 container statuses recorded)
Jan  3 02:25:08.302: INFO: 	Container cert-manager ready: true, restart count 4
Jan  3 02:25:08.302: INFO: cert-manager-webhook-78cdb5fdf-frlxh from cert-manager started at 2023-01-03 01:35:12 +0000 UTC (1 container statuses recorded)
Jan  3 02:25:08.302: INFO: 	Container cert-manager ready: true, restart count 0
Jan  3 02:25:08.302: INFO: console-59c484c98b-mcqpt from console-system started at 2023-01-03 01:41:24 +0000 UTC (1 container statuses recorded)
Jan  3 02:25:08.302: INFO: 	Container console ready: true, restart count 0
Jan  3 02:25:08.302: INFO: hyperauth-5cbb4d58b8-qpc2d from hyperauth started at 2023-01-03 01:37:01 +0000 UTC (1 container statuses recorded)
Jan  3 02:25:08.302: INFO: 	Container hyperauth ready: true, restart count 0
Jan  3 02:25:08.302: INFO: postgresql-c6b9bb88b-fk4pn from hyperauth started at 2023-01-03 02:07:41 +0000 UTC (1 container statuses recorded)
Jan  3 02:25:08.302: INFO: 	Container postgresql ready: true, restart count 0
Jan  3 02:25:08.302: INFO: hypercloud5-api-server-8c944885b-mk6vb from hypercloud5-system started at 2023-01-03 01:38:50 +0000 UTC (1 container statuses recorded)
Jan  3 02:25:08.302: INFO: 	Container hypercloud5-api-server ready: true, restart count 4
Jan  3 02:25:08.302: INFO: timescaledb-0 from hypercloud5-system started at 2023-01-03 01:38:50 +0000 UTC (1 container statuses recorded)
Jan  3 02:25:08.302: INFO: 	Container timescaledb ready: true, restart count 0
Jan  3 02:25:08.302: INFO: calico-node-9p9ff from kube-system started at 2022-12-28 00:54:44 +0000 UTC (1 container statuses recorded)
Jan  3 02:25:08.302: INFO: 	Container calico-node ready: true, restart count 0
Jan  3 02:25:08.302: INFO: coredns-565d847f94-dp5jm from kube-system started at 2023-01-03 02:07:41 +0000 UTC (1 container statuses recorded)
Jan  3 02:25:08.302: INFO: 	Container coredns ready: true, restart count 0
Jan  3 02:25:08.302: INFO: kube-proxy-jplgg from kube-system started at 2022-12-28 00:54:44 +0000 UTC (1 container statuses recorded)
Jan  3 02:25:08.302: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  3 02:25:08.302: INFO: snapshot-controller-545d7f8994-vg2ld from kube-system started at 2022-12-29 08:24:10 +0000 UTC (1 container statuses recorded)
Jan  3 02:25:08.302: INFO: 	Container snapshot-controller ready: true, restart count 3
Jan  3 02:25:08.302: INFO: csi-cephfsplugin-provisioner-69f54fb566-jxcpx from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (5 container statuses recorded)
Jan  3 02:25:08.302: INFO: 	Container csi-attacher ready: true, restart count 0
Jan  3 02:25:08.302: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Jan  3 02:25:08.302: INFO: 	Container csi-provisioner ready: true, restart count 0
Jan  3 02:25:08.302: INFO: 	Container csi-resizer ready: true, restart count 0
Jan  3 02:25:08.302: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jan  3 02:25:08.302: INFO: csi-cephfsplugin-stnqm from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (2 container statuses recorded)
Jan  3 02:25:08.302: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Jan  3 02:25:08.302: INFO: 	Container driver-registrar ready: true, restart count 0
Jan  3 02:25:08.302: INFO: csi-rbdplugin-provisioner-6f689965cc-wl454 from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (5 container statuses recorded)
Jan  3 02:25:08.302: INFO: 	Container csi-attacher ready: true, restart count 0
Jan  3 02:25:08.302: INFO: 	Container csi-provisioner ready: true, restart count 0
Jan  3 02:25:08.302: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jan  3 02:25:08.302: INFO: 	Container csi-resizer ready: true, restart count 0
Jan  3 02:25:08.302: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jan  3 02:25:08.302: INFO: csi-rbdplugin-t2hxd from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (2 container statuses recorded)
Jan  3 02:25:08.302: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jan  3 02:25:08.302: INFO: 	Container driver-registrar ready: true, restart count 0
Jan  3 02:25:08.302: INFO: rook-ceph-crashcollector-cncf-node1-846d9dbb97-scfg5 from rook-ceph started at 2023-01-02 01:35:36 +0000 UTC (1 container statuses recorded)
Jan  3 02:25:08.302: INFO: 	Container ceph-crash ready: true, restart count 0
Jan  3 02:25:08.302: INFO: rook-ceph-mds-myfs-a-58f7884986-6th9v from rook-ceph started at 2023-01-02 01:35:36 +0000 UTC (1 container statuses recorded)
Jan  3 02:25:08.302: INFO: 	Container mds ready: true, restart count 0
Jan  3 02:25:08.302: INFO: rook-ceph-mgr-a-7949c65bd-kjzgc from rook-ceph started at 2022-12-29 08:25:47 +0000 UTC (2 container statuses recorded)
Jan  3 02:25:08.302: INFO: 	Container mgr ready: true, restart count 0
Jan  3 02:25:08.302: INFO: 	Container watch-active ready: true, restart count 0
Jan  3 02:25:08.302: INFO: rook-ceph-mon-a-bdd44bcfb-s5hjx from rook-ceph started at 2022-12-29 08:24:58 +0000 UTC (1 container statuses recorded)
Jan  3 02:25:08.302: INFO: 	Container mon ready: true, restart count 0
Jan  3 02:25:08.302: INFO: rook-ceph-operator-7f56d78d46-z2j6r from rook-ceph started at 2023-01-02 01:29:14 +0000 UTC (1 container statuses recorded)
Jan  3 02:25:08.302: INFO: 	Container rook-ceph-operator ready: true, restart count 0
Jan  3 02:25:08.302: INFO: rook-ceph-osd-2-5d8bc49787-jd84z from rook-ceph started at 2023-01-02 01:35:03 +0000 UTC (1 container statuses recorded)
Jan  3 02:25:08.302: INFO: 	Container osd ready: true, restart count 0
Jan  3 02:25:08.302: INFO: rook-ceph-osd-prepare-cncf-node1-89n8c from rook-ceph started at 2023-01-03 01:40:56 +0000 UTC (1 container statuses recorded)
Jan  3 02:25:08.302: INFO: 	Container provision ready: false, restart count 0
Jan  3 02:25:08.302: INFO: rook-ceph-tools-75fb64866c-bcxp7 from rook-ceph started at 2022-12-30 02:06:28 +0000 UTC (1 container statuses recorded)
Jan  3 02:25:08.302: INFO: 	Container rook-ceph-tools ready: true, restart count 0
Jan  3 02:25:08.302: INFO: sonobuoy from sonobuoy started at 2023-01-03 01:46:49 +0000 UTC (1 container statuses recorded)
Jan  3 02:25:08.302: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan  3 02:25:08.302: INFO: sonobuoy-e2e-job-a15b13ec9aa249f2 from sonobuoy started at 2023-01-03 01:46:49 +0000 UTC (2 container statuses recorded)
Jan  3 02:25:08.302: INFO: 	Container e2e ready: true, restart count 0
Jan  3 02:25:08.302: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  3 02:25:08.302: INFO: sonobuoy-systemd-logs-daemon-set-58995b31e0564399-gndlm from sonobuoy started at 2023-01-03 01:46:49 +0000 UTC (2 container statuses recorded)
Jan  3 02:25:08.302: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  3 02:25:08.302: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  3 02:25:08.302: INFO: 
Logging pods the apiserver thinks is on node cncf-node2 before test
Jan  3 02:25:08.312: INFO: hyperauth-5cbb4d58b8-pqhc5 from hyperauth started at 2023-01-03 01:37:01 +0000 UTC (1 container statuses recorded)
Jan  3 02:25:08.312: INFO: 	Container hyperauth ready: true, restart count 0
Jan  3 02:25:08.312: INFO: calico-node-5f5qs from kube-system started at 2022-12-29 05:30:03 +0000 UTC (1 container statuses recorded)
Jan  3 02:25:08.312: INFO: 	Container calico-node ready: true, restart count 0
Jan  3 02:25:08.312: INFO: coredns-565d847f94-gqs74 from kube-system started at 2023-01-03 02:07:41 +0000 UTC (1 container statuses recorded)
Jan  3 02:25:08.312: INFO: 	Container coredns ready: true, restart count 0
Jan  3 02:25:08.312: INFO: kube-proxy-ddlsm from kube-system started at 2022-12-29 05:30:03 +0000 UTC (1 container statuses recorded)
Jan  3 02:25:08.312: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  3 02:25:08.312: INFO: snapshot-controller-545d7f8994-qgm6l from kube-system started at 2022-12-29 08:24:10 +0000 UTC (1 container statuses recorded)
Jan  3 02:25:08.312: INFO: 	Container snapshot-controller ready: true, restart count 0
Jan  3 02:25:08.312: INFO: csi-cephfsplugin-hxtjf from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (2 container statuses recorded)
Jan  3 02:25:08.312: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Jan  3 02:25:08.312: INFO: 	Container driver-registrar ready: true, restart count 0
Jan  3 02:25:08.312: INFO: csi-cephfsplugin-provisioner-69f54fb566-xhc7t from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (5 container statuses recorded)
Jan  3 02:25:08.312: INFO: 	Container csi-attacher ready: true, restart count 0
Jan  3 02:25:08.312: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Jan  3 02:25:08.312: INFO: 	Container csi-provisioner ready: true, restart count 0
Jan  3 02:25:08.312: INFO: 	Container csi-resizer ready: true, restart count 0
Jan  3 02:25:08.312: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jan  3 02:25:08.312: INFO: csi-rbdplugin-provisioner-6f689965cc-2mqrd from rook-ceph started at 2023-01-03 02:07:41 +0000 UTC (5 container statuses recorded)
Jan  3 02:25:08.312: INFO: 	Container csi-attacher ready: true, restart count 0
Jan  3 02:25:08.312: INFO: 	Container csi-provisioner ready: true, restart count 0
Jan  3 02:25:08.312: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jan  3 02:25:08.312: INFO: 	Container csi-resizer ready: true, restart count 0
Jan  3 02:25:08.312: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jan  3 02:25:08.312: INFO: csi-rbdplugin-qk6jk from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (2 container statuses recorded)
Jan  3 02:25:08.312: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jan  3 02:25:08.312: INFO: 	Container driver-registrar ready: true, restart count 0
Jan  3 02:25:08.312: INFO: rook-ceph-crashcollector-cncf-node2-54fb78d5b8-nncld from rook-ceph started at 2023-01-03 02:07:41 +0000 UTC (1 container statuses recorded)
Jan  3 02:25:08.312: INFO: 	Container ceph-crash ready: true, restart count 0
Jan  3 02:25:08.312: INFO: rook-ceph-mds-myfs-b-65b48b8c9d-l4wlk from rook-ceph started at 2023-01-03 02:07:41 +0000 UTC (1 container statuses recorded)
Jan  3 02:25:08.312: INFO: 	Container mds ready: true, restart count 0
Jan  3 02:25:08.312: INFO: rook-ceph-mgr-b-856c746f6-lchkv from rook-ceph started at 2023-01-03 02:07:41 +0000 UTC (2 container statuses recorded)
Jan  3 02:25:08.312: INFO: 	Container mgr ready: true, restart count 0
Jan  3 02:25:08.312: INFO: 	Container watch-active ready: true, restart count 0
Jan  3 02:25:08.312: INFO: rook-ceph-mon-c-6c468d6b45-m7rqj from rook-ceph started at 2022-12-29 08:25:33 +0000 UTC (1 container statuses recorded)
Jan  3 02:25:08.312: INFO: 	Container mon ready: true, restart count 0
Jan  3 02:25:08.312: INFO: rook-ceph-osd-0-7d877c8d99-hq5fw from rook-ceph started at 2023-01-02 01:35:02 +0000 UTC (1 container statuses recorded)
Jan  3 02:25:08.312: INFO: 	Container osd ready: true, restart count 0
Jan  3 02:25:08.312: INFO: rook-ceph-osd-prepare-cncf-node2-rnwb6 from rook-ceph started at 2023-01-03 01:40:59 +0000 UTC (1 container statuses recorded)
Jan  3 02:25:08.312: INFO: 	Container provision ready: false, restart count 0
Jan  3 02:25:08.312: INFO: sonobuoy-systemd-logs-daemon-set-58995b31e0564399-wjpkt from sonobuoy started at 2023-01-03 01:46:49 +0000 UTC (2 container statuses recorded)
Jan  3 02:25:08.312: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  3 02:25:08.312: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/03/23 02:25:08.312
Jan  3 02:25:08.318: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-1550" to be "running"
Jan  3 02:25:08.324: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 6.457723ms
Jan  3 02:25:10.327: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.008994429s
Jan  3 02:25:10.327: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/03/23 02:25:10.329
STEP: Trying to apply a random label on the found node. 01/03/23 02:25:10.502
STEP: verifying the node has the label kubernetes.io/e2e-a84458d0-d5f5-4302-9a56-d059816de7de 95 01/03/23 02:25:10.509
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 01/03/23 02:25:10.538
Jan  3 02:25:10.543: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-1550" to be "not pending"
Jan  3 02:25:10.563: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 20.434184ms
Jan  3 02:25:12.565: INFO: Pod "pod4": Phase="Running", Reason="", readiness=false. Elapsed: 2.022476845s
Jan  3 02:25:12.565: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.21.7.5 on the node which pod4 resides and expect not scheduled 01/03/23 02:25:12.565
Jan  3 02:25:12.570: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-1550" to be "not pending"
Jan  3 02:25:12.629: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.615415ms
Jan  3 02:25:14.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.060570376s
Jan  3 02:25:16.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.060791917s
Jan  3 02:25:18.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.061230902s
Jan  3 02:25:20.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.061301907s
Jan  3 02:25:22.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.062115029s
Jan  3 02:25:24.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.061999626s
Jan  3 02:25:26.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.061446429s
Jan  3 02:25:28.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.06273121s
Jan  3 02:25:30.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.061546633s
Jan  3 02:25:32.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.061353988s
Jan  3 02:25:34.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.062462674s
Jan  3 02:25:36.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.06166585s
Jan  3 02:25:38.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.062212903s
Jan  3 02:25:40.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.061609949s
Jan  3 02:25:42.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.062067282s
Jan  3 02:25:44.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.061890838s
Jan  3 02:25:46.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.061542001s
Jan  3 02:25:48.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.061172403s
Jan  3 02:25:50.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.061354728s
Jan  3 02:25:52.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.061770682s
Jan  3 02:25:54.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.061655125s
Jan  3 02:25:56.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.060568145s
Jan  3 02:25:58.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.062573947s
Jan  3 02:26:00.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.061735683s
Jan  3 02:26:02.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.061973672s
Jan  3 02:26:04.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.062060483s
Jan  3 02:26:06.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.061618725s
Jan  3 02:26:08.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.062471971s
Jan  3 02:26:10.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.061325444s
Jan  3 02:26:12.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.061738206s
Jan  3 02:26:14.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.061604235s
Jan  3 02:26:16.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.061509847s
Jan  3 02:26:18.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.06120246s
Jan  3 02:26:20.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.061803783s
Jan  3 02:26:22.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.062480625s
Jan  3 02:26:24.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.061874054s
Jan  3 02:26:26.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.061452874s
Jan  3 02:26:28.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.06244239s
Jan  3 02:26:30.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.06122039s
Jan  3 02:26:32.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.06150945s
Jan  3 02:26:34.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.061975432s
Jan  3 02:26:36.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.06134091s
Jan  3 02:26:38.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.061759465s
Jan  3 02:26:40.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.062652441s
Jan  3 02:26:42.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.062406561s
Jan  3 02:26:44.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.0623388s
Jan  3 02:26:46.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.061227231s
Jan  3 02:26:48.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.061128714s
Jan  3 02:26:50.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.062237279s
Jan  3 02:26:52.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.061647877s
Jan  3 02:26:54.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.062540277s
Jan  3 02:26:56.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.06168423s
Jan  3 02:26:58.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.062430032s
Jan  3 02:27:00.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.061602805s
Jan  3 02:27:02.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.062328561s
Jan  3 02:27:04.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.062036715s
Jan  3 02:27:06.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.061374901s
Jan  3 02:27:08.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.06281478s
Jan  3 02:27:10.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.062005964s
Jan  3 02:27:12.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.062230012s
Jan  3 02:27:14.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.061946796s
Jan  3 02:27:16.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.061310702s
Jan  3 02:27:18.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.060880505s
Jan  3 02:27:20.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.062164829s
Jan  3 02:27:22.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.062275772s
Jan  3 02:27:24.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.062275186s
Jan  3 02:27:26.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.060850827s
Jan  3 02:27:28.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.062042657s
Jan  3 02:27:30.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.062140008s
Jan  3 02:27:32.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.062323388s
Jan  3 02:27:34.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.060554931s
Jan  3 02:27:36.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.061198643s
Jan  3 02:27:38.644: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.073480317s
Jan  3 02:27:40.655: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.085046471s
Jan  3 02:27:42.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.06199956s
Jan  3 02:27:44.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.06189002s
Jan  3 02:27:46.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.061526992s
Jan  3 02:27:48.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.061774106s
Jan  3 02:27:50.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.061810371s
Jan  3 02:27:52.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.061212382s
Jan  3 02:27:54.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.062271408s
Jan  3 02:27:56.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.061791881s
Jan  3 02:27:58.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.061889812s
Jan  3 02:28:00.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.061963032s
Jan  3 02:28:02.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.062312884s
Jan  3 02:28:04.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.062058846s
Jan  3 02:28:06.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.061278899s
Jan  3 02:28:08.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.062660715s
Jan  3 02:28:10.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.062061915s
Jan  3 02:28:12.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.062584435s
Jan  3 02:28:14.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.062660395s
Jan  3 02:28:16.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.061944961s
Jan  3 02:28:18.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.06178829s
Jan  3 02:28:20.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.062213699s
Jan  3 02:28:22.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.061642193s
Jan  3 02:28:24.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.062531948s
Jan  3 02:28:26.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.061639218s
Jan  3 02:28:28.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.062823808s
Jan  3 02:28:30.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.061986654s
Jan  3 02:28:32.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.062357798s
Jan  3 02:28:34.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.062502972s
Jan  3 02:28:36.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.060831893s
Jan  3 02:28:38.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.061928646s
Jan  3 02:28:40.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.061905459s
Jan  3 02:28:42.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.062257823s
Jan  3 02:28:44.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.062132256s
Jan  3 02:28:46.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.061205806s
Jan  3 02:28:48.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.061070089s
Jan  3 02:28:50.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.062389357s
Jan  3 02:28:52.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.062143679s
Jan  3 02:28:54.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.062037919s
Jan  3 02:28:56.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.06134408s
Jan  3 02:28:58.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.062582407s
Jan  3 02:29:00.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.062854859s
Jan  3 02:29:02.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.061324842s
Jan  3 02:29:04.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.062433544s
Jan  3 02:29:06.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.061595077s
Jan  3 02:29:08.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.062262572s
Jan  3 02:29:10.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.062270574s
Jan  3 02:29:12.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.061757227s
Jan  3 02:29:14.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.062141045s
Jan  3 02:29:16.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.061362261s
Jan  3 02:29:18.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.061534967s
Jan  3 02:29:20.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.062566332s
Jan  3 02:29:22.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.062104797s
Jan  3 02:29:24.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.061050067s
Jan  3 02:29:26.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.061495223s
Jan  3 02:29:28.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.062236029s
Jan  3 02:29:30.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.062330364s
Jan  3 02:29:32.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.061852034s
Jan  3 02:29:34.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.062134189s
Jan  3 02:29:36.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.061316069s
Jan  3 02:29:38.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.062263905s
Jan  3 02:29:40.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.062317928s
Jan  3 02:29:42.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.061391291s
Jan  3 02:29:44.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.062021163s
Jan  3 02:29:46.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.061374939s
Jan  3 02:29:48.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.06119692s
Jan  3 02:29:50.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.061689559s
Jan  3 02:29:52.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.061859654s
Jan  3 02:29:54.647: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.07641553s
Jan  3 02:29:56.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.061244541s
Jan  3 02:29:58.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.064975203s
Jan  3 02:30:00.652: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.082301501s
Jan  3 02:30:02.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.062313232s
Jan  3 02:30:04.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.061998921s
Jan  3 02:30:06.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.06119644s
Jan  3 02:30:08.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.062320534s
Jan  3 02:30:10.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.062299176s
Jan  3 02:30:12.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.062406059s
Jan  3 02:30:12.634: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.063929832s
STEP: removing the label kubernetes.io/e2e-a84458d0-d5f5-4302-9a56-d059816de7de off the node cncf-master 01/03/23 02:30:12.634
STEP: verifying the node doesn't have the label kubernetes.io/e2e-a84458d0-d5f5-4302-9a56-d059816de7de 01/03/23 02:30:12.641
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jan  3 02:30:12.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1550" for this suite. 01/03/23 02:30:12.652
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","completed":124,"skipped":2249,"failed":0}
------------------------------
• [SLOW TEST] [304.407 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:25:08.248
    Jan  3 02:25:08.248: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename sched-pred 01/03/23 02:25:08.249
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:25:08.257
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:25:08.258
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jan  3 02:25:08.262: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan  3 02:25:08.279: INFO: Waiting for terminating namespaces to be deleted...
    Jan  3 02:25:08.280: INFO: 
    Logging pods the apiserver thinks is on node cncf-master before test
    Jan  3 02:25:08.289: INFO: calico-kube-controllers-58dbc876ff-xt8wh from kube-system started at 2023-01-03 02:07:41 +0000 UTC (1 container statuses recorded)
    Jan  3 02:25:08.289: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Jan  3 02:25:08.289: INFO: calico-node-zxb7d from kube-system started at 2022-12-28 00:23:10 +0000 UTC (1 container statuses recorded)
    Jan  3 02:25:08.289: INFO: 	Container calico-node ready: true, restart count 0
    Jan  3 02:25:08.289: INFO: etcd-cncf-master from kube-system started at 2022-12-28 00:22:57 +0000 UTC (1 container statuses recorded)
    Jan  3 02:25:08.289: INFO: 	Container etcd ready: true, restart count 0
    Jan  3 02:25:08.289: INFO: kube-apiserver-cncf-master from kube-system started at 2023-01-03 01:37:53 +0000 UTC (1 container statuses recorded)
    Jan  3 02:25:08.289: INFO: 	Container kube-apiserver ready: true, restart count 0
    Jan  3 02:25:08.289: INFO: kube-controller-manager-cncf-master from kube-system started at 2022-12-28 00:22:57 +0000 UTC (1 container statuses recorded)
    Jan  3 02:25:08.289: INFO: 	Container kube-controller-manager ready: true, restart count 2
    Jan  3 02:25:08.289: INFO: kube-proxy-qvggp from kube-system started at 2022-12-28 00:23:10 +0000 UTC (1 container statuses recorded)
    Jan  3 02:25:08.289: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  3 02:25:08.289: INFO: kube-scheduler-cncf-master from kube-system started at 2022-12-28 00:22:56 +0000 UTC (1 container statuses recorded)
    Jan  3 02:25:08.289: INFO: 	Container kube-scheduler ready: true, restart count 2
    Jan  3 02:25:08.289: INFO: agnhost-host-aliases39524e23-5b87-48bc-b14e-cc0b6db4db92 from kubelet-test-9357 started at 2023-01-03 02:25:00 +0000 UTC (1 container statuses recorded)
    Jan  3 02:25:08.289: INFO: 	Container agnhost-container ready: false, restart count 0
    Jan  3 02:25:08.289: INFO: pod-qos-class-5ddd4a92-8a6e-44e9-b8b2-502ac886b043 from pods-4509 started at 2023-01-03 02:24:39 +0000 UTC (1 container statuses recorded)
    Jan  3 02:25:08.289: INFO: 	Container agnhost ready: false, restart count 0
    Jan  3 02:25:08.289: INFO: csi-cephfsplugin-gj7gj from rook-ceph started at 2023-01-03 02:07:49 +0000 UTC (2 container statuses recorded)
    Jan  3 02:25:08.289: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
    Jan  3 02:25:08.289: INFO: 	Container driver-registrar ready: true, restart count 0
    Jan  3 02:25:08.289: INFO: csi-rbdplugin-vwr2v from rook-ceph started at 2023-01-03 02:07:47 +0000 UTC (2 container statuses recorded)
    Jan  3 02:25:08.289: INFO: 	Container csi-rbdplugin ready: true, restart count 0
    Jan  3 02:25:08.289: INFO: 	Container driver-registrar ready: true, restart count 0
    Jan  3 02:25:08.289: INFO: rook-ceph-crashcollector-cncf-master-7569fb669b-slpjd from rook-ceph started at 2023-01-03 02:07:59 +0000 UTC (1 container statuses recorded)
    Jan  3 02:25:08.289: INFO: 	Container ceph-crash ready: true, restart count 0
    Jan  3 02:25:08.289: INFO: rook-ceph-mon-b-66ffc4d4d9-85fqv from rook-ceph started at 2023-01-03 02:07:41 +0000 UTC (1 container statuses recorded)
    Jan  3 02:25:08.289: INFO: 	Container mon ready: true, restart count 0
    Jan  3 02:25:08.289: INFO: rook-ceph-osd-1-58d74d96db-zg5pm from rook-ceph started at 2023-01-03 02:07:41 +0000 UTC (1 container statuses recorded)
    Jan  3 02:25:08.289: INFO: 	Container osd ready: true, restart count 0
    Jan  3 02:25:08.289: INFO: sonobuoy-systemd-logs-daemon-set-58995b31e0564399-fxv6h from sonobuoy started at 2023-01-03 01:46:49 +0000 UTC (2 container statuses recorded)
    Jan  3 02:25:08.289: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  3 02:25:08.289: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  3 02:25:08.289: INFO: 
    Logging pods the apiserver thinks is on node cncf-node1 before test
    Jan  3 02:25:08.302: INFO: traefik-7c456f74d8-b7b6n from api-gateway-system started at 2023-01-03 01:41:21 +0000 UTC (1 container statuses recorded)
    Jan  3 02:25:08.302: INFO: 	Container traefik ready: true, restart count 0
    Jan  3 02:25:08.302: INFO: cert-manager-754886db57-z7pm7 from cert-manager started at 2023-01-03 01:35:12 +0000 UTC (1 container statuses recorded)
    Jan  3 02:25:08.302: INFO: 	Container cert-manager ready: true, restart count 1
    Jan  3 02:25:08.302: INFO: cert-manager-cainjector-66b4c7c78d-vc87n from cert-manager started at 2023-01-03 01:35:12 +0000 UTC (1 container statuses recorded)
    Jan  3 02:25:08.302: INFO: 	Container cert-manager ready: true, restart count 4
    Jan  3 02:25:08.302: INFO: cert-manager-webhook-78cdb5fdf-frlxh from cert-manager started at 2023-01-03 01:35:12 +0000 UTC (1 container statuses recorded)
    Jan  3 02:25:08.302: INFO: 	Container cert-manager ready: true, restart count 0
    Jan  3 02:25:08.302: INFO: console-59c484c98b-mcqpt from console-system started at 2023-01-03 01:41:24 +0000 UTC (1 container statuses recorded)
    Jan  3 02:25:08.302: INFO: 	Container console ready: true, restart count 0
    Jan  3 02:25:08.302: INFO: hyperauth-5cbb4d58b8-qpc2d from hyperauth started at 2023-01-03 01:37:01 +0000 UTC (1 container statuses recorded)
    Jan  3 02:25:08.302: INFO: 	Container hyperauth ready: true, restart count 0
    Jan  3 02:25:08.302: INFO: postgresql-c6b9bb88b-fk4pn from hyperauth started at 2023-01-03 02:07:41 +0000 UTC (1 container statuses recorded)
    Jan  3 02:25:08.302: INFO: 	Container postgresql ready: true, restart count 0
    Jan  3 02:25:08.302: INFO: hypercloud5-api-server-8c944885b-mk6vb from hypercloud5-system started at 2023-01-03 01:38:50 +0000 UTC (1 container statuses recorded)
    Jan  3 02:25:08.302: INFO: 	Container hypercloud5-api-server ready: true, restart count 4
    Jan  3 02:25:08.302: INFO: timescaledb-0 from hypercloud5-system started at 2023-01-03 01:38:50 +0000 UTC (1 container statuses recorded)
    Jan  3 02:25:08.302: INFO: 	Container timescaledb ready: true, restart count 0
    Jan  3 02:25:08.302: INFO: calico-node-9p9ff from kube-system started at 2022-12-28 00:54:44 +0000 UTC (1 container statuses recorded)
    Jan  3 02:25:08.302: INFO: 	Container calico-node ready: true, restart count 0
    Jan  3 02:25:08.302: INFO: coredns-565d847f94-dp5jm from kube-system started at 2023-01-03 02:07:41 +0000 UTC (1 container statuses recorded)
    Jan  3 02:25:08.302: INFO: 	Container coredns ready: true, restart count 0
    Jan  3 02:25:08.302: INFO: kube-proxy-jplgg from kube-system started at 2022-12-28 00:54:44 +0000 UTC (1 container statuses recorded)
    Jan  3 02:25:08.302: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  3 02:25:08.302: INFO: snapshot-controller-545d7f8994-vg2ld from kube-system started at 2022-12-29 08:24:10 +0000 UTC (1 container statuses recorded)
    Jan  3 02:25:08.302: INFO: 	Container snapshot-controller ready: true, restart count 3
    Jan  3 02:25:08.302: INFO: csi-cephfsplugin-provisioner-69f54fb566-jxcpx from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (5 container statuses recorded)
    Jan  3 02:25:08.302: INFO: 	Container csi-attacher ready: true, restart count 0
    Jan  3 02:25:08.302: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
    Jan  3 02:25:08.302: INFO: 	Container csi-provisioner ready: true, restart count 0
    Jan  3 02:25:08.302: INFO: 	Container csi-resizer ready: true, restart count 0
    Jan  3 02:25:08.302: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Jan  3 02:25:08.302: INFO: csi-cephfsplugin-stnqm from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (2 container statuses recorded)
    Jan  3 02:25:08.302: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
    Jan  3 02:25:08.302: INFO: 	Container driver-registrar ready: true, restart count 0
    Jan  3 02:25:08.302: INFO: csi-rbdplugin-provisioner-6f689965cc-wl454 from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (5 container statuses recorded)
    Jan  3 02:25:08.302: INFO: 	Container csi-attacher ready: true, restart count 0
    Jan  3 02:25:08.302: INFO: 	Container csi-provisioner ready: true, restart count 0
    Jan  3 02:25:08.302: INFO: 	Container csi-rbdplugin ready: true, restart count 0
    Jan  3 02:25:08.302: INFO: 	Container csi-resizer ready: true, restart count 0
    Jan  3 02:25:08.302: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Jan  3 02:25:08.302: INFO: csi-rbdplugin-t2hxd from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (2 container statuses recorded)
    Jan  3 02:25:08.302: INFO: 	Container csi-rbdplugin ready: true, restart count 0
    Jan  3 02:25:08.302: INFO: 	Container driver-registrar ready: true, restart count 0
    Jan  3 02:25:08.302: INFO: rook-ceph-crashcollector-cncf-node1-846d9dbb97-scfg5 from rook-ceph started at 2023-01-02 01:35:36 +0000 UTC (1 container statuses recorded)
    Jan  3 02:25:08.302: INFO: 	Container ceph-crash ready: true, restart count 0
    Jan  3 02:25:08.302: INFO: rook-ceph-mds-myfs-a-58f7884986-6th9v from rook-ceph started at 2023-01-02 01:35:36 +0000 UTC (1 container statuses recorded)
    Jan  3 02:25:08.302: INFO: 	Container mds ready: true, restart count 0
    Jan  3 02:25:08.302: INFO: rook-ceph-mgr-a-7949c65bd-kjzgc from rook-ceph started at 2022-12-29 08:25:47 +0000 UTC (2 container statuses recorded)
    Jan  3 02:25:08.302: INFO: 	Container mgr ready: true, restart count 0
    Jan  3 02:25:08.302: INFO: 	Container watch-active ready: true, restart count 0
    Jan  3 02:25:08.302: INFO: rook-ceph-mon-a-bdd44bcfb-s5hjx from rook-ceph started at 2022-12-29 08:24:58 +0000 UTC (1 container statuses recorded)
    Jan  3 02:25:08.302: INFO: 	Container mon ready: true, restart count 0
    Jan  3 02:25:08.302: INFO: rook-ceph-operator-7f56d78d46-z2j6r from rook-ceph started at 2023-01-02 01:29:14 +0000 UTC (1 container statuses recorded)
    Jan  3 02:25:08.302: INFO: 	Container rook-ceph-operator ready: true, restart count 0
    Jan  3 02:25:08.302: INFO: rook-ceph-osd-2-5d8bc49787-jd84z from rook-ceph started at 2023-01-02 01:35:03 +0000 UTC (1 container statuses recorded)
    Jan  3 02:25:08.302: INFO: 	Container osd ready: true, restart count 0
    Jan  3 02:25:08.302: INFO: rook-ceph-osd-prepare-cncf-node1-89n8c from rook-ceph started at 2023-01-03 01:40:56 +0000 UTC (1 container statuses recorded)
    Jan  3 02:25:08.302: INFO: 	Container provision ready: false, restart count 0
    Jan  3 02:25:08.302: INFO: rook-ceph-tools-75fb64866c-bcxp7 from rook-ceph started at 2022-12-30 02:06:28 +0000 UTC (1 container statuses recorded)
    Jan  3 02:25:08.302: INFO: 	Container rook-ceph-tools ready: true, restart count 0
    Jan  3 02:25:08.302: INFO: sonobuoy from sonobuoy started at 2023-01-03 01:46:49 +0000 UTC (1 container statuses recorded)
    Jan  3 02:25:08.302: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan  3 02:25:08.302: INFO: sonobuoy-e2e-job-a15b13ec9aa249f2 from sonobuoy started at 2023-01-03 01:46:49 +0000 UTC (2 container statuses recorded)
    Jan  3 02:25:08.302: INFO: 	Container e2e ready: true, restart count 0
    Jan  3 02:25:08.302: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  3 02:25:08.302: INFO: sonobuoy-systemd-logs-daemon-set-58995b31e0564399-gndlm from sonobuoy started at 2023-01-03 01:46:49 +0000 UTC (2 container statuses recorded)
    Jan  3 02:25:08.302: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  3 02:25:08.302: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  3 02:25:08.302: INFO: 
    Logging pods the apiserver thinks is on node cncf-node2 before test
    Jan  3 02:25:08.312: INFO: hyperauth-5cbb4d58b8-pqhc5 from hyperauth started at 2023-01-03 01:37:01 +0000 UTC (1 container statuses recorded)
    Jan  3 02:25:08.312: INFO: 	Container hyperauth ready: true, restart count 0
    Jan  3 02:25:08.312: INFO: calico-node-5f5qs from kube-system started at 2022-12-29 05:30:03 +0000 UTC (1 container statuses recorded)
    Jan  3 02:25:08.312: INFO: 	Container calico-node ready: true, restart count 0
    Jan  3 02:25:08.312: INFO: coredns-565d847f94-gqs74 from kube-system started at 2023-01-03 02:07:41 +0000 UTC (1 container statuses recorded)
    Jan  3 02:25:08.312: INFO: 	Container coredns ready: true, restart count 0
    Jan  3 02:25:08.312: INFO: kube-proxy-ddlsm from kube-system started at 2022-12-29 05:30:03 +0000 UTC (1 container statuses recorded)
    Jan  3 02:25:08.312: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  3 02:25:08.312: INFO: snapshot-controller-545d7f8994-qgm6l from kube-system started at 2022-12-29 08:24:10 +0000 UTC (1 container statuses recorded)
    Jan  3 02:25:08.312: INFO: 	Container snapshot-controller ready: true, restart count 0
    Jan  3 02:25:08.312: INFO: csi-cephfsplugin-hxtjf from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (2 container statuses recorded)
    Jan  3 02:25:08.312: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
    Jan  3 02:25:08.312: INFO: 	Container driver-registrar ready: true, restart count 0
    Jan  3 02:25:08.312: INFO: csi-cephfsplugin-provisioner-69f54fb566-xhc7t from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (5 container statuses recorded)
    Jan  3 02:25:08.312: INFO: 	Container csi-attacher ready: true, restart count 0
    Jan  3 02:25:08.312: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
    Jan  3 02:25:08.312: INFO: 	Container csi-provisioner ready: true, restart count 0
    Jan  3 02:25:08.312: INFO: 	Container csi-resizer ready: true, restart count 0
    Jan  3 02:25:08.312: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Jan  3 02:25:08.312: INFO: csi-rbdplugin-provisioner-6f689965cc-2mqrd from rook-ceph started at 2023-01-03 02:07:41 +0000 UTC (5 container statuses recorded)
    Jan  3 02:25:08.312: INFO: 	Container csi-attacher ready: true, restart count 0
    Jan  3 02:25:08.312: INFO: 	Container csi-provisioner ready: true, restart count 0
    Jan  3 02:25:08.312: INFO: 	Container csi-rbdplugin ready: true, restart count 0
    Jan  3 02:25:08.312: INFO: 	Container csi-resizer ready: true, restart count 0
    Jan  3 02:25:08.312: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Jan  3 02:25:08.312: INFO: csi-rbdplugin-qk6jk from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (2 container statuses recorded)
    Jan  3 02:25:08.312: INFO: 	Container csi-rbdplugin ready: true, restart count 0
    Jan  3 02:25:08.312: INFO: 	Container driver-registrar ready: true, restart count 0
    Jan  3 02:25:08.312: INFO: rook-ceph-crashcollector-cncf-node2-54fb78d5b8-nncld from rook-ceph started at 2023-01-03 02:07:41 +0000 UTC (1 container statuses recorded)
    Jan  3 02:25:08.312: INFO: 	Container ceph-crash ready: true, restart count 0
    Jan  3 02:25:08.312: INFO: rook-ceph-mds-myfs-b-65b48b8c9d-l4wlk from rook-ceph started at 2023-01-03 02:07:41 +0000 UTC (1 container statuses recorded)
    Jan  3 02:25:08.312: INFO: 	Container mds ready: true, restart count 0
    Jan  3 02:25:08.312: INFO: rook-ceph-mgr-b-856c746f6-lchkv from rook-ceph started at 2023-01-03 02:07:41 +0000 UTC (2 container statuses recorded)
    Jan  3 02:25:08.312: INFO: 	Container mgr ready: true, restart count 0
    Jan  3 02:25:08.312: INFO: 	Container watch-active ready: true, restart count 0
    Jan  3 02:25:08.312: INFO: rook-ceph-mon-c-6c468d6b45-m7rqj from rook-ceph started at 2022-12-29 08:25:33 +0000 UTC (1 container statuses recorded)
    Jan  3 02:25:08.312: INFO: 	Container mon ready: true, restart count 0
    Jan  3 02:25:08.312: INFO: rook-ceph-osd-0-7d877c8d99-hq5fw from rook-ceph started at 2023-01-02 01:35:02 +0000 UTC (1 container statuses recorded)
    Jan  3 02:25:08.312: INFO: 	Container osd ready: true, restart count 0
    Jan  3 02:25:08.312: INFO: rook-ceph-osd-prepare-cncf-node2-rnwb6 from rook-ceph started at 2023-01-03 01:40:59 +0000 UTC (1 container statuses recorded)
    Jan  3 02:25:08.312: INFO: 	Container provision ready: false, restart count 0
    Jan  3 02:25:08.312: INFO: sonobuoy-systemd-logs-daemon-set-58995b31e0564399-wjpkt from sonobuoy started at 2023-01-03 01:46:49 +0000 UTC (2 container statuses recorded)
    Jan  3 02:25:08.312: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  3 02:25:08.312: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:699
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/03/23 02:25:08.312
    Jan  3 02:25:08.318: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-1550" to be "running"
    Jan  3 02:25:08.324: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 6.457723ms
    Jan  3 02:25:10.327: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.008994429s
    Jan  3 02:25:10.327: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/03/23 02:25:10.329
    STEP: Trying to apply a random label on the found node. 01/03/23 02:25:10.502
    STEP: verifying the node has the label kubernetes.io/e2e-a84458d0-d5f5-4302-9a56-d059816de7de 95 01/03/23 02:25:10.509
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 01/03/23 02:25:10.538
    Jan  3 02:25:10.543: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-1550" to be "not pending"
    Jan  3 02:25:10.563: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 20.434184ms
    Jan  3 02:25:12.565: INFO: Pod "pod4": Phase="Running", Reason="", readiness=false. Elapsed: 2.022476845s
    Jan  3 02:25:12.565: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.21.7.5 on the node which pod4 resides and expect not scheduled 01/03/23 02:25:12.565
    Jan  3 02:25:12.570: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-1550" to be "not pending"
    Jan  3 02:25:12.629: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.615415ms
    Jan  3 02:25:14.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.060570376s
    Jan  3 02:25:16.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.060791917s
    Jan  3 02:25:18.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.061230902s
    Jan  3 02:25:20.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.061301907s
    Jan  3 02:25:22.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.062115029s
    Jan  3 02:25:24.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.061999626s
    Jan  3 02:25:26.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.061446429s
    Jan  3 02:25:28.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.06273121s
    Jan  3 02:25:30.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.061546633s
    Jan  3 02:25:32.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.061353988s
    Jan  3 02:25:34.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.062462674s
    Jan  3 02:25:36.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.06166585s
    Jan  3 02:25:38.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.062212903s
    Jan  3 02:25:40.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.061609949s
    Jan  3 02:25:42.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.062067282s
    Jan  3 02:25:44.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.061890838s
    Jan  3 02:25:46.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.061542001s
    Jan  3 02:25:48.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.061172403s
    Jan  3 02:25:50.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.061354728s
    Jan  3 02:25:52.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.061770682s
    Jan  3 02:25:54.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.061655125s
    Jan  3 02:25:56.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.060568145s
    Jan  3 02:25:58.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.062573947s
    Jan  3 02:26:00.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.061735683s
    Jan  3 02:26:02.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.061973672s
    Jan  3 02:26:04.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.062060483s
    Jan  3 02:26:06.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.061618725s
    Jan  3 02:26:08.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.062471971s
    Jan  3 02:26:10.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.061325444s
    Jan  3 02:26:12.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.061738206s
    Jan  3 02:26:14.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.061604235s
    Jan  3 02:26:16.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.061509847s
    Jan  3 02:26:18.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.06120246s
    Jan  3 02:26:20.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.061803783s
    Jan  3 02:26:22.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.062480625s
    Jan  3 02:26:24.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.061874054s
    Jan  3 02:26:26.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.061452874s
    Jan  3 02:26:28.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.06244239s
    Jan  3 02:26:30.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.06122039s
    Jan  3 02:26:32.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.06150945s
    Jan  3 02:26:34.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.061975432s
    Jan  3 02:26:36.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.06134091s
    Jan  3 02:26:38.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.061759465s
    Jan  3 02:26:40.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.062652441s
    Jan  3 02:26:42.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.062406561s
    Jan  3 02:26:44.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.0623388s
    Jan  3 02:26:46.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.061227231s
    Jan  3 02:26:48.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.061128714s
    Jan  3 02:26:50.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.062237279s
    Jan  3 02:26:52.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.061647877s
    Jan  3 02:26:54.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.062540277s
    Jan  3 02:26:56.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.06168423s
    Jan  3 02:26:58.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.062430032s
    Jan  3 02:27:00.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.061602805s
    Jan  3 02:27:02.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.062328561s
    Jan  3 02:27:04.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.062036715s
    Jan  3 02:27:06.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.061374901s
    Jan  3 02:27:08.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.06281478s
    Jan  3 02:27:10.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.062005964s
    Jan  3 02:27:12.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.062230012s
    Jan  3 02:27:14.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.061946796s
    Jan  3 02:27:16.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.061310702s
    Jan  3 02:27:18.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.060880505s
    Jan  3 02:27:20.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.062164829s
    Jan  3 02:27:22.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.062275772s
    Jan  3 02:27:24.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.062275186s
    Jan  3 02:27:26.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.060850827s
    Jan  3 02:27:28.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.062042657s
    Jan  3 02:27:30.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.062140008s
    Jan  3 02:27:32.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.062323388s
    Jan  3 02:27:34.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.060554931s
    Jan  3 02:27:36.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.061198643s
    Jan  3 02:27:38.644: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.073480317s
    Jan  3 02:27:40.655: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.085046471s
    Jan  3 02:27:42.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.06199956s
    Jan  3 02:27:44.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.06189002s
    Jan  3 02:27:46.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.061526992s
    Jan  3 02:27:48.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.061774106s
    Jan  3 02:27:50.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.061810371s
    Jan  3 02:27:52.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.061212382s
    Jan  3 02:27:54.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.062271408s
    Jan  3 02:27:56.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.061791881s
    Jan  3 02:27:58.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.061889812s
    Jan  3 02:28:00.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.061963032s
    Jan  3 02:28:02.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.062312884s
    Jan  3 02:28:04.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.062058846s
    Jan  3 02:28:06.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.061278899s
    Jan  3 02:28:08.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.062660715s
    Jan  3 02:28:10.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.062061915s
    Jan  3 02:28:12.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.062584435s
    Jan  3 02:28:14.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.062660395s
    Jan  3 02:28:16.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.061944961s
    Jan  3 02:28:18.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.06178829s
    Jan  3 02:28:20.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.062213699s
    Jan  3 02:28:22.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.061642193s
    Jan  3 02:28:24.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.062531948s
    Jan  3 02:28:26.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.061639218s
    Jan  3 02:28:28.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.062823808s
    Jan  3 02:28:30.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.061986654s
    Jan  3 02:28:32.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.062357798s
    Jan  3 02:28:34.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.062502972s
    Jan  3 02:28:36.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.060831893s
    Jan  3 02:28:38.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.061928646s
    Jan  3 02:28:40.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.061905459s
    Jan  3 02:28:42.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.062257823s
    Jan  3 02:28:44.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.062132256s
    Jan  3 02:28:46.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.061205806s
    Jan  3 02:28:48.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.061070089s
    Jan  3 02:28:50.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.062389357s
    Jan  3 02:28:52.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.062143679s
    Jan  3 02:28:54.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.062037919s
    Jan  3 02:28:56.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.06134408s
    Jan  3 02:28:58.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.062582407s
    Jan  3 02:29:00.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.062854859s
    Jan  3 02:29:02.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.061324842s
    Jan  3 02:29:04.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.062433544s
    Jan  3 02:29:06.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.061595077s
    Jan  3 02:29:08.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.062262572s
    Jan  3 02:29:10.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.062270574s
    Jan  3 02:29:12.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.061757227s
    Jan  3 02:29:14.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.062141045s
    Jan  3 02:29:16.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.061362261s
    Jan  3 02:29:18.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.061534967s
    Jan  3 02:29:20.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.062566332s
    Jan  3 02:29:22.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.062104797s
    Jan  3 02:29:24.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.061050067s
    Jan  3 02:29:26.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.061495223s
    Jan  3 02:29:28.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.062236029s
    Jan  3 02:29:30.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.062330364s
    Jan  3 02:29:32.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.061852034s
    Jan  3 02:29:34.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.062134189s
    Jan  3 02:29:36.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.061316069s
    Jan  3 02:29:38.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.062263905s
    Jan  3 02:29:40.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.062317928s
    Jan  3 02:29:42.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.061391291s
    Jan  3 02:29:44.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.062021163s
    Jan  3 02:29:46.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.061374939s
    Jan  3 02:29:48.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.06119692s
    Jan  3 02:29:50.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.061689559s
    Jan  3 02:29:52.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.061859654s
    Jan  3 02:29:54.647: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.07641553s
    Jan  3 02:29:56.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.061244541s
    Jan  3 02:29:58.635: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.064975203s
    Jan  3 02:30:00.652: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.082301501s
    Jan  3 02:30:02.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.062313232s
    Jan  3 02:30:04.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.061998921s
    Jan  3 02:30:06.631: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.06119644s
    Jan  3 02:30:08.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.062320534s
    Jan  3 02:30:10.632: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.062299176s
    Jan  3 02:30:12.633: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.062406059s
    Jan  3 02:30:12.634: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.063929832s
    STEP: removing the label kubernetes.io/e2e-a84458d0-d5f5-4302-9a56-d059816de7de off the node cncf-master 01/03/23 02:30:12.634
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-a84458d0-d5f5-4302-9a56-d059816de7de 01/03/23 02:30:12.641
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jan  3 02:30:12.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-1550" for this suite. 01/03/23 02:30:12.652
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:30:12.656
Jan  3 02:30:12.656: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename configmap 01/03/23 02:30:12.657
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:30:12.685
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:30:12.687
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
STEP: Creating configMap with name configmap-test-volume-map-13281e31-33e7-49ed-ba86-772d2d6cfc4b 01/03/23 02:30:12.693
STEP: Creating a pod to test consume configMaps 01/03/23 02:30:12.698
Jan  3 02:30:12.735: INFO: Waiting up to 5m0s for pod "pod-configmaps-39f6abdb-d81f-4369-919b-8b270dae3902" in namespace "configmap-4737" to be "Succeeded or Failed"
Jan  3 02:30:12.738: INFO: Pod "pod-configmaps-39f6abdb-d81f-4369-919b-8b270dae3902": Phase="Pending", Reason="", readiness=false. Elapsed: 3.550613ms
Jan  3 02:30:14.741: INFO: Pod "pod-configmaps-39f6abdb-d81f-4369-919b-8b270dae3902": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006288122s
Jan  3 02:30:16.741: INFO: Pod "pod-configmaps-39f6abdb-d81f-4369-919b-8b270dae3902": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006346481s
STEP: Saw pod success 01/03/23 02:30:16.741
Jan  3 02:30:16.741: INFO: Pod "pod-configmaps-39f6abdb-d81f-4369-919b-8b270dae3902" satisfied condition "Succeeded or Failed"
Jan  3 02:30:16.743: INFO: Trying to get logs from node cncf-master pod pod-configmaps-39f6abdb-d81f-4369-919b-8b270dae3902 container agnhost-container: <nil>
STEP: delete the pod 01/03/23 02:30:16.754
Jan  3 02:30:16.779: INFO: Waiting for pod pod-configmaps-39f6abdb-d81f-4369-919b-8b270dae3902 to disappear
Jan  3 02:30:16.785: INFO: Pod pod-configmaps-39f6abdb-d81f-4369-919b-8b270dae3902 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan  3 02:30:16.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4737" for this suite. 01/03/23 02:30:16.788
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":125,"skipped":2253,"failed":0}
------------------------------
• [4.173 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:30:12.656
    Jan  3 02:30:12.656: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename configmap 01/03/23 02:30:12.657
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:30:12.685
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:30:12.687
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:88
    STEP: Creating configMap with name configmap-test-volume-map-13281e31-33e7-49ed-ba86-772d2d6cfc4b 01/03/23 02:30:12.693
    STEP: Creating a pod to test consume configMaps 01/03/23 02:30:12.698
    Jan  3 02:30:12.735: INFO: Waiting up to 5m0s for pod "pod-configmaps-39f6abdb-d81f-4369-919b-8b270dae3902" in namespace "configmap-4737" to be "Succeeded or Failed"
    Jan  3 02:30:12.738: INFO: Pod "pod-configmaps-39f6abdb-d81f-4369-919b-8b270dae3902": Phase="Pending", Reason="", readiness=false. Elapsed: 3.550613ms
    Jan  3 02:30:14.741: INFO: Pod "pod-configmaps-39f6abdb-d81f-4369-919b-8b270dae3902": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006288122s
    Jan  3 02:30:16.741: INFO: Pod "pod-configmaps-39f6abdb-d81f-4369-919b-8b270dae3902": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006346481s
    STEP: Saw pod success 01/03/23 02:30:16.741
    Jan  3 02:30:16.741: INFO: Pod "pod-configmaps-39f6abdb-d81f-4369-919b-8b270dae3902" satisfied condition "Succeeded or Failed"
    Jan  3 02:30:16.743: INFO: Trying to get logs from node cncf-master pod pod-configmaps-39f6abdb-d81f-4369-919b-8b270dae3902 container agnhost-container: <nil>
    STEP: delete the pod 01/03/23 02:30:16.754
    Jan  3 02:30:16.779: INFO: Waiting for pod pod-configmaps-39f6abdb-d81f-4369-919b-8b270dae3902 to disappear
    Jan  3 02:30:16.785: INFO: Pod pod-configmaps-39f6abdb-d81f-4369-919b-8b270dae3902 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan  3 02:30:16.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4737" for this suite. 01/03/23 02:30:16.788
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:30:16.83
Jan  3 02:30:16.830: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename var-expansion 01/03/23 02:30:16.831
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:30:16.842
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:30:16.844
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
STEP: Creating a pod to test substitution in volume subpath 01/03/23 02:30:16.846
Jan  3 02:30:16.866: INFO: Waiting up to 5m0s for pod "var-expansion-c60e99d2-a199-4f13-9cc7-153ed4d99a75" in namespace "var-expansion-2659" to be "Succeeded or Failed"
Jan  3 02:30:16.877: INFO: Pod "var-expansion-c60e99d2-a199-4f13-9cc7-153ed4d99a75": Phase="Pending", Reason="", readiness=false. Elapsed: 10.09385ms
Jan  3 02:30:18.879: INFO: Pod "var-expansion-c60e99d2-a199-4f13-9cc7-153ed4d99a75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012548968s
Jan  3 02:30:20.879: INFO: Pod "var-expansion-c60e99d2-a199-4f13-9cc7-153ed4d99a75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012604731s
STEP: Saw pod success 01/03/23 02:30:20.879
Jan  3 02:30:20.879: INFO: Pod "var-expansion-c60e99d2-a199-4f13-9cc7-153ed4d99a75" satisfied condition "Succeeded or Failed"
Jan  3 02:30:20.881: INFO: Trying to get logs from node cncf-master pod var-expansion-c60e99d2-a199-4f13-9cc7-153ed4d99a75 container dapi-container: <nil>
STEP: delete the pod 01/03/23 02:30:20.884
Jan  3 02:30:20.937: INFO: Waiting for pod var-expansion-c60e99d2-a199-4f13-9cc7-153ed4d99a75 to disappear
Jan  3 02:30:20.945: INFO: Pod var-expansion-c60e99d2-a199-4f13-9cc7-153ed4d99a75 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan  3 02:30:20.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2659" for this suite. 01/03/23 02:30:20.948
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","completed":126,"skipped":2262,"failed":0}
------------------------------
• [4.121 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:30:16.83
    Jan  3 02:30:16.830: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename var-expansion 01/03/23 02:30:16.831
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:30:16.842
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:30:16.844
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:111
    STEP: Creating a pod to test substitution in volume subpath 01/03/23 02:30:16.846
    Jan  3 02:30:16.866: INFO: Waiting up to 5m0s for pod "var-expansion-c60e99d2-a199-4f13-9cc7-153ed4d99a75" in namespace "var-expansion-2659" to be "Succeeded or Failed"
    Jan  3 02:30:16.877: INFO: Pod "var-expansion-c60e99d2-a199-4f13-9cc7-153ed4d99a75": Phase="Pending", Reason="", readiness=false. Elapsed: 10.09385ms
    Jan  3 02:30:18.879: INFO: Pod "var-expansion-c60e99d2-a199-4f13-9cc7-153ed4d99a75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012548968s
    Jan  3 02:30:20.879: INFO: Pod "var-expansion-c60e99d2-a199-4f13-9cc7-153ed4d99a75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012604731s
    STEP: Saw pod success 01/03/23 02:30:20.879
    Jan  3 02:30:20.879: INFO: Pod "var-expansion-c60e99d2-a199-4f13-9cc7-153ed4d99a75" satisfied condition "Succeeded or Failed"
    Jan  3 02:30:20.881: INFO: Trying to get logs from node cncf-master pod var-expansion-c60e99d2-a199-4f13-9cc7-153ed4d99a75 container dapi-container: <nil>
    STEP: delete the pod 01/03/23 02:30:20.884
    Jan  3 02:30:20.937: INFO: Waiting for pod var-expansion-c60e99d2-a199-4f13-9cc7-153ed4d99a75 to disappear
    Jan  3 02:30:20.945: INFO: Pod var-expansion-c60e99d2-a199-4f13-9cc7-153ed4d99a75 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan  3 02:30:20.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-2659" for this suite. 01/03/23 02:30:20.948
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:30:20.951
Jan  3 02:30:20.951: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename emptydir-wrapper 01/03/23 02:30:20.952
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:30:20.963
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:30:20.965
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Jan  3 02:30:21.000: INFO: Waiting up to 5m0s for pod "pod-secrets-34c7b990-338c-4016-a204-50169216e21c" in namespace "emptydir-wrapper-4006" to be "running and ready"
Jan  3 02:30:21.026: INFO: Pod "pod-secrets-34c7b990-338c-4016-a204-50169216e21c": Phase="Pending", Reason="", readiness=false. Elapsed: 25.69555ms
Jan  3 02:30:21.026: INFO: The phase of Pod pod-secrets-34c7b990-338c-4016-a204-50169216e21c is Pending, waiting for it to be Running (with Ready = true)
Jan  3 02:30:23.027: INFO: Pod "pod-secrets-34c7b990-338c-4016-a204-50169216e21c": Phase="Running", Reason="", readiness=true. Elapsed: 2.027571378s
Jan  3 02:30:23.027: INFO: The phase of Pod pod-secrets-34c7b990-338c-4016-a204-50169216e21c is Running (Ready = true)
Jan  3 02:30:23.027: INFO: Pod "pod-secrets-34c7b990-338c-4016-a204-50169216e21c" satisfied condition "running and ready"
STEP: Cleaning up the secret 01/03/23 02:30:23.029
STEP: Cleaning up the configmap 01/03/23 02:30:23.031
STEP: Cleaning up the pod 01/03/23 02:30:23.063
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Jan  3 02:30:23.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4006" for this suite. 01/03/23 02:30:23.09
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","completed":127,"skipped":2264,"failed":0}
------------------------------
• [2.142 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:30:20.951
    Jan  3 02:30:20.951: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename emptydir-wrapper 01/03/23 02:30:20.952
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:30:20.963
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:30:20.965
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Jan  3 02:30:21.000: INFO: Waiting up to 5m0s for pod "pod-secrets-34c7b990-338c-4016-a204-50169216e21c" in namespace "emptydir-wrapper-4006" to be "running and ready"
    Jan  3 02:30:21.026: INFO: Pod "pod-secrets-34c7b990-338c-4016-a204-50169216e21c": Phase="Pending", Reason="", readiness=false. Elapsed: 25.69555ms
    Jan  3 02:30:21.026: INFO: The phase of Pod pod-secrets-34c7b990-338c-4016-a204-50169216e21c is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 02:30:23.027: INFO: Pod "pod-secrets-34c7b990-338c-4016-a204-50169216e21c": Phase="Running", Reason="", readiness=true. Elapsed: 2.027571378s
    Jan  3 02:30:23.027: INFO: The phase of Pod pod-secrets-34c7b990-338c-4016-a204-50169216e21c is Running (Ready = true)
    Jan  3 02:30:23.027: INFO: Pod "pod-secrets-34c7b990-338c-4016-a204-50169216e21c" satisfied condition "running and ready"
    STEP: Cleaning up the secret 01/03/23 02:30:23.029
    STEP: Cleaning up the configmap 01/03/23 02:30:23.031
    STEP: Cleaning up the pod 01/03/23 02:30:23.063
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Jan  3 02:30:23.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-4006" for this suite. 01/03/23 02:30:23.09
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:30:23.096
Jan  3 02:30:23.096: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename services 01/03/23 02:30:23.097
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:30:23.119
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:30:23.121
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
STEP: creating a service nodeport-service with the type=NodePort in namespace services-9099 01/03/23 02:30:23.123
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/03/23 02:30:23.163
STEP: creating service externalsvc in namespace services-9099 01/03/23 02:30:23.163
STEP: creating replication controller externalsvc in namespace services-9099 01/03/23 02:30:23.194
I0103 02:30:23.198330      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-9099, replica count: 2
I0103 02:30:26.250401      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 01/03/23 02:30:26.252
Jan  3 02:30:26.295: INFO: Creating new exec pod
Jan  3 02:30:26.311: INFO: Waiting up to 5m0s for pod "execpodvkhwm" in namespace "services-9099" to be "running"
Jan  3 02:30:26.318: INFO: Pod "execpodvkhwm": Phase="Pending", Reason="", readiness=false. Elapsed: 7.060339ms
Jan  3 02:30:28.332: INFO: Pod "execpodvkhwm": Phase="Running", Reason="", readiness=true. Elapsed: 2.020591469s
Jan  3 02:30:28.332: INFO: Pod "execpodvkhwm" satisfied condition "running"
Jan  3 02:30:28.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-9099 exec execpodvkhwm -- /bin/sh -x -c nslookup nodeport-service.services-9099.svc.cluster.local'
Jan  3 02:30:28.515: INFO: stderr: "+ nslookup nodeport-service.services-9099.svc.cluster.local\n"
Jan  3 02:30:28.515: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-9099.svc.cluster.local\tcanonical name = externalsvc.services-9099.svc.cluster.local.\nName:\texternalsvc.services-9099.svc.cluster.local\nAddress: 10.96.151.244\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-9099, will wait for the garbage collector to delete the pods 01/03/23 02:30:28.515
Jan  3 02:30:28.578: INFO: Deleting ReplicationController externalsvc took: 10.882241ms
Jan  3 02:30:28.678: INFO: Terminating ReplicationController externalsvc pods took: 100.416704ms
Jan  3 02:30:30.640: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  3 02:30:30.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9099" for this suite. 01/03/23 02:30:30.68
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","completed":128,"skipped":2333,"failed":0}
------------------------------
• [SLOW TEST] [7.595 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:30:23.096
    Jan  3 02:30:23.096: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename services 01/03/23 02:30:23.097
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:30:23.119
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:30:23.121
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1523
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-9099 01/03/23 02:30:23.123
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/03/23 02:30:23.163
    STEP: creating service externalsvc in namespace services-9099 01/03/23 02:30:23.163
    STEP: creating replication controller externalsvc in namespace services-9099 01/03/23 02:30:23.194
    I0103 02:30:23.198330      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-9099, replica count: 2
    I0103 02:30:26.250401      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 01/03/23 02:30:26.252
    Jan  3 02:30:26.295: INFO: Creating new exec pod
    Jan  3 02:30:26.311: INFO: Waiting up to 5m0s for pod "execpodvkhwm" in namespace "services-9099" to be "running"
    Jan  3 02:30:26.318: INFO: Pod "execpodvkhwm": Phase="Pending", Reason="", readiness=false. Elapsed: 7.060339ms
    Jan  3 02:30:28.332: INFO: Pod "execpodvkhwm": Phase="Running", Reason="", readiness=true. Elapsed: 2.020591469s
    Jan  3 02:30:28.332: INFO: Pod "execpodvkhwm" satisfied condition "running"
    Jan  3 02:30:28.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-9099 exec execpodvkhwm -- /bin/sh -x -c nslookup nodeport-service.services-9099.svc.cluster.local'
    Jan  3 02:30:28.515: INFO: stderr: "+ nslookup nodeport-service.services-9099.svc.cluster.local\n"
    Jan  3 02:30:28.515: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-9099.svc.cluster.local\tcanonical name = externalsvc.services-9099.svc.cluster.local.\nName:\texternalsvc.services-9099.svc.cluster.local\nAddress: 10.96.151.244\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-9099, will wait for the garbage collector to delete the pods 01/03/23 02:30:28.515
    Jan  3 02:30:28.578: INFO: Deleting ReplicationController externalsvc took: 10.882241ms
    Jan  3 02:30:28.678: INFO: Terminating ReplicationController externalsvc pods took: 100.416704ms
    Jan  3 02:30:30.640: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  3 02:30:30.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9099" for this suite. 01/03/23 02:30:30.68
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:30:30.693
Jan  3 02:30:30.693: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename emptydir 01/03/23 02:30:30.694
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:30:30.713
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:30:30.715
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
STEP: Creating a pod to test emptydir 0777 on tmpfs 01/03/23 02:30:30.717
Jan  3 02:30:30.727: INFO: Waiting up to 5m0s for pod "pod-2339529e-1c02-41fb-b6c6-e77691d97f77" in namespace "emptydir-4893" to be "Succeeded or Failed"
Jan  3 02:30:30.777: INFO: Pod "pod-2339529e-1c02-41fb-b6c6-e77691d97f77": Phase="Pending", Reason="", readiness=false. Elapsed: 49.942787ms
Jan  3 02:30:32.780: INFO: Pod "pod-2339529e-1c02-41fb-b6c6-e77691d97f77": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053095182s
Jan  3 02:30:34.780: INFO: Pod "pod-2339529e-1c02-41fb-b6c6-e77691d97f77": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052922909s
STEP: Saw pod success 01/03/23 02:30:34.78
Jan  3 02:30:34.780: INFO: Pod "pod-2339529e-1c02-41fb-b6c6-e77691d97f77" satisfied condition "Succeeded or Failed"
Jan  3 02:30:34.782: INFO: Trying to get logs from node cncf-master pod pod-2339529e-1c02-41fb-b6c6-e77691d97f77 container test-container: <nil>
STEP: delete the pod 01/03/23 02:30:34.785
Jan  3 02:30:34.817: INFO: Waiting for pod pod-2339529e-1c02-41fb-b6c6-e77691d97f77 to disappear
Jan  3 02:30:34.818: INFO: Pod pod-2339529e-1c02-41fb-b6c6-e77691d97f77 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan  3 02:30:34.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4893" for this suite. 01/03/23 02:30:34.821
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":129,"skipped":2357,"failed":0}
------------------------------
• [4.130 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:30:30.693
    Jan  3 02:30:30.693: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename emptydir 01/03/23 02:30:30.694
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:30:30.713
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:30:30.715
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:116
    STEP: Creating a pod to test emptydir 0777 on tmpfs 01/03/23 02:30:30.717
    Jan  3 02:30:30.727: INFO: Waiting up to 5m0s for pod "pod-2339529e-1c02-41fb-b6c6-e77691d97f77" in namespace "emptydir-4893" to be "Succeeded or Failed"
    Jan  3 02:30:30.777: INFO: Pod "pod-2339529e-1c02-41fb-b6c6-e77691d97f77": Phase="Pending", Reason="", readiness=false. Elapsed: 49.942787ms
    Jan  3 02:30:32.780: INFO: Pod "pod-2339529e-1c02-41fb-b6c6-e77691d97f77": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053095182s
    Jan  3 02:30:34.780: INFO: Pod "pod-2339529e-1c02-41fb-b6c6-e77691d97f77": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052922909s
    STEP: Saw pod success 01/03/23 02:30:34.78
    Jan  3 02:30:34.780: INFO: Pod "pod-2339529e-1c02-41fb-b6c6-e77691d97f77" satisfied condition "Succeeded or Failed"
    Jan  3 02:30:34.782: INFO: Trying to get logs from node cncf-master pod pod-2339529e-1c02-41fb-b6c6-e77691d97f77 container test-container: <nil>
    STEP: delete the pod 01/03/23 02:30:34.785
    Jan  3 02:30:34.817: INFO: Waiting for pod pod-2339529e-1c02-41fb-b6c6-e77691d97f77 to disappear
    Jan  3 02:30:34.818: INFO: Pod pod-2339529e-1c02-41fb-b6c6-e77691d97f77 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan  3 02:30:34.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4893" for this suite. 01/03/23 02:30:34.821
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:30:34.824
Jan  3 02:30:34.824: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename emptydir 01/03/23 02:30:34.825
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:30:34.833
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:30:34.835
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
STEP: Creating a pod to test emptydir 0666 on node default medium 01/03/23 02:30:34.837
Jan  3 02:30:34.856: INFO: Waiting up to 5m0s for pod "pod-d7b2c8e2-5984-4695-9939-62ad50f26623" in namespace "emptydir-7285" to be "Succeeded or Failed"
Jan  3 02:30:34.862: INFO: Pod "pod-d7b2c8e2-5984-4695-9939-62ad50f26623": Phase="Pending", Reason="", readiness=false. Elapsed: 5.834865ms
Jan  3 02:30:36.865: INFO: Pod "pod-d7b2c8e2-5984-4695-9939-62ad50f26623": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008633826s
Jan  3 02:30:38.866: INFO: Pod "pod-d7b2c8e2-5984-4695-9939-62ad50f26623": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009375973s
STEP: Saw pod success 01/03/23 02:30:38.866
Jan  3 02:30:38.866: INFO: Pod "pod-d7b2c8e2-5984-4695-9939-62ad50f26623" satisfied condition "Succeeded or Failed"
Jan  3 02:30:38.867: INFO: Trying to get logs from node cncf-master pod pod-d7b2c8e2-5984-4695-9939-62ad50f26623 container test-container: <nil>
STEP: delete the pod 01/03/23 02:30:38.87
Jan  3 02:30:38.932: INFO: Waiting for pod pod-d7b2c8e2-5984-4695-9939-62ad50f26623 to disappear
Jan  3 02:30:38.937: INFO: Pod pod-d7b2c8e2-5984-4695-9939-62ad50f26623 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan  3 02:30:38.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7285" for this suite. 01/03/23 02:30:38.94
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":130,"skipped":2363,"failed":0}
------------------------------
• [4.119 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:30:34.824
    Jan  3 02:30:34.824: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename emptydir 01/03/23 02:30:34.825
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:30:34.833
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:30:34.835
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:176
    STEP: Creating a pod to test emptydir 0666 on node default medium 01/03/23 02:30:34.837
    Jan  3 02:30:34.856: INFO: Waiting up to 5m0s for pod "pod-d7b2c8e2-5984-4695-9939-62ad50f26623" in namespace "emptydir-7285" to be "Succeeded or Failed"
    Jan  3 02:30:34.862: INFO: Pod "pod-d7b2c8e2-5984-4695-9939-62ad50f26623": Phase="Pending", Reason="", readiness=false. Elapsed: 5.834865ms
    Jan  3 02:30:36.865: INFO: Pod "pod-d7b2c8e2-5984-4695-9939-62ad50f26623": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008633826s
    Jan  3 02:30:38.866: INFO: Pod "pod-d7b2c8e2-5984-4695-9939-62ad50f26623": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009375973s
    STEP: Saw pod success 01/03/23 02:30:38.866
    Jan  3 02:30:38.866: INFO: Pod "pod-d7b2c8e2-5984-4695-9939-62ad50f26623" satisfied condition "Succeeded or Failed"
    Jan  3 02:30:38.867: INFO: Trying to get logs from node cncf-master pod pod-d7b2c8e2-5984-4695-9939-62ad50f26623 container test-container: <nil>
    STEP: delete the pod 01/03/23 02:30:38.87
    Jan  3 02:30:38.932: INFO: Waiting for pod pod-d7b2c8e2-5984-4695-9939-62ad50f26623 to disappear
    Jan  3 02:30:38.937: INFO: Pod pod-d7b2c8e2-5984-4695-9939-62ad50f26623 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan  3 02:30:38.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-7285" for this suite. 01/03/23 02:30:38.94
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:30:38.943
Jan  3 02:30:38.943: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename limitrange 01/03/23 02:30:38.944
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:30:38.971
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:30:38.973
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
STEP: Creating a LimitRange 01/03/23 02:30:38.975
STEP: Setting up watch 01/03/23 02:30:38.975
STEP: Submitting a LimitRange 01/03/23 02:30:39.088
STEP: Verifying LimitRange creation was observed 01/03/23 02:30:39.097
STEP: Fetching the LimitRange to ensure it has proper values 01/03/23 02:30:39.097
Jan  3 02:30:39.103: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan  3 02:30:39.103: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 01/03/23 02:30:39.103
STEP: Ensuring Pod has resource requirements applied from LimitRange 01/03/23 02:30:39.107
Jan  3 02:30:39.109: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan  3 02:30:39.109: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 01/03/23 02:30:39.109
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 01/03/23 02:30:39.131
Jan  3 02:30:39.154: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jan  3 02:30:39.154: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 01/03/23 02:30:39.154
STEP: Failing to create a Pod with more than max resources 01/03/23 02:30:39.157
STEP: Updating a LimitRange 01/03/23 02:30:39.16
STEP: Verifying LimitRange updating is effective 01/03/23 02:30:39.17
STEP: Creating a Pod with less than former min resources 01/03/23 02:30:41.173
STEP: Failing to create a Pod with more than max resources 01/03/23 02:30:41.177
STEP: Deleting a LimitRange 01/03/23 02:30:41.18
STEP: Verifying the LimitRange was deleted 01/03/23 02:30:41.217
Jan  3 02:30:46.220: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 01/03/23 02:30:46.22
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:187
Jan  3 02:30:46.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-9998" for this suite. 01/03/23 02:30:46.23
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","completed":131,"skipped":2363,"failed":0}
------------------------------
• [SLOW TEST] [7.326 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:30:38.943
    Jan  3 02:30:38.943: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename limitrange 01/03/23 02:30:38.944
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:30:38.971
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:30:38.973
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:57
    STEP: Creating a LimitRange 01/03/23 02:30:38.975
    STEP: Setting up watch 01/03/23 02:30:38.975
    STEP: Submitting a LimitRange 01/03/23 02:30:39.088
    STEP: Verifying LimitRange creation was observed 01/03/23 02:30:39.097
    STEP: Fetching the LimitRange to ensure it has proper values 01/03/23 02:30:39.097
    Jan  3 02:30:39.103: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jan  3 02:30:39.103: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 01/03/23 02:30:39.103
    STEP: Ensuring Pod has resource requirements applied from LimitRange 01/03/23 02:30:39.107
    Jan  3 02:30:39.109: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jan  3 02:30:39.109: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 01/03/23 02:30:39.109
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 01/03/23 02:30:39.131
    Jan  3 02:30:39.154: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Jan  3 02:30:39.154: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 01/03/23 02:30:39.154
    STEP: Failing to create a Pod with more than max resources 01/03/23 02:30:39.157
    STEP: Updating a LimitRange 01/03/23 02:30:39.16
    STEP: Verifying LimitRange updating is effective 01/03/23 02:30:39.17
    STEP: Creating a Pod with less than former min resources 01/03/23 02:30:41.173
    STEP: Failing to create a Pod with more than max resources 01/03/23 02:30:41.177
    STEP: Deleting a LimitRange 01/03/23 02:30:41.18
    STEP: Verifying the LimitRange was deleted 01/03/23 02:30:41.217
    Jan  3 02:30:46.220: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 01/03/23 02:30:46.22
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:187
    Jan  3 02:30:46.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "limitrange-9998" for this suite. 01/03/23 02:30:46.23
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:30:46.27
Jan  3 02:30:46.270: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename crd-publish-openapi 01/03/23 02:30:46.271
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:30:46.28
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:30:46.282
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 01/03/23 02:30:46.284
Jan  3 02:30:46.285: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 01/03/23 02:31:06.487
Jan  3 02:31:06.488: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
Jan  3 02:31:11.878: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  3 02:31:32.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8694" for this suite. 01/03/23 02:31:32.747
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","completed":132,"skipped":2375,"failed":0}
------------------------------
• [SLOW TEST] [46.480 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:30:46.27
    Jan  3 02:30:46.270: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename crd-publish-openapi 01/03/23 02:30:46.271
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:30:46.28
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:30:46.282
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:308
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 01/03/23 02:30:46.284
    Jan  3 02:30:46.285: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 01/03/23 02:31:06.487
    Jan  3 02:31:06.488: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    Jan  3 02:31:11.878: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  3 02:31:32.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-8694" for this suite. 01/03/23 02:31:32.747
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:31:32.751
Jan  3 02:31:32.751: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename configmap 01/03/23 02:31:32.752
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:31:32.761
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:31:32.763
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
STEP: Creating configMap with name cm-test-opt-del-63ede7ab-a71d-416b-9bdc-e5e585eba5c7 01/03/23 02:31:32.797
STEP: Creating configMap with name cm-test-opt-upd-99356931-8bf9-424a-a4e9-0a2383f1b989 01/03/23 02:31:32.8
STEP: Creating the pod 01/03/23 02:31:32.807
Jan  3 02:31:32.816: INFO: Waiting up to 5m0s for pod "pod-configmaps-36ae2a49-889e-4b3c-b077-4550c5486a1d" in namespace "configmap-8827" to be "running and ready"
Jan  3 02:31:32.831: INFO: Pod "pod-configmaps-36ae2a49-889e-4b3c-b077-4550c5486a1d": Phase="Pending", Reason="", readiness=false. Elapsed: 14.469299ms
Jan  3 02:31:32.831: INFO: The phase of Pod pod-configmaps-36ae2a49-889e-4b3c-b077-4550c5486a1d is Pending, waiting for it to be Running (with Ready = true)
Jan  3 02:31:34.833: INFO: Pod "pod-configmaps-36ae2a49-889e-4b3c-b077-4550c5486a1d": Phase="Running", Reason="", readiness=true. Elapsed: 2.016949361s
Jan  3 02:31:34.833: INFO: The phase of Pod pod-configmaps-36ae2a49-889e-4b3c-b077-4550c5486a1d is Running (Ready = true)
Jan  3 02:31:34.833: INFO: Pod "pod-configmaps-36ae2a49-889e-4b3c-b077-4550c5486a1d" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-63ede7ab-a71d-416b-9bdc-e5e585eba5c7 01/03/23 02:31:34.843
STEP: Updating configmap cm-test-opt-upd-99356931-8bf9-424a-a4e9-0a2383f1b989 01/03/23 02:31:34.846
STEP: Creating configMap with name cm-test-opt-create-631529fe-bf46-4baf-bafa-80e056e146f5 01/03/23 02:31:34.853
STEP: waiting to observe update in volume 01/03/23 02:31:34.891
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan  3 02:31:36.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8827" for this suite. 01/03/23 02:31:36.913
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":133,"skipped":2394,"failed":0}
------------------------------
• [4.165 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:31:32.751
    Jan  3 02:31:32.751: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename configmap 01/03/23 02:31:32.752
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:31:32.761
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:31:32.763
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:239
    STEP: Creating configMap with name cm-test-opt-del-63ede7ab-a71d-416b-9bdc-e5e585eba5c7 01/03/23 02:31:32.797
    STEP: Creating configMap with name cm-test-opt-upd-99356931-8bf9-424a-a4e9-0a2383f1b989 01/03/23 02:31:32.8
    STEP: Creating the pod 01/03/23 02:31:32.807
    Jan  3 02:31:32.816: INFO: Waiting up to 5m0s for pod "pod-configmaps-36ae2a49-889e-4b3c-b077-4550c5486a1d" in namespace "configmap-8827" to be "running and ready"
    Jan  3 02:31:32.831: INFO: Pod "pod-configmaps-36ae2a49-889e-4b3c-b077-4550c5486a1d": Phase="Pending", Reason="", readiness=false. Elapsed: 14.469299ms
    Jan  3 02:31:32.831: INFO: The phase of Pod pod-configmaps-36ae2a49-889e-4b3c-b077-4550c5486a1d is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 02:31:34.833: INFO: Pod "pod-configmaps-36ae2a49-889e-4b3c-b077-4550c5486a1d": Phase="Running", Reason="", readiness=true. Elapsed: 2.016949361s
    Jan  3 02:31:34.833: INFO: The phase of Pod pod-configmaps-36ae2a49-889e-4b3c-b077-4550c5486a1d is Running (Ready = true)
    Jan  3 02:31:34.833: INFO: Pod "pod-configmaps-36ae2a49-889e-4b3c-b077-4550c5486a1d" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-63ede7ab-a71d-416b-9bdc-e5e585eba5c7 01/03/23 02:31:34.843
    STEP: Updating configmap cm-test-opt-upd-99356931-8bf9-424a-a4e9-0a2383f1b989 01/03/23 02:31:34.846
    STEP: Creating configMap with name cm-test-opt-create-631529fe-bf46-4baf-bafa-80e056e146f5 01/03/23 02:31:34.853
    STEP: waiting to observe update in volume 01/03/23 02:31:34.891
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan  3 02:31:36.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-8827" for this suite. 01/03/23 02:31:36.913
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:31:36.916
Jan  3 02:31:36.916: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename kubectl 01/03/23 02:31:36.917
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:31:36.927
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:31:36.93
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1492
STEP: creating the pod 01/03/23 02:31:36.932
Jan  3 02:31:36.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-940 create -f -'
Jan  3 02:31:37.759: INFO: stderr: ""
Jan  3 02:31:37.759: INFO: stdout: "pod/pause created\n"
Jan  3 02:31:37.759: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jan  3 02:31:37.759: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-940" to be "running and ready"
Jan  3 02:31:37.765: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.230943ms
Jan  3 02:31:37.766: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'cncf-master' to be 'Running' but was 'Pending'
Jan  3 02:31:39.769: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.009297429s
Jan  3 02:31:39.769: INFO: Pod "pause" satisfied condition "running and ready"
Jan  3 02:31:39.769: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
STEP: adding the label testing-label with value testing-label-value to a pod 01/03/23 02:31:39.769
Jan  3 02:31:39.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-940 label pods pause testing-label=testing-label-value'
Jan  3 02:31:39.831: INFO: stderr: ""
Jan  3 02:31:39.831: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 01/03/23 02:31:39.831
Jan  3 02:31:39.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-940 get pod pause -L testing-label'
Jan  3 02:31:39.904: INFO: stderr: ""
Jan  3 02:31:39.904: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 01/03/23 02:31:39.904
Jan  3 02:31:39.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-940 label pods pause testing-label-'
Jan  3 02:31:39.975: INFO: stderr: ""
Jan  3 02:31:39.975: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 01/03/23 02:31:39.975
Jan  3 02:31:39.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-940 get pod pause -L testing-label'
Jan  3 02:31:40.034: INFO: stderr: ""
Jan  3 02:31:40.034: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1498
STEP: using delete to clean up resources 01/03/23 02:31:40.034
Jan  3 02:31:40.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-940 delete --grace-period=0 --force -f -'
Jan  3 02:31:40.112: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  3 02:31:40.112: INFO: stdout: "pod \"pause\" force deleted\n"
Jan  3 02:31:40.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-940 get rc,svc -l name=pause --no-headers'
Jan  3 02:31:40.174: INFO: stderr: "No resources found in kubectl-940 namespace.\n"
Jan  3 02:31:40.174: INFO: stdout: ""
Jan  3 02:31:40.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-940 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan  3 02:31:40.230: INFO: stderr: ""
Jan  3 02:31:40.230: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan  3 02:31:40.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-940" for this suite. 01/03/23 02:31:40.233
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","completed":134,"skipped":2403,"failed":0}
------------------------------
• [3.319 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1490
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:31:36.916
    Jan  3 02:31:36.916: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename kubectl 01/03/23 02:31:36.917
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:31:36.927
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:31:36.93
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1492
    STEP: creating the pod 01/03/23 02:31:36.932
    Jan  3 02:31:36.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-940 create -f -'
    Jan  3 02:31:37.759: INFO: stderr: ""
    Jan  3 02:31:37.759: INFO: stdout: "pod/pause created\n"
    Jan  3 02:31:37.759: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Jan  3 02:31:37.759: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-940" to be "running and ready"
    Jan  3 02:31:37.765: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.230943ms
    Jan  3 02:31:37.766: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'cncf-master' to be 'Running' but was 'Pending'
    Jan  3 02:31:39.769: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.009297429s
    Jan  3 02:31:39.769: INFO: Pod "pause" satisfied condition "running and ready"
    Jan  3 02:31:39.769: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1507
    STEP: adding the label testing-label with value testing-label-value to a pod 01/03/23 02:31:39.769
    Jan  3 02:31:39.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-940 label pods pause testing-label=testing-label-value'
    Jan  3 02:31:39.831: INFO: stderr: ""
    Jan  3 02:31:39.831: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 01/03/23 02:31:39.831
    Jan  3 02:31:39.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-940 get pod pause -L testing-label'
    Jan  3 02:31:39.904: INFO: stderr: ""
    Jan  3 02:31:39.904: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 01/03/23 02:31:39.904
    Jan  3 02:31:39.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-940 label pods pause testing-label-'
    Jan  3 02:31:39.975: INFO: stderr: ""
    Jan  3 02:31:39.975: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 01/03/23 02:31:39.975
    Jan  3 02:31:39.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-940 get pod pause -L testing-label'
    Jan  3 02:31:40.034: INFO: stderr: ""
    Jan  3 02:31:40.034: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1498
    STEP: using delete to clean up resources 01/03/23 02:31:40.034
    Jan  3 02:31:40.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-940 delete --grace-period=0 --force -f -'
    Jan  3 02:31:40.112: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  3 02:31:40.112: INFO: stdout: "pod \"pause\" force deleted\n"
    Jan  3 02:31:40.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-940 get rc,svc -l name=pause --no-headers'
    Jan  3 02:31:40.174: INFO: stderr: "No resources found in kubectl-940 namespace.\n"
    Jan  3 02:31:40.174: INFO: stdout: ""
    Jan  3 02:31:40.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-940 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan  3 02:31:40.230: INFO: stderr: ""
    Jan  3 02:31:40.230: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan  3 02:31:40.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-940" for this suite. 01/03/23 02:31:40.233
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:31:40.237
Jan  3 02:31:40.237: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename projected 01/03/23 02:31:40.237
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:31:40.265
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:31:40.266
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
STEP: Creating secret with name projected-secret-test-f8cc9331-9557-48ed-aae0-ceaae8d64d0b 01/03/23 02:31:40.268
STEP: Creating a pod to test consume secrets 01/03/23 02:31:40.283
Jan  3 02:31:40.291: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-cf0f7c88-b700-42f6-a55d-99a7be0b356e" in namespace "projected-9574" to be "Succeeded or Failed"
Jan  3 02:31:40.311: INFO: Pod "pod-projected-secrets-cf0f7c88-b700-42f6-a55d-99a7be0b356e": Phase="Pending", Reason="", readiness=false. Elapsed: 20.446343ms
Jan  3 02:31:42.341: INFO: Pod "pod-projected-secrets-cf0f7c88-b700-42f6-a55d-99a7be0b356e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050388737s
Jan  3 02:31:44.315: INFO: Pod "pod-projected-secrets-cf0f7c88-b700-42f6-a55d-99a7be0b356e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02356471s
Jan  3 02:31:46.314: INFO: Pod "pod-projected-secrets-cf0f7c88-b700-42f6-a55d-99a7be0b356e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022664286s
STEP: Saw pod success 01/03/23 02:31:46.314
Jan  3 02:31:46.314: INFO: Pod "pod-projected-secrets-cf0f7c88-b700-42f6-a55d-99a7be0b356e" satisfied condition "Succeeded or Failed"
Jan  3 02:31:46.315: INFO: Trying to get logs from node cncf-master pod pod-projected-secrets-cf0f7c88-b700-42f6-a55d-99a7be0b356e container secret-volume-test: <nil>
STEP: delete the pod 01/03/23 02:31:46.319
Jan  3 02:31:46.340: INFO: Waiting for pod pod-projected-secrets-cf0f7c88-b700-42f6-a55d-99a7be0b356e to disappear
Jan  3 02:31:46.345: INFO: Pod pod-projected-secrets-cf0f7c88-b700-42f6-a55d-99a7be0b356e no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan  3 02:31:46.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9574" for this suite. 01/03/23 02:31:46.348
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":135,"skipped":2453,"failed":0}
------------------------------
• [SLOW TEST] [6.134 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:31:40.237
    Jan  3 02:31:40.237: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename projected 01/03/23 02:31:40.237
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:31:40.265
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:31:40.266
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:118
    STEP: Creating secret with name projected-secret-test-f8cc9331-9557-48ed-aae0-ceaae8d64d0b 01/03/23 02:31:40.268
    STEP: Creating a pod to test consume secrets 01/03/23 02:31:40.283
    Jan  3 02:31:40.291: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-cf0f7c88-b700-42f6-a55d-99a7be0b356e" in namespace "projected-9574" to be "Succeeded or Failed"
    Jan  3 02:31:40.311: INFO: Pod "pod-projected-secrets-cf0f7c88-b700-42f6-a55d-99a7be0b356e": Phase="Pending", Reason="", readiness=false. Elapsed: 20.446343ms
    Jan  3 02:31:42.341: INFO: Pod "pod-projected-secrets-cf0f7c88-b700-42f6-a55d-99a7be0b356e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050388737s
    Jan  3 02:31:44.315: INFO: Pod "pod-projected-secrets-cf0f7c88-b700-42f6-a55d-99a7be0b356e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02356471s
    Jan  3 02:31:46.314: INFO: Pod "pod-projected-secrets-cf0f7c88-b700-42f6-a55d-99a7be0b356e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022664286s
    STEP: Saw pod success 01/03/23 02:31:46.314
    Jan  3 02:31:46.314: INFO: Pod "pod-projected-secrets-cf0f7c88-b700-42f6-a55d-99a7be0b356e" satisfied condition "Succeeded or Failed"
    Jan  3 02:31:46.315: INFO: Trying to get logs from node cncf-master pod pod-projected-secrets-cf0f7c88-b700-42f6-a55d-99a7be0b356e container secret-volume-test: <nil>
    STEP: delete the pod 01/03/23 02:31:46.319
    Jan  3 02:31:46.340: INFO: Waiting for pod pod-projected-secrets-cf0f7c88-b700-42f6-a55d-99a7be0b356e to disappear
    Jan  3 02:31:46.345: INFO: Pod pod-projected-secrets-cf0f7c88-b700-42f6-a55d-99a7be0b356e no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan  3 02:31:46.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9574" for this suite. 01/03/23 02:31:46.348
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:31:46.371
Jan  3 02:31:46.371: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename deployment 01/03/23 02:31:46.372
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:31:46.391
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:31:46.393
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 01/03/23 02:31:46.4
STEP: waiting for Deployment to be created 01/03/23 02:31:46.404
STEP: waiting for all Replicas to be Ready 01/03/23 02:31:46.405
Jan  3 02:31:46.405: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan  3 02:31:46.405: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan  3 02:31:46.431: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan  3 02:31:46.431: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan  3 02:31:46.463: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan  3 02:31:46.463: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan  3 02:31:46.586: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan  3 02:31:46.586: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan  3 02:31:47.490: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan  3 02:31:47.490: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan  3 02:31:47.753: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 01/03/23 02:31:47.753
W0103 02:31:47.760964      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan  3 02:31:47.762: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 01/03/23 02:31:47.762
Jan  3 02:31:47.763: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 0
Jan  3 02:31:47.763: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 0
Jan  3 02:31:47.763: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 0
Jan  3 02:31:47.763: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 0
Jan  3 02:31:47.763: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 0
Jan  3 02:31:47.763: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 0
Jan  3 02:31:47.763: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 0
Jan  3 02:31:47.763: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 0
Jan  3 02:31:47.763: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 1
Jan  3 02:31:47.763: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 1
Jan  3 02:31:47.763: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 2
Jan  3 02:31:47.763: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 2
Jan  3 02:31:47.763: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 2
Jan  3 02:31:47.763: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 2
Jan  3 02:31:47.781: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 2
Jan  3 02:31:47.781: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 2
Jan  3 02:31:47.810: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 2
Jan  3 02:31:47.810: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 2
Jan  3 02:31:47.887: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 1
Jan  3 02:31:47.887: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 1
Jan  3 02:31:49.526: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 2
Jan  3 02:31:49.526: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 2
Jan  3 02:31:49.547: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 1
STEP: listing Deployments 01/03/23 02:31:49.547
Jan  3 02:31:49.563: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 01/03/23 02:31:49.563
Jan  3 02:31:49.571: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 01/03/23 02:31:49.571
Jan  3 02:31:49.580: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan  3 02:31:49.581: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan  3 02:31:49.601: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan  3 02:31:49.658: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan  3 02:31:49.687: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan  3 02:31:50.559: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan  3 02:31:50.580: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan  3 02:31:50.592: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan  3 02:31:50.603: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan  3 02:31:50.609: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan  3 02:31:51.787: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 01/03/23 02:31:51.815
STEP: fetching the DeploymentStatus 01/03/23 02:31:51.819
Jan  3 02:31:51.822: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 1
Jan  3 02:31:51.822: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 1
Jan  3 02:31:51.822: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 1
Jan  3 02:31:51.822: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 1
Jan  3 02:31:51.823: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 1
Jan  3 02:31:51.823: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 2
Jan  3 02:31:51.823: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 2
Jan  3 02:31:51.823: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 2
Jan  3 02:31:51.823: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 2
Jan  3 02:31:51.823: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 2
Jan  3 02:31:51.823: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 3
STEP: deleting the Deployment 01/03/23 02:31:51.823
Jan  3 02:31:51.827: INFO: observed event type MODIFIED
Jan  3 02:31:51.827: INFO: observed event type MODIFIED
Jan  3 02:31:51.827: INFO: observed event type MODIFIED
Jan  3 02:31:51.827: INFO: observed event type MODIFIED
Jan  3 02:31:51.827: INFO: observed event type MODIFIED
Jan  3 02:31:51.827: INFO: observed event type MODIFIED
Jan  3 02:31:51.827: INFO: observed event type MODIFIED
Jan  3 02:31:51.827: INFO: observed event type MODIFIED
Jan  3 02:31:51.827: INFO: observed event type MODIFIED
Jan  3 02:31:51.827: INFO: observed event type MODIFIED
Jan  3 02:31:51.827: INFO: observed event type MODIFIED
Jan  3 02:31:51.827: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan  3 02:31:51.838: INFO: Log out all the ReplicaSets if there is no deployment created
Jan  3 02:31:51.840: INFO: ReplicaSet "test-deployment-54cc775c4b":
&ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-1810  c6c3324a-ecb2-47c8-b947-ef522b0d47f7 994243 4 2023-01-03 02:31:47 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[createdTime:2023-01-03T11:31:46.401841584+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2 updatedTime:2023-01-03T11:31:46.401841584+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [{apps/v1 Deployment test-deployment dc90aaa4-6eb3-4190-96f4-fea3aea3bd73 0xc006895657 0xc006895658}] [] [{kube-controller-manager Update apps/v1 2023-01-03 02:31:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:createdTime":{},"f:creator":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{},"f:updatedTime":{},"f:updater":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dc90aaa4-6eb3-4190-96f4-fea3aea3bd73\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-03 02:31:51 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006895710 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jan  3 02:31:51.846: INFO: pod: "test-deployment-54cc775c4b-k5hth":
&Pod{ObjectMeta:{test-deployment-54cc775c4b-k5hth test-deployment-54cc775c4b- deployment-1810  eb8a1132-44f0-4a23-8ffb-b387637ea99e 994237 0 2023-01-03 02:31:47 +0000 UTC 2023-01-03 02:31:52 +0000 UTC 0xc006895c40 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:3fe6dc589e9c73a1e8b4bf9adab3d275f9a82b191d0ac991aa59798e6ca980f8 cni.projectcalico.org/podIP:10.244.246.76/32 cni.projectcalico.org/podIPs:10.244.246.76/32 createdTime:2023-01-03T11:31:47.800482027+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T11:31:47.800482027+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet test-deployment-54cc775c4b c6c3324a-ecb2-47c8-b947-ef522b0d47f7 0xc006895cc7 0xc006895cc8}] [] [{kube-controller-manager Update v1 2023-01-03 02:31:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c6c3324a-ecb2-47c8-b947-ef522b0d47f7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-03 02:31:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-03 02:31:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.246.76\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7frhk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7frhk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-master,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:31:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:31:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:31:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:31:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.5,PodIP:10.244.246.76,StartTime:2023-01-03 02:31:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-03 02:31:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:cri-o://8a1071867a0b49b189fa7a4b7d27c459af8a021181fddd95686a7ad943f54c39,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.246.76,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan  3 02:31:51.846: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
&ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-1810  2fa8e141-57a3-40ad-8517-dee391841a99 994233 2 2023-01-03 02:31:49 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[createdTime:2023-01-03T11:31:49.583232616+09:00 creator:system:serviceaccount:kube-system:deployment-controller deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3 updatedTime:2023-01-03T11:31:49.583232616+09:00 updater:system:serviceaccount:kube-system:deployment-controller] [{apps/v1 Deployment test-deployment dc90aaa4-6eb3-4190-96f4-fea3aea3bd73 0xc0068957a7 0xc0068957a8}] [] [{kube-controller-manager Update apps/v1 2023-01-03 02:31:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dc90aaa4-6eb3-4190-96f4-fea3aea3bd73\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-03 02:31:51 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006895850 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Jan  3 02:31:51.852: INFO: pod: "test-deployment-7c7d8d58c8-5dm5s":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-5dm5s test-deployment-7c7d8d58c8- deployment-1810  1651a2fc-dff6-41e6-acaa-524cb83a0815 994175 0 2023-01-03 02:31:49 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:61460c443ac808801c5555cf0423bebac90733842a4e129503abb044fd21debd cni.projectcalico.org/podIP:10.244.246.75/32 cni.projectcalico.org/podIPs:10.244.246.75/32 createdTime:2023-01-03T11:31:49.596735814+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T11:31:49.596735814+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 2fa8e141-57a3-40ad-8517-dee391841a99 0xc0048e9f27 0xc0048e9f28}] [] [{kube-controller-manager Update v1 2023-01-03 02:31:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2fa8e141-57a3-40ad-8517-dee391841a99\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-03 02:31:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-03 02:31:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.246.75\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-skkfq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-skkfq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-master,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:31:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:31:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:31:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:31:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.5,PodIP:10.244.246.75,StartTime:2023-01-03 02:31:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-03 02:31:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://cb76eb544e82efbb9516d6a591d0b22588d8ebb0fb8b8c2e00a09fb60ff813a6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.246.75,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan  3 02:31:51.852: INFO: pod: "test-deployment-7c7d8d58c8-kdmv2":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-kdmv2 test-deployment-7c7d8d58c8- deployment-1810  6c50eb29-9ee9-49c9-b517-17837372f88e 994232 0 2023-01-03 02:31:50 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:355e11fd44275321d6e69f68a215ce93d5940a8b24f70b7a156b8b0b14d93ba1 cni.projectcalico.org/podIP:10.244.232.17/32 cni.projectcalico.org/podIPs:10.244.232.17/32 createdTime:2023-01-03T11:31:50.573166109+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T11:31:50.573166109+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 2fa8e141-57a3-40ad-8517-dee391841a99 0xc00692a187 0xc00692a188}] [] [{kube-controller-manager Update v1 2023-01-03 02:31:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2fa8e141-57a3-40ad-8517-dee391841a99\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-03 02:31:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-03 02:31:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.232.17\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zwl4x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zwl4x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:31:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:31:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:31:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:31:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.7,PodIP:10.244.232.17,StartTime:2023-01-03 02:31:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-03 02:31:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://93726ecbe33998dd82f4135b1b9201dee1a0595eaf6e02f0f01b655dbe3462a5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.232.17,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan  3 02:31:51.852: INFO: ReplicaSet "test-deployment-8594bb6fdd":
&ReplicaSet{ObjectMeta:{test-deployment-8594bb6fdd  deployment-1810  409b01c4-fb0e-47d2-af6b-9e9250c910b4 994129 3 2023-01-03 02:31:46 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[createdTime:2023-01-03T11:31:46.401841584+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:31:46.401841584+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [{apps/v1 Deployment test-deployment dc90aaa4-6eb3-4190-96f4-fea3aea3bd73 0xc0068958e7 0xc0068958e8}] [] [{kube-controller-manager Update apps/v1 2023-01-03 02:31:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:createdTime":{},"f:creator":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{},"f:updatedTime":{},"f:updater":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dc90aaa4-6eb3-4190-96f4-fea3aea3bd73\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-03 02:31:49 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8594bb6fdd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006895980 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan  3 02:31:51.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1810" for this suite. 01/03/23 02:31:51.879
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","completed":136,"skipped":2464,"failed":0}
------------------------------
• [SLOW TEST] [5.510 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:31:46.371
    Jan  3 02:31:46.371: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename deployment 01/03/23 02:31:46.372
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:31:46.391
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:31:46.393
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 01/03/23 02:31:46.4
    STEP: waiting for Deployment to be created 01/03/23 02:31:46.404
    STEP: waiting for all Replicas to be Ready 01/03/23 02:31:46.405
    Jan  3 02:31:46.405: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan  3 02:31:46.405: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan  3 02:31:46.431: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan  3 02:31:46.431: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan  3 02:31:46.463: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan  3 02:31:46.463: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan  3 02:31:46.586: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan  3 02:31:46.586: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan  3 02:31:47.490: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jan  3 02:31:47.490: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jan  3 02:31:47.753: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 01/03/23 02:31:47.753
    W0103 02:31:47.760964      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan  3 02:31:47.762: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 01/03/23 02:31:47.762
    Jan  3 02:31:47.763: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 0
    Jan  3 02:31:47.763: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 0
    Jan  3 02:31:47.763: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 0
    Jan  3 02:31:47.763: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 0
    Jan  3 02:31:47.763: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 0
    Jan  3 02:31:47.763: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 0
    Jan  3 02:31:47.763: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 0
    Jan  3 02:31:47.763: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 0
    Jan  3 02:31:47.763: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 1
    Jan  3 02:31:47.763: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 1
    Jan  3 02:31:47.763: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 2
    Jan  3 02:31:47.763: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 2
    Jan  3 02:31:47.763: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 2
    Jan  3 02:31:47.763: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 2
    Jan  3 02:31:47.781: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 2
    Jan  3 02:31:47.781: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 2
    Jan  3 02:31:47.810: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 2
    Jan  3 02:31:47.810: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 2
    Jan  3 02:31:47.887: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 1
    Jan  3 02:31:47.887: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 1
    Jan  3 02:31:49.526: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 2
    Jan  3 02:31:49.526: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 2
    Jan  3 02:31:49.547: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 1
    STEP: listing Deployments 01/03/23 02:31:49.547
    Jan  3 02:31:49.563: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 01/03/23 02:31:49.563
    Jan  3 02:31:49.571: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 01/03/23 02:31:49.571
    Jan  3 02:31:49.580: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  3 02:31:49.581: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  3 02:31:49.601: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  3 02:31:49.658: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  3 02:31:49.687: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  3 02:31:50.559: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  3 02:31:50.580: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  3 02:31:50.592: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  3 02:31:50.603: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  3 02:31:50.609: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  3 02:31:51.787: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 01/03/23 02:31:51.815
    STEP: fetching the DeploymentStatus 01/03/23 02:31:51.819
    Jan  3 02:31:51.822: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 1
    Jan  3 02:31:51.822: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 1
    Jan  3 02:31:51.822: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 1
    Jan  3 02:31:51.822: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 1
    Jan  3 02:31:51.823: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 1
    Jan  3 02:31:51.823: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 2
    Jan  3 02:31:51.823: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 2
    Jan  3 02:31:51.823: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 2
    Jan  3 02:31:51.823: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 2
    Jan  3 02:31:51.823: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 2
    Jan  3 02:31:51.823: INFO: observed Deployment test-deployment in namespace deployment-1810 with ReadyReplicas 3
    STEP: deleting the Deployment 01/03/23 02:31:51.823
    Jan  3 02:31:51.827: INFO: observed event type MODIFIED
    Jan  3 02:31:51.827: INFO: observed event type MODIFIED
    Jan  3 02:31:51.827: INFO: observed event type MODIFIED
    Jan  3 02:31:51.827: INFO: observed event type MODIFIED
    Jan  3 02:31:51.827: INFO: observed event type MODIFIED
    Jan  3 02:31:51.827: INFO: observed event type MODIFIED
    Jan  3 02:31:51.827: INFO: observed event type MODIFIED
    Jan  3 02:31:51.827: INFO: observed event type MODIFIED
    Jan  3 02:31:51.827: INFO: observed event type MODIFIED
    Jan  3 02:31:51.827: INFO: observed event type MODIFIED
    Jan  3 02:31:51.827: INFO: observed event type MODIFIED
    Jan  3 02:31:51.827: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan  3 02:31:51.838: INFO: Log out all the ReplicaSets if there is no deployment created
    Jan  3 02:31:51.840: INFO: ReplicaSet "test-deployment-54cc775c4b":
    &ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-1810  c6c3324a-ecb2-47c8-b947-ef522b0d47f7 994243 4 2023-01-03 02:31:47 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[createdTime:2023-01-03T11:31:46.401841584+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2 updatedTime:2023-01-03T11:31:46.401841584+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [{apps/v1 Deployment test-deployment dc90aaa4-6eb3-4190-96f4-fea3aea3bd73 0xc006895657 0xc006895658}] [] [{kube-controller-manager Update apps/v1 2023-01-03 02:31:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:createdTime":{},"f:creator":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{},"f:updatedTime":{},"f:updater":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dc90aaa4-6eb3-4190-96f4-fea3aea3bd73\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-03 02:31:51 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006895710 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Jan  3 02:31:51.846: INFO: pod: "test-deployment-54cc775c4b-k5hth":
    &Pod{ObjectMeta:{test-deployment-54cc775c4b-k5hth test-deployment-54cc775c4b- deployment-1810  eb8a1132-44f0-4a23-8ffb-b387637ea99e 994237 0 2023-01-03 02:31:47 +0000 UTC 2023-01-03 02:31:52 +0000 UTC 0xc006895c40 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:3fe6dc589e9c73a1e8b4bf9adab3d275f9a82b191d0ac991aa59798e6ca980f8 cni.projectcalico.org/podIP:10.244.246.76/32 cni.projectcalico.org/podIPs:10.244.246.76/32 createdTime:2023-01-03T11:31:47.800482027+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T11:31:47.800482027+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet test-deployment-54cc775c4b c6c3324a-ecb2-47c8-b947-ef522b0d47f7 0xc006895cc7 0xc006895cc8}] [] [{kube-controller-manager Update v1 2023-01-03 02:31:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c6c3324a-ecb2-47c8-b947-ef522b0d47f7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-03 02:31:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-03 02:31:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.246.76\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7frhk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7frhk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-master,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:31:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:31:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:31:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:31:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.5,PodIP:10.244.246.76,StartTime:2023-01-03 02:31:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-03 02:31:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:cri-o://8a1071867a0b49b189fa7a4b7d27c459af8a021181fddd95686a7ad943f54c39,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.246.76,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan  3 02:31:51.846: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
    &ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-1810  2fa8e141-57a3-40ad-8517-dee391841a99 994233 2 2023-01-03 02:31:49 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[createdTime:2023-01-03T11:31:49.583232616+09:00 creator:system:serviceaccount:kube-system:deployment-controller deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3 updatedTime:2023-01-03T11:31:49.583232616+09:00 updater:system:serviceaccount:kube-system:deployment-controller] [{apps/v1 Deployment test-deployment dc90aaa4-6eb3-4190-96f4-fea3aea3bd73 0xc0068957a7 0xc0068957a8}] [] [{kube-controller-manager Update apps/v1 2023-01-03 02:31:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dc90aaa4-6eb3-4190-96f4-fea3aea3bd73\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-03 02:31:51 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006895850 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Jan  3 02:31:51.852: INFO: pod: "test-deployment-7c7d8d58c8-5dm5s":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-5dm5s test-deployment-7c7d8d58c8- deployment-1810  1651a2fc-dff6-41e6-acaa-524cb83a0815 994175 0 2023-01-03 02:31:49 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:61460c443ac808801c5555cf0423bebac90733842a4e129503abb044fd21debd cni.projectcalico.org/podIP:10.244.246.75/32 cni.projectcalico.org/podIPs:10.244.246.75/32 createdTime:2023-01-03T11:31:49.596735814+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T11:31:49.596735814+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 2fa8e141-57a3-40ad-8517-dee391841a99 0xc0048e9f27 0xc0048e9f28}] [] [{kube-controller-manager Update v1 2023-01-03 02:31:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2fa8e141-57a3-40ad-8517-dee391841a99\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-03 02:31:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-03 02:31:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.246.75\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-skkfq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-skkfq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-master,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:31:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:31:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:31:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:31:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.5,PodIP:10.244.246.75,StartTime:2023-01-03 02:31:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-03 02:31:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://cb76eb544e82efbb9516d6a591d0b22588d8ebb0fb8b8c2e00a09fb60ff813a6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.246.75,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan  3 02:31:51.852: INFO: pod: "test-deployment-7c7d8d58c8-kdmv2":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-kdmv2 test-deployment-7c7d8d58c8- deployment-1810  6c50eb29-9ee9-49c9-b517-17837372f88e 994232 0 2023-01-03 02:31:50 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:355e11fd44275321d6e69f68a215ce93d5940a8b24f70b7a156b8b0b14d93ba1 cni.projectcalico.org/podIP:10.244.232.17/32 cni.projectcalico.org/podIPs:10.244.232.17/32 createdTime:2023-01-03T11:31:50.573166109+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T11:31:50.573166109+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 2fa8e141-57a3-40ad-8517-dee391841a99 0xc00692a187 0xc00692a188}] [] [{kube-controller-manager Update v1 2023-01-03 02:31:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2fa8e141-57a3-40ad-8517-dee391841a99\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-03 02:31:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-03 02:31:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.232.17\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zwl4x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zwl4x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:31:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:31:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:31:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:31:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.7,PodIP:10.244.232.17,StartTime:2023-01-03 02:31:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-03 02:31:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://93726ecbe33998dd82f4135b1b9201dee1a0595eaf6e02f0f01b655dbe3462a5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.232.17,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan  3 02:31:51.852: INFO: ReplicaSet "test-deployment-8594bb6fdd":
    &ReplicaSet{ObjectMeta:{test-deployment-8594bb6fdd  deployment-1810  409b01c4-fb0e-47d2-af6b-9e9250c910b4 994129 3 2023-01-03 02:31:46 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[createdTime:2023-01-03T11:31:46.401841584+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:31:46.401841584+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [{apps/v1 Deployment test-deployment dc90aaa4-6eb3-4190-96f4-fea3aea3bd73 0xc0068958e7 0xc0068958e8}] [] [{kube-controller-manager Update apps/v1 2023-01-03 02:31:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:createdTime":{},"f:creator":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{},"f:updatedTime":{},"f:updater":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dc90aaa4-6eb3-4190-96f4-fea3aea3bd73\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-03 02:31:49 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8594bb6fdd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006895980 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan  3 02:31:51.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-1810" for this suite. 01/03/23 02:31:51.879
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:31:51.881
Jan  3 02:31:51.882: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename webhook 01/03/23 02:31:51.883
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:31:51.927
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:31:51.928
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/03/23 02:31:51.955
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/23 02:31:52.313
STEP: Deploying the webhook pod 01/03/23 02:31:52.318
STEP: Wait for the deployment to be ready 01/03/23 02:31:52.336
Jan  3 02:31:52.364: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/03/23 02:31:54.37
STEP: Verifying the service has paired with the endpoint 01/03/23 02:31:54.385
Jan  3 02:31:55.385: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 01/03/23 02:31:55.387
STEP: create a pod that should be updated by the webhook 01/03/23 02:31:55.398
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  3 02:31:55.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8208" for this suite. 01/03/23 02:31:55.437
STEP: Destroying namespace "webhook-8208-markers" for this suite. 01/03/23 02:31:55.454
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","completed":137,"skipped":2465,"failed":0}
------------------------------
• [3.664 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:31:51.881
    Jan  3 02:31:51.882: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename webhook 01/03/23 02:31:51.883
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:31:51.927
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:31:51.928
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/03/23 02:31:51.955
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/23 02:31:52.313
    STEP: Deploying the webhook pod 01/03/23 02:31:52.318
    STEP: Wait for the deployment to be ready 01/03/23 02:31:52.336
    Jan  3 02:31:52.364: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/03/23 02:31:54.37
    STEP: Verifying the service has paired with the endpoint 01/03/23 02:31:54.385
    Jan  3 02:31:55.385: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:263
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 01/03/23 02:31:55.387
    STEP: create a pod that should be updated by the webhook 01/03/23 02:31:55.398
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  3 02:31:55.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8208" for this suite. 01/03/23 02:31:55.437
    STEP: Destroying namespace "webhook-8208-markers" for this suite. 01/03/23 02:31:55.454
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:31:55.546
Jan  3 02:31:55.546: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename projected 01/03/23 02:31:55.547
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:31:55.573
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:31:55.576
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
STEP: Creating the pod 01/03/23 02:31:55.577
Jan  3 02:31:55.631: INFO: Waiting up to 5m0s for pod "annotationupdate9261e274-de62-4ecd-b82c-f80437e5dfaa" in namespace "projected-93" to be "running and ready"
Jan  3 02:31:55.637: INFO: Pod "annotationupdate9261e274-de62-4ecd-b82c-f80437e5dfaa": Phase="Pending", Reason="", readiness=false. Elapsed: 5.787454ms
Jan  3 02:31:55.637: INFO: The phase of Pod annotationupdate9261e274-de62-4ecd-b82c-f80437e5dfaa is Pending, waiting for it to be Running (with Ready = true)
Jan  3 02:31:57.640: INFO: Pod "annotationupdate9261e274-de62-4ecd-b82c-f80437e5dfaa": Phase="Running", Reason="", readiness=true. Elapsed: 2.009113221s
Jan  3 02:31:57.640: INFO: The phase of Pod annotationupdate9261e274-de62-4ecd-b82c-f80437e5dfaa is Running (Ready = true)
Jan  3 02:31:57.640: INFO: Pod "annotationupdate9261e274-de62-4ecd-b82c-f80437e5dfaa" satisfied condition "running and ready"
Jan  3 02:31:58.151: INFO: Successfully updated pod "annotationupdate9261e274-de62-4ecd-b82c-f80437e5dfaa"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan  3 02:32:02.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-93" for this suite. 01/03/23 02:32:02.181
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","completed":138,"skipped":2473,"failed":0}
------------------------------
• [SLOW TEST] [6.637 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:31:55.546
    Jan  3 02:31:55.546: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename projected 01/03/23 02:31:55.547
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:31:55.573
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:31:55.576
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:161
    STEP: Creating the pod 01/03/23 02:31:55.577
    Jan  3 02:31:55.631: INFO: Waiting up to 5m0s for pod "annotationupdate9261e274-de62-4ecd-b82c-f80437e5dfaa" in namespace "projected-93" to be "running and ready"
    Jan  3 02:31:55.637: INFO: Pod "annotationupdate9261e274-de62-4ecd-b82c-f80437e5dfaa": Phase="Pending", Reason="", readiness=false. Elapsed: 5.787454ms
    Jan  3 02:31:55.637: INFO: The phase of Pod annotationupdate9261e274-de62-4ecd-b82c-f80437e5dfaa is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 02:31:57.640: INFO: Pod "annotationupdate9261e274-de62-4ecd-b82c-f80437e5dfaa": Phase="Running", Reason="", readiness=true. Elapsed: 2.009113221s
    Jan  3 02:31:57.640: INFO: The phase of Pod annotationupdate9261e274-de62-4ecd-b82c-f80437e5dfaa is Running (Ready = true)
    Jan  3 02:31:57.640: INFO: Pod "annotationupdate9261e274-de62-4ecd-b82c-f80437e5dfaa" satisfied condition "running and ready"
    Jan  3 02:31:58.151: INFO: Successfully updated pod "annotationupdate9261e274-de62-4ecd-b82c-f80437e5dfaa"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan  3 02:32:02.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-93" for this suite. 01/03/23 02:32:02.181
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:32:02.184
Jan  3 02:32:02.184: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename emptydir 01/03/23 02:32:02.185
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:32:02.202
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:32:02.204
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
STEP: Creating a pod to test emptydir 0666 on tmpfs 01/03/23 02:32:02.206
Jan  3 02:32:02.211: INFO: Waiting up to 5m0s for pod "pod-0b6141d2-22f7-47fc-9a32-e85234365500" in namespace "emptydir-323" to be "Succeeded or Failed"
Jan  3 02:32:02.223: INFO: Pod "pod-0b6141d2-22f7-47fc-9a32-e85234365500": Phase="Pending", Reason="", readiness=false. Elapsed: 11.423148ms
Jan  3 02:32:04.225: INFO: Pod "pod-0b6141d2-22f7-47fc-9a32-e85234365500": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013964309s
Jan  3 02:32:06.225: INFO: Pod "pod-0b6141d2-22f7-47fc-9a32-e85234365500": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013789763s
Jan  3 02:32:08.226: INFO: Pod "pod-0b6141d2-22f7-47fc-9a32-e85234365500": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015165402s
STEP: Saw pod success 01/03/23 02:32:08.226
Jan  3 02:32:08.226: INFO: Pod "pod-0b6141d2-22f7-47fc-9a32-e85234365500" satisfied condition "Succeeded or Failed"
Jan  3 02:32:08.228: INFO: Trying to get logs from node cncf-master pod pod-0b6141d2-22f7-47fc-9a32-e85234365500 container test-container: <nil>
STEP: delete the pod 01/03/23 02:32:08.232
Jan  3 02:32:08.254: INFO: Waiting for pod pod-0b6141d2-22f7-47fc-9a32-e85234365500 to disappear
Jan  3 02:32:08.260: INFO: Pod pod-0b6141d2-22f7-47fc-9a32-e85234365500 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan  3 02:32:08.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-323" for this suite. 01/03/23 02:32:08.263
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":139,"skipped":2493,"failed":0}
------------------------------
• [SLOW TEST] [6.081 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:32:02.184
    Jan  3 02:32:02.184: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename emptydir 01/03/23 02:32:02.185
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:32:02.202
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:32:02.204
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:136
    STEP: Creating a pod to test emptydir 0666 on tmpfs 01/03/23 02:32:02.206
    Jan  3 02:32:02.211: INFO: Waiting up to 5m0s for pod "pod-0b6141d2-22f7-47fc-9a32-e85234365500" in namespace "emptydir-323" to be "Succeeded or Failed"
    Jan  3 02:32:02.223: INFO: Pod "pod-0b6141d2-22f7-47fc-9a32-e85234365500": Phase="Pending", Reason="", readiness=false. Elapsed: 11.423148ms
    Jan  3 02:32:04.225: INFO: Pod "pod-0b6141d2-22f7-47fc-9a32-e85234365500": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013964309s
    Jan  3 02:32:06.225: INFO: Pod "pod-0b6141d2-22f7-47fc-9a32-e85234365500": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013789763s
    Jan  3 02:32:08.226: INFO: Pod "pod-0b6141d2-22f7-47fc-9a32-e85234365500": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015165402s
    STEP: Saw pod success 01/03/23 02:32:08.226
    Jan  3 02:32:08.226: INFO: Pod "pod-0b6141d2-22f7-47fc-9a32-e85234365500" satisfied condition "Succeeded or Failed"
    Jan  3 02:32:08.228: INFO: Trying to get logs from node cncf-master pod pod-0b6141d2-22f7-47fc-9a32-e85234365500 container test-container: <nil>
    STEP: delete the pod 01/03/23 02:32:08.232
    Jan  3 02:32:08.254: INFO: Waiting for pod pod-0b6141d2-22f7-47fc-9a32-e85234365500 to disappear
    Jan  3 02:32:08.260: INFO: Pod pod-0b6141d2-22f7-47fc-9a32-e85234365500 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan  3 02:32:08.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-323" for this suite. 01/03/23 02:32:08.263
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:32:08.266
Jan  3 02:32:08.266: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename podtemplate 01/03/23 02:32:08.267
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:32:08.283
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:32:08.284
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Jan  3 02:32:08.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-799" for this suite. 01/03/23 02:32:08.371
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","completed":140,"skipped":2518,"failed":0}
------------------------------
• [0.107 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:32:08.266
    Jan  3 02:32:08.266: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename podtemplate 01/03/23 02:32:08.267
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:32:08.283
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:32:08.284
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Jan  3 02:32:08.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-799" for this suite. 01/03/23 02:32:08.371
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:32:08.375
Jan  3 02:32:08.375: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename init-container 01/03/23 02:32:08.376
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:32:08.397
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:32:08.399
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
STEP: creating the pod 01/03/23 02:32:08.401
Jan  3 02:32:08.401: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jan  3 02:32:13.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7763" for this suite. 01/03/23 02:32:13.576
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","completed":141,"skipped":2569,"failed":0}
------------------------------
• [SLOW TEST] [5.203 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:32:08.375
    Jan  3 02:32:08.375: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename init-container 01/03/23 02:32:08.376
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:32:08.397
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:32:08.399
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:176
    STEP: creating the pod 01/03/23 02:32:08.401
    Jan  3 02:32:08.401: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan  3 02:32:13.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-7763" for this suite. 01/03/23 02:32:13.576
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:32:13.579
Jan  3 02:32:13.579: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename gc 01/03/23 02:32:13.58
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:32:13.598
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:32:13.6
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 01/03/23 02:32:13.601
STEP: delete the rc 01/03/23 02:32:18.622
STEP: wait for all pods to be garbage collected 01/03/23 02:32:18.628
STEP: Gathering metrics 01/03/23 02:32:23.632
Jan  3 02:32:23.640: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cncf-master" in namespace "kube-system" to be "running and ready"
Jan  3 02:32:23.641: INFO: Pod "kube-controller-manager-cncf-master": Phase="Running", Reason="", readiness=true. Elapsed: 1.683971ms
Jan  3 02:32:23.641: INFO: The phase of Pod kube-controller-manager-cncf-master is Running (Ready = true)
Jan  3 02:32:23.641: INFO: Pod "kube-controller-manager-cncf-master" satisfied condition "running and ready"
Jan  3 02:32:23.699: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan  3 02:32:23.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4494" for this suite. 01/03/23 02:32:23.703
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","completed":142,"skipped":2592,"failed":0}
------------------------------
• [SLOW TEST] [10.126 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:32:13.579
    Jan  3 02:32:13.579: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename gc 01/03/23 02:32:13.58
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:32:13.598
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:32:13.6
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 01/03/23 02:32:13.601
    STEP: delete the rc 01/03/23 02:32:18.622
    STEP: wait for all pods to be garbage collected 01/03/23 02:32:18.628
    STEP: Gathering metrics 01/03/23 02:32:23.632
    Jan  3 02:32:23.640: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cncf-master" in namespace "kube-system" to be "running and ready"
    Jan  3 02:32:23.641: INFO: Pod "kube-controller-manager-cncf-master": Phase="Running", Reason="", readiness=true. Elapsed: 1.683971ms
    Jan  3 02:32:23.641: INFO: The phase of Pod kube-controller-manager-cncf-master is Running (Ready = true)
    Jan  3 02:32:23.641: INFO: Pod "kube-controller-manager-cncf-master" satisfied condition "running and ready"
    Jan  3 02:32:23.699: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan  3 02:32:23.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-4494" for this suite. 01/03/23 02:32:23.703
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:32:23.706
Jan  3 02:32:23.706: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename projected 01/03/23 02:32:23.707
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:32:23.741
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:32:23.743
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
STEP: Creating projection with secret that has name projected-secret-test-a89bdf36-2af7-4837-8772-b8e3131824c3 01/03/23 02:32:23.745
STEP: Creating a pod to test consume secrets 01/03/23 02:32:23.748
Jan  3 02:32:23.792: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-540a0d1a-dd2e-4566-a31c-aa81081ac9ce" in namespace "projected-4901" to be "Succeeded or Failed"
Jan  3 02:32:23.812: INFO: Pod "pod-projected-secrets-540a0d1a-dd2e-4566-a31c-aa81081ac9ce": Phase="Pending", Reason="", readiness=false. Elapsed: 19.964533ms
Jan  3 02:32:25.815: INFO: Pod "pod-projected-secrets-540a0d1a-dd2e-4566-a31c-aa81081ac9ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022782904s
Jan  3 02:32:27.820: INFO: Pod "pod-projected-secrets-540a0d1a-dd2e-4566-a31c-aa81081ac9ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028452773s
STEP: Saw pod success 01/03/23 02:32:27.82
Jan  3 02:32:27.821: INFO: Pod "pod-projected-secrets-540a0d1a-dd2e-4566-a31c-aa81081ac9ce" satisfied condition "Succeeded or Failed"
Jan  3 02:32:27.822: INFO: Trying to get logs from node cncf-master pod pod-projected-secrets-540a0d1a-dd2e-4566-a31c-aa81081ac9ce container projected-secret-volume-test: <nil>
STEP: delete the pod 01/03/23 02:32:27.825
Jan  3 02:32:27.839: INFO: Waiting for pod pod-projected-secrets-540a0d1a-dd2e-4566-a31c-aa81081ac9ce to disappear
Jan  3 02:32:27.854: INFO: Pod pod-projected-secrets-540a0d1a-dd2e-4566-a31c-aa81081ac9ce no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan  3 02:32:27.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4901" for this suite. 01/03/23 02:32:27.856
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":143,"skipped":2605,"failed":0}
------------------------------
• [4.153 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:32:23.706
    Jan  3 02:32:23.706: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename projected 01/03/23 02:32:23.707
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:32:23.741
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:32:23.743
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:55
    STEP: Creating projection with secret that has name projected-secret-test-a89bdf36-2af7-4837-8772-b8e3131824c3 01/03/23 02:32:23.745
    STEP: Creating a pod to test consume secrets 01/03/23 02:32:23.748
    Jan  3 02:32:23.792: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-540a0d1a-dd2e-4566-a31c-aa81081ac9ce" in namespace "projected-4901" to be "Succeeded or Failed"
    Jan  3 02:32:23.812: INFO: Pod "pod-projected-secrets-540a0d1a-dd2e-4566-a31c-aa81081ac9ce": Phase="Pending", Reason="", readiness=false. Elapsed: 19.964533ms
    Jan  3 02:32:25.815: INFO: Pod "pod-projected-secrets-540a0d1a-dd2e-4566-a31c-aa81081ac9ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022782904s
    Jan  3 02:32:27.820: INFO: Pod "pod-projected-secrets-540a0d1a-dd2e-4566-a31c-aa81081ac9ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028452773s
    STEP: Saw pod success 01/03/23 02:32:27.82
    Jan  3 02:32:27.821: INFO: Pod "pod-projected-secrets-540a0d1a-dd2e-4566-a31c-aa81081ac9ce" satisfied condition "Succeeded or Failed"
    Jan  3 02:32:27.822: INFO: Trying to get logs from node cncf-master pod pod-projected-secrets-540a0d1a-dd2e-4566-a31c-aa81081ac9ce container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/03/23 02:32:27.825
    Jan  3 02:32:27.839: INFO: Waiting for pod pod-projected-secrets-540a0d1a-dd2e-4566-a31c-aa81081ac9ce to disappear
    Jan  3 02:32:27.854: INFO: Pod pod-projected-secrets-540a0d1a-dd2e-4566-a31c-aa81081ac9ce no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan  3 02:32:27.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4901" for this suite. 01/03/23 02:32:27.856
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:32:27.86
Jan  3 02:32:27.860: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename container-probe 01/03/23 02:32:27.861
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:32:27.868
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:32:27.87
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan  3 02:33:27.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5942" for this suite. 01/03/23 02:33:27.897
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","completed":144,"skipped":2647,"failed":0}
------------------------------
• [SLOW TEST] [60.039 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:32:27.86
    Jan  3 02:32:27.860: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename container-probe 01/03/23 02:32:27.861
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:32:27.868
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:32:27.87
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:104
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan  3 02:33:27.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-5942" for this suite. 01/03/23 02:33:27.897
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:309
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:33:27.901
Jan  3 02:33:27.901: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename job 01/03/23 02:33:27.902
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:33:27.915
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:33:27.917
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:309
STEP: Creating a job 01/03/23 02:33:27.919
STEP: Ensuring active pods == parallelism 01/03/23 02:33:27.932
STEP: delete a job 01/03/23 02:33:29.936
STEP: deleting Job.batch foo in namespace job-8899, will wait for the garbage collector to delete the pods 01/03/23 02:33:29.936
Jan  3 02:33:29.990: INFO: Deleting Job.batch foo took: 2.496859ms
Jan  3 02:33:30.091: INFO: Terminating Job.batch foo pods took: 100.41241ms
STEP: Ensuring job was deleted 01/03/23 02:34:02.991
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan  3 02:34:02.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8899" for this suite. 01/03/23 02:34:02.996
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","completed":145,"skipped":2662,"failed":0}
------------------------------
• [SLOW TEST] [35.098 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:33:27.901
    Jan  3 02:33:27.901: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename job 01/03/23 02:33:27.902
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:33:27.915
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:33:27.917
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:309
    STEP: Creating a job 01/03/23 02:33:27.919
    STEP: Ensuring active pods == parallelism 01/03/23 02:33:27.932
    STEP: delete a job 01/03/23 02:33:29.936
    STEP: deleting Job.batch foo in namespace job-8899, will wait for the garbage collector to delete the pods 01/03/23 02:33:29.936
    Jan  3 02:33:29.990: INFO: Deleting Job.batch foo took: 2.496859ms
    Jan  3 02:33:30.091: INFO: Terminating Job.batch foo pods took: 100.41241ms
    STEP: Ensuring job was deleted 01/03/23 02:34:02.991
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan  3 02:34:02.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-8899" for this suite. 01/03/23 02:34:02.996
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:34:03.001
Jan  3 02:34:03.001: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename configmap 01/03/23 02:34:03.002
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:34:03.036
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:34:03.038
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan  3 02:34:03.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-981" for this suite. 01/03/23 02:34:03.094
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","completed":146,"skipped":2694,"failed":0}
------------------------------
• [0.095 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:34:03.001
    Jan  3 02:34:03.001: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename configmap 01/03/23 02:34:03.002
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:34:03.036
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:34:03.038
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:503
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan  3 02:34:03.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-981" for this suite. 01/03/23 02:34:03.094
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:34:03.098
Jan  3 02:34:03.098: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename sched-pred 01/03/23 02:34:03.099
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:34:03.105
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:34:03.107
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan  3 02:34:03.110: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan  3 02:34:03.127: INFO: Waiting for terminating namespaces to be deleted...
Jan  3 02:34:03.129: INFO: 
Logging pods the apiserver thinks is on node cncf-master before test
Jan  3 02:34:03.157: INFO: calico-kube-controllers-58dbc876ff-xt8wh from kube-system started at 2023-01-03 02:07:41 +0000 UTC (1 container statuses recorded)
Jan  3 02:34:03.157: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan  3 02:34:03.157: INFO: calico-node-zxb7d from kube-system started at 2022-12-28 00:23:10 +0000 UTC (1 container statuses recorded)
Jan  3 02:34:03.157: INFO: 	Container calico-node ready: true, restart count 0
Jan  3 02:34:03.157: INFO: etcd-cncf-master from kube-system started at 2022-12-28 00:22:57 +0000 UTC (1 container statuses recorded)
Jan  3 02:34:03.157: INFO: 	Container etcd ready: true, restart count 0
Jan  3 02:34:03.157: INFO: kube-apiserver-cncf-master from kube-system started at 2023-01-03 01:37:53 +0000 UTC (1 container statuses recorded)
Jan  3 02:34:03.157: INFO: 	Container kube-apiserver ready: true, restart count 0
Jan  3 02:34:03.157: INFO: kube-controller-manager-cncf-master from kube-system started at 2022-12-28 00:22:57 +0000 UTC (1 container statuses recorded)
Jan  3 02:34:03.157: INFO: 	Container kube-controller-manager ready: true, restart count 2
Jan  3 02:34:03.157: INFO: kube-proxy-qvggp from kube-system started at 2022-12-28 00:23:10 +0000 UTC (1 container statuses recorded)
Jan  3 02:34:03.157: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  3 02:34:03.157: INFO: kube-scheduler-cncf-master from kube-system started at 2022-12-28 00:22:56 +0000 UTC (1 container statuses recorded)
Jan  3 02:34:03.157: INFO: 	Container kube-scheduler ready: true, restart count 2
Jan  3 02:34:03.157: INFO: csi-cephfsplugin-gj7gj from rook-ceph started at 2023-01-03 02:07:49 +0000 UTC (2 container statuses recorded)
Jan  3 02:34:03.157: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Jan  3 02:34:03.157: INFO: 	Container driver-registrar ready: true, restart count 0
Jan  3 02:34:03.157: INFO: csi-rbdplugin-vwr2v from rook-ceph started at 2023-01-03 02:07:47 +0000 UTC (2 container statuses recorded)
Jan  3 02:34:03.157: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jan  3 02:34:03.157: INFO: 	Container driver-registrar ready: true, restart count 0
Jan  3 02:34:03.157: INFO: rook-ceph-crashcollector-cncf-master-7569fb669b-slpjd from rook-ceph started at 2023-01-03 02:07:59 +0000 UTC (1 container statuses recorded)
Jan  3 02:34:03.157: INFO: 	Container ceph-crash ready: true, restart count 0
Jan  3 02:34:03.157: INFO: rook-ceph-mon-b-66ffc4d4d9-85fqv from rook-ceph started at 2023-01-03 02:07:41 +0000 UTC (1 container statuses recorded)
Jan  3 02:34:03.157: INFO: 	Container mon ready: true, restart count 0
Jan  3 02:34:03.157: INFO: rook-ceph-osd-1-58d74d96db-zg5pm from rook-ceph started at 2023-01-03 02:07:41 +0000 UTC (1 container statuses recorded)
Jan  3 02:34:03.157: INFO: 	Container osd ready: true, restart count 0
Jan  3 02:34:03.157: INFO: sonobuoy-systemd-logs-daemon-set-58995b31e0564399-fxv6h from sonobuoy started at 2023-01-03 01:46:49 +0000 UTC (2 container statuses recorded)
Jan  3 02:34:03.157: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  3 02:34:03.157: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  3 02:34:03.157: INFO: 
Logging pods the apiserver thinks is on node cncf-node1 before test
Jan  3 02:34:03.171: INFO: traefik-7c456f74d8-b7b6n from api-gateway-system started at 2023-01-03 01:41:21 +0000 UTC (1 container statuses recorded)
Jan  3 02:34:03.171: INFO: 	Container traefik ready: true, restart count 0
Jan  3 02:34:03.171: INFO: cert-manager-754886db57-z7pm7 from cert-manager started at 2023-01-03 01:35:12 +0000 UTC (1 container statuses recorded)
Jan  3 02:34:03.171: INFO: 	Container cert-manager ready: true, restart count 1
Jan  3 02:34:03.171: INFO: cert-manager-cainjector-66b4c7c78d-vc87n from cert-manager started at 2023-01-03 01:35:12 +0000 UTC (1 container statuses recorded)
Jan  3 02:34:03.171: INFO: 	Container cert-manager ready: true, restart count 4
Jan  3 02:34:03.171: INFO: cert-manager-webhook-78cdb5fdf-frlxh from cert-manager started at 2023-01-03 01:35:12 +0000 UTC (1 container statuses recorded)
Jan  3 02:34:03.171: INFO: 	Container cert-manager ready: true, restart count 0
Jan  3 02:34:03.171: INFO: console-59c484c98b-mcqpt from console-system started at 2023-01-03 01:41:24 +0000 UTC (1 container statuses recorded)
Jan  3 02:34:03.171: INFO: 	Container console ready: true, restart count 0
Jan  3 02:34:03.171: INFO: hyperauth-5cbb4d58b8-qpc2d from hyperauth started at 2023-01-03 01:37:01 +0000 UTC (1 container statuses recorded)
Jan  3 02:34:03.171: INFO: 	Container hyperauth ready: true, restart count 0
Jan  3 02:34:03.171: INFO: postgresql-c6b9bb88b-fk4pn from hyperauth started at 2023-01-03 02:07:41 +0000 UTC (1 container statuses recorded)
Jan  3 02:34:03.171: INFO: 	Container postgresql ready: true, restart count 0
Jan  3 02:34:03.171: INFO: hypercloud5-api-server-8c944885b-mk6vb from hypercloud5-system started at 2023-01-03 01:38:50 +0000 UTC (1 container statuses recorded)
Jan  3 02:34:03.171: INFO: 	Container hypercloud5-api-server ready: true, restart count 4
Jan  3 02:34:03.171: INFO: timescaledb-0 from hypercloud5-system started at 2023-01-03 01:38:50 +0000 UTC (1 container statuses recorded)
Jan  3 02:34:03.171: INFO: 	Container timescaledb ready: true, restart count 0
Jan  3 02:34:03.171: INFO: calico-node-9p9ff from kube-system started at 2022-12-28 00:54:44 +0000 UTC (1 container statuses recorded)
Jan  3 02:34:03.171: INFO: 	Container calico-node ready: true, restart count 0
Jan  3 02:34:03.171: INFO: coredns-565d847f94-dp5jm from kube-system started at 2023-01-03 02:07:41 +0000 UTC (1 container statuses recorded)
Jan  3 02:34:03.171: INFO: 	Container coredns ready: true, restart count 0
Jan  3 02:34:03.171: INFO: kube-proxy-jplgg from kube-system started at 2022-12-28 00:54:44 +0000 UTC (1 container statuses recorded)
Jan  3 02:34:03.171: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  3 02:34:03.171: INFO: snapshot-controller-545d7f8994-vg2ld from kube-system started at 2022-12-29 08:24:10 +0000 UTC (1 container statuses recorded)
Jan  3 02:34:03.171: INFO: 	Container snapshot-controller ready: true, restart count 3
Jan  3 02:34:03.171: INFO: csi-cephfsplugin-provisioner-69f54fb566-jxcpx from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (5 container statuses recorded)
Jan  3 02:34:03.171: INFO: 	Container csi-attacher ready: true, restart count 0
Jan  3 02:34:03.171: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Jan  3 02:34:03.171: INFO: 	Container csi-provisioner ready: true, restart count 0
Jan  3 02:34:03.171: INFO: 	Container csi-resizer ready: true, restart count 0
Jan  3 02:34:03.171: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jan  3 02:34:03.171: INFO: csi-cephfsplugin-stnqm from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (2 container statuses recorded)
Jan  3 02:34:03.171: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Jan  3 02:34:03.172: INFO: 	Container driver-registrar ready: true, restart count 0
Jan  3 02:34:03.172: INFO: csi-rbdplugin-provisioner-6f689965cc-wl454 from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (5 container statuses recorded)
Jan  3 02:34:03.172: INFO: 	Container csi-attacher ready: true, restart count 0
Jan  3 02:34:03.172: INFO: 	Container csi-provisioner ready: true, restart count 0
Jan  3 02:34:03.172: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jan  3 02:34:03.172: INFO: 	Container csi-resizer ready: true, restart count 0
Jan  3 02:34:03.172: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jan  3 02:34:03.172: INFO: csi-rbdplugin-t2hxd from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (2 container statuses recorded)
Jan  3 02:34:03.172: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jan  3 02:34:03.172: INFO: 	Container driver-registrar ready: true, restart count 0
Jan  3 02:34:03.172: INFO: rook-ceph-crashcollector-cncf-node1-846d9dbb97-scfg5 from rook-ceph started at 2023-01-02 01:35:36 +0000 UTC (1 container statuses recorded)
Jan  3 02:34:03.172: INFO: 	Container ceph-crash ready: true, restart count 0
Jan  3 02:34:03.172: INFO: rook-ceph-mds-myfs-a-58f7884986-6th9v from rook-ceph started at 2023-01-02 01:35:36 +0000 UTC (1 container statuses recorded)
Jan  3 02:34:03.172: INFO: 	Container mds ready: true, restart count 0
Jan  3 02:34:03.172: INFO: rook-ceph-mgr-a-7949c65bd-kjzgc from rook-ceph started at 2022-12-29 08:25:47 +0000 UTC (2 container statuses recorded)
Jan  3 02:34:03.172: INFO: 	Container mgr ready: true, restart count 0
Jan  3 02:34:03.172: INFO: 	Container watch-active ready: true, restart count 0
Jan  3 02:34:03.172: INFO: rook-ceph-mon-a-bdd44bcfb-s5hjx from rook-ceph started at 2022-12-29 08:24:58 +0000 UTC (1 container statuses recorded)
Jan  3 02:34:03.172: INFO: 	Container mon ready: true, restart count 0
Jan  3 02:34:03.172: INFO: rook-ceph-operator-7f56d78d46-z2j6r from rook-ceph started at 2023-01-02 01:29:14 +0000 UTC (1 container statuses recorded)
Jan  3 02:34:03.172: INFO: 	Container rook-ceph-operator ready: true, restart count 0
Jan  3 02:34:03.172: INFO: rook-ceph-osd-2-5d8bc49787-jd84z from rook-ceph started at 2023-01-02 01:35:03 +0000 UTC (1 container statuses recorded)
Jan  3 02:34:03.172: INFO: 	Container osd ready: true, restart count 0
Jan  3 02:34:03.172: INFO: rook-ceph-osd-prepare-cncf-node1-89n8c from rook-ceph started at 2023-01-03 01:40:56 +0000 UTC (1 container statuses recorded)
Jan  3 02:34:03.172: INFO: 	Container provision ready: false, restart count 0
Jan  3 02:34:03.172: INFO: rook-ceph-tools-75fb64866c-bcxp7 from rook-ceph started at 2022-12-30 02:06:28 +0000 UTC (1 container statuses recorded)
Jan  3 02:34:03.172: INFO: 	Container rook-ceph-tools ready: true, restart count 0
Jan  3 02:34:03.172: INFO: sonobuoy from sonobuoy started at 2023-01-03 01:46:49 +0000 UTC (1 container statuses recorded)
Jan  3 02:34:03.172: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan  3 02:34:03.172: INFO: sonobuoy-e2e-job-a15b13ec9aa249f2 from sonobuoy started at 2023-01-03 01:46:49 +0000 UTC (2 container statuses recorded)
Jan  3 02:34:03.172: INFO: 	Container e2e ready: true, restart count 0
Jan  3 02:34:03.172: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  3 02:34:03.172: INFO: sonobuoy-systemd-logs-daemon-set-58995b31e0564399-gndlm from sonobuoy started at 2023-01-03 01:46:49 +0000 UTC (2 container statuses recorded)
Jan  3 02:34:03.172: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  3 02:34:03.172: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  3 02:34:03.172: INFO: 
Logging pods the apiserver thinks is on node cncf-node2 before test
Jan  3 02:34:03.183: INFO: hyperauth-5cbb4d58b8-pqhc5 from hyperauth started at 2023-01-03 01:37:01 +0000 UTC (1 container statuses recorded)
Jan  3 02:34:03.183: INFO: 	Container hyperauth ready: true, restart count 0
Jan  3 02:34:03.183: INFO: calico-node-5f5qs from kube-system started at 2022-12-29 05:30:03 +0000 UTC (1 container statuses recorded)
Jan  3 02:34:03.183: INFO: 	Container calico-node ready: true, restart count 0
Jan  3 02:34:03.183: INFO: coredns-565d847f94-gqs74 from kube-system started at 2023-01-03 02:07:41 +0000 UTC (1 container statuses recorded)
Jan  3 02:34:03.183: INFO: 	Container coredns ready: true, restart count 0
Jan  3 02:34:03.183: INFO: kube-proxy-ddlsm from kube-system started at 2022-12-29 05:30:03 +0000 UTC (1 container statuses recorded)
Jan  3 02:34:03.183: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  3 02:34:03.183: INFO: snapshot-controller-545d7f8994-qgm6l from kube-system started at 2022-12-29 08:24:10 +0000 UTC (1 container statuses recorded)
Jan  3 02:34:03.183: INFO: 	Container snapshot-controller ready: true, restart count 0
Jan  3 02:34:03.183: INFO: csi-cephfsplugin-hxtjf from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (2 container statuses recorded)
Jan  3 02:34:03.183: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Jan  3 02:34:03.183: INFO: 	Container driver-registrar ready: true, restart count 0
Jan  3 02:34:03.183: INFO: csi-cephfsplugin-provisioner-69f54fb566-xhc7t from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (5 container statuses recorded)
Jan  3 02:34:03.183: INFO: 	Container csi-attacher ready: true, restart count 0
Jan  3 02:34:03.183: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Jan  3 02:34:03.183: INFO: 	Container csi-provisioner ready: true, restart count 0
Jan  3 02:34:03.183: INFO: 	Container csi-resizer ready: true, restart count 0
Jan  3 02:34:03.183: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jan  3 02:34:03.183: INFO: csi-rbdplugin-provisioner-6f689965cc-2mqrd from rook-ceph started at 2023-01-03 02:07:41 +0000 UTC (5 container statuses recorded)
Jan  3 02:34:03.183: INFO: 	Container csi-attacher ready: true, restart count 0
Jan  3 02:34:03.183: INFO: 	Container csi-provisioner ready: true, restart count 0
Jan  3 02:34:03.183: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jan  3 02:34:03.183: INFO: 	Container csi-resizer ready: true, restart count 0
Jan  3 02:34:03.183: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jan  3 02:34:03.183: INFO: csi-rbdplugin-qk6jk from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (2 container statuses recorded)
Jan  3 02:34:03.183: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jan  3 02:34:03.183: INFO: 	Container driver-registrar ready: true, restart count 0
Jan  3 02:34:03.183: INFO: rook-ceph-crashcollector-cncf-node2-54fb78d5b8-nncld from rook-ceph started at 2023-01-03 02:07:41 +0000 UTC (1 container statuses recorded)
Jan  3 02:34:03.183: INFO: 	Container ceph-crash ready: true, restart count 0
Jan  3 02:34:03.184: INFO: rook-ceph-mds-myfs-b-65b48b8c9d-l4wlk from rook-ceph started at 2023-01-03 02:07:41 +0000 UTC (1 container statuses recorded)
Jan  3 02:34:03.184: INFO: 	Container mds ready: true, restart count 0
Jan  3 02:34:03.184: INFO: rook-ceph-mgr-b-856c746f6-lchkv from rook-ceph started at 2023-01-03 02:07:41 +0000 UTC (2 container statuses recorded)
Jan  3 02:34:03.184: INFO: 	Container mgr ready: true, restart count 0
Jan  3 02:34:03.184: INFO: 	Container watch-active ready: true, restart count 0
Jan  3 02:34:03.184: INFO: rook-ceph-mon-c-6c468d6b45-m7rqj from rook-ceph started at 2022-12-29 08:25:33 +0000 UTC (1 container statuses recorded)
Jan  3 02:34:03.184: INFO: 	Container mon ready: true, restart count 0
Jan  3 02:34:03.184: INFO: rook-ceph-osd-0-7d877c8d99-hq5fw from rook-ceph started at 2023-01-02 01:35:02 +0000 UTC (1 container statuses recorded)
Jan  3 02:34:03.184: INFO: 	Container osd ready: true, restart count 0
Jan  3 02:34:03.184: INFO: rook-ceph-osd-prepare-cncf-node2-rnwb6 from rook-ceph started at 2023-01-03 01:40:59 +0000 UTC (1 container statuses recorded)
Jan  3 02:34:03.184: INFO: 	Container provision ready: false, restart count 0
Jan  3 02:34:03.184: INFO: sonobuoy-systemd-logs-daemon-set-58995b31e0564399-wjpkt from sonobuoy started at 2023-01-03 01:46:49 +0000 UTC (2 container statuses recorded)
Jan  3 02:34:03.184: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  3 02:34:03.184: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/03/23 02:34:03.184
Jan  3 02:34:03.189: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-8281" to be "running"
Jan  3 02:34:03.190: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.386317ms
Jan  3 02:34:05.208: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.01871028s
Jan  3 02:34:05.208: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/03/23 02:34:05.209
STEP: Trying to apply a random label on the found node. 01/03/23 02:34:05.225
STEP: verifying the node has the label kubernetes.io/e2e-2b3ee44c-a831-4bbc-90f6-7f6c907ce381 42 01/03/23 02:34:05.232
STEP: Trying to relaunch the pod, now with labels. 01/03/23 02:34:05.246
Jan  3 02:34:05.251: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-8281" to be "not pending"
Jan  3 02:34:05.253: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 2.418102ms
Jan  3 02:34:07.257: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.005590598s
Jan  3 02:34:07.257: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-2b3ee44c-a831-4bbc-90f6-7f6c907ce381 off the node cncf-master 01/03/23 02:34:07.258
STEP: verifying the node doesn't have the label kubernetes.io/e2e-2b3ee44c-a831-4bbc-90f6-7f6c907ce381 01/03/23 02:34:07.265
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jan  3 02:34:07.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8281" for this suite. 01/03/23 02:34:07.279
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","completed":147,"skipped":2750,"failed":0}
------------------------------
• [4.183 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:34:03.098
    Jan  3 02:34:03.098: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename sched-pred 01/03/23 02:34:03.099
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:34:03.105
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:34:03.107
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jan  3 02:34:03.110: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan  3 02:34:03.127: INFO: Waiting for terminating namespaces to be deleted...
    Jan  3 02:34:03.129: INFO: 
    Logging pods the apiserver thinks is on node cncf-master before test
    Jan  3 02:34:03.157: INFO: calico-kube-controllers-58dbc876ff-xt8wh from kube-system started at 2023-01-03 02:07:41 +0000 UTC (1 container statuses recorded)
    Jan  3 02:34:03.157: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Jan  3 02:34:03.157: INFO: calico-node-zxb7d from kube-system started at 2022-12-28 00:23:10 +0000 UTC (1 container statuses recorded)
    Jan  3 02:34:03.157: INFO: 	Container calico-node ready: true, restart count 0
    Jan  3 02:34:03.157: INFO: etcd-cncf-master from kube-system started at 2022-12-28 00:22:57 +0000 UTC (1 container statuses recorded)
    Jan  3 02:34:03.157: INFO: 	Container etcd ready: true, restart count 0
    Jan  3 02:34:03.157: INFO: kube-apiserver-cncf-master from kube-system started at 2023-01-03 01:37:53 +0000 UTC (1 container statuses recorded)
    Jan  3 02:34:03.157: INFO: 	Container kube-apiserver ready: true, restart count 0
    Jan  3 02:34:03.157: INFO: kube-controller-manager-cncf-master from kube-system started at 2022-12-28 00:22:57 +0000 UTC (1 container statuses recorded)
    Jan  3 02:34:03.157: INFO: 	Container kube-controller-manager ready: true, restart count 2
    Jan  3 02:34:03.157: INFO: kube-proxy-qvggp from kube-system started at 2022-12-28 00:23:10 +0000 UTC (1 container statuses recorded)
    Jan  3 02:34:03.157: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  3 02:34:03.157: INFO: kube-scheduler-cncf-master from kube-system started at 2022-12-28 00:22:56 +0000 UTC (1 container statuses recorded)
    Jan  3 02:34:03.157: INFO: 	Container kube-scheduler ready: true, restart count 2
    Jan  3 02:34:03.157: INFO: csi-cephfsplugin-gj7gj from rook-ceph started at 2023-01-03 02:07:49 +0000 UTC (2 container statuses recorded)
    Jan  3 02:34:03.157: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
    Jan  3 02:34:03.157: INFO: 	Container driver-registrar ready: true, restart count 0
    Jan  3 02:34:03.157: INFO: csi-rbdplugin-vwr2v from rook-ceph started at 2023-01-03 02:07:47 +0000 UTC (2 container statuses recorded)
    Jan  3 02:34:03.157: INFO: 	Container csi-rbdplugin ready: true, restart count 0
    Jan  3 02:34:03.157: INFO: 	Container driver-registrar ready: true, restart count 0
    Jan  3 02:34:03.157: INFO: rook-ceph-crashcollector-cncf-master-7569fb669b-slpjd from rook-ceph started at 2023-01-03 02:07:59 +0000 UTC (1 container statuses recorded)
    Jan  3 02:34:03.157: INFO: 	Container ceph-crash ready: true, restart count 0
    Jan  3 02:34:03.157: INFO: rook-ceph-mon-b-66ffc4d4d9-85fqv from rook-ceph started at 2023-01-03 02:07:41 +0000 UTC (1 container statuses recorded)
    Jan  3 02:34:03.157: INFO: 	Container mon ready: true, restart count 0
    Jan  3 02:34:03.157: INFO: rook-ceph-osd-1-58d74d96db-zg5pm from rook-ceph started at 2023-01-03 02:07:41 +0000 UTC (1 container statuses recorded)
    Jan  3 02:34:03.157: INFO: 	Container osd ready: true, restart count 0
    Jan  3 02:34:03.157: INFO: sonobuoy-systemd-logs-daemon-set-58995b31e0564399-fxv6h from sonobuoy started at 2023-01-03 01:46:49 +0000 UTC (2 container statuses recorded)
    Jan  3 02:34:03.157: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  3 02:34:03.157: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  3 02:34:03.157: INFO: 
    Logging pods the apiserver thinks is on node cncf-node1 before test
    Jan  3 02:34:03.171: INFO: traefik-7c456f74d8-b7b6n from api-gateway-system started at 2023-01-03 01:41:21 +0000 UTC (1 container statuses recorded)
    Jan  3 02:34:03.171: INFO: 	Container traefik ready: true, restart count 0
    Jan  3 02:34:03.171: INFO: cert-manager-754886db57-z7pm7 from cert-manager started at 2023-01-03 01:35:12 +0000 UTC (1 container statuses recorded)
    Jan  3 02:34:03.171: INFO: 	Container cert-manager ready: true, restart count 1
    Jan  3 02:34:03.171: INFO: cert-manager-cainjector-66b4c7c78d-vc87n from cert-manager started at 2023-01-03 01:35:12 +0000 UTC (1 container statuses recorded)
    Jan  3 02:34:03.171: INFO: 	Container cert-manager ready: true, restart count 4
    Jan  3 02:34:03.171: INFO: cert-manager-webhook-78cdb5fdf-frlxh from cert-manager started at 2023-01-03 01:35:12 +0000 UTC (1 container statuses recorded)
    Jan  3 02:34:03.171: INFO: 	Container cert-manager ready: true, restart count 0
    Jan  3 02:34:03.171: INFO: console-59c484c98b-mcqpt from console-system started at 2023-01-03 01:41:24 +0000 UTC (1 container statuses recorded)
    Jan  3 02:34:03.171: INFO: 	Container console ready: true, restart count 0
    Jan  3 02:34:03.171: INFO: hyperauth-5cbb4d58b8-qpc2d from hyperauth started at 2023-01-03 01:37:01 +0000 UTC (1 container statuses recorded)
    Jan  3 02:34:03.171: INFO: 	Container hyperauth ready: true, restart count 0
    Jan  3 02:34:03.171: INFO: postgresql-c6b9bb88b-fk4pn from hyperauth started at 2023-01-03 02:07:41 +0000 UTC (1 container statuses recorded)
    Jan  3 02:34:03.171: INFO: 	Container postgresql ready: true, restart count 0
    Jan  3 02:34:03.171: INFO: hypercloud5-api-server-8c944885b-mk6vb from hypercloud5-system started at 2023-01-03 01:38:50 +0000 UTC (1 container statuses recorded)
    Jan  3 02:34:03.171: INFO: 	Container hypercloud5-api-server ready: true, restart count 4
    Jan  3 02:34:03.171: INFO: timescaledb-0 from hypercloud5-system started at 2023-01-03 01:38:50 +0000 UTC (1 container statuses recorded)
    Jan  3 02:34:03.171: INFO: 	Container timescaledb ready: true, restart count 0
    Jan  3 02:34:03.171: INFO: calico-node-9p9ff from kube-system started at 2022-12-28 00:54:44 +0000 UTC (1 container statuses recorded)
    Jan  3 02:34:03.171: INFO: 	Container calico-node ready: true, restart count 0
    Jan  3 02:34:03.171: INFO: coredns-565d847f94-dp5jm from kube-system started at 2023-01-03 02:07:41 +0000 UTC (1 container statuses recorded)
    Jan  3 02:34:03.171: INFO: 	Container coredns ready: true, restart count 0
    Jan  3 02:34:03.171: INFO: kube-proxy-jplgg from kube-system started at 2022-12-28 00:54:44 +0000 UTC (1 container statuses recorded)
    Jan  3 02:34:03.171: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  3 02:34:03.171: INFO: snapshot-controller-545d7f8994-vg2ld from kube-system started at 2022-12-29 08:24:10 +0000 UTC (1 container statuses recorded)
    Jan  3 02:34:03.171: INFO: 	Container snapshot-controller ready: true, restart count 3
    Jan  3 02:34:03.171: INFO: csi-cephfsplugin-provisioner-69f54fb566-jxcpx from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (5 container statuses recorded)
    Jan  3 02:34:03.171: INFO: 	Container csi-attacher ready: true, restart count 0
    Jan  3 02:34:03.171: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
    Jan  3 02:34:03.171: INFO: 	Container csi-provisioner ready: true, restart count 0
    Jan  3 02:34:03.171: INFO: 	Container csi-resizer ready: true, restart count 0
    Jan  3 02:34:03.171: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Jan  3 02:34:03.171: INFO: csi-cephfsplugin-stnqm from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (2 container statuses recorded)
    Jan  3 02:34:03.171: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
    Jan  3 02:34:03.172: INFO: 	Container driver-registrar ready: true, restart count 0
    Jan  3 02:34:03.172: INFO: csi-rbdplugin-provisioner-6f689965cc-wl454 from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (5 container statuses recorded)
    Jan  3 02:34:03.172: INFO: 	Container csi-attacher ready: true, restart count 0
    Jan  3 02:34:03.172: INFO: 	Container csi-provisioner ready: true, restart count 0
    Jan  3 02:34:03.172: INFO: 	Container csi-rbdplugin ready: true, restart count 0
    Jan  3 02:34:03.172: INFO: 	Container csi-resizer ready: true, restart count 0
    Jan  3 02:34:03.172: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Jan  3 02:34:03.172: INFO: csi-rbdplugin-t2hxd from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (2 container statuses recorded)
    Jan  3 02:34:03.172: INFO: 	Container csi-rbdplugin ready: true, restart count 0
    Jan  3 02:34:03.172: INFO: 	Container driver-registrar ready: true, restart count 0
    Jan  3 02:34:03.172: INFO: rook-ceph-crashcollector-cncf-node1-846d9dbb97-scfg5 from rook-ceph started at 2023-01-02 01:35:36 +0000 UTC (1 container statuses recorded)
    Jan  3 02:34:03.172: INFO: 	Container ceph-crash ready: true, restart count 0
    Jan  3 02:34:03.172: INFO: rook-ceph-mds-myfs-a-58f7884986-6th9v from rook-ceph started at 2023-01-02 01:35:36 +0000 UTC (1 container statuses recorded)
    Jan  3 02:34:03.172: INFO: 	Container mds ready: true, restart count 0
    Jan  3 02:34:03.172: INFO: rook-ceph-mgr-a-7949c65bd-kjzgc from rook-ceph started at 2022-12-29 08:25:47 +0000 UTC (2 container statuses recorded)
    Jan  3 02:34:03.172: INFO: 	Container mgr ready: true, restart count 0
    Jan  3 02:34:03.172: INFO: 	Container watch-active ready: true, restart count 0
    Jan  3 02:34:03.172: INFO: rook-ceph-mon-a-bdd44bcfb-s5hjx from rook-ceph started at 2022-12-29 08:24:58 +0000 UTC (1 container statuses recorded)
    Jan  3 02:34:03.172: INFO: 	Container mon ready: true, restart count 0
    Jan  3 02:34:03.172: INFO: rook-ceph-operator-7f56d78d46-z2j6r from rook-ceph started at 2023-01-02 01:29:14 +0000 UTC (1 container statuses recorded)
    Jan  3 02:34:03.172: INFO: 	Container rook-ceph-operator ready: true, restart count 0
    Jan  3 02:34:03.172: INFO: rook-ceph-osd-2-5d8bc49787-jd84z from rook-ceph started at 2023-01-02 01:35:03 +0000 UTC (1 container statuses recorded)
    Jan  3 02:34:03.172: INFO: 	Container osd ready: true, restart count 0
    Jan  3 02:34:03.172: INFO: rook-ceph-osd-prepare-cncf-node1-89n8c from rook-ceph started at 2023-01-03 01:40:56 +0000 UTC (1 container statuses recorded)
    Jan  3 02:34:03.172: INFO: 	Container provision ready: false, restart count 0
    Jan  3 02:34:03.172: INFO: rook-ceph-tools-75fb64866c-bcxp7 from rook-ceph started at 2022-12-30 02:06:28 +0000 UTC (1 container statuses recorded)
    Jan  3 02:34:03.172: INFO: 	Container rook-ceph-tools ready: true, restart count 0
    Jan  3 02:34:03.172: INFO: sonobuoy from sonobuoy started at 2023-01-03 01:46:49 +0000 UTC (1 container statuses recorded)
    Jan  3 02:34:03.172: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan  3 02:34:03.172: INFO: sonobuoy-e2e-job-a15b13ec9aa249f2 from sonobuoy started at 2023-01-03 01:46:49 +0000 UTC (2 container statuses recorded)
    Jan  3 02:34:03.172: INFO: 	Container e2e ready: true, restart count 0
    Jan  3 02:34:03.172: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  3 02:34:03.172: INFO: sonobuoy-systemd-logs-daemon-set-58995b31e0564399-gndlm from sonobuoy started at 2023-01-03 01:46:49 +0000 UTC (2 container statuses recorded)
    Jan  3 02:34:03.172: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  3 02:34:03.172: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  3 02:34:03.172: INFO: 
    Logging pods the apiserver thinks is on node cncf-node2 before test
    Jan  3 02:34:03.183: INFO: hyperauth-5cbb4d58b8-pqhc5 from hyperauth started at 2023-01-03 01:37:01 +0000 UTC (1 container statuses recorded)
    Jan  3 02:34:03.183: INFO: 	Container hyperauth ready: true, restart count 0
    Jan  3 02:34:03.183: INFO: calico-node-5f5qs from kube-system started at 2022-12-29 05:30:03 +0000 UTC (1 container statuses recorded)
    Jan  3 02:34:03.183: INFO: 	Container calico-node ready: true, restart count 0
    Jan  3 02:34:03.183: INFO: coredns-565d847f94-gqs74 from kube-system started at 2023-01-03 02:07:41 +0000 UTC (1 container statuses recorded)
    Jan  3 02:34:03.183: INFO: 	Container coredns ready: true, restart count 0
    Jan  3 02:34:03.183: INFO: kube-proxy-ddlsm from kube-system started at 2022-12-29 05:30:03 +0000 UTC (1 container statuses recorded)
    Jan  3 02:34:03.183: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  3 02:34:03.183: INFO: snapshot-controller-545d7f8994-qgm6l from kube-system started at 2022-12-29 08:24:10 +0000 UTC (1 container statuses recorded)
    Jan  3 02:34:03.183: INFO: 	Container snapshot-controller ready: true, restart count 0
    Jan  3 02:34:03.183: INFO: csi-cephfsplugin-hxtjf from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (2 container statuses recorded)
    Jan  3 02:34:03.183: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
    Jan  3 02:34:03.183: INFO: 	Container driver-registrar ready: true, restart count 0
    Jan  3 02:34:03.183: INFO: csi-cephfsplugin-provisioner-69f54fb566-xhc7t from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (5 container statuses recorded)
    Jan  3 02:34:03.183: INFO: 	Container csi-attacher ready: true, restart count 0
    Jan  3 02:34:03.183: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
    Jan  3 02:34:03.183: INFO: 	Container csi-provisioner ready: true, restart count 0
    Jan  3 02:34:03.183: INFO: 	Container csi-resizer ready: true, restart count 0
    Jan  3 02:34:03.183: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Jan  3 02:34:03.183: INFO: csi-rbdplugin-provisioner-6f689965cc-2mqrd from rook-ceph started at 2023-01-03 02:07:41 +0000 UTC (5 container statuses recorded)
    Jan  3 02:34:03.183: INFO: 	Container csi-attacher ready: true, restart count 0
    Jan  3 02:34:03.183: INFO: 	Container csi-provisioner ready: true, restart count 0
    Jan  3 02:34:03.183: INFO: 	Container csi-rbdplugin ready: true, restart count 0
    Jan  3 02:34:03.183: INFO: 	Container csi-resizer ready: true, restart count 0
    Jan  3 02:34:03.183: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Jan  3 02:34:03.183: INFO: csi-rbdplugin-qk6jk from rook-ceph started at 2022-12-29 08:24:18 +0000 UTC (2 container statuses recorded)
    Jan  3 02:34:03.183: INFO: 	Container csi-rbdplugin ready: true, restart count 0
    Jan  3 02:34:03.183: INFO: 	Container driver-registrar ready: true, restart count 0
    Jan  3 02:34:03.183: INFO: rook-ceph-crashcollector-cncf-node2-54fb78d5b8-nncld from rook-ceph started at 2023-01-03 02:07:41 +0000 UTC (1 container statuses recorded)
    Jan  3 02:34:03.183: INFO: 	Container ceph-crash ready: true, restart count 0
    Jan  3 02:34:03.184: INFO: rook-ceph-mds-myfs-b-65b48b8c9d-l4wlk from rook-ceph started at 2023-01-03 02:07:41 +0000 UTC (1 container statuses recorded)
    Jan  3 02:34:03.184: INFO: 	Container mds ready: true, restart count 0
    Jan  3 02:34:03.184: INFO: rook-ceph-mgr-b-856c746f6-lchkv from rook-ceph started at 2023-01-03 02:07:41 +0000 UTC (2 container statuses recorded)
    Jan  3 02:34:03.184: INFO: 	Container mgr ready: true, restart count 0
    Jan  3 02:34:03.184: INFO: 	Container watch-active ready: true, restart count 0
    Jan  3 02:34:03.184: INFO: rook-ceph-mon-c-6c468d6b45-m7rqj from rook-ceph started at 2022-12-29 08:25:33 +0000 UTC (1 container statuses recorded)
    Jan  3 02:34:03.184: INFO: 	Container mon ready: true, restart count 0
    Jan  3 02:34:03.184: INFO: rook-ceph-osd-0-7d877c8d99-hq5fw from rook-ceph started at 2023-01-02 01:35:02 +0000 UTC (1 container statuses recorded)
    Jan  3 02:34:03.184: INFO: 	Container osd ready: true, restart count 0
    Jan  3 02:34:03.184: INFO: rook-ceph-osd-prepare-cncf-node2-rnwb6 from rook-ceph started at 2023-01-03 01:40:59 +0000 UTC (1 container statuses recorded)
    Jan  3 02:34:03.184: INFO: 	Container provision ready: false, restart count 0
    Jan  3 02:34:03.184: INFO: sonobuoy-systemd-logs-daemon-set-58995b31e0564399-wjpkt from sonobuoy started at 2023-01-03 01:46:49 +0000 UTC (2 container statuses recorded)
    Jan  3 02:34:03.184: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  3 02:34:03.184: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:461
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/03/23 02:34:03.184
    Jan  3 02:34:03.189: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-8281" to be "running"
    Jan  3 02:34:03.190: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.386317ms
    Jan  3 02:34:05.208: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.01871028s
    Jan  3 02:34:05.208: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/03/23 02:34:05.209
    STEP: Trying to apply a random label on the found node. 01/03/23 02:34:05.225
    STEP: verifying the node has the label kubernetes.io/e2e-2b3ee44c-a831-4bbc-90f6-7f6c907ce381 42 01/03/23 02:34:05.232
    STEP: Trying to relaunch the pod, now with labels. 01/03/23 02:34:05.246
    Jan  3 02:34:05.251: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-8281" to be "not pending"
    Jan  3 02:34:05.253: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 2.418102ms
    Jan  3 02:34:07.257: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.005590598s
    Jan  3 02:34:07.257: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-2b3ee44c-a831-4bbc-90f6-7f6c907ce381 off the node cncf-master 01/03/23 02:34:07.258
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-2b3ee44c-a831-4bbc-90f6-7f6c907ce381 01/03/23 02:34:07.265
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jan  3 02:34:07.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-8281" for this suite. 01/03/23 02:34:07.279
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:34:07.282
Jan  3 02:34:07.282: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename gc 01/03/23 02:34:07.283
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:34:07.317
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:34:07.319
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 01/03/23 02:34:07.32
STEP: Wait for the Deployment to create new ReplicaSet 01/03/23 02:34:07.334
STEP: delete the deployment 01/03/23 02:34:07.847
STEP: wait for all rs to be garbage collected 01/03/23 02:34:07.849
STEP: expected 0 rs, got 1 rs 01/03/23 02:34:07.855
STEP: expected 0 pods, got 2 pods 01/03/23 02:34:07.872
STEP: Gathering metrics 01/03/23 02:34:08.377
Jan  3 02:34:08.404: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cncf-master" in namespace "kube-system" to be "running and ready"
Jan  3 02:34:08.406: INFO: Pod "kube-controller-manager-cncf-master": Phase="Running", Reason="", readiness=true. Elapsed: 1.55361ms
Jan  3 02:34:08.406: INFO: The phase of Pod kube-controller-manager-cncf-master is Running (Ready = true)
Jan  3 02:34:08.406: INFO: Pod "kube-controller-manager-cncf-master" satisfied condition "running and ready"
Jan  3 02:34:08.465: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan  3 02:34:08.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2073" for this suite. 01/03/23 02:34:08.468
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","completed":148,"skipped":2752,"failed":0}
------------------------------
• [1.196 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:34:07.282
    Jan  3 02:34:07.282: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename gc 01/03/23 02:34:07.283
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:34:07.317
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:34:07.319
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 01/03/23 02:34:07.32
    STEP: Wait for the Deployment to create new ReplicaSet 01/03/23 02:34:07.334
    STEP: delete the deployment 01/03/23 02:34:07.847
    STEP: wait for all rs to be garbage collected 01/03/23 02:34:07.849
    STEP: expected 0 rs, got 1 rs 01/03/23 02:34:07.855
    STEP: expected 0 pods, got 2 pods 01/03/23 02:34:07.872
    STEP: Gathering metrics 01/03/23 02:34:08.377
    Jan  3 02:34:08.404: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cncf-master" in namespace "kube-system" to be "running and ready"
    Jan  3 02:34:08.406: INFO: Pod "kube-controller-manager-cncf-master": Phase="Running", Reason="", readiness=true. Elapsed: 1.55361ms
    Jan  3 02:34:08.406: INFO: The phase of Pod kube-controller-manager-cncf-master is Running (Ready = true)
    Jan  3 02:34:08.406: INFO: Pod "kube-controller-manager-cncf-master" satisfied condition "running and ready"
    Jan  3 02:34:08.465: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan  3 02:34:08.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-2073" for this suite. 01/03/23 02:34:08.468
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:34:08.478
Jan  3 02:34:08.478: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename replicaset 01/03/23 02:34:08.479
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:34:08.489
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:34:08.491
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 01/03/23 02:34:08.493
Jan  3 02:34:08.533: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-4807" to be "running and ready"
Jan  3 02:34:08.540: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 7.295233ms
Jan  3 02:34:08.540: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan  3 02:34:10.543: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.010367069s
Jan  3 02:34:10.543: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Jan  3 02:34:10.543: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 01/03/23 02:34:10.545
STEP: Then the orphan pod is adopted 01/03/23 02:34:10.549
STEP: When the matched label of one of its pods change 01/03/23 02:34:11.58
Jan  3 02:34:11.582: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 01/03/23 02:34:11.587
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan  3 02:34:12.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4807" for this suite. 01/03/23 02:34:12.636
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","completed":149,"skipped":2763,"failed":0}
------------------------------
• [4.161 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:34:08.478
    Jan  3 02:34:08.478: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename replicaset 01/03/23 02:34:08.479
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:34:08.489
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:34:08.491
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 01/03/23 02:34:08.493
    Jan  3 02:34:08.533: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-4807" to be "running and ready"
    Jan  3 02:34:08.540: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 7.295233ms
    Jan  3 02:34:08.540: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 02:34:10.543: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.010367069s
    Jan  3 02:34:10.543: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Jan  3 02:34:10.543: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 01/03/23 02:34:10.545
    STEP: Then the orphan pod is adopted 01/03/23 02:34:10.549
    STEP: When the matched label of one of its pods change 01/03/23 02:34:11.58
    Jan  3 02:34:11.582: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 01/03/23 02:34:11.587
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan  3 02:34:12.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-4807" for this suite. 01/03/23 02:34:12.636
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:34:12.64
Jan  3 02:34:12.640: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename replication-controller 01/03/23 02:34:12.641
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:34:12.655
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:34:12.657
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
STEP: Given a Pod with a 'name' label pod-adoption is created 01/03/23 02:34:12.659
Jan  3 02:34:12.675: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-4417" to be "running and ready"
Jan  3 02:34:12.695: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 19.725714ms
Jan  3 02:34:12.695: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jan  3 02:34:14.697: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.022347875s
Jan  3 02:34:14.697: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Jan  3 02:34:14.697: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 01/03/23 02:34:14.699
STEP: Then the orphan pod is adopted 01/03/23 02:34:14.703
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan  3 02:34:15.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4417" for this suite. 01/03/23 02:34:15.719
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","completed":150,"skipped":2772,"failed":0}
------------------------------
• [3.082 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:34:12.64
    Jan  3 02:34:12.640: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename replication-controller 01/03/23 02:34:12.641
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:34:12.655
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:34:12.657
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:91
    STEP: Given a Pod with a 'name' label pod-adoption is created 01/03/23 02:34:12.659
    Jan  3 02:34:12.675: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-4417" to be "running and ready"
    Jan  3 02:34:12.695: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 19.725714ms
    Jan  3 02:34:12.695: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 02:34:14.697: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.022347875s
    Jan  3 02:34:14.697: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Jan  3 02:34:14.697: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 01/03/23 02:34:14.699
    STEP: Then the orphan pod is adopted 01/03/23 02:34:14.703
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan  3 02:34:15.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-4417" for this suite. 01/03/23 02:34:15.719
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:34:15.722
Jan  3 02:34:15.722: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename container-probe 01/03/23 02:34:15.723
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:34:15.736
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:34:15.737
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
STEP: Creating pod liveness-afae5ef6-58af-43ac-9de1-fd16f45d8a8a in namespace container-probe-6391 01/03/23 02:34:15.739
Jan  3 02:34:15.758: INFO: Waiting up to 5m0s for pod "liveness-afae5ef6-58af-43ac-9de1-fd16f45d8a8a" in namespace "container-probe-6391" to be "not pending"
Jan  3 02:34:15.764: INFO: Pod "liveness-afae5ef6-58af-43ac-9de1-fd16f45d8a8a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.798886ms
Jan  3 02:34:17.767: INFO: Pod "liveness-afae5ef6-58af-43ac-9de1-fd16f45d8a8a": Phase="Running", Reason="", readiness=true. Elapsed: 2.009624736s
Jan  3 02:34:17.767: INFO: Pod "liveness-afae5ef6-58af-43ac-9de1-fd16f45d8a8a" satisfied condition "not pending"
Jan  3 02:34:17.767: INFO: Started pod liveness-afae5ef6-58af-43ac-9de1-fd16f45d8a8a in namespace container-probe-6391
STEP: checking the pod's current state and verifying that restartCount is present 01/03/23 02:34:17.767
Jan  3 02:34:17.769: INFO: Initial restart count of pod liveness-afae5ef6-58af-43ac-9de1-fd16f45d8a8a is 0
Jan  3 02:34:37.874: INFO: Restart count of pod container-probe-6391/liveness-afae5ef6-58af-43ac-9de1-fd16f45d8a8a is now 1 (20.105495717s elapsed)
Jan  3 02:34:57.904: INFO: Restart count of pod container-probe-6391/liveness-afae5ef6-58af-43ac-9de1-fd16f45d8a8a is now 2 (40.135226562s elapsed)
Jan  3 02:35:17.932: INFO: Restart count of pod container-probe-6391/liveness-afae5ef6-58af-43ac-9de1-fd16f45d8a8a is now 3 (1m0.162871027s elapsed)
Jan  3 02:35:38.034: INFO: Restart count of pod container-probe-6391/liveness-afae5ef6-58af-43ac-9de1-fd16f45d8a8a is now 4 (1m20.265311725s elapsed)
Jan  3 02:36:40.143: INFO: Restart count of pod container-probe-6391/liveness-afae5ef6-58af-43ac-9de1-fd16f45d8a8a is now 5 (2m22.374680905s elapsed)
STEP: deleting the pod 01/03/23 02:36:40.143
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan  3 02:36:40.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6391" for this suite. 01/03/23 02:36:40.225
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","completed":151,"skipped":2778,"failed":0}
------------------------------
• [SLOW TEST] [144.506 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:34:15.722
    Jan  3 02:34:15.722: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename container-probe 01/03/23 02:34:15.723
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:34:15.736
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:34:15.737
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:195
    STEP: Creating pod liveness-afae5ef6-58af-43ac-9de1-fd16f45d8a8a in namespace container-probe-6391 01/03/23 02:34:15.739
    Jan  3 02:34:15.758: INFO: Waiting up to 5m0s for pod "liveness-afae5ef6-58af-43ac-9de1-fd16f45d8a8a" in namespace "container-probe-6391" to be "not pending"
    Jan  3 02:34:15.764: INFO: Pod "liveness-afae5ef6-58af-43ac-9de1-fd16f45d8a8a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.798886ms
    Jan  3 02:34:17.767: INFO: Pod "liveness-afae5ef6-58af-43ac-9de1-fd16f45d8a8a": Phase="Running", Reason="", readiness=true. Elapsed: 2.009624736s
    Jan  3 02:34:17.767: INFO: Pod "liveness-afae5ef6-58af-43ac-9de1-fd16f45d8a8a" satisfied condition "not pending"
    Jan  3 02:34:17.767: INFO: Started pod liveness-afae5ef6-58af-43ac-9de1-fd16f45d8a8a in namespace container-probe-6391
    STEP: checking the pod's current state and verifying that restartCount is present 01/03/23 02:34:17.767
    Jan  3 02:34:17.769: INFO: Initial restart count of pod liveness-afae5ef6-58af-43ac-9de1-fd16f45d8a8a is 0
    Jan  3 02:34:37.874: INFO: Restart count of pod container-probe-6391/liveness-afae5ef6-58af-43ac-9de1-fd16f45d8a8a is now 1 (20.105495717s elapsed)
    Jan  3 02:34:57.904: INFO: Restart count of pod container-probe-6391/liveness-afae5ef6-58af-43ac-9de1-fd16f45d8a8a is now 2 (40.135226562s elapsed)
    Jan  3 02:35:17.932: INFO: Restart count of pod container-probe-6391/liveness-afae5ef6-58af-43ac-9de1-fd16f45d8a8a is now 3 (1m0.162871027s elapsed)
    Jan  3 02:35:38.034: INFO: Restart count of pod container-probe-6391/liveness-afae5ef6-58af-43ac-9de1-fd16f45d8a8a is now 4 (1m20.265311725s elapsed)
    Jan  3 02:36:40.143: INFO: Restart count of pod container-probe-6391/liveness-afae5ef6-58af-43ac-9de1-fd16f45d8a8a is now 5 (2m22.374680905s elapsed)
    STEP: deleting the pod 01/03/23 02:36:40.143
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan  3 02:36:40.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-6391" for this suite. 01/03/23 02:36:40.225
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:36:40.229
Jan  3 02:36:40.229: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename downward-api 01/03/23 02:36:40.23
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:36:40.242
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:36:40.244
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
STEP: Creating a pod to test downward API volume plugin 01/03/23 02:36:40.255
Jan  3 02:36:40.261: INFO: Waiting up to 5m0s for pod "downwardapi-volume-627098eb-66a1-4847-a790-72e6425443cf" in namespace "downward-api-2860" to be "Succeeded or Failed"
Jan  3 02:36:40.272: INFO: Pod "downwardapi-volume-627098eb-66a1-4847-a790-72e6425443cf": Phase="Pending", Reason="", readiness=false. Elapsed: 11.113196ms
Jan  3 02:36:42.275: INFO: Pod "downwardapi-volume-627098eb-66a1-4847-a790-72e6425443cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014154726s
Jan  3 02:36:44.275: INFO: Pod "downwardapi-volume-627098eb-66a1-4847-a790-72e6425443cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013820171s
STEP: Saw pod success 01/03/23 02:36:44.275
Jan  3 02:36:44.275: INFO: Pod "downwardapi-volume-627098eb-66a1-4847-a790-72e6425443cf" satisfied condition "Succeeded or Failed"
Jan  3 02:36:44.276: INFO: Trying to get logs from node cncf-master pod downwardapi-volume-627098eb-66a1-4847-a790-72e6425443cf container client-container: <nil>
STEP: delete the pod 01/03/23 02:36:44.287
Jan  3 02:36:44.304: INFO: Waiting for pod downwardapi-volume-627098eb-66a1-4847-a790-72e6425443cf to disappear
Jan  3 02:36:44.310: INFO: Pod downwardapi-volume-627098eb-66a1-4847-a790-72e6425443cf no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan  3 02:36:44.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2860" for this suite. 01/03/23 02:36:44.312
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","completed":152,"skipped":2810,"failed":0}
------------------------------
• [4.086 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:36:40.229
    Jan  3 02:36:40.229: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename downward-api 01/03/23 02:36:40.23
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:36:40.242
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:36:40.244
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:206
    STEP: Creating a pod to test downward API volume plugin 01/03/23 02:36:40.255
    Jan  3 02:36:40.261: INFO: Waiting up to 5m0s for pod "downwardapi-volume-627098eb-66a1-4847-a790-72e6425443cf" in namespace "downward-api-2860" to be "Succeeded or Failed"
    Jan  3 02:36:40.272: INFO: Pod "downwardapi-volume-627098eb-66a1-4847-a790-72e6425443cf": Phase="Pending", Reason="", readiness=false. Elapsed: 11.113196ms
    Jan  3 02:36:42.275: INFO: Pod "downwardapi-volume-627098eb-66a1-4847-a790-72e6425443cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014154726s
    Jan  3 02:36:44.275: INFO: Pod "downwardapi-volume-627098eb-66a1-4847-a790-72e6425443cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013820171s
    STEP: Saw pod success 01/03/23 02:36:44.275
    Jan  3 02:36:44.275: INFO: Pod "downwardapi-volume-627098eb-66a1-4847-a790-72e6425443cf" satisfied condition "Succeeded or Failed"
    Jan  3 02:36:44.276: INFO: Trying to get logs from node cncf-master pod downwardapi-volume-627098eb-66a1-4847-a790-72e6425443cf container client-container: <nil>
    STEP: delete the pod 01/03/23 02:36:44.287
    Jan  3 02:36:44.304: INFO: Waiting for pod downwardapi-volume-627098eb-66a1-4847-a790-72e6425443cf to disappear
    Jan  3 02:36:44.310: INFO: Pod downwardapi-volume-627098eb-66a1-4847-a790-72e6425443cf no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan  3 02:36:44.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2860" for this suite. 01/03/23 02:36:44.312
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:36:44.316
Jan  3 02:36:44.316: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename events 01/03/23 02:36:44.317
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:36:44.327
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:36:44.329
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 01/03/23 02:36:44.331
Jan  3 02:36:44.344: INFO: created test-event-1
Jan  3 02:36:44.386: INFO: created test-event-2
Jan  3 02:36:44.389: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 01/03/23 02:36:44.389
STEP: delete collection of events 01/03/23 02:36:44.395
Jan  3 02:36:44.396: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 01/03/23 02:36:44.408
Jan  3 02:36:44.408: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Jan  3 02:36:44.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-61" for this suite. 01/03/23 02:36:44.415
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","completed":153,"skipped":2842,"failed":0}
------------------------------
• [0.101 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:36:44.316
    Jan  3 02:36:44.316: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename events 01/03/23 02:36:44.317
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:36:44.327
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:36:44.329
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 01/03/23 02:36:44.331
    Jan  3 02:36:44.344: INFO: created test-event-1
    Jan  3 02:36:44.386: INFO: created test-event-2
    Jan  3 02:36:44.389: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 01/03/23 02:36:44.389
    STEP: delete collection of events 01/03/23 02:36:44.395
    Jan  3 02:36:44.396: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 01/03/23 02:36:44.408
    Jan  3 02:36:44.408: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Jan  3 02:36:44.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-61" for this suite. 01/03/23 02:36:44.415
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:36:44.419
Jan  3 02:36:44.419: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename kubectl 01/03/23 02:36:44.42
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:36:44.43
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:36:44.432
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
STEP: creating Agnhost RC 01/03/23 02:36:44.433
Jan  3 02:36:44.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-1227 create -f -'
Jan  3 02:36:44.649: INFO: stderr: ""
Jan  3 02:36:44.649: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/03/23 02:36:44.649
Jan  3 02:36:45.652: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  3 02:36:45.652: INFO: Found 0 / 1
Jan  3 02:36:46.652: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  3 02:36:46.652: INFO: Found 1 / 1
Jan  3 02:36:46.652: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 01/03/23 02:36:46.652
Jan  3 02:36:46.653: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  3 02:36:46.653: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan  3 02:36:46.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-1227 patch pod agnhost-primary-85hbn -p {"metadata":{"annotations":{"x":"y"}}}'
Jan  3 02:36:46.731: INFO: stderr: ""
Jan  3 02:36:46.731: INFO: stdout: "pod/agnhost-primary-85hbn patched\n"
STEP: checking annotations 01/03/23 02:36:46.731
Jan  3 02:36:46.774: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  3 02:36:46.775: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan  3 02:36:46.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1227" for this suite. 01/03/23 02:36:46.777
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","completed":154,"skipped":2859,"failed":0}
------------------------------
• [2.362 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1644
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:36:44.419
    Jan  3 02:36:44.419: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename kubectl 01/03/23 02:36:44.42
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:36:44.43
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:36:44.432
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1650
    STEP: creating Agnhost RC 01/03/23 02:36:44.433
    Jan  3 02:36:44.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-1227 create -f -'
    Jan  3 02:36:44.649: INFO: stderr: ""
    Jan  3 02:36:44.649: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/03/23 02:36:44.649
    Jan  3 02:36:45.652: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  3 02:36:45.652: INFO: Found 0 / 1
    Jan  3 02:36:46.652: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  3 02:36:46.652: INFO: Found 1 / 1
    Jan  3 02:36:46.652: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 01/03/23 02:36:46.652
    Jan  3 02:36:46.653: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  3 02:36:46.653: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan  3 02:36:46.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-1227 patch pod agnhost-primary-85hbn -p {"metadata":{"annotations":{"x":"y"}}}'
    Jan  3 02:36:46.731: INFO: stderr: ""
    Jan  3 02:36:46.731: INFO: stdout: "pod/agnhost-primary-85hbn patched\n"
    STEP: checking annotations 01/03/23 02:36:46.731
    Jan  3 02:36:46.774: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  3 02:36:46.775: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan  3 02:36:46.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1227" for this suite. 01/03/23 02:36:46.777
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:36:46.78
Jan  3 02:36:46.780: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename var-expansion 01/03/23 02:36:46.781
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:36:46.803
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:36:46.805
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
Jan  3 02:36:46.812: INFO: Waiting up to 2m0s for pod "var-expansion-c398f62b-0e56-4ab2-a5fe-96ed136def1a" in namespace "var-expansion-9202" to be "container 0 failed with reason CreateContainerConfigError"
Jan  3 02:36:46.826: INFO: Pod "var-expansion-c398f62b-0e56-4ab2-a5fe-96ed136def1a": Phase="Pending", Reason="", readiness=false. Elapsed: 13.403967ms
Jan  3 02:36:48.829: INFO: Pod "var-expansion-c398f62b-0e56-4ab2-a5fe-96ed136def1a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01642115s
Jan  3 02:36:48.829: INFO: Pod "var-expansion-c398f62b-0e56-4ab2-a5fe-96ed136def1a" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jan  3 02:36:48.829: INFO: Deleting pod "var-expansion-c398f62b-0e56-4ab2-a5fe-96ed136def1a" in namespace "var-expansion-9202"
Jan  3 02:36:48.832: INFO: Wait up to 5m0s for pod "var-expansion-c398f62b-0e56-4ab2-a5fe-96ed136def1a" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan  3 02:36:52.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9202" for this suite. 01/03/23 02:36:52.885
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","completed":155,"skipped":2862,"failed":0}
------------------------------
• [SLOW TEST] [6.107 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:36:46.78
    Jan  3 02:36:46.780: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename var-expansion 01/03/23 02:36:46.781
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:36:46.803
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:36:46.805
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:151
    Jan  3 02:36:46.812: INFO: Waiting up to 2m0s for pod "var-expansion-c398f62b-0e56-4ab2-a5fe-96ed136def1a" in namespace "var-expansion-9202" to be "container 0 failed with reason CreateContainerConfigError"
    Jan  3 02:36:46.826: INFO: Pod "var-expansion-c398f62b-0e56-4ab2-a5fe-96ed136def1a": Phase="Pending", Reason="", readiness=false. Elapsed: 13.403967ms
    Jan  3 02:36:48.829: INFO: Pod "var-expansion-c398f62b-0e56-4ab2-a5fe-96ed136def1a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01642115s
    Jan  3 02:36:48.829: INFO: Pod "var-expansion-c398f62b-0e56-4ab2-a5fe-96ed136def1a" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jan  3 02:36:48.829: INFO: Deleting pod "var-expansion-c398f62b-0e56-4ab2-a5fe-96ed136def1a" in namespace "var-expansion-9202"
    Jan  3 02:36:48.832: INFO: Wait up to 5m0s for pod "var-expansion-c398f62b-0e56-4ab2-a5fe-96ed136def1a" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan  3 02:36:52.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-9202" for this suite. 01/03/23 02:36:52.885
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:36:52.888
Jan  3 02:36:52.888: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename containers 01/03/23 02:36:52.889
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:36:52.899
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:36:52.901
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
Jan  3 02:36:52.917: INFO: Waiting up to 5m0s for pod "client-containers-9c5bcc37-70ed-4498-962d-184b026f4a02" in namespace "containers-9436" to be "running"
Jan  3 02:36:52.922: INFO: Pod "client-containers-9c5bcc37-70ed-4498-962d-184b026f4a02": Phase="Pending", Reason="", readiness=false. Elapsed: 5.911947ms
Jan  3 02:36:54.925: INFO: Pod "client-containers-9c5bcc37-70ed-4498-962d-184b026f4a02": Phase="Running", Reason="", readiness=true. Elapsed: 2.008267377s
Jan  3 02:36:54.925: INFO: Pod "client-containers-9c5bcc37-70ed-4498-962d-184b026f4a02" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jan  3 02:36:54.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9436" for this suite. 01/03/23 02:36:54.931
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","completed":156,"skipped":2867,"failed":0}
------------------------------
• [2.046 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:36:52.888
    Jan  3 02:36:52.888: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename containers 01/03/23 02:36:52.889
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:36:52.899
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:36:52.901
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:38
    Jan  3 02:36:52.917: INFO: Waiting up to 5m0s for pod "client-containers-9c5bcc37-70ed-4498-962d-184b026f4a02" in namespace "containers-9436" to be "running"
    Jan  3 02:36:52.922: INFO: Pod "client-containers-9c5bcc37-70ed-4498-962d-184b026f4a02": Phase="Pending", Reason="", readiness=false. Elapsed: 5.911947ms
    Jan  3 02:36:54.925: INFO: Pod "client-containers-9c5bcc37-70ed-4498-962d-184b026f4a02": Phase="Running", Reason="", readiness=true. Elapsed: 2.008267377s
    Jan  3 02:36:54.925: INFO: Pod "client-containers-9c5bcc37-70ed-4498-962d-184b026f4a02" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jan  3 02:36:54.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-9436" for this suite. 01/03/23 02:36:54.931
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:36:54.934
Jan  3 02:36:54.934: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename configmap 01/03/23 02:36:54.935
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:36:54.951
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:36:54.953
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
STEP: Creating configMap that has name configmap-test-emptyKey-7acae184-1189-4ce6-b325-bea38f42cf3e 01/03/23 02:36:54.955
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jan  3 02:36:54.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6230" for this suite. 01/03/23 02:36:54.968
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","completed":157,"skipped":2867,"failed":0}
------------------------------
• [0.037 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:36:54.934
    Jan  3 02:36:54.934: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename configmap 01/03/23 02:36:54.935
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:36:54.951
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:36:54.953
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:137
    STEP: Creating configMap that has name configmap-test-emptyKey-7acae184-1189-4ce6-b325-bea38f42cf3e 01/03/23 02:36:54.955
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jan  3 02:36:54.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6230" for this suite. 01/03/23 02:36:54.968
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:36:54.972
Jan  3 02:36:54.972: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename pods 01/03/23 02:36:54.972
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:36:54.979
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:36:54.981
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
Jan  3 02:36:54.983: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: creating the pod 01/03/23 02:36:54.983
STEP: submitting the pod to kubernetes 01/03/23 02:36:54.984
Jan  3 02:36:55.024: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-8c6a390b-b67a-4e65-a053-81f5b360ef47" in namespace "pods-6371" to be "running and ready"
Jan  3 02:36:55.031: INFO: Pod "pod-logs-websocket-8c6a390b-b67a-4e65-a053-81f5b360ef47": Phase="Pending", Reason="", readiness=false. Elapsed: 7.503358ms
Jan  3 02:36:55.031: INFO: The phase of Pod pod-logs-websocket-8c6a390b-b67a-4e65-a053-81f5b360ef47 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 02:36:57.034: INFO: Pod "pod-logs-websocket-8c6a390b-b67a-4e65-a053-81f5b360ef47": Phase="Running", Reason="", readiness=true. Elapsed: 2.009965331s
Jan  3 02:36:57.034: INFO: The phase of Pod pod-logs-websocket-8c6a390b-b67a-4e65-a053-81f5b360ef47 is Running (Ready = true)
Jan  3 02:36:57.034: INFO: Pod "pod-logs-websocket-8c6a390b-b67a-4e65-a053-81f5b360ef47" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan  3 02:36:57.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6371" for this suite. 01/03/23 02:36:57.045
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","completed":158,"skipped":2885,"failed":0}
------------------------------
• [2.076 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:36:54.972
    Jan  3 02:36:54.972: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename pods 01/03/23 02:36:54.972
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:36:54.979
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:36:54.981
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:617
    Jan  3 02:36:54.983: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: creating the pod 01/03/23 02:36:54.983
    STEP: submitting the pod to kubernetes 01/03/23 02:36:54.984
    Jan  3 02:36:55.024: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-8c6a390b-b67a-4e65-a053-81f5b360ef47" in namespace "pods-6371" to be "running and ready"
    Jan  3 02:36:55.031: INFO: Pod "pod-logs-websocket-8c6a390b-b67a-4e65-a053-81f5b360ef47": Phase="Pending", Reason="", readiness=false. Elapsed: 7.503358ms
    Jan  3 02:36:55.031: INFO: The phase of Pod pod-logs-websocket-8c6a390b-b67a-4e65-a053-81f5b360ef47 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 02:36:57.034: INFO: Pod "pod-logs-websocket-8c6a390b-b67a-4e65-a053-81f5b360ef47": Phase="Running", Reason="", readiness=true. Elapsed: 2.009965331s
    Jan  3 02:36:57.034: INFO: The phase of Pod pod-logs-websocket-8c6a390b-b67a-4e65-a053-81f5b360ef47 is Running (Ready = true)
    Jan  3 02:36:57.034: INFO: Pod "pod-logs-websocket-8c6a390b-b67a-4e65-a053-81f5b360ef47" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan  3 02:36:57.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-6371" for this suite. 01/03/23 02:36:57.045
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:36:57.048
Jan  3 02:36:57.048: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename services 01/03/23 02:36:57.048
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:36:57.066
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:36:57.067
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6038 01/03/23 02:36:57.069
STEP: changing the ExternalName service to type=ClusterIP 01/03/23 02:36:57.1
STEP: creating replication controller externalname-service in namespace services-6038 01/03/23 02:36:57.152
I0103 02:36:57.163628      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6038, replica count: 2
I0103 02:37:00.216171      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  3 02:37:00.216: INFO: Creating new exec pod
Jan  3 02:37:00.221: INFO: Waiting up to 5m0s for pod "execpodklhj5" in namespace "services-6038" to be "running"
Jan  3 02:37:00.244: INFO: Pod "execpodklhj5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.667637ms
Jan  3 02:37:02.261: INFO: Pod "execpodklhj5": Phase="Running", Reason="", readiness=true. Elapsed: 2.03977942s
Jan  3 02:37:02.261: INFO: Pod "execpodklhj5" satisfied condition "running"
Jan  3 02:37:03.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-6038 exec execpodklhj5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan  3 02:37:03.397: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan  3 02:37:03.397: INFO: stdout: "externalname-service-tpf45"
Jan  3 02:37:03.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-6038 exec execpodklhj5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.127.136 80'
Jan  3 02:37:03.517: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.127.136 80\nConnection to 10.96.127.136 80 port [tcp/http] succeeded!\n"
Jan  3 02:37:03.517: INFO: stdout: "externalname-service-tpf45"
Jan  3 02:37:03.517: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  3 02:37:03.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6038" for this suite. 01/03/23 02:37:03.552
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","completed":159,"skipped":2889,"failed":0}
------------------------------
• [SLOW TEST] [6.512 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:36:57.048
    Jan  3 02:36:57.048: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename services 01/03/23 02:36:57.048
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:36:57.066
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:36:57.067
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1404
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-6038 01/03/23 02:36:57.069
    STEP: changing the ExternalName service to type=ClusterIP 01/03/23 02:36:57.1
    STEP: creating replication controller externalname-service in namespace services-6038 01/03/23 02:36:57.152
    I0103 02:36:57.163628      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6038, replica count: 2
    I0103 02:37:00.216171      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  3 02:37:00.216: INFO: Creating new exec pod
    Jan  3 02:37:00.221: INFO: Waiting up to 5m0s for pod "execpodklhj5" in namespace "services-6038" to be "running"
    Jan  3 02:37:00.244: INFO: Pod "execpodklhj5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.667637ms
    Jan  3 02:37:02.261: INFO: Pod "execpodklhj5": Phase="Running", Reason="", readiness=true. Elapsed: 2.03977942s
    Jan  3 02:37:02.261: INFO: Pod "execpodklhj5" satisfied condition "running"
    Jan  3 02:37:03.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-6038 exec execpodklhj5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jan  3 02:37:03.397: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan  3 02:37:03.397: INFO: stdout: "externalname-service-tpf45"
    Jan  3 02:37:03.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-6038 exec execpodklhj5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.127.136 80'
    Jan  3 02:37:03.517: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.127.136 80\nConnection to 10.96.127.136 80 port [tcp/http] succeeded!\n"
    Jan  3 02:37:03.517: INFO: stdout: "externalname-service-tpf45"
    Jan  3 02:37:03.517: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  3 02:37:03.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6038" for this suite. 01/03/23 02:37:03.552
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:37:03.56
Jan  3 02:37:03.560: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename projected 01/03/23 02:37:03.561
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:37:03.57
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:37:03.571
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
STEP: Creating the pod 01/03/23 02:37:03.573
Jan  3 02:37:03.589: INFO: Waiting up to 5m0s for pod "labelsupdate3e188342-9e77-43b4-bf25-519aa46914fe" in namespace "projected-1153" to be "running and ready"
Jan  3 02:37:03.593: INFO: Pod "labelsupdate3e188342-9e77-43b4-bf25-519aa46914fe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.061241ms
Jan  3 02:37:03.593: INFO: The phase of Pod labelsupdate3e188342-9e77-43b4-bf25-519aa46914fe is Pending, waiting for it to be Running (with Ready = true)
Jan  3 02:37:05.595: INFO: Pod "labelsupdate3e188342-9e77-43b4-bf25-519aa46914fe": Phase="Running", Reason="", readiness=true. Elapsed: 2.006340092s
Jan  3 02:37:05.595: INFO: The phase of Pod labelsupdate3e188342-9e77-43b4-bf25-519aa46914fe is Running (Ready = true)
Jan  3 02:37:05.595: INFO: Pod "labelsupdate3e188342-9e77-43b4-bf25-519aa46914fe" satisfied condition "running and ready"
Jan  3 02:37:06.122: INFO: Successfully updated pod "labelsupdate3e188342-9e77-43b4-bf25-519aa46914fe"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan  3 02:37:08.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1153" for this suite. 01/03/23 02:37:08.145
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","completed":160,"skipped":2910,"failed":0}
------------------------------
• [4.588 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:37:03.56
    Jan  3 02:37:03.560: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename projected 01/03/23 02:37:03.561
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:37:03.57
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:37:03.571
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:129
    STEP: Creating the pod 01/03/23 02:37:03.573
    Jan  3 02:37:03.589: INFO: Waiting up to 5m0s for pod "labelsupdate3e188342-9e77-43b4-bf25-519aa46914fe" in namespace "projected-1153" to be "running and ready"
    Jan  3 02:37:03.593: INFO: Pod "labelsupdate3e188342-9e77-43b4-bf25-519aa46914fe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.061241ms
    Jan  3 02:37:03.593: INFO: The phase of Pod labelsupdate3e188342-9e77-43b4-bf25-519aa46914fe is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 02:37:05.595: INFO: Pod "labelsupdate3e188342-9e77-43b4-bf25-519aa46914fe": Phase="Running", Reason="", readiness=true. Elapsed: 2.006340092s
    Jan  3 02:37:05.595: INFO: The phase of Pod labelsupdate3e188342-9e77-43b4-bf25-519aa46914fe is Running (Ready = true)
    Jan  3 02:37:05.595: INFO: Pod "labelsupdate3e188342-9e77-43b4-bf25-519aa46914fe" satisfied condition "running and ready"
    Jan  3 02:37:06.122: INFO: Successfully updated pod "labelsupdate3e188342-9e77-43b4-bf25-519aa46914fe"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan  3 02:37:08.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1153" for this suite. 01/03/23 02:37:08.145
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:37:08.149
Jan  3 02:37:08.149: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename custom-resource-definition 01/03/23 02:37:08.15
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:37:08.164
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:37:08.166
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Jan  3 02:37:08.168: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  3 02:37:11.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3040" for this suite. 01/03/23 02:37:11.299
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","completed":161,"skipped":2915,"failed":0}
------------------------------
• [3.153 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:37:08.149
    Jan  3 02:37:08.149: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename custom-resource-definition 01/03/23 02:37:08.15
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:37:08.164
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:37:08.166
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Jan  3 02:37:08.168: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  3 02:37:11.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-3040" for this suite. 01/03/23 02:37:11.299
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:37:11.302
Jan  3 02:37:11.302: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename ingress 01/03/23 02:37:11.303
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:37:11.314
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:37:11.316
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 01/03/23 02:37:11.317
STEP: getting /apis/networking.k8s.io 01/03/23 02:37:11.319
STEP: getting /apis/networking.k8s.iov1 01/03/23 02:37:11.32
STEP: creating 01/03/23 02:37:11.321
STEP: getting 01/03/23 02:37:11.342
STEP: listing 01/03/23 02:37:11.361
STEP: watching 01/03/23 02:37:11.363
Jan  3 02:37:11.363: INFO: starting watch
STEP: cluster-wide listing 01/03/23 02:37:11.364
STEP: cluster-wide watching 01/03/23 02:37:11.365
Jan  3 02:37:11.365: INFO: starting watch
STEP: patching 01/03/23 02:37:11.366
STEP: updating 01/03/23 02:37:11.376
Jan  3 02:37:11.415: INFO: waiting for watch events with expected annotations
Jan  3 02:37:11.415: INFO: saw patched and updated annotations
STEP: patching /status 01/03/23 02:37:11.415
STEP: updating /status 01/03/23 02:37:11.418
STEP: get /status 01/03/23 02:37:11.424
STEP: deleting 01/03/23 02:37:11.446
STEP: deleting a collection 01/03/23 02:37:11.452
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:187
Jan  3 02:37:11.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-4697" for this suite. 01/03/23 02:37:11.465
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","completed":162,"skipped":2940,"failed":0}
------------------------------
• [0.165 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:37:11.302
    Jan  3 02:37:11.302: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename ingress 01/03/23 02:37:11.303
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:37:11.314
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:37:11.316
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 01/03/23 02:37:11.317
    STEP: getting /apis/networking.k8s.io 01/03/23 02:37:11.319
    STEP: getting /apis/networking.k8s.iov1 01/03/23 02:37:11.32
    STEP: creating 01/03/23 02:37:11.321
    STEP: getting 01/03/23 02:37:11.342
    STEP: listing 01/03/23 02:37:11.361
    STEP: watching 01/03/23 02:37:11.363
    Jan  3 02:37:11.363: INFO: starting watch
    STEP: cluster-wide listing 01/03/23 02:37:11.364
    STEP: cluster-wide watching 01/03/23 02:37:11.365
    Jan  3 02:37:11.365: INFO: starting watch
    STEP: patching 01/03/23 02:37:11.366
    STEP: updating 01/03/23 02:37:11.376
    Jan  3 02:37:11.415: INFO: waiting for watch events with expected annotations
    Jan  3 02:37:11.415: INFO: saw patched and updated annotations
    STEP: patching /status 01/03/23 02:37:11.415
    STEP: updating /status 01/03/23 02:37:11.418
    STEP: get /status 01/03/23 02:37:11.424
    STEP: deleting 01/03/23 02:37:11.446
    STEP: deleting a collection 01/03/23 02:37:11.452
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:187
    Jan  3 02:37:11.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingress-4697" for this suite. 01/03/23 02:37:11.465
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:37:11.468
Jan  3 02:37:11.468: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename proxy 01/03/23 02:37:11.468
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:37:11.48
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:37:11.482
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 01/03/23 02:37:11.51
STEP: creating replication controller proxy-service-czh5m in namespace proxy-7990 01/03/23 02:37:11.51
I0103 02:37:11.550170      23 runners.go:193] Created replication controller with name: proxy-service-czh5m, namespace: proxy-7990, replica count: 1
I0103 02:37:12.601896      23 runners.go:193] proxy-service-czh5m Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0103 02:37:13.602610      23 runners.go:193] proxy-service-czh5m Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  3 02:37:13.604: INFO: setup took 2.120878106s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 01/03/23 02:37:13.604
Jan  3 02:37:13.607: INFO: (0) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:160/proxy/: foo (200; 2.572367ms)
Jan  3 02:37:13.607: INFO: (0) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:160/proxy/: foo (200; 2.753026ms)
Jan  3 02:37:13.607: INFO: (0) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:162/proxy/: bar (200; 2.896424ms)
Jan  3 02:37:13.607: INFO: (0) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:162/proxy/: bar (200; 2.845123ms)
Jan  3 02:37:13.608: INFO: (0) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/rewriteme">test<... (200; 3.498173ms)
Jan  3 02:37:13.608: INFO: (0) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname1/proxy/: foo (200; 3.600015ms)
Jan  3 02:37:13.608: INFO: (0) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname2/proxy/: bar (200; 3.564465ms)
Jan  3 02:37:13.609: INFO: (0) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname2/proxy/: bar (200; 4.284648ms)
Jan  3 02:37:13.609: INFO: (0) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/rewriteme">... (200; 4.32241ms)
Jan  3 02:37:13.609: INFO: (0) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/rewriteme">test</a> (200; 4.354181ms)
Jan  3 02:37:13.609: INFO: (0) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname1/proxy/: foo (200; 4.345404ms)
Jan  3 02:37:13.613: INFO: (0) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:462/proxy/: tls qux (200; 8.430004ms)
Jan  3 02:37:13.613: INFO: (0) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname2/proxy/: tls qux (200; 8.459679ms)
Jan  3 02:37:13.614: INFO: (0) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname1/proxy/: tls baz (200; 9.611222ms)
Jan  3 02:37:13.614: INFO: (0) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:460/proxy/: tls baz (200; 9.645876ms)
Jan  3 02:37:13.614: INFO: (0) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/tlsrewritem... (200; 9.857626ms)
Jan  3 02:37:13.617: INFO: (1) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/rewriteme">test</a> (200; 2.527608ms)
Jan  3 02:37:13.617: INFO: (1) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:160/proxy/: foo (200; 2.617584ms)
Jan  3 02:37:13.617: INFO: (1) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:162/proxy/: bar (200; 2.618684ms)
Jan  3 02:37:13.617: INFO: (1) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/rewriteme">test<... (200; 2.72146ms)
Jan  3 02:37:13.617: INFO: (1) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname2/proxy/: bar (200; 2.763547ms)
Jan  3 02:37:13.618: INFO: (1) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname1/proxy/: foo (200; 3.297951ms)
Jan  3 02:37:13.618: INFO: (1) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname2/proxy/: tls qux (200; 3.241215ms)
Jan  3 02:37:13.618: INFO: (1) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:162/proxy/: bar (200; 3.30435ms)
Jan  3 02:37:13.618: INFO: (1) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname2/proxy/: bar (200; 3.273971ms)
Jan  3 02:37:13.618: INFO: (1) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname1/proxy/: tls baz (200; 3.271436ms)
Jan  3 02:37:13.618: INFO: (1) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/rewriteme">... (200; 3.347552ms)
Jan  3 02:37:13.618: INFO: (1) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname1/proxy/: foo (200; 3.256027ms)
Jan  3 02:37:13.618: INFO: (1) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:160/proxy/: foo (200; 3.453853ms)
Jan  3 02:37:13.618: INFO: (1) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:460/proxy/: tls baz (200; 3.932327ms)
Jan  3 02:37:13.618: INFO: (1) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/tlsrewritem... (200; 4.123507ms)
Jan  3 02:37:13.619: INFO: (1) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:462/proxy/: tls qux (200; 4.343923ms)
Jan  3 02:37:13.621: INFO: (2) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:462/proxy/: tls qux (200; 2.024706ms)
Jan  3 02:37:13.625: INFO: (2) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname1/proxy/: tls baz (200; 6.303643ms)
Jan  3 02:37:13.625: INFO: (2) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname1/proxy/: foo (200; 6.409005ms)
Jan  3 02:37:13.625: INFO: (2) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname1/proxy/: foo (200; 6.31371ms)
Jan  3 02:37:13.625: INFO: (2) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname2/proxy/: tls qux (200; 6.376955ms)
Jan  3 02:37:13.625: INFO: (2) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname2/proxy/: bar (200; 6.344856ms)
Jan  3 02:37:13.626: INFO: (2) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname2/proxy/: bar (200; 6.972156ms)
Jan  3 02:37:13.626: INFO: (2) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/rewriteme">test<... (200; 6.965056ms)
Jan  3 02:37:13.626: INFO: (2) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:162/proxy/: bar (200; 6.995562ms)
Jan  3 02:37:13.626: INFO: (2) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/rewriteme">... (200; 7.351985ms)
Jan  3 02:37:13.626: INFO: (2) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:162/proxy/: bar (200; 7.369944ms)
Jan  3 02:37:13.626: INFO: (2) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:160/proxy/: foo (200; 7.346443ms)
Jan  3 02:37:13.626: INFO: (2) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/rewriteme">test</a> (200; 7.41265ms)
Jan  3 02:37:13.626: INFO: (2) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:460/proxy/: tls baz (200; 7.364462ms)
Jan  3 02:37:13.626: INFO: (2) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:160/proxy/: foo (200; 7.51806ms)
Jan  3 02:37:13.626: INFO: (2) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/tlsrewritem... (200; 7.568734ms)
Jan  3 02:37:13.629: INFO: (3) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:160/proxy/: foo (200; 2.67076ms)
Jan  3 02:37:13.630: INFO: (3) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname1/proxy/: foo (200; 3.098634ms)
Jan  3 02:37:13.630: INFO: (3) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname1/proxy/: foo (200; 3.125263ms)
Jan  3 02:37:13.630: INFO: (3) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/rewriteme">... (200; 3.116456ms)
Jan  3 02:37:13.630: INFO: (3) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:460/proxy/: tls baz (200; 3.121354ms)
Jan  3 02:37:13.630: INFO: (3) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname1/proxy/: tls baz (200; 3.496975ms)
Jan  3 02:37:13.630: INFO: (3) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname2/proxy/: tls qux (200; 3.526608ms)
Jan  3 02:37:13.630: INFO: (3) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname2/proxy/: bar (200; 3.545934ms)
Jan  3 02:37:13.630: INFO: (3) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname2/proxy/: bar (200; 3.622691ms)
Jan  3 02:37:13.630: INFO: (3) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:160/proxy/: foo (200; 3.64118ms)
Jan  3 02:37:13.630: INFO: (3) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/rewriteme">test<... (200; 3.655793ms)
Jan  3 02:37:13.630: INFO: (3) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/rewriteme">test</a> (200; 3.836569ms)
Jan  3 02:37:13.630: INFO: (3) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:162/proxy/: bar (200; 3.834529ms)
Jan  3 02:37:13.630: INFO: (3) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/tlsrewritem... (200; 3.869867ms)
Jan  3 02:37:13.630: INFO: (3) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:462/proxy/: tls qux (200; 3.902018ms)
Jan  3 02:37:13.630: INFO: (3) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:162/proxy/: bar (200; 3.855574ms)
Jan  3 02:37:13.633: INFO: (4) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname2/proxy/: bar (200; 2.79772ms)
Jan  3 02:37:13.633: INFO: (4) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname1/proxy/: foo (200; 2.836009ms)
Jan  3 02:37:13.633: INFO: (4) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname1/proxy/: tls baz (200; 2.865837ms)
Jan  3 02:37:13.633: INFO: (4) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname2/proxy/: tls qux (200; 2.870079ms)
Jan  3 02:37:13.634: INFO: (4) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:460/proxy/: tls baz (200; 2.93021ms)
Jan  3 02:37:13.634: INFO: (4) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/rewriteme">test<... (200; 2.993512ms)
Jan  3 02:37:13.634: INFO: (4) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/rewriteme">test</a> (200; 3.053314ms)
Jan  3 02:37:13.634: INFO: (4) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:160/proxy/: foo (200; 3.13083ms)
Jan  3 02:37:13.634: INFO: (4) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/rewriteme">... (200; 3.199187ms)
Jan  3 02:37:13.634: INFO: (4) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:162/proxy/: bar (200; 3.207258ms)
Jan  3 02:37:13.634: INFO: (4) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname1/proxy/: foo (200; 3.325298ms)
Jan  3 02:37:13.634: INFO: (4) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname2/proxy/: bar (200; 3.361136ms)
Jan  3 02:37:13.634: INFO: (4) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:162/proxy/: bar (200; 3.379294ms)
Jan  3 02:37:13.634: INFO: (4) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:462/proxy/: tls qux (200; 3.577107ms)
Jan  3 02:37:13.634: INFO: (4) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:160/proxy/: foo (200; 3.641354ms)
Jan  3 02:37:13.634: INFO: (4) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/tlsrewritem... (200; 3.640237ms)
Jan  3 02:37:13.637: INFO: (5) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/rewriteme">test<... (200; 2.455593ms)
Jan  3 02:37:13.637: INFO: (5) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:160/proxy/: foo (200; 2.677191ms)
Jan  3 02:37:13.637: INFO: (5) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/rewriteme">test</a> (200; 2.753063ms)
Jan  3 02:37:13.637: INFO: (5) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:162/proxy/: bar (200; 2.767405ms)
Jan  3 02:37:13.637: INFO: (5) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname1/proxy/: foo (200; 2.934904ms)
Jan  3 02:37:13.637: INFO: (5) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:460/proxy/: tls baz (200; 2.968359ms)
Jan  3 02:37:13.638: INFO: (5) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:160/proxy/: foo (200; 3.426532ms)
Jan  3 02:37:13.638: INFO: (5) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname2/proxy/: bar (200; 3.486179ms)
Jan  3 02:37:13.638: INFO: (5) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/rewriteme">... (200; 3.482652ms)
Jan  3 02:37:13.638: INFO: (5) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname1/proxy/: foo (200; 3.527785ms)
Jan  3 02:37:13.638: INFO: (5) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname2/proxy/: bar (200; 3.511827ms)
Jan  3 02:37:13.638: INFO: (5) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname2/proxy/: tls qux (200; 3.493839ms)
Jan  3 02:37:13.638: INFO: (5) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname1/proxy/: tls baz (200; 3.509547ms)
Jan  3 02:37:13.638: INFO: (5) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/tlsrewritem... (200; 3.639804ms)
Jan  3 02:37:13.638: INFO: (5) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:462/proxy/: tls qux (200; 3.657769ms)
Jan  3 02:37:13.638: INFO: (5) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:162/proxy/: bar (200; 3.684293ms)
Jan  3 02:37:13.643: INFO: (6) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/rewriteme">... (200; 5.358261ms)
Jan  3 02:37:13.643: INFO: (6) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/tlsrewritem... (200; 5.36648ms)
Jan  3 02:37:13.643: INFO: (6) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:160/proxy/: foo (200; 5.366933ms)
Jan  3 02:37:13.644: INFO: (6) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:160/proxy/: foo (200; 5.981675ms)
Jan  3 02:37:13.644: INFO: (6) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname2/proxy/: bar (200; 6.022304ms)
Jan  3 02:37:13.644: INFO: (6) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/rewriteme">test</a> (200; 5.981836ms)
Jan  3 02:37:13.644: INFO: (6) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname2/proxy/: tls qux (200; 6.010324ms)
Jan  3 02:37:13.644: INFO: (6) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname1/proxy/: foo (200; 6.040546ms)
Jan  3 02:37:13.644: INFO: (6) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname2/proxy/: bar (200; 6.018123ms)
Jan  3 02:37:13.644: INFO: (6) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:460/proxy/: tls baz (200; 6.195055ms)
Jan  3 02:37:13.644: INFO: (6) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:162/proxy/: bar (200; 6.241749ms)
Jan  3 02:37:13.644: INFO: (6) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname1/proxy/: foo (200; 6.244366ms)
Jan  3 02:37:13.644: INFO: (6) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:162/proxy/: bar (200; 6.349334ms)
Jan  3 02:37:13.644: INFO: (6) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname1/proxy/: tls baz (200; 6.29807ms)
Jan  3 02:37:13.644: INFO: (6) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:462/proxy/: tls qux (200; 6.272069ms)
Jan  3 02:37:13.644: INFO: (6) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/rewriteme">test<... (200; 6.368421ms)
Jan  3 02:37:13.647: INFO: (7) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname2/proxy/: bar (200; 2.816175ms)
Jan  3 02:37:13.647: INFO: (7) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname1/proxy/: foo (200; 2.815672ms)
Jan  3 02:37:13.647: INFO: (7) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/rewriteme">... (200; 2.809274ms)
Jan  3 02:37:13.648: INFO: (7) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname1/proxy/: tls baz (200; 3.083282ms)
Jan  3 02:37:13.648: INFO: (7) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname2/proxy/: tls qux (200; 3.033796ms)
Jan  3 02:37:13.648: INFO: (7) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname2/proxy/: bar (200; 3.170505ms)
Jan  3 02:37:13.648: INFO: (7) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname1/proxy/: foo (200; 3.217372ms)
Jan  3 02:37:13.648: INFO: (7) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:162/proxy/: bar (200; 3.345555ms)
Jan  3 02:37:13.648: INFO: (7) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:460/proxy/: tls baz (200; 3.483298ms)
Jan  3 02:37:13.648: INFO: (7) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/tlsrewritem... (200; 3.50272ms)
Jan  3 02:37:13.648: INFO: (7) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:162/proxy/: bar (200; 3.495916ms)
Jan  3 02:37:13.648: INFO: (7) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:160/proxy/: foo (200; 3.520118ms)
Jan  3 02:37:13.648: INFO: (7) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/rewriteme">test</a> (200; 3.492308ms)
Jan  3 02:37:13.648: INFO: (7) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/rewriteme">test<... (200; 3.554764ms)
Jan  3 02:37:13.648: INFO: (7) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:160/proxy/: foo (200; 3.663816ms)
Jan  3 02:37:13.648: INFO: (7) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:462/proxy/: tls qux (200; 3.648987ms)
Jan  3 02:37:13.651: INFO: (8) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname1/proxy/: foo (200; 2.508889ms)
Jan  3 02:37:13.651: INFO: (8) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:162/proxy/: bar (200; 2.53677ms)
Jan  3 02:37:13.651: INFO: (8) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/rewriteme">test</a> (200; 2.699377ms)
Jan  3 02:37:13.651: INFO: (8) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname2/proxy/: bar (200; 3.172112ms)
Jan  3 02:37:13.652: INFO: (8) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/rewriteme">... (200; 3.327148ms)
Jan  3 02:37:13.652: INFO: (8) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname1/proxy/: foo (200; 3.325376ms)
Jan  3 02:37:13.652: INFO: (8) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname2/proxy/: bar (200; 3.553406ms)
Jan  3 02:37:13.652: INFO: (8) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:160/proxy/: foo (200; 3.648743ms)
Jan  3 02:37:13.652: INFO: (8) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname1/proxy/: tls baz (200; 3.819808ms)
Jan  3 02:37:13.652: INFO: (8) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:460/proxy/: tls baz (200; 3.806963ms)
Jan  3 02:37:13.652: INFO: (8) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:162/proxy/: bar (200; 3.845871ms)
Jan  3 02:37:13.652: INFO: (8) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:462/proxy/: tls qux (200; 3.764767ms)
Jan  3 02:37:13.652: INFO: (8) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/rewriteme">test<... (200; 3.86287ms)
Jan  3 02:37:13.652: INFO: (8) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/tlsrewritem... (200; 3.845314ms)
Jan  3 02:37:13.652: INFO: (8) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname2/proxy/: tls qux (200; 3.849641ms)
Jan  3 02:37:13.652: INFO: (8) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:160/proxy/: foo (200; 3.940552ms)
Jan  3 02:37:13.654: INFO: (9) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:162/proxy/: bar (200; 2.158685ms)
Jan  3 02:37:13.661: INFO: (9) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:160/proxy/: foo (200; 8.998577ms)
Jan  3 02:37:13.661: INFO: (9) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:160/proxy/: foo (200; 9.073612ms)
Jan  3 02:37:13.661: INFO: (9) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/rewriteme">test<... (200; 9.175813ms)
Jan  3 02:37:13.661: INFO: (9) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/rewriteme">test</a> (200; 9.269574ms)
Jan  3 02:37:13.662: INFO: (9) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname1/proxy/: foo (200; 10.081404ms)
Jan  3 02:37:13.662: INFO: (9) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname2/proxy/: bar (200; 10.084998ms)
Jan  3 02:37:13.662: INFO: (9) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:162/proxy/: bar (200; 10.098283ms)
Jan  3 02:37:13.662: INFO: (9) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname1/proxy/: foo (200; 10.09088ms)
Jan  3 02:37:13.662: INFO: (9) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname2/proxy/: tls qux (200; 10.063956ms)
Jan  3 02:37:13.662: INFO: (9) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname1/proxy/: tls baz (200; 10.245183ms)
Jan  3 02:37:13.662: INFO: (9) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/tlsrewritem... (200; 10.200898ms)
Jan  3 02:37:13.662: INFO: (9) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:462/proxy/: tls qux (200; 10.238397ms)
Jan  3 02:37:13.662: INFO: (9) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/rewriteme">... (200; 10.329418ms)
Jan  3 02:37:13.662: INFO: (9) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname2/proxy/: bar (200; 10.273579ms)
Jan  3 02:37:13.662: INFO: (9) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:460/proxy/: tls baz (200; 10.325094ms)
Jan  3 02:37:13.701: INFO: (10) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:162/proxy/: bar (200; 38.592524ms)
Jan  3 02:37:13.701: INFO: (10) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:460/proxy/: tls baz (200; 38.73549ms)
Jan  3 02:37:13.701: INFO: (10) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/rewriteme">test<... (200; 38.755639ms)
Jan  3 02:37:13.702: INFO: (10) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:160/proxy/: foo (200; 39.171588ms)
Jan  3 02:37:13.702: INFO: (10) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/rewriteme">test</a> (200; 39.216149ms)
Jan  3 02:37:13.702: INFO: (10) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:162/proxy/: bar (200; 39.317629ms)
Jan  3 02:37:13.702: INFO: (10) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname2/proxy/: bar (200; 39.673902ms)
Jan  3 02:37:13.702: INFO: (10) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname1/proxy/: foo (200; 39.738875ms)
Jan  3 02:37:13.702: INFO: (10) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname1/proxy/: foo (200; 39.777888ms)
Jan  3 02:37:13.702: INFO: (10) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/rewriteme">... (200; 39.853048ms)
Jan  3 02:37:13.703: INFO: (10) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname2/proxy/: bar (200; 39.853211ms)
Jan  3 02:37:13.703: INFO: (10) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname1/proxy/: tls baz (200; 39.954647ms)
Jan  3 02:37:13.703: INFO: (10) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname2/proxy/: tls qux (200; 40.009101ms)
Jan  3 02:37:13.703: INFO: (10) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:160/proxy/: foo (200; 40.11688ms)
Jan  3 02:37:13.703: INFO: (10) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:462/proxy/: tls qux (200; 40.141837ms)
Jan  3 02:37:13.703: INFO: (10) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/tlsrewritem... (200; 40.26496ms)
Jan  3 02:37:13.709: INFO: (11) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:160/proxy/: foo (200; 6.295917ms)
Jan  3 02:37:13.709: INFO: (11) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:162/proxy/: bar (200; 6.375912ms)
Jan  3 02:37:13.709: INFO: (11) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:462/proxy/: tls qux (200; 6.326555ms)
Jan  3 02:37:13.710: INFO: (11) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:460/proxy/: tls baz (200; 6.836505ms)
Jan  3 02:37:13.710: INFO: (11) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname1/proxy/: foo (200; 7.200716ms)
Jan  3 02:37:13.710: INFO: (11) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/rewriteme">test</a> (200; 7.304524ms)
Jan  3 02:37:13.710: INFO: (11) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname1/proxy/: foo (200; 7.363705ms)
Jan  3 02:37:13.710: INFO: (11) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname2/proxy/: bar (200; 7.496029ms)
Jan  3 02:37:13.710: INFO: (11) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname2/proxy/: tls qux (200; 7.486324ms)
Jan  3 02:37:13.710: INFO: (11) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname2/proxy/: bar (200; 7.478894ms)
Jan  3 02:37:13.710: INFO: (11) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname1/proxy/: tls baz (200; 7.60562ms)
Jan  3 02:37:13.711: INFO: (11) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:160/proxy/: foo (200; 7.628502ms)
Jan  3 02:37:13.711: INFO: (11) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/rewriteme">... (200; 7.747973ms)
Jan  3 02:37:13.711: INFO: (11) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:162/proxy/: bar (200; 7.683951ms)
Jan  3 02:37:13.711: INFO: (11) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/rewriteme">test<... (200; 7.685862ms)
Jan  3 02:37:13.711: INFO: (11) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/tlsrewritem... (200; 7.888164ms)
Jan  3 02:37:13.713: INFO: (12) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/rewriteme">... (200; 2.223697ms)
Jan  3 02:37:13.713: INFO: (12) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:160/proxy/: foo (200; 2.423352ms)
Jan  3 02:37:13.713: INFO: (12) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/rewriteme">test</a> (200; 2.466801ms)
Jan  3 02:37:13.713: INFO: (12) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/tlsrewritem... (200; 2.411019ms)
Jan  3 02:37:13.714: INFO: (12) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:460/proxy/: tls baz (200; 2.780633ms)
Jan  3 02:37:13.714: INFO: (12) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname1/proxy/: foo (200; 2.916574ms)
Jan  3 02:37:13.714: INFO: (12) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname2/proxy/: bar (200; 3.071261ms)
Jan  3 02:37:13.714: INFO: (12) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname2/proxy/: tls qux (200; 3.05788ms)
Jan  3 02:37:13.714: INFO: (12) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname1/proxy/: tls baz (200; 3.037235ms)
Jan  3 02:37:13.714: INFO: (12) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname1/proxy/: foo (200; 3.02586ms)
Jan  3 02:37:13.714: INFO: (12) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:162/proxy/: bar (200; 3.311822ms)
Jan  3 02:37:13.714: INFO: (12) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname2/proxy/: bar (200; 3.271307ms)
Jan  3 02:37:13.714: INFO: (12) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:160/proxy/: foo (200; 3.34321ms)
Jan  3 02:37:13.714: INFO: (12) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/rewriteme">test<... (200; 3.356351ms)
Jan  3 02:37:13.714: INFO: (12) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:462/proxy/: tls qux (200; 3.41844ms)
Jan  3 02:37:13.714: INFO: (12) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:162/proxy/: bar (200; 3.579472ms)
Jan  3 02:37:13.717: INFO: (13) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:162/proxy/: bar (200; 2.428053ms)
Jan  3 02:37:13.717: INFO: (13) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:160/proxy/: foo (200; 2.477058ms)
Jan  3 02:37:13.717: INFO: (13) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:160/proxy/: foo (200; 2.472654ms)
Jan  3 02:37:13.717: INFO: (13) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/rewriteme">test</a> (200; 2.527329ms)
Jan  3 02:37:13.717: INFO: (13) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/rewriteme">... (200; 2.694338ms)
Jan  3 02:37:13.717: INFO: (13) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:460/proxy/: tls baz (200; 2.669297ms)
Jan  3 02:37:13.717: INFO: (13) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/rewriteme">test<... (200; 2.795475ms)
Jan  3 02:37:13.717: INFO: (13) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/tlsrewritem... (200; 2.885432ms)
Jan  3 02:37:13.717: INFO: (13) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:462/proxy/: tls qux (200; 3.025036ms)
Jan  3 02:37:13.718: INFO: (13) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname2/proxy/: bar (200; 3.327042ms)
Jan  3 02:37:13.718: INFO: (13) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname1/proxy/: foo (200; 3.424298ms)
Jan  3 02:37:13.718: INFO: (13) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname1/proxy/: foo (200; 3.405004ms)
Jan  3 02:37:13.718: INFO: (13) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname2/proxy/: tls qux (200; 3.407559ms)
Jan  3 02:37:13.718: INFO: (13) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname1/proxy/: tls baz (200; 3.401894ms)
Jan  3 02:37:13.718: INFO: (13) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:162/proxy/: bar (200; 3.530266ms)
Jan  3 02:37:13.718: INFO: (13) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname2/proxy/: bar (200; 3.490261ms)
Jan  3 02:37:13.720: INFO: (14) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/rewriteme">test<... (200; 2.160004ms)
Jan  3 02:37:13.721: INFO: (14) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:162/proxy/: bar (200; 2.571914ms)
Jan  3 02:37:13.721: INFO: (14) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname2/proxy/: bar (200; 2.708927ms)
Jan  3 02:37:13.721: INFO: (14) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:460/proxy/: tls baz (200; 2.712957ms)
Jan  3 02:37:13.721: INFO: (14) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname1/proxy/: tls baz (200; 2.905199ms)
Jan  3 02:37:13.721: INFO: (14) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname1/proxy/: foo (200; 2.873352ms)
Jan  3 02:37:13.721: INFO: (14) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/rewriteme">... (200; 2.913149ms)
Jan  3 02:37:13.721: INFO: (14) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname2/proxy/: bar (200; 3.026789ms)
Jan  3 02:37:13.721: INFO: (14) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:162/proxy/: bar (200; 3.013172ms)
Jan  3 02:37:13.721: INFO: (14) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname2/proxy/: tls qux (200; 3.011699ms)
Jan  3 02:37:13.721: INFO: (14) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname1/proxy/: foo (200; 3.12671ms)
Jan  3 02:37:13.721: INFO: (14) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:462/proxy/: tls qux (200; 3.342173ms)
Jan  3 02:37:13.721: INFO: (14) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:160/proxy/: foo (200; 3.306898ms)
Jan  3 02:37:13.721: INFO: (14) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:160/proxy/: foo (200; 3.449139ms)
Jan  3 02:37:13.722: INFO: (14) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/tlsrewritem... (200; 3.527687ms)
Jan  3 02:37:13.722: INFO: (14) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/rewriteme">test</a> (200; 3.425968ms)
Jan  3 02:37:13.724: INFO: (15) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:460/proxy/: tls baz (200; 2.631436ms)
Jan  3 02:37:13.724: INFO: (15) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname1/proxy/: foo (200; 2.919527ms)
Jan  3 02:37:13.725: INFO: (15) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:162/proxy/: bar (200; 2.841251ms)
Jan  3 02:37:13.725: INFO: (15) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/rewriteme">... (200; 3.262134ms)
Jan  3 02:37:13.725: INFO: (15) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:162/proxy/: bar (200; 3.298814ms)
Jan  3 02:37:13.725: INFO: (15) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/rewriteme">test<... (200; 3.37875ms)
Jan  3 02:37:13.725: INFO: (15) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname2/proxy/: tls qux (200; 3.418991ms)
Jan  3 02:37:13.725: INFO: (15) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname1/proxy/: foo (200; 3.347198ms)
Jan  3 02:37:13.725: INFO: (15) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname1/proxy/: tls baz (200; 3.406681ms)
Jan  3 02:37:13.725: INFO: (15) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname2/proxy/: bar (200; 3.382844ms)
Jan  3 02:37:13.725: INFO: (15) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:160/proxy/: foo (200; 3.417914ms)
Jan  3 02:37:13.725: INFO: (15) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/rewriteme">test</a> (200; 3.497074ms)
Jan  3 02:37:13.725: INFO: (15) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:160/proxy/: foo (200; 3.510269ms)
Jan  3 02:37:13.725: INFO: (15) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname2/proxy/: bar (200; 3.458859ms)
Jan  3 02:37:13.725: INFO: (15) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/tlsrewritem... (200; 3.751427ms)
Jan  3 02:37:13.725: INFO: (15) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:462/proxy/: tls qux (200; 3.71018ms)
Jan  3 02:37:13.728: INFO: (16) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:160/proxy/: foo (200; 2.401982ms)
Jan  3 02:37:13.728: INFO: (16) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/tlsrewritem... (200; 2.396475ms)
Jan  3 02:37:13.728: INFO: (16) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname1/proxy/: foo (200; 2.963003ms)
Jan  3 02:37:13.728: INFO: (16) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname1/proxy/: tls baz (200; 3.015705ms)
Jan  3 02:37:13.729: INFO: (16) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/rewriteme">test</a> (200; 3.146ms)
Jan  3 02:37:13.729: INFO: (16) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname2/proxy/: tls qux (200; 3.192254ms)
Jan  3 02:37:13.729: INFO: (16) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname1/proxy/: foo (200; 3.235894ms)
Jan  3 02:37:13.729: INFO: (16) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/rewriteme">... (200; 3.183189ms)
Jan  3 02:37:13.729: INFO: (16) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname2/proxy/: bar (200; 3.176088ms)
Jan  3 02:37:13.729: INFO: (16) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:162/proxy/: bar (200; 3.195834ms)
Jan  3 02:37:13.729: INFO: (16) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:462/proxy/: tls qux (200; 3.320019ms)
Jan  3 02:37:13.729: INFO: (16) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname2/proxy/: bar (200; 3.615181ms)
Jan  3 02:37:13.729: INFO: (16) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:162/proxy/: bar (200; 3.700185ms)
Jan  3 02:37:13.729: INFO: (16) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:460/proxy/: tls baz (200; 3.706025ms)
Jan  3 02:37:13.729: INFO: (16) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:160/proxy/: foo (200; 3.665492ms)
Jan  3 02:37:13.729: INFO: (16) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/rewriteme">test<... (200; 3.765833ms)
Jan  3 02:37:13.731: INFO: (17) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/tlsrewritem... (200; 2.258753ms)
Jan  3 02:37:13.731: INFO: (17) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/rewriteme">test</a> (200; 2.27239ms)
Jan  3 02:37:13.732: INFO: (17) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/rewriteme">... (200; 2.393622ms)
Jan  3 02:37:13.732: INFO: (17) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:160/proxy/: foo (200; 2.648732ms)
Jan  3 02:37:13.732: INFO: (17) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname1/proxy/: foo (200; 2.915167ms)
Jan  3 02:37:13.732: INFO: (17) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname2/proxy/: bar (200; 3.085222ms)
Jan  3 02:37:13.732: INFO: (17) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname2/proxy/: bar (200; 3.227501ms)
Jan  3 02:37:13.732: INFO: (17) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname1/proxy/: foo (200; 3.19498ms)
Jan  3 02:37:13.732: INFO: (17) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:460/proxy/: tls baz (200; 3.233963ms)
Jan  3 02:37:13.732: INFO: (17) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname2/proxy/: tls qux (200; 3.218592ms)
Jan  3 02:37:13.732: INFO: (17) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname1/proxy/: tls baz (200; 3.326806ms)
Jan  3 02:37:13.732: INFO: (17) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:162/proxy/: bar (200; 3.356347ms)
Jan  3 02:37:13.733: INFO: (17) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:160/proxy/: foo (200; 3.497142ms)
Jan  3 02:37:13.733: INFO: (17) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:162/proxy/: bar (200; 3.613154ms)
Jan  3 02:37:13.733: INFO: (17) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/rewriteme">test<... (200; 3.611516ms)
Jan  3 02:37:13.733: INFO: (17) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:462/proxy/: tls qux (200; 3.65713ms)
Jan  3 02:37:13.735: INFO: (18) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:162/proxy/: bar (200; 2.175385ms)
Jan  3 02:37:13.735: INFO: (18) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/tlsrewritem... (200; 2.322514ms)
Jan  3 02:37:13.736: INFO: (18) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname2/proxy/: bar (200; 2.804671ms)
Jan  3 02:37:13.736: INFO: (18) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname2/proxy/: bar (200; 2.80527ms)
Jan  3 02:37:13.736: INFO: (18) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname2/proxy/: tls qux (200; 2.803848ms)
Jan  3 02:37:13.736: INFO: (18) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname1/proxy/: foo (200; 2.96403ms)
Jan  3 02:37:13.736: INFO: (18) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:162/proxy/: bar (200; 3.054382ms)
Jan  3 02:37:13.736: INFO: (18) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:460/proxy/: tls baz (200; 3.105664ms)
Jan  3 02:37:13.736: INFO: (18) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname1/proxy/: tls baz (200; 3.194174ms)
Jan  3 02:37:13.736: INFO: (18) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:160/proxy/: foo (200; 3.183578ms)
Jan  3 02:37:13.736: INFO: (18) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:160/proxy/: foo (200; 3.308713ms)
Jan  3 02:37:13.736: INFO: (18) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:462/proxy/: tls qux (200; 3.244668ms)
Jan  3 02:37:13.736: INFO: (18) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname1/proxy/: foo (200; 3.452626ms)
Jan  3 02:37:13.736: INFO: (18) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/rewriteme">test<... (200; 3.568818ms)
Jan  3 02:37:13.736: INFO: (18) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/rewriteme">test</a> (200; 3.530317ms)
Jan  3 02:37:13.736: INFO: (18) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/rewriteme">... (200; 3.564799ms)
Jan  3 02:37:13.738: INFO: (19) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:162/proxy/: bar (200; 1.738621ms)
Jan  3 02:37:13.739: INFO: (19) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:162/proxy/: bar (200; 2.152937ms)
Jan  3 02:37:13.739: INFO: (19) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:160/proxy/: foo (200; 2.249616ms)
Jan  3 02:37:13.739: INFO: (19) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/tlsrewritem... (200; 2.397851ms)
Jan  3 02:37:13.740: INFO: (19) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname1/proxy/: foo (200; 3.003595ms)
Jan  3 02:37:13.740: INFO: (19) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname2/proxy/: bar (200; 3.076351ms)
Jan  3 02:37:13.740: INFO: (19) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname1/proxy/: foo (200; 3.246469ms)
Jan  3 02:37:13.740: INFO: (19) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname2/proxy/: bar (200; 3.320976ms)
Jan  3 02:37:13.740: INFO: (19) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/rewriteme">test<... (200; 3.280799ms)
Jan  3 02:37:13.740: INFO: (19) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:160/proxy/: foo (200; 3.441766ms)
Jan  3 02:37:13.740: INFO: (19) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname1/proxy/: tls baz (200; 3.391269ms)
Jan  3 02:37:13.740: INFO: (19) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:460/proxy/: tls baz (200; 3.398787ms)
Jan  3 02:37:13.740: INFO: (19) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname2/proxy/: tls qux (200; 3.497782ms)
Jan  3 02:37:13.740: INFO: (19) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/rewriteme">... (200; 3.427806ms)
Jan  3 02:37:13.740: INFO: (19) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/rewriteme">test</a> (200; 3.494685ms)
Jan  3 02:37:13.740: INFO: (19) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:462/proxy/: tls qux (200; 3.450228ms)
STEP: deleting ReplicationController proxy-service-czh5m in namespace proxy-7990, will wait for the garbage collector to delete the pods 01/03/23 02:37:13.74
Jan  3 02:37:13.821: INFO: Deleting ReplicationController proxy-service-czh5m took: 27.763536ms
Jan  3 02:37:13.921: INFO: Terminating ReplicationController proxy-service-czh5m pods took: 100.878687ms
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Jan  3 02:37:16.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7990" for this suite. 01/03/23 02:37:16.426
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","completed":163,"skipped":2950,"failed":0}
------------------------------
• [4.960 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:37:11.468
    Jan  3 02:37:11.468: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename proxy 01/03/23 02:37:11.468
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:37:11.48
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:37:11.482
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 01/03/23 02:37:11.51
    STEP: creating replication controller proxy-service-czh5m in namespace proxy-7990 01/03/23 02:37:11.51
    I0103 02:37:11.550170      23 runners.go:193] Created replication controller with name: proxy-service-czh5m, namespace: proxy-7990, replica count: 1
    I0103 02:37:12.601896      23 runners.go:193] proxy-service-czh5m Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0103 02:37:13.602610      23 runners.go:193] proxy-service-czh5m Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  3 02:37:13.604: INFO: setup took 2.120878106s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 01/03/23 02:37:13.604
    Jan  3 02:37:13.607: INFO: (0) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:160/proxy/: foo (200; 2.572367ms)
    Jan  3 02:37:13.607: INFO: (0) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:160/proxy/: foo (200; 2.753026ms)
    Jan  3 02:37:13.607: INFO: (0) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:162/proxy/: bar (200; 2.896424ms)
    Jan  3 02:37:13.607: INFO: (0) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:162/proxy/: bar (200; 2.845123ms)
    Jan  3 02:37:13.608: INFO: (0) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/rewriteme">test<... (200; 3.498173ms)
    Jan  3 02:37:13.608: INFO: (0) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname1/proxy/: foo (200; 3.600015ms)
    Jan  3 02:37:13.608: INFO: (0) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname2/proxy/: bar (200; 3.564465ms)
    Jan  3 02:37:13.609: INFO: (0) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname2/proxy/: bar (200; 4.284648ms)
    Jan  3 02:37:13.609: INFO: (0) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/rewriteme">... (200; 4.32241ms)
    Jan  3 02:37:13.609: INFO: (0) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/rewriteme">test</a> (200; 4.354181ms)
    Jan  3 02:37:13.609: INFO: (0) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname1/proxy/: foo (200; 4.345404ms)
    Jan  3 02:37:13.613: INFO: (0) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:462/proxy/: tls qux (200; 8.430004ms)
    Jan  3 02:37:13.613: INFO: (0) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname2/proxy/: tls qux (200; 8.459679ms)
    Jan  3 02:37:13.614: INFO: (0) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname1/proxy/: tls baz (200; 9.611222ms)
    Jan  3 02:37:13.614: INFO: (0) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:460/proxy/: tls baz (200; 9.645876ms)
    Jan  3 02:37:13.614: INFO: (0) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/tlsrewritem... (200; 9.857626ms)
    Jan  3 02:37:13.617: INFO: (1) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/rewriteme">test</a> (200; 2.527608ms)
    Jan  3 02:37:13.617: INFO: (1) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:160/proxy/: foo (200; 2.617584ms)
    Jan  3 02:37:13.617: INFO: (1) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:162/proxy/: bar (200; 2.618684ms)
    Jan  3 02:37:13.617: INFO: (1) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/rewriteme">test<... (200; 2.72146ms)
    Jan  3 02:37:13.617: INFO: (1) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname2/proxy/: bar (200; 2.763547ms)
    Jan  3 02:37:13.618: INFO: (1) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname1/proxy/: foo (200; 3.297951ms)
    Jan  3 02:37:13.618: INFO: (1) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname2/proxy/: tls qux (200; 3.241215ms)
    Jan  3 02:37:13.618: INFO: (1) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:162/proxy/: bar (200; 3.30435ms)
    Jan  3 02:37:13.618: INFO: (1) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname2/proxy/: bar (200; 3.273971ms)
    Jan  3 02:37:13.618: INFO: (1) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname1/proxy/: tls baz (200; 3.271436ms)
    Jan  3 02:37:13.618: INFO: (1) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/rewriteme">... (200; 3.347552ms)
    Jan  3 02:37:13.618: INFO: (1) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname1/proxy/: foo (200; 3.256027ms)
    Jan  3 02:37:13.618: INFO: (1) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:160/proxy/: foo (200; 3.453853ms)
    Jan  3 02:37:13.618: INFO: (1) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:460/proxy/: tls baz (200; 3.932327ms)
    Jan  3 02:37:13.618: INFO: (1) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/tlsrewritem... (200; 4.123507ms)
    Jan  3 02:37:13.619: INFO: (1) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:462/proxy/: tls qux (200; 4.343923ms)
    Jan  3 02:37:13.621: INFO: (2) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:462/proxy/: tls qux (200; 2.024706ms)
    Jan  3 02:37:13.625: INFO: (2) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname1/proxy/: tls baz (200; 6.303643ms)
    Jan  3 02:37:13.625: INFO: (2) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname1/proxy/: foo (200; 6.409005ms)
    Jan  3 02:37:13.625: INFO: (2) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname1/proxy/: foo (200; 6.31371ms)
    Jan  3 02:37:13.625: INFO: (2) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname2/proxy/: tls qux (200; 6.376955ms)
    Jan  3 02:37:13.625: INFO: (2) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname2/proxy/: bar (200; 6.344856ms)
    Jan  3 02:37:13.626: INFO: (2) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname2/proxy/: bar (200; 6.972156ms)
    Jan  3 02:37:13.626: INFO: (2) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/rewriteme">test<... (200; 6.965056ms)
    Jan  3 02:37:13.626: INFO: (2) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:162/proxy/: bar (200; 6.995562ms)
    Jan  3 02:37:13.626: INFO: (2) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/rewriteme">... (200; 7.351985ms)
    Jan  3 02:37:13.626: INFO: (2) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:162/proxy/: bar (200; 7.369944ms)
    Jan  3 02:37:13.626: INFO: (2) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:160/proxy/: foo (200; 7.346443ms)
    Jan  3 02:37:13.626: INFO: (2) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/rewriteme">test</a> (200; 7.41265ms)
    Jan  3 02:37:13.626: INFO: (2) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:460/proxy/: tls baz (200; 7.364462ms)
    Jan  3 02:37:13.626: INFO: (2) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:160/proxy/: foo (200; 7.51806ms)
    Jan  3 02:37:13.626: INFO: (2) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/tlsrewritem... (200; 7.568734ms)
    Jan  3 02:37:13.629: INFO: (3) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:160/proxy/: foo (200; 2.67076ms)
    Jan  3 02:37:13.630: INFO: (3) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname1/proxy/: foo (200; 3.098634ms)
    Jan  3 02:37:13.630: INFO: (3) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname1/proxy/: foo (200; 3.125263ms)
    Jan  3 02:37:13.630: INFO: (3) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/rewriteme">... (200; 3.116456ms)
    Jan  3 02:37:13.630: INFO: (3) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:460/proxy/: tls baz (200; 3.121354ms)
    Jan  3 02:37:13.630: INFO: (3) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname1/proxy/: tls baz (200; 3.496975ms)
    Jan  3 02:37:13.630: INFO: (3) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname2/proxy/: tls qux (200; 3.526608ms)
    Jan  3 02:37:13.630: INFO: (3) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname2/proxy/: bar (200; 3.545934ms)
    Jan  3 02:37:13.630: INFO: (3) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname2/proxy/: bar (200; 3.622691ms)
    Jan  3 02:37:13.630: INFO: (3) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:160/proxy/: foo (200; 3.64118ms)
    Jan  3 02:37:13.630: INFO: (3) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/rewriteme">test<... (200; 3.655793ms)
    Jan  3 02:37:13.630: INFO: (3) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/rewriteme">test</a> (200; 3.836569ms)
    Jan  3 02:37:13.630: INFO: (3) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:162/proxy/: bar (200; 3.834529ms)
    Jan  3 02:37:13.630: INFO: (3) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/tlsrewritem... (200; 3.869867ms)
    Jan  3 02:37:13.630: INFO: (3) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:462/proxy/: tls qux (200; 3.902018ms)
    Jan  3 02:37:13.630: INFO: (3) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:162/proxy/: bar (200; 3.855574ms)
    Jan  3 02:37:13.633: INFO: (4) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname2/proxy/: bar (200; 2.79772ms)
    Jan  3 02:37:13.633: INFO: (4) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname1/proxy/: foo (200; 2.836009ms)
    Jan  3 02:37:13.633: INFO: (4) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname1/proxy/: tls baz (200; 2.865837ms)
    Jan  3 02:37:13.633: INFO: (4) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname2/proxy/: tls qux (200; 2.870079ms)
    Jan  3 02:37:13.634: INFO: (4) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:460/proxy/: tls baz (200; 2.93021ms)
    Jan  3 02:37:13.634: INFO: (4) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/rewriteme">test<... (200; 2.993512ms)
    Jan  3 02:37:13.634: INFO: (4) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/rewriteme">test</a> (200; 3.053314ms)
    Jan  3 02:37:13.634: INFO: (4) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:160/proxy/: foo (200; 3.13083ms)
    Jan  3 02:37:13.634: INFO: (4) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/rewriteme">... (200; 3.199187ms)
    Jan  3 02:37:13.634: INFO: (4) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:162/proxy/: bar (200; 3.207258ms)
    Jan  3 02:37:13.634: INFO: (4) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname1/proxy/: foo (200; 3.325298ms)
    Jan  3 02:37:13.634: INFO: (4) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname2/proxy/: bar (200; 3.361136ms)
    Jan  3 02:37:13.634: INFO: (4) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:162/proxy/: bar (200; 3.379294ms)
    Jan  3 02:37:13.634: INFO: (4) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:462/proxy/: tls qux (200; 3.577107ms)
    Jan  3 02:37:13.634: INFO: (4) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:160/proxy/: foo (200; 3.641354ms)
    Jan  3 02:37:13.634: INFO: (4) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/tlsrewritem... (200; 3.640237ms)
    Jan  3 02:37:13.637: INFO: (5) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/rewriteme">test<... (200; 2.455593ms)
    Jan  3 02:37:13.637: INFO: (5) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:160/proxy/: foo (200; 2.677191ms)
    Jan  3 02:37:13.637: INFO: (5) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/rewriteme">test</a> (200; 2.753063ms)
    Jan  3 02:37:13.637: INFO: (5) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:162/proxy/: bar (200; 2.767405ms)
    Jan  3 02:37:13.637: INFO: (5) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname1/proxy/: foo (200; 2.934904ms)
    Jan  3 02:37:13.637: INFO: (5) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:460/proxy/: tls baz (200; 2.968359ms)
    Jan  3 02:37:13.638: INFO: (5) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:160/proxy/: foo (200; 3.426532ms)
    Jan  3 02:37:13.638: INFO: (5) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname2/proxy/: bar (200; 3.486179ms)
    Jan  3 02:37:13.638: INFO: (5) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/rewriteme">... (200; 3.482652ms)
    Jan  3 02:37:13.638: INFO: (5) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname1/proxy/: foo (200; 3.527785ms)
    Jan  3 02:37:13.638: INFO: (5) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname2/proxy/: bar (200; 3.511827ms)
    Jan  3 02:37:13.638: INFO: (5) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname2/proxy/: tls qux (200; 3.493839ms)
    Jan  3 02:37:13.638: INFO: (5) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname1/proxy/: tls baz (200; 3.509547ms)
    Jan  3 02:37:13.638: INFO: (5) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/tlsrewritem... (200; 3.639804ms)
    Jan  3 02:37:13.638: INFO: (5) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:462/proxy/: tls qux (200; 3.657769ms)
    Jan  3 02:37:13.638: INFO: (5) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:162/proxy/: bar (200; 3.684293ms)
    Jan  3 02:37:13.643: INFO: (6) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/rewriteme">... (200; 5.358261ms)
    Jan  3 02:37:13.643: INFO: (6) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/tlsrewritem... (200; 5.36648ms)
    Jan  3 02:37:13.643: INFO: (6) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:160/proxy/: foo (200; 5.366933ms)
    Jan  3 02:37:13.644: INFO: (6) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:160/proxy/: foo (200; 5.981675ms)
    Jan  3 02:37:13.644: INFO: (6) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname2/proxy/: bar (200; 6.022304ms)
    Jan  3 02:37:13.644: INFO: (6) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/rewriteme">test</a> (200; 5.981836ms)
    Jan  3 02:37:13.644: INFO: (6) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname2/proxy/: tls qux (200; 6.010324ms)
    Jan  3 02:37:13.644: INFO: (6) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname1/proxy/: foo (200; 6.040546ms)
    Jan  3 02:37:13.644: INFO: (6) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname2/proxy/: bar (200; 6.018123ms)
    Jan  3 02:37:13.644: INFO: (6) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:460/proxy/: tls baz (200; 6.195055ms)
    Jan  3 02:37:13.644: INFO: (6) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:162/proxy/: bar (200; 6.241749ms)
    Jan  3 02:37:13.644: INFO: (6) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname1/proxy/: foo (200; 6.244366ms)
    Jan  3 02:37:13.644: INFO: (6) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:162/proxy/: bar (200; 6.349334ms)
    Jan  3 02:37:13.644: INFO: (6) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname1/proxy/: tls baz (200; 6.29807ms)
    Jan  3 02:37:13.644: INFO: (6) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:462/proxy/: tls qux (200; 6.272069ms)
    Jan  3 02:37:13.644: INFO: (6) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/rewriteme">test<... (200; 6.368421ms)
    Jan  3 02:37:13.647: INFO: (7) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname2/proxy/: bar (200; 2.816175ms)
    Jan  3 02:37:13.647: INFO: (7) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname1/proxy/: foo (200; 2.815672ms)
    Jan  3 02:37:13.647: INFO: (7) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/rewriteme">... (200; 2.809274ms)
    Jan  3 02:37:13.648: INFO: (7) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname1/proxy/: tls baz (200; 3.083282ms)
    Jan  3 02:37:13.648: INFO: (7) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname2/proxy/: tls qux (200; 3.033796ms)
    Jan  3 02:37:13.648: INFO: (7) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname2/proxy/: bar (200; 3.170505ms)
    Jan  3 02:37:13.648: INFO: (7) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname1/proxy/: foo (200; 3.217372ms)
    Jan  3 02:37:13.648: INFO: (7) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:162/proxy/: bar (200; 3.345555ms)
    Jan  3 02:37:13.648: INFO: (7) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:460/proxy/: tls baz (200; 3.483298ms)
    Jan  3 02:37:13.648: INFO: (7) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/tlsrewritem... (200; 3.50272ms)
    Jan  3 02:37:13.648: INFO: (7) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:162/proxy/: bar (200; 3.495916ms)
    Jan  3 02:37:13.648: INFO: (7) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:160/proxy/: foo (200; 3.520118ms)
    Jan  3 02:37:13.648: INFO: (7) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/rewriteme">test</a> (200; 3.492308ms)
    Jan  3 02:37:13.648: INFO: (7) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/rewriteme">test<... (200; 3.554764ms)
    Jan  3 02:37:13.648: INFO: (7) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:160/proxy/: foo (200; 3.663816ms)
    Jan  3 02:37:13.648: INFO: (7) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:462/proxy/: tls qux (200; 3.648987ms)
    Jan  3 02:37:13.651: INFO: (8) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname1/proxy/: foo (200; 2.508889ms)
    Jan  3 02:37:13.651: INFO: (8) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:162/proxy/: bar (200; 2.53677ms)
    Jan  3 02:37:13.651: INFO: (8) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/rewriteme">test</a> (200; 2.699377ms)
    Jan  3 02:37:13.651: INFO: (8) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname2/proxy/: bar (200; 3.172112ms)
    Jan  3 02:37:13.652: INFO: (8) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/rewriteme">... (200; 3.327148ms)
    Jan  3 02:37:13.652: INFO: (8) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname1/proxy/: foo (200; 3.325376ms)
    Jan  3 02:37:13.652: INFO: (8) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname2/proxy/: bar (200; 3.553406ms)
    Jan  3 02:37:13.652: INFO: (8) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:160/proxy/: foo (200; 3.648743ms)
    Jan  3 02:37:13.652: INFO: (8) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname1/proxy/: tls baz (200; 3.819808ms)
    Jan  3 02:37:13.652: INFO: (8) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:460/proxy/: tls baz (200; 3.806963ms)
    Jan  3 02:37:13.652: INFO: (8) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:162/proxy/: bar (200; 3.845871ms)
    Jan  3 02:37:13.652: INFO: (8) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:462/proxy/: tls qux (200; 3.764767ms)
    Jan  3 02:37:13.652: INFO: (8) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/rewriteme">test<... (200; 3.86287ms)
    Jan  3 02:37:13.652: INFO: (8) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/tlsrewritem... (200; 3.845314ms)
    Jan  3 02:37:13.652: INFO: (8) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname2/proxy/: tls qux (200; 3.849641ms)
    Jan  3 02:37:13.652: INFO: (8) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:160/proxy/: foo (200; 3.940552ms)
    Jan  3 02:37:13.654: INFO: (9) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:162/proxy/: bar (200; 2.158685ms)
    Jan  3 02:37:13.661: INFO: (9) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:160/proxy/: foo (200; 8.998577ms)
    Jan  3 02:37:13.661: INFO: (9) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:160/proxy/: foo (200; 9.073612ms)
    Jan  3 02:37:13.661: INFO: (9) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/rewriteme">test<... (200; 9.175813ms)
    Jan  3 02:37:13.661: INFO: (9) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/rewriteme">test</a> (200; 9.269574ms)
    Jan  3 02:37:13.662: INFO: (9) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname1/proxy/: foo (200; 10.081404ms)
    Jan  3 02:37:13.662: INFO: (9) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname2/proxy/: bar (200; 10.084998ms)
    Jan  3 02:37:13.662: INFO: (9) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:162/proxy/: bar (200; 10.098283ms)
    Jan  3 02:37:13.662: INFO: (9) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname1/proxy/: foo (200; 10.09088ms)
    Jan  3 02:37:13.662: INFO: (9) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname2/proxy/: tls qux (200; 10.063956ms)
    Jan  3 02:37:13.662: INFO: (9) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname1/proxy/: tls baz (200; 10.245183ms)
    Jan  3 02:37:13.662: INFO: (9) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/tlsrewritem... (200; 10.200898ms)
    Jan  3 02:37:13.662: INFO: (9) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:462/proxy/: tls qux (200; 10.238397ms)
    Jan  3 02:37:13.662: INFO: (9) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/rewriteme">... (200; 10.329418ms)
    Jan  3 02:37:13.662: INFO: (9) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname2/proxy/: bar (200; 10.273579ms)
    Jan  3 02:37:13.662: INFO: (9) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:460/proxy/: tls baz (200; 10.325094ms)
    Jan  3 02:37:13.701: INFO: (10) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:162/proxy/: bar (200; 38.592524ms)
    Jan  3 02:37:13.701: INFO: (10) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:460/proxy/: tls baz (200; 38.73549ms)
    Jan  3 02:37:13.701: INFO: (10) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/rewriteme">test<... (200; 38.755639ms)
    Jan  3 02:37:13.702: INFO: (10) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:160/proxy/: foo (200; 39.171588ms)
    Jan  3 02:37:13.702: INFO: (10) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/rewriteme">test</a> (200; 39.216149ms)
    Jan  3 02:37:13.702: INFO: (10) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:162/proxy/: bar (200; 39.317629ms)
    Jan  3 02:37:13.702: INFO: (10) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname2/proxy/: bar (200; 39.673902ms)
    Jan  3 02:37:13.702: INFO: (10) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname1/proxy/: foo (200; 39.738875ms)
    Jan  3 02:37:13.702: INFO: (10) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname1/proxy/: foo (200; 39.777888ms)
    Jan  3 02:37:13.702: INFO: (10) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/rewriteme">... (200; 39.853048ms)
    Jan  3 02:37:13.703: INFO: (10) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname2/proxy/: bar (200; 39.853211ms)
    Jan  3 02:37:13.703: INFO: (10) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname1/proxy/: tls baz (200; 39.954647ms)
    Jan  3 02:37:13.703: INFO: (10) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname2/proxy/: tls qux (200; 40.009101ms)
    Jan  3 02:37:13.703: INFO: (10) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:160/proxy/: foo (200; 40.11688ms)
    Jan  3 02:37:13.703: INFO: (10) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:462/proxy/: tls qux (200; 40.141837ms)
    Jan  3 02:37:13.703: INFO: (10) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/tlsrewritem... (200; 40.26496ms)
    Jan  3 02:37:13.709: INFO: (11) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:160/proxy/: foo (200; 6.295917ms)
    Jan  3 02:37:13.709: INFO: (11) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:162/proxy/: bar (200; 6.375912ms)
    Jan  3 02:37:13.709: INFO: (11) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:462/proxy/: tls qux (200; 6.326555ms)
    Jan  3 02:37:13.710: INFO: (11) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:460/proxy/: tls baz (200; 6.836505ms)
    Jan  3 02:37:13.710: INFO: (11) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname1/proxy/: foo (200; 7.200716ms)
    Jan  3 02:37:13.710: INFO: (11) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/rewriteme">test</a> (200; 7.304524ms)
    Jan  3 02:37:13.710: INFO: (11) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname1/proxy/: foo (200; 7.363705ms)
    Jan  3 02:37:13.710: INFO: (11) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname2/proxy/: bar (200; 7.496029ms)
    Jan  3 02:37:13.710: INFO: (11) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname2/proxy/: tls qux (200; 7.486324ms)
    Jan  3 02:37:13.710: INFO: (11) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname2/proxy/: bar (200; 7.478894ms)
    Jan  3 02:37:13.710: INFO: (11) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname1/proxy/: tls baz (200; 7.60562ms)
    Jan  3 02:37:13.711: INFO: (11) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:160/proxy/: foo (200; 7.628502ms)
    Jan  3 02:37:13.711: INFO: (11) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/rewriteme">... (200; 7.747973ms)
    Jan  3 02:37:13.711: INFO: (11) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:162/proxy/: bar (200; 7.683951ms)
    Jan  3 02:37:13.711: INFO: (11) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/rewriteme">test<... (200; 7.685862ms)
    Jan  3 02:37:13.711: INFO: (11) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/tlsrewritem... (200; 7.888164ms)
    Jan  3 02:37:13.713: INFO: (12) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/rewriteme">... (200; 2.223697ms)
    Jan  3 02:37:13.713: INFO: (12) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:160/proxy/: foo (200; 2.423352ms)
    Jan  3 02:37:13.713: INFO: (12) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/rewriteme">test</a> (200; 2.466801ms)
    Jan  3 02:37:13.713: INFO: (12) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/tlsrewritem... (200; 2.411019ms)
    Jan  3 02:37:13.714: INFO: (12) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:460/proxy/: tls baz (200; 2.780633ms)
    Jan  3 02:37:13.714: INFO: (12) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname1/proxy/: foo (200; 2.916574ms)
    Jan  3 02:37:13.714: INFO: (12) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname2/proxy/: bar (200; 3.071261ms)
    Jan  3 02:37:13.714: INFO: (12) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname2/proxy/: tls qux (200; 3.05788ms)
    Jan  3 02:37:13.714: INFO: (12) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname1/proxy/: tls baz (200; 3.037235ms)
    Jan  3 02:37:13.714: INFO: (12) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname1/proxy/: foo (200; 3.02586ms)
    Jan  3 02:37:13.714: INFO: (12) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:162/proxy/: bar (200; 3.311822ms)
    Jan  3 02:37:13.714: INFO: (12) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname2/proxy/: bar (200; 3.271307ms)
    Jan  3 02:37:13.714: INFO: (12) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:160/proxy/: foo (200; 3.34321ms)
    Jan  3 02:37:13.714: INFO: (12) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/rewriteme">test<... (200; 3.356351ms)
    Jan  3 02:37:13.714: INFO: (12) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:462/proxy/: tls qux (200; 3.41844ms)
    Jan  3 02:37:13.714: INFO: (12) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:162/proxy/: bar (200; 3.579472ms)
    Jan  3 02:37:13.717: INFO: (13) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:162/proxy/: bar (200; 2.428053ms)
    Jan  3 02:37:13.717: INFO: (13) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:160/proxy/: foo (200; 2.477058ms)
    Jan  3 02:37:13.717: INFO: (13) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:160/proxy/: foo (200; 2.472654ms)
    Jan  3 02:37:13.717: INFO: (13) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/rewriteme">test</a> (200; 2.527329ms)
    Jan  3 02:37:13.717: INFO: (13) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/rewriteme">... (200; 2.694338ms)
    Jan  3 02:37:13.717: INFO: (13) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:460/proxy/: tls baz (200; 2.669297ms)
    Jan  3 02:37:13.717: INFO: (13) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/rewriteme">test<... (200; 2.795475ms)
    Jan  3 02:37:13.717: INFO: (13) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/tlsrewritem... (200; 2.885432ms)
    Jan  3 02:37:13.717: INFO: (13) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:462/proxy/: tls qux (200; 3.025036ms)
    Jan  3 02:37:13.718: INFO: (13) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname2/proxy/: bar (200; 3.327042ms)
    Jan  3 02:37:13.718: INFO: (13) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname1/proxy/: foo (200; 3.424298ms)
    Jan  3 02:37:13.718: INFO: (13) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname1/proxy/: foo (200; 3.405004ms)
    Jan  3 02:37:13.718: INFO: (13) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname2/proxy/: tls qux (200; 3.407559ms)
    Jan  3 02:37:13.718: INFO: (13) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname1/proxy/: tls baz (200; 3.401894ms)
    Jan  3 02:37:13.718: INFO: (13) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:162/proxy/: bar (200; 3.530266ms)
    Jan  3 02:37:13.718: INFO: (13) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname2/proxy/: bar (200; 3.490261ms)
    Jan  3 02:37:13.720: INFO: (14) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/rewriteme">test<... (200; 2.160004ms)
    Jan  3 02:37:13.721: INFO: (14) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:162/proxy/: bar (200; 2.571914ms)
    Jan  3 02:37:13.721: INFO: (14) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname2/proxy/: bar (200; 2.708927ms)
    Jan  3 02:37:13.721: INFO: (14) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:460/proxy/: tls baz (200; 2.712957ms)
    Jan  3 02:37:13.721: INFO: (14) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname1/proxy/: tls baz (200; 2.905199ms)
    Jan  3 02:37:13.721: INFO: (14) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname1/proxy/: foo (200; 2.873352ms)
    Jan  3 02:37:13.721: INFO: (14) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/rewriteme">... (200; 2.913149ms)
    Jan  3 02:37:13.721: INFO: (14) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname2/proxy/: bar (200; 3.026789ms)
    Jan  3 02:37:13.721: INFO: (14) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:162/proxy/: bar (200; 3.013172ms)
    Jan  3 02:37:13.721: INFO: (14) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname2/proxy/: tls qux (200; 3.011699ms)
    Jan  3 02:37:13.721: INFO: (14) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname1/proxy/: foo (200; 3.12671ms)
    Jan  3 02:37:13.721: INFO: (14) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:462/proxy/: tls qux (200; 3.342173ms)
    Jan  3 02:37:13.721: INFO: (14) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:160/proxy/: foo (200; 3.306898ms)
    Jan  3 02:37:13.721: INFO: (14) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:160/proxy/: foo (200; 3.449139ms)
    Jan  3 02:37:13.722: INFO: (14) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/tlsrewritem... (200; 3.527687ms)
    Jan  3 02:37:13.722: INFO: (14) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/rewriteme">test</a> (200; 3.425968ms)
    Jan  3 02:37:13.724: INFO: (15) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:460/proxy/: tls baz (200; 2.631436ms)
    Jan  3 02:37:13.724: INFO: (15) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname1/proxy/: foo (200; 2.919527ms)
    Jan  3 02:37:13.725: INFO: (15) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:162/proxy/: bar (200; 2.841251ms)
    Jan  3 02:37:13.725: INFO: (15) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/rewriteme">... (200; 3.262134ms)
    Jan  3 02:37:13.725: INFO: (15) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:162/proxy/: bar (200; 3.298814ms)
    Jan  3 02:37:13.725: INFO: (15) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/rewriteme">test<... (200; 3.37875ms)
    Jan  3 02:37:13.725: INFO: (15) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname2/proxy/: tls qux (200; 3.418991ms)
    Jan  3 02:37:13.725: INFO: (15) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname1/proxy/: foo (200; 3.347198ms)
    Jan  3 02:37:13.725: INFO: (15) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname1/proxy/: tls baz (200; 3.406681ms)
    Jan  3 02:37:13.725: INFO: (15) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname2/proxy/: bar (200; 3.382844ms)
    Jan  3 02:37:13.725: INFO: (15) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:160/proxy/: foo (200; 3.417914ms)
    Jan  3 02:37:13.725: INFO: (15) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/rewriteme">test</a> (200; 3.497074ms)
    Jan  3 02:37:13.725: INFO: (15) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:160/proxy/: foo (200; 3.510269ms)
    Jan  3 02:37:13.725: INFO: (15) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname2/proxy/: bar (200; 3.458859ms)
    Jan  3 02:37:13.725: INFO: (15) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/tlsrewritem... (200; 3.751427ms)
    Jan  3 02:37:13.725: INFO: (15) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:462/proxy/: tls qux (200; 3.71018ms)
    Jan  3 02:37:13.728: INFO: (16) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:160/proxy/: foo (200; 2.401982ms)
    Jan  3 02:37:13.728: INFO: (16) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/tlsrewritem... (200; 2.396475ms)
    Jan  3 02:37:13.728: INFO: (16) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname1/proxy/: foo (200; 2.963003ms)
    Jan  3 02:37:13.728: INFO: (16) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname1/proxy/: tls baz (200; 3.015705ms)
    Jan  3 02:37:13.729: INFO: (16) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/rewriteme">test</a> (200; 3.146ms)
    Jan  3 02:37:13.729: INFO: (16) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname2/proxy/: tls qux (200; 3.192254ms)
    Jan  3 02:37:13.729: INFO: (16) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname1/proxy/: foo (200; 3.235894ms)
    Jan  3 02:37:13.729: INFO: (16) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/rewriteme">... (200; 3.183189ms)
    Jan  3 02:37:13.729: INFO: (16) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname2/proxy/: bar (200; 3.176088ms)
    Jan  3 02:37:13.729: INFO: (16) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:162/proxy/: bar (200; 3.195834ms)
    Jan  3 02:37:13.729: INFO: (16) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:462/proxy/: tls qux (200; 3.320019ms)
    Jan  3 02:37:13.729: INFO: (16) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname2/proxy/: bar (200; 3.615181ms)
    Jan  3 02:37:13.729: INFO: (16) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:162/proxy/: bar (200; 3.700185ms)
    Jan  3 02:37:13.729: INFO: (16) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:460/proxy/: tls baz (200; 3.706025ms)
    Jan  3 02:37:13.729: INFO: (16) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:160/proxy/: foo (200; 3.665492ms)
    Jan  3 02:37:13.729: INFO: (16) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/rewriteme">test<... (200; 3.765833ms)
    Jan  3 02:37:13.731: INFO: (17) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/tlsrewritem... (200; 2.258753ms)
    Jan  3 02:37:13.731: INFO: (17) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/rewriteme">test</a> (200; 2.27239ms)
    Jan  3 02:37:13.732: INFO: (17) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/rewriteme">... (200; 2.393622ms)
    Jan  3 02:37:13.732: INFO: (17) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:160/proxy/: foo (200; 2.648732ms)
    Jan  3 02:37:13.732: INFO: (17) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname1/proxy/: foo (200; 2.915167ms)
    Jan  3 02:37:13.732: INFO: (17) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname2/proxy/: bar (200; 3.085222ms)
    Jan  3 02:37:13.732: INFO: (17) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname2/proxy/: bar (200; 3.227501ms)
    Jan  3 02:37:13.732: INFO: (17) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname1/proxy/: foo (200; 3.19498ms)
    Jan  3 02:37:13.732: INFO: (17) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:460/proxy/: tls baz (200; 3.233963ms)
    Jan  3 02:37:13.732: INFO: (17) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname2/proxy/: tls qux (200; 3.218592ms)
    Jan  3 02:37:13.732: INFO: (17) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname1/proxy/: tls baz (200; 3.326806ms)
    Jan  3 02:37:13.732: INFO: (17) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:162/proxy/: bar (200; 3.356347ms)
    Jan  3 02:37:13.733: INFO: (17) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:160/proxy/: foo (200; 3.497142ms)
    Jan  3 02:37:13.733: INFO: (17) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:162/proxy/: bar (200; 3.613154ms)
    Jan  3 02:37:13.733: INFO: (17) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/rewriteme">test<... (200; 3.611516ms)
    Jan  3 02:37:13.733: INFO: (17) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:462/proxy/: tls qux (200; 3.65713ms)
    Jan  3 02:37:13.735: INFO: (18) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:162/proxy/: bar (200; 2.175385ms)
    Jan  3 02:37:13.735: INFO: (18) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/tlsrewritem... (200; 2.322514ms)
    Jan  3 02:37:13.736: INFO: (18) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname2/proxy/: bar (200; 2.804671ms)
    Jan  3 02:37:13.736: INFO: (18) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname2/proxy/: bar (200; 2.80527ms)
    Jan  3 02:37:13.736: INFO: (18) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname2/proxy/: tls qux (200; 2.803848ms)
    Jan  3 02:37:13.736: INFO: (18) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname1/proxy/: foo (200; 2.96403ms)
    Jan  3 02:37:13.736: INFO: (18) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:162/proxy/: bar (200; 3.054382ms)
    Jan  3 02:37:13.736: INFO: (18) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:460/proxy/: tls baz (200; 3.105664ms)
    Jan  3 02:37:13.736: INFO: (18) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname1/proxy/: tls baz (200; 3.194174ms)
    Jan  3 02:37:13.736: INFO: (18) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:160/proxy/: foo (200; 3.183578ms)
    Jan  3 02:37:13.736: INFO: (18) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:160/proxy/: foo (200; 3.308713ms)
    Jan  3 02:37:13.736: INFO: (18) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:462/proxy/: tls qux (200; 3.244668ms)
    Jan  3 02:37:13.736: INFO: (18) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname1/proxy/: foo (200; 3.452626ms)
    Jan  3 02:37:13.736: INFO: (18) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/rewriteme">test<... (200; 3.568818ms)
    Jan  3 02:37:13.736: INFO: (18) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/rewriteme">test</a> (200; 3.530317ms)
    Jan  3 02:37:13.736: INFO: (18) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/rewriteme">... (200; 3.564799ms)
    Jan  3 02:37:13.738: INFO: (19) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:162/proxy/: bar (200; 1.738621ms)
    Jan  3 02:37:13.739: INFO: (19) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:162/proxy/: bar (200; 2.152937ms)
    Jan  3 02:37:13.739: INFO: (19) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:160/proxy/: foo (200; 2.249616ms)
    Jan  3 02:37:13.739: INFO: (19) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:443/proxy/tlsrewritem... (200; 2.397851ms)
    Jan  3 02:37:13.740: INFO: (19) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname1/proxy/: foo (200; 3.003595ms)
    Jan  3 02:37:13.740: INFO: (19) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname2/proxy/: bar (200; 3.076351ms)
    Jan  3 02:37:13.740: INFO: (19) /api/v1/namespaces/proxy-7990/services/proxy-service-czh5m:portname1/proxy/: foo (200; 3.246469ms)
    Jan  3 02:37:13.740: INFO: (19) /api/v1/namespaces/proxy-7990/services/http:proxy-service-czh5m:portname2/proxy/: bar (200; 3.320976ms)
    Jan  3 02:37:13.740: INFO: (19) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp:1080/proxy/rewriteme">test<... (200; 3.280799ms)
    Jan  3 02:37:13.740: INFO: (19) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:160/proxy/: foo (200; 3.441766ms)
    Jan  3 02:37:13.740: INFO: (19) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname1/proxy/: tls baz (200; 3.391269ms)
    Jan  3 02:37:13.740: INFO: (19) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:460/proxy/: tls baz (200; 3.398787ms)
    Jan  3 02:37:13.740: INFO: (19) /api/v1/namespaces/proxy-7990/services/https:proxy-service-czh5m:tlsportname2/proxy/: tls qux (200; 3.497782ms)
    Jan  3 02:37:13.740: INFO: (19) /api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/http:proxy-service-czh5m-49bsp:1080/proxy/rewriteme">... (200; 3.427806ms)
    Jan  3 02:37:13.740: INFO: (19) /api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/: <a href="/api/v1/namespaces/proxy-7990/pods/proxy-service-czh5m-49bsp/proxy/rewriteme">test</a> (200; 3.494685ms)
    Jan  3 02:37:13.740: INFO: (19) /api/v1/namespaces/proxy-7990/pods/https:proxy-service-czh5m-49bsp:462/proxy/: tls qux (200; 3.450228ms)
    STEP: deleting ReplicationController proxy-service-czh5m in namespace proxy-7990, will wait for the garbage collector to delete the pods 01/03/23 02:37:13.74
    Jan  3 02:37:13.821: INFO: Deleting ReplicationController proxy-service-czh5m took: 27.763536ms
    Jan  3 02:37:13.921: INFO: Terminating ReplicationController proxy-service-czh5m pods took: 100.878687ms
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Jan  3 02:37:16.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-7990" for this suite. 01/03/23 02:37:16.426
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:37:16.429
Jan  3 02:37:16.430: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename kubectl 01/03/23 02:37:16.431
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:37:16.44
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:37:16.442
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/03/23 02:37:16.444
Jan  3 02:37:16.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-5752 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan  3 02:37:16.504: INFO: stderr: ""
Jan  3 02:37:16.504: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 01/03/23 02:37:16.504
Jan  3 02:37:16.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-5752 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Jan  3 02:37:18.020: INFO: stderr: ""
Jan  3 02:37:18.020: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/03/23 02:37:18.02
Jan  3 02:37:18.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-5752 delete pods e2e-test-httpd-pod'
Jan  3 02:37:20.313: INFO: stderr: ""
Jan  3 02:37:20.313: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan  3 02:37:20.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5752" for this suite. 01/03/23 02:37:20.317
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","completed":164,"skipped":2972,"failed":0}
------------------------------
• [3.890 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:954
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:960

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:37:16.429
    Jan  3 02:37:16.430: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename kubectl 01/03/23 02:37:16.431
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:37:16.44
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:37:16.442
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:960
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/03/23 02:37:16.444
    Jan  3 02:37:16.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-5752 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jan  3 02:37:16.504: INFO: stderr: ""
    Jan  3 02:37:16.504: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 01/03/23 02:37:16.504
    Jan  3 02:37:16.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-5752 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
    Jan  3 02:37:18.020: INFO: stderr: ""
    Jan  3 02:37:18.020: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/03/23 02:37:18.02
    Jan  3 02:37:18.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-5752 delete pods e2e-test-httpd-pod'
    Jan  3 02:37:20.313: INFO: stderr: ""
    Jan  3 02:37:20.313: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan  3 02:37:20.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5752" for this suite. 01/03/23 02:37:20.317
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:37:20.32
Jan  3 02:37:20.320: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename crd-publish-openapi 01/03/23 02:37:20.321
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:37:20.351
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:37:20.352
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
Jan  3 02:37:20.355: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 01/03/23 02:37:25.759
Jan  3 02:37:25.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-5180 --namespace=crd-publish-openapi-5180 create -f -'
Jan  3 02:37:26.699: INFO: stderr: ""
Jan  3 02:37:26.700: INFO: stdout: "e2e-test-crd-publish-openapi-4603-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan  3 02:37:26.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-5180 --namespace=crd-publish-openapi-5180 delete e2e-test-crd-publish-openapi-4603-crds test-foo'
Jan  3 02:37:26.770: INFO: stderr: ""
Jan  3 02:37:26.770: INFO: stdout: "e2e-test-crd-publish-openapi-4603-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jan  3 02:37:26.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-5180 --namespace=crd-publish-openapi-5180 apply -f -'
Jan  3 02:37:26.983: INFO: stderr: ""
Jan  3 02:37:26.983: INFO: stdout: "e2e-test-crd-publish-openapi-4603-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan  3 02:37:26.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-5180 --namespace=crd-publish-openapi-5180 delete e2e-test-crd-publish-openapi-4603-crds test-foo'
Jan  3 02:37:27.044: INFO: stderr: ""
Jan  3 02:37:27.044: INFO: stdout: "e2e-test-crd-publish-openapi-4603-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 01/03/23 02:37:27.044
Jan  3 02:37:27.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-5180 --namespace=crd-publish-openapi-5180 create -f -'
Jan  3 02:37:27.250: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 01/03/23 02:37:27.25
Jan  3 02:37:27.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-5180 --namespace=crd-publish-openapi-5180 create -f -'
Jan  3 02:37:27.455: INFO: rc: 1
Jan  3 02:37:27.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-5180 --namespace=crd-publish-openapi-5180 apply -f -'
Jan  3 02:37:27.663: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 01/03/23 02:37:27.663
Jan  3 02:37:27.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-5180 --namespace=crd-publish-openapi-5180 create -f -'
Jan  3 02:37:27.880: INFO: rc: 1
Jan  3 02:37:27.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-5180 --namespace=crd-publish-openapi-5180 apply -f -'
Jan  3 02:37:28.112: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 01/03/23 02:37:28.112
Jan  3 02:37:28.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-5180 explain e2e-test-crd-publish-openapi-4603-crds'
Jan  3 02:37:28.348: INFO: stderr: ""
Jan  3 02:37:28.348: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4603-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 01/03/23 02:37:28.348
Jan  3 02:37:28.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-5180 explain e2e-test-crd-publish-openapi-4603-crds.metadata'
Jan  3 02:37:28.586: INFO: stderr: ""
Jan  3 02:37:28.586: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4603-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jan  3 02:37:28.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-5180 explain e2e-test-crd-publish-openapi-4603-crds.spec'
Jan  3 02:37:28.794: INFO: stderr: ""
Jan  3 02:37:28.794: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4603-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jan  3 02:37:28.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-5180 explain e2e-test-crd-publish-openapi-4603-crds.spec.bars'
Jan  3 02:37:29.003: INFO: stderr: ""
Jan  3 02:37:29.003: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4603-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 01/03/23 02:37:29.003
Jan  3 02:37:29.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-5180 explain e2e-test-crd-publish-openapi-4603-crds.spec.bars2'
Jan  3 02:37:29.211: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  3 02:37:34.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5180" for this suite. 01/03/23 02:37:34.598
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","completed":165,"skipped":2984,"failed":0}
------------------------------
• [SLOW TEST] [14.280 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:37:20.32
    Jan  3 02:37:20.320: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename crd-publish-openapi 01/03/23 02:37:20.321
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:37:20.351
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:37:20.352
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:68
    Jan  3 02:37:20.355: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 01/03/23 02:37:25.759
    Jan  3 02:37:25.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-5180 --namespace=crd-publish-openapi-5180 create -f -'
    Jan  3 02:37:26.699: INFO: stderr: ""
    Jan  3 02:37:26.700: INFO: stdout: "e2e-test-crd-publish-openapi-4603-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jan  3 02:37:26.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-5180 --namespace=crd-publish-openapi-5180 delete e2e-test-crd-publish-openapi-4603-crds test-foo'
    Jan  3 02:37:26.770: INFO: stderr: ""
    Jan  3 02:37:26.770: INFO: stdout: "e2e-test-crd-publish-openapi-4603-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Jan  3 02:37:26.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-5180 --namespace=crd-publish-openapi-5180 apply -f -'
    Jan  3 02:37:26.983: INFO: stderr: ""
    Jan  3 02:37:26.983: INFO: stdout: "e2e-test-crd-publish-openapi-4603-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jan  3 02:37:26.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-5180 --namespace=crd-publish-openapi-5180 delete e2e-test-crd-publish-openapi-4603-crds test-foo'
    Jan  3 02:37:27.044: INFO: stderr: ""
    Jan  3 02:37:27.044: INFO: stdout: "e2e-test-crd-publish-openapi-4603-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 01/03/23 02:37:27.044
    Jan  3 02:37:27.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-5180 --namespace=crd-publish-openapi-5180 create -f -'
    Jan  3 02:37:27.250: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 01/03/23 02:37:27.25
    Jan  3 02:37:27.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-5180 --namespace=crd-publish-openapi-5180 create -f -'
    Jan  3 02:37:27.455: INFO: rc: 1
    Jan  3 02:37:27.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-5180 --namespace=crd-publish-openapi-5180 apply -f -'
    Jan  3 02:37:27.663: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 01/03/23 02:37:27.663
    Jan  3 02:37:27.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-5180 --namespace=crd-publish-openapi-5180 create -f -'
    Jan  3 02:37:27.880: INFO: rc: 1
    Jan  3 02:37:27.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-5180 --namespace=crd-publish-openapi-5180 apply -f -'
    Jan  3 02:37:28.112: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 01/03/23 02:37:28.112
    Jan  3 02:37:28.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-5180 explain e2e-test-crd-publish-openapi-4603-crds'
    Jan  3 02:37:28.348: INFO: stderr: ""
    Jan  3 02:37:28.348: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4603-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 01/03/23 02:37:28.348
    Jan  3 02:37:28.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-5180 explain e2e-test-crd-publish-openapi-4603-crds.metadata'
    Jan  3 02:37:28.586: INFO: stderr: ""
    Jan  3 02:37:28.586: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4603-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Jan  3 02:37:28.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-5180 explain e2e-test-crd-publish-openapi-4603-crds.spec'
    Jan  3 02:37:28.794: INFO: stderr: ""
    Jan  3 02:37:28.794: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4603-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Jan  3 02:37:28.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-5180 explain e2e-test-crd-publish-openapi-4603-crds.spec.bars'
    Jan  3 02:37:29.003: INFO: stderr: ""
    Jan  3 02:37:29.003: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4603-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 01/03/23 02:37:29.003
    Jan  3 02:37:29.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-5180 explain e2e-test-crd-publish-openapi-4603-crds.spec.bars2'
    Jan  3 02:37:29.211: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  3 02:37:34.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-5180" for this suite. 01/03/23 02:37:34.598
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:37:34.601
Jan  3 02:37:34.601: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename controllerrevisions 01/03/23 02:37:34.602
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:37:34.619
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:37:34.621
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-7vq4m-daemon-set" 01/03/23 02:37:34.642
STEP: Check that daemon pods launch on every node of the cluster. 01/03/23 02:37:34.646
Jan  3 02:37:34.650: INFO: Number of nodes with available pods controlled by daemonset e2e-7vq4m-daemon-set: 0
Jan  3 02:37:34.650: INFO: Node cncf-master is running 0 daemon pod, expected 1
Jan  3 02:37:35.664: INFO: Number of nodes with available pods controlled by daemonset e2e-7vq4m-daemon-set: 0
Jan  3 02:37:35.664: INFO: Node cncf-master is running 0 daemon pod, expected 1
Jan  3 02:37:36.656: INFO: Number of nodes with available pods controlled by daemonset e2e-7vq4m-daemon-set: 3
Jan  3 02:37:36.656: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-7vq4m-daemon-set
STEP: Confirm DaemonSet "e2e-7vq4m-daemon-set" successfully created with "daemonset-name=e2e-7vq4m-daemon-set" label 01/03/23 02:37:36.658
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-7vq4m-daemon-set" 01/03/23 02:37:36.661
Jan  3 02:37:36.663: INFO: Located ControllerRevision: "e2e-7vq4m-daemon-set-679f5b5776"
STEP: Patching ControllerRevision "e2e-7vq4m-daemon-set-679f5b5776" 01/03/23 02:37:36.664
Jan  3 02:37:36.668: INFO: e2e-7vq4m-daemon-set-679f5b5776 has been patched
STEP: Create a new ControllerRevision 01/03/23 02:37:36.668
Jan  3 02:37:36.672: INFO: Created ControllerRevision: e2e-7vq4m-daemon-set-fd98b7cc4
STEP: Confirm that there are two ControllerRevisions 01/03/23 02:37:36.672
Jan  3 02:37:36.672: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan  3 02:37:36.687: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-7vq4m-daemon-set-679f5b5776" 01/03/23 02:37:36.687
STEP: Confirm that there is only one ControllerRevision 01/03/23 02:37:36.689
Jan  3 02:37:36.689: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan  3 02:37:36.694: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-7vq4m-daemon-set-fd98b7cc4" 01/03/23 02:37:36.695
Jan  3 02:37:36.699: INFO: e2e-7vq4m-daemon-set-fd98b7cc4 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 01/03/23 02:37:36.699
W0103 02:37:36.706218      23 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 01/03/23 02:37:36.706
Jan  3 02:37:36.706: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan  3 02:37:37.751: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan  3 02:37:37.754: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-7vq4m-daemon-set-fd98b7cc4=updated" 01/03/23 02:37:37.754
STEP: Confirm that there is only one ControllerRevision 01/03/23 02:37:37.757
Jan  3 02:37:37.757: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan  3 02:37:37.763: INFO: Found 1 ControllerRevisions
Jan  3 02:37:37.764: INFO: ControllerRevision "e2e-7vq4m-daemon-set-6fbb6c74d7" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-7vq4m-daemon-set" 01/03/23 02:37:37.766
STEP: deleting DaemonSet.extensions e2e-7vq4m-daemon-set in namespace controllerrevisions-6941, will wait for the garbage collector to delete the pods 01/03/23 02:37:37.766
Jan  3 02:37:37.821: INFO: Deleting DaemonSet.extensions e2e-7vq4m-daemon-set took: 2.539758ms
Jan  3 02:37:37.921: INFO: Terminating DaemonSet.extensions e2e-7vq4m-daemon-set pods took: 100.508067ms
Jan  3 02:37:39.631: INFO: Number of nodes with available pods controlled by daemonset e2e-7vq4m-daemon-set: 0
Jan  3 02:37:39.631: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-7vq4m-daemon-set
Jan  3 02:37:39.632: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"996872"},"items":null}

Jan  3 02:37:39.634: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"996872"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:187
Jan  3 02:37:39.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "controllerrevisions-6941" for this suite. 01/03/23 02:37:39.644
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]","completed":166,"skipped":3001,"failed":0}
------------------------------
• [SLOW TEST] [5.046 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:37:34.601
    Jan  3 02:37:34.601: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename controllerrevisions 01/03/23 02:37:34.602
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:37:34.619
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:37:34.621
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-7vq4m-daemon-set" 01/03/23 02:37:34.642
    STEP: Check that daemon pods launch on every node of the cluster. 01/03/23 02:37:34.646
    Jan  3 02:37:34.650: INFO: Number of nodes with available pods controlled by daemonset e2e-7vq4m-daemon-set: 0
    Jan  3 02:37:34.650: INFO: Node cncf-master is running 0 daemon pod, expected 1
    Jan  3 02:37:35.664: INFO: Number of nodes with available pods controlled by daemonset e2e-7vq4m-daemon-set: 0
    Jan  3 02:37:35.664: INFO: Node cncf-master is running 0 daemon pod, expected 1
    Jan  3 02:37:36.656: INFO: Number of nodes with available pods controlled by daemonset e2e-7vq4m-daemon-set: 3
    Jan  3 02:37:36.656: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-7vq4m-daemon-set
    STEP: Confirm DaemonSet "e2e-7vq4m-daemon-set" successfully created with "daemonset-name=e2e-7vq4m-daemon-set" label 01/03/23 02:37:36.658
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-7vq4m-daemon-set" 01/03/23 02:37:36.661
    Jan  3 02:37:36.663: INFO: Located ControllerRevision: "e2e-7vq4m-daemon-set-679f5b5776"
    STEP: Patching ControllerRevision "e2e-7vq4m-daemon-set-679f5b5776" 01/03/23 02:37:36.664
    Jan  3 02:37:36.668: INFO: e2e-7vq4m-daemon-set-679f5b5776 has been patched
    STEP: Create a new ControllerRevision 01/03/23 02:37:36.668
    Jan  3 02:37:36.672: INFO: Created ControllerRevision: e2e-7vq4m-daemon-set-fd98b7cc4
    STEP: Confirm that there are two ControllerRevisions 01/03/23 02:37:36.672
    Jan  3 02:37:36.672: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan  3 02:37:36.687: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-7vq4m-daemon-set-679f5b5776" 01/03/23 02:37:36.687
    STEP: Confirm that there is only one ControllerRevision 01/03/23 02:37:36.689
    Jan  3 02:37:36.689: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan  3 02:37:36.694: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-7vq4m-daemon-set-fd98b7cc4" 01/03/23 02:37:36.695
    Jan  3 02:37:36.699: INFO: e2e-7vq4m-daemon-set-fd98b7cc4 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 01/03/23 02:37:36.699
    W0103 02:37:36.706218      23 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 01/03/23 02:37:36.706
    Jan  3 02:37:36.706: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan  3 02:37:37.751: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan  3 02:37:37.754: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-7vq4m-daemon-set-fd98b7cc4=updated" 01/03/23 02:37:37.754
    STEP: Confirm that there is only one ControllerRevision 01/03/23 02:37:37.757
    Jan  3 02:37:37.757: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan  3 02:37:37.763: INFO: Found 1 ControllerRevisions
    Jan  3 02:37:37.764: INFO: ControllerRevision "e2e-7vq4m-daemon-set-6fbb6c74d7" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-7vq4m-daemon-set" 01/03/23 02:37:37.766
    STEP: deleting DaemonSet.extensions e2e-7vq4m-daemon-set in namespace controllerrevisions-6941, will wait for the garbage collector to delete the pods 01/03/23 02:37:37.766
    Jan  3 02:37:37.821: INFO: Deleting DaemonSet.extensions e2e-7vq4m-daemon-set took: 2.539758ms
    Jan  3 02:37:37.921: INFO: Terminating DaemonSet.extensions e2e-7vq4m-daemon-set pods took: 100.508067ms
    Jan  3 02:37:39.631: INFO: Number of nodes with available pods controlled by daemonset e2e-7vq4m-daemon-set: 0
    Jan  3 02:37:39.631: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-7vq4m-daemon-set
    Jan  3 02:37:39.632: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"996872"},"items":null}

    Jan  3 02:37:39.634: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"996872"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:187
    Jan  3 02:37:39.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "controllerrevisions-6941" for this suite. 01/03/23 02:37:39.644
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:37:39.647
Jan  3 02:37:39.647: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename replicaset 01/03/23 02:37:39.648
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:37:39.66
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:37:39.663
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Jan  3 02:37:39.665: INFO: Creating ReplicaSet my-hostname-basic-cbce7d66-d84b-4554-844e-bac0229bbb23
Jan  3 02:37:39.688: INFO: Pod name my-hostname-basic-cbce7d66-d84b-4554-844e-bac0229bbb23: Found 0 pods out of 1
Jan  3 02:37:44.696: INFO: Pod name my-hostname-basic-cbce7d66-d84b-4554-844e-bac0229bbb23: Found 1 pods out of 1
Jan  3 02:37:44.696: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-cbce7d66-d84b-4554-844e-bac0229bbb23" is running
Jan  3 02:37:44.696: INFO: Waiting up to 5m0s for pod "my-hostname-basic-cbce7d66-d84b-4554-844e-bac0229bbb23-b4hdg" in namespace "replicaset-5870" to be "running"
Jan  3 02:37:44.701: INFO: Pod "my-hostname-basic-cbce7d66-d84b-4554-844e-bac0229bbb23-b4hdg": Phase="Running", Reason="", readiness=true. Elapsed: 5.262183ms
Jan  3 02:37:44.701: INFO: Pod "my-hostname-basic-cbce7d66-d84b-4554-844e-bac0229bbb23-b4hdg" satisfied condition "running"
Jan  3 02:37:44.701: INFO: Pod "my-hostname-basic-cbce7d66-d84b-4554-844e-bac0229bbb23-b4hdg" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-03 02:37:39 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-03 02:37:41 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-03 02:37:41 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-03 02:37:39 +0000 UTC Reason: Message:}])
Jan  3 02:37:44.701: INFO: Trying to dial the pod
Jan  3 02:37:49.709: INFO: Controller my-hostname-basic-cbce7d66-d84b-4554-844e-bac0229bbb23: Got expected result from replica 1 [my-hostname-basic-cbce7d66-d84b-4554-844e-bac0229bbb23-b4hdg]: "my-hostname-basic-cbce7d66-d84b-4554-844e-bac0229bbb23-b4hdg", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan  3 02:37:49.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5870" for this suite. 01/03/23 02:37:49.712
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","completed":167,"skipped":3008,"failed":0}
------------------------------
• [SLOW TEST] [10.068 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:37:39.647
    Jan  3 02:37:39.647: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename replicaset 01/03/23 02:37:39.648
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:37:39.66
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:37:39.663
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Jan  3 02:37:39.665: INFO: Creating ReplicaSet my-hostname-basic-cbce7d66-d84b-4554-844e-bac0229bbb23
    Jan  3 02:37:39.688: INFO: Pod name my-hostname-basic-cbce7d66-d84b-4554-844e-bac0229bbb23: Found 0 pods out of 1
    Jan  3 02:37:44.696: INFO: Pod name my-hostname-basic-cbce7d66-d84b-4554-844e-bac0229bbb23: Found 1 pods out of 1
    Jan  3 02:37:44.696: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-cbce7d66-d84b-4554-844e-bac0229bbb23" is running
    Jan  3 02:37:44.696: INFO: Waiting up to 5m0s for pod "my-hostname-basic-cbce7d66-d84b-4554-844e-bac0229bbb23-b4hdg" in namespace "replicaset-5870" to be "running"
    Jan  3 02:37:44.701: INFO: Pod "my-hostname-basic-cbce7d66-d84b-4554-844e-bac0229bbb23-b4hdg": Phase="Running", Reason="", readiness=true. Elapsed: 5.262183ms
    Jan  3 02:37:44.701: INFO: Pod "my-hostname-basic-cbce7d66-d84b-4554-844e-bac0229bbb23-b4hdg" satisfied condition "running"
    Jan  3 02:37:44.701: INFO: Pod "my-hostname-basic-cbce7d66-d84b-4554-844e-bac0229bbb23-b4hdg" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-03 02:37:39 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-03 02:37:41 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-03 02:37:41 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-03 02:37:39 +0000 UTC Reason: Message:}])
    Jan  3 02:37:44.701: INFO: Trying to dial the pod
    Jan  3 02:37:49.709: INFO: Controller my-hostname-basic-cbce7d66-d84b-4554-844e-bac0229bbb23: Got expected result from replica 1 [my-hostname-basic-cbce7d66-d84b-4554-844e-bac0229bbb23-b4hdg]: "my-hostname-basic-cbce7d66-d84b-4554-844e-bac0229bbb23-b4hdg", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan  3 02:37:49.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-5870" for this suite. 01/03/23 02:37:49.712
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:37:49.715
Jan  3 02:37:49.715: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename container-runtime 01/03/23 02:37:49.717
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:37:49.73
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:37:49.732
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
STEP: create the container 01/03/23 02:37:49.734
STEP: wait for the container to reach Succeeded 01/03/23 02:37:49.777
STEP: get the container status 01/03/23 02:37:53.793
STEP: the container should be terminated 01/03/23 02:37:53.795
STEP: the termination message should be set 01/03/23 02:37:53.795
Jan  3 02:37:53.795: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 01/03/23 02:37:53.795
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan  3 02:37:53.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4330" for this suite. 01/03/23 02:37:53.861
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","completed":168,"skipped":3010,"failed":0}
------------------------------
• [4.148 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:37:49.715
    Jan  3 02:37:49.715: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename container-runtime 01/03/23 02:37:49.717
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:37:49.73
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:37:49.732
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194
    STEP: create the container 01/03/23 02:37:49.734
    STEP: wait for the container to reach Succeeded 01/03/23 02:37:49.777
    STEP: get the container status 01/03/23 02:37:53.793
    STEP: the container should be terminated 01/03/23 02:37:53.795
    STEP: the termination message should be set 01/03/23 02:37:53.795
    Jan  3 02:37:53.795: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 01/03/23 02:37:53.795
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan  3 02:37:53.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-4330" for this suite. 01/03/23 02:37:53.861
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:37:53.864
Jan  3 02:37:53.864: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename configmap 01/03/23 02:37:53.865
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:37:53.881
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:37:53.882
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
STEP: Creating configMap with name configmap-test-volume-map-056cf9d2-c457-48c6-ae9a-10445eed60b7 01/03/23 02:37:53.884
STEP: Creating a pod to test consume configMaps 01/03/23 02:37:53.887
Jan  3 02:37:53.907: INFO: Waiting up to 5m0s for pod "pod-configmaps-06162934-c947-41a2-883e-58c2a3434195" in namespace "configmap-1043" to be "Succeeded or Failed"
Jan  3 02:37:53.913: INFO: Pod "pod-configmaps-06162934-c947-41a2-883e-58c2a3434195": Phase="Pending", Reason="", readiness=false. Elapsed: 6.568095ms
Jan  3 02:37:55.915: INFO: Pod "pod-configmaps-06162934-c947-41a2-883e-58c2a3434195": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008840143s
Jan  3 02:37:57.922: INFO: Pod "pod-configmaps-06162934-c947-41a2-883e-58c2a3434195": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015681051s
STEP: Saw pod success 01/03/23 02:37:57.922
Jan  3 02:37:57.922: INFO: Pod "pod-configmaps-06162934-c947-41a2-883e-58c2a3434195" satisfied condition "Succeeded or Failed"
Jan  3 02:37:57.924: INFO: Trying to get logs from node cncf-master pod pod-configmaps-06162934-c947-41a2-883e-58c2a3434195 container agnhost-container: <nil>
STEP: delete the pod 01/03/23 02:37:57.927
Jan  3 02:37:57.948: INFO: Waiting for pod pod-configmaps-06162934-c947-41a2-883e-58c2a3434195 to disappear
Jan  3 02:37:57.953: INFO: Pod pod-configmaps-06162934-c947-41a2-883e-58c2a3434195 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan  3 02:37:57.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1043" for this suite. 01/03/23 02:37:57.956
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":169,"skipped":3039,"failed":0}
------------------------------
• [4.095 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:37:53.864
    Jan  3 02:37:53.864: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename configmap 01/03/23 02:37:53.865
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:37:53.881
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:37:53.882
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:108
    STEP: Creating configMap with name configmap-test-volume-map-056cf9d2-c457-48c6-ae9a-10445eed60b7 01/03/23 02:37:53.884
    STEP: Creating a pod to test consume configMaps 01/03/23 02:37:53.887
    Jan  3 02:37:53.907: INFO: Waiting up to 5m0s for pod "pod-configmaps-06162934-c947-41a2-883e-58c2a3434195" in namespace "configmap-1043" to be "Succeeded or Failed"
    Jan  3 02:37:53.913: INFO: Pod "pod-configmaps-06162934-c947-41a2-883e-58c2a3434195": Phase="Pending", Reason="", readiness=false. Elapsed: 6.568095ms
    Jan  3 02:37:55.915: INFO: Pod "pod-configmaps-06162934-c947-41a2-883e-58c2a3434195": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008840143s
    Jan  3 02:37:57.922: INFO: Pod "pod-configmaps-06162934-c947-41a2-883e-58c2a3434195": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015681051s
    STEP: Saw pod success 01/03/23 02:37:57.922
    Jan  3 02:37:57.922: INFO: Pod "pod-configmaps-06162934-c947-41a2-883e-58c2a3434195" satisfied condition "Succeeded or Failed"
    Jan  3 02:37:57.924: INFO: Trying to get logs from node cncf-master pod pod-configmaps-06162934-c947-41a2-883e-58c2a3434195 container agnhost-container: <nil>
    STEP: delete the pod 01/03/23 02:37:57.927
    Jan  3 02:37:57.948: INFO: Waiting for pod pod-configmaps-06162934-c947-41a2-883e-58c2a3434195 to disappear
    Jan  3 02:37:57.953: INFO: Pod pod-configmaps-06162934-c947-41a2-883e-58c2a3434195 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan  3 02:37:57.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1043" for this suite. 01/03/23 02:37:57.956
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:37:57.959
Jan  3 02:37:57.959: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename daemonsets 01/03/23 02:37:57.96
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:37:57.981
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:37:57.983
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
Jan  3 02:37:58.018: INFO: Create a RollingUpdate DaemonSet
Jan  3 02:37:58.022: INFO: Check that daemon pods launch on every node of the cluster
Jan  3 02:37:58.055: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  3 02:37:58.055: INFO: Node cncf-master is running 0 daemon pod, expected 1
Jan  3 02:37:59.066: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  3 02:37:59.066: INFO: Node cncf-master is running 0 daemon pod, expected 1
Jan  3 02:38:00.079: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan  3 02:38:00.079: INFO: Node cncf-node2 is running 0 daemon pod, expected 1
Jan  3 02:38:01.060: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan  3 02:38:01.060: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
Jan  3 02:38:01.060: INFO: Update the DaemonSet to trigger a rollout
Jan  3 02:38:01.065: INFO: Updating DaemonSet daemon-set
Jan  3 02:38:03.082: INFO: Roll back the DaemonSet before rollout is complete
Jan  3 02:38:03.086: INFO: Updating DaemonSet daemon-set
Jan  3 02:38:03.086: INFO: Make sure DaemonSet rollback is complete
Jan  3 02:38:03.110: INFO: Wrong image for pod: daemon-set-xlp6s. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Jan  3 02:38:03.110: INFO: Pod daemon-set-xlp6s is not available
Jan  3 02:38:10.131: INFO: Pod daemon-set-2kq65 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/03/23 02:38:10.138
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-109, will wait for the garbage collector to delete the pods 01/03/23 02:38:10.139
Jan  3 02:38:10.207: INFO: Deleting DaemonSet.extensions daemon-set took: 15.686478ms
Jan  3 02:38:10.308: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.835388ms
Jan  3 02:38:11.710: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  3 02:38:11.710: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan  3 02:38:11.712: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"997252"},"items":null}

Jan  3 02:38:11.713: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"997252"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan  3 02:38:11.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-109" for this suite. 01/03/23 02:38:11.724
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","completed":170,"skipped":3039,"failed":0}
------------------------------
• [SLOW TEST] [13.768 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:37:57.959
    Jan  3 02:37:57.959: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename daemonsets 01/03/23 02:37:57.96
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:37:57.981
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:37:57.983
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:431
    Jan  3 02:37:58.018: INFO: Create a RollingUpdate DaemonSet
    Jan  3 02:37:58.022: INFO: Check that daemon pods launch on every node of the cluster
    Jan  3 02:37:58.055: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  3 02:37:58.055: INFO: Node cncf-master is running 0 daemon pod, expected 1
    Jan  3 02:37:59.066: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  3 02:37:59.066: INFO: Node cncf-master is running 0 daemon pod, expected 1
    Jan  3 02:38:00.079: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan  3 02:38:00.079: INFO: Node cncf-node2 is running 0 daemon pod, expected 1
    Jan  3 02:38:01.060: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan  3 02:38:01.060: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    Jan  3 02:38:01.060: INFO: Update the DaemonSet to trigger a rollout
    Jan  3 02:38:01.065: INFO: Updating DaemonSet daemon-set
    Jan  3 02:38:03.082: INFO: Roll back the DaemonSet before rollout is complete
    Jan  3 02:38:03.086: INFO: Updating DaemonSet daemon-set
    Jan  3 02:38:03.086: INFO: Make sure DaemonSet rollback is complete
    Jan  3 02:38:03.110: INFO: Wrong image for pod: daemon-set-xlp6s. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
    Jan  3 02:38:03.110: INFO: Pod daemon-set-xlp6s is not available
    Jan  3 02:38:10.131: INFO: Pod daemon-set-2kq65 is not available
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/03/23 02:38:10.138
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-109, will wait for the garbage collector to delete the pods 01/03/23 02:38:10.139
    Jan  3 02:38:10.207: INFO: Deleting DaemonSet.extensions daemon-set took: 15.686478ms
    Jan  3 02:38:10.308: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.835388ms
    Jan  3 02:38:11.710: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  3 02:38:11.710: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan  3 02:38:11.712: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"997252"},"items":null}

    Jan  3 02:38:11.713: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"997252"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan  3 02:38:11.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-109" for this suite. 01/03/23 02:38:11.724
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:38:11.727
Jan  3 02:38:11.727: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename pods 01/03/23 02:38:11.728
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:38:11.745
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:38:11.746
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
STEP: creating the pod 01/03/23 02:38:11.748
STEP: submitting the pod to kubernetes 01/03/23 02:38:11.748
Jan  3 02:38:11.754: INFO: Waiting up to 5m0s for pod "pod-update-c5f96623-a88b-4e4a-a743-79a433cc0269" in namespace "pods-5534" to be "running and ready"
Jan  3 02:38:11.768: INFO: Pod "pod-update-c5f96623-a88b-4e4a-a743-79a433cc0269": Phase="Pending", Reason="", readiness=false. Elapsed: 14.47471ms
Jan  3 02:38:11.768: INFO: The phase of Pod pod-update-c5f96623-a88b-4e4a-a743-79a433cc0269 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 02:38:13.771: INFO: Pod "pod-update-c5f96623-a88b-4e4a-a743-79a433cc0269": Phase="Running", Reason="", readiness=true. Elapsed: 2.017831385s
Jan  3 02:38:13.771: INFO: The phase of Pod pod-update-c5f96623-a88b-4e4a-a743-79a433cc0269 is Running (Ready = true)
Jan  3 02:38:13.771: INFO: Pod "pod-update-c5f96623-a88b-4e4a-a743-79a433cc0269" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 01/03/23 02:38:13.773
STEP: updating the pod 01/03/23 02:38:13.775
Jan  3 02:38:14.281: INFO: Successfully updated pod "pod-update-c5f96623-a88b-4e4a-a743-79a433cc0269"
Jan  3 02:38:14.281: INFO: Waiting up to 5m0s for pod "pod-update-c5f96623-a88b-4e4a-a743-79a433cc0269" in namespace "pods-5534" to be "running"
Jan  3 02:38:14.290: INFO: Pod "pod-update-c5f96623-a88b-4e4a-a743-79a433cc0269": Phase="Running", Reason="", readiness=true. Elapsed: 8.67598ms
Jan  3 02:38:14.290: INFO: Pod "pod-update-c5f96623-a88b-4e4a-a743-79a433cc0269" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 01/03/23 02:38:14.29
Jan  3 02:38:14.292: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan  3 02:38:14.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5534" for this suite. 01/03/23 02:38:14.295
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","completed":171,"skipped":3041,"failed":0}
------------------------------
• [2.570 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:38:11.727
    Jan  3 02:38:11.727: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename pods 01/03/23 02:38:11.728
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:38:11.745
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:38:11.746
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:343
    STEP: creating the pod 01/03/23 02:38:11.748
    STEP: submitting the pod to kubernetes 01/03/23 02:38:11.748
    Jan  3 02:38:11.754: INFO: Waiting up to 5m0s for pod "pod-update-c5f96623-a88b-4e4a-a743-79a433cc0269" in namespace "pods-5534" to be "running and ready"
    Jan  3 02:38:11.768: INFO: Pod "pod-update-c5f96623-a88b-4e4a-a743-79a433cc0269": Phase="Pending", Reason="", readiness=false. Elapsed: 14.47471ms
    Jan  3 02:38:11.768: INFO: The phase of Pod pod-update-c5f96623-a88b-4e4a-a743-79a433cc0269 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 02:38:13.771: INFO: Pod "pod-update-c5f96623-a88b-4e4a-a743-79a433cc0269": Phase="Running", Reason="", readiness=true. Elapsed: 2.017831385s
    Jan  3 02:38:13.771: INFO: The phase of Pod pod-update-c5f96623-a88b-4e4a-a743-79a433cc0269 is Running (Ready = true)
    Jan  3 02:38:13.771: INFO: Pod "pod-update-c5f96623-a88b-4e4a-a743-79a433cc0269" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 01/03/23 02:38:13.773
    STEP: updating the pod 01/03/23 02:38:13.775
    Jan  3 02:38:14.281: INFO: Successfully updated pod "pod-update-c5f96623-a88b-4e4a-a743-79a433cc0269"
    Jan  3 02:38:14.281: INFO: Waiting up to 5m0s for pod "pod-update-c5f96623-a88b-4e4a-a743-79a433cc0269" in namespace "pods-5534" to be "running"
    Jan  3 02:38:14.290: INFO: Pod "pod-update-c5f96623-a88b-4e4a-a743-79a433cc0269": Phase="Running", Reason="", readiness=true. Elapsed: 8.67598ms
    Jan  3 02:38:14.290: INFO: Pod "pod-update-c5f96623-a88b-4e4a-a743-79a433cc0269" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 01/03/23 02:38:14.29
    Jan  3 02:38:14.292: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan  3 02:38:14.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-5534" for this suite. 01/03/23 02:38:14.295
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Services
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:38:14.298
Jan  3 02:38:14.298: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename services 01/03/23 02:38:14.299
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:38:14.307
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:38:14.309
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
STEP: creating service in namespace services-6976 01/03/23 02:38:14.311
Jan  3 02:38:14.316: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-6976" to be "running and ready"
Jan  3 02:38:14.325: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 8.668094ms
Jan  3 02:38:14.325: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jan  3 02:38:16.334: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.017719317s
Jan  3 02:38:16.334: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Jan  3 02:38:16.334: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Jan  3 02:38:16.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-6976 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jan  3 02:38:16.483: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jan  3 02:38:16.483: INFO: stdout: "iptables"
Jan  3 02:38:16.483: INFO: proxyMode: iptables
Jan  3 02:38:16.505: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jan  3 02:38:16.511: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-6976 01/03/23 02:38:16.511
STEP: creating replication controller affinity-nodeport-timeout in namespace services-6976 01/03/23 02:38:16.534
I0103 02:38:16.540006      23 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-6976, replica count: 3
I0103 02:38:19.590821      23 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  3 02:38:19.600: INFO: Creating new exec pod
Jan  3 02:38:19.604: INFO: Waiting up to 5m0s for pod "execpod-affinityfwh67" in namespace "services-6976" to be "running"
Jan  3 02:38:19.609: INFO: Pod "execpod-affinityfwh67": Phase="Pending", Reason="", readiness=false. Elapsed: 4.589621ms
Jan  3 02:38:21.611: INFO: Pod "execpod-affinityfwh67": Phase="Running", Reason="", readiness=true. Elapsed: 2.006835806s
Jan  3 02:38:21.611: INFO: Pod "execpod-affinityfwh67" satisfied condition "running"
Jan  3 02:38:22.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-6976 exec execpod-affinityfwh67 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Jan  3 02:38:22.751: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Jan  3 02:38:22.751: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  3 02:38:22.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-6976 exec execpod-affinityfwh67 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.139.7 80'
Jan  3 02:38:22.871: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.139.7 80\nConnection to 10.96.139.7 80 port [tcp/http] succeeded!\n"
Jan  3 02:38:22.871: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  3 02:38:22.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-6976 exec execpod-affinityfwh67 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.7.5 32089'
Jan  3 02:38:22.983: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.7.5 32089\nConnection to 172.21.7.5 32089 port [tcp/*] succeeded!\n"
Jan  3 02:38:22.983: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  3 02:38:22.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-6976 exec execpod-affinityfwh67 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.7.7 32089'
Jan  3 02:38:23.102: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.7.7 32089\nConnection to 172.21.7.7 32089 port [tcp/*] succeeded!\n"
Jan  3 02:38:23.102: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  3 02:38:23.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-6976 exec execpod-affinityfwh67 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.7.5:32089/ ; done'
Jan  3 02:38:23.305: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:32089/\n"
Jan  3 02:38:23.305: INFO: stdout: "\naffinity-nodeport-timeout-7pch7\naffinity-nodeport-timeout-7pch7\naffinity-nodeport-timeout-7pch7\naffinity-nodeport-timeout-7pch7\naffinity-nodeport-timeout-7pch7\naffinity-nodeport-timeout-7pch7\naffinity-nodeport-timeout-7pch7\naffinity-nodeport-timeout-7pch7\naffinity-nodeport-timeout-7pch7\naffinity-nodeport-timeout-7pch7\naffinity-nodeport-timeout-7pch7\naffinity-nodeport-timeout-7pch7\naffinity-nodeport-timeout-7pch7\naffinity-nodeport-timeout-7pch7\naffinity-nodeport-timeout-7pch7\naffinity-nodeport-timeout-7pch7"
Jan  3 02:38:23.305: INFO: Received response from host: affinity-nodeport-timeout-7pch7
Jan  3 02:38:23.305: INFO: Received response from host: affinity-nodeport-timeout-7pch7
Jan  3 02:38:23.305: INFO: Received response from host: affinity-nodeport-timeout-7pch7
Jan  3 02:38:23.305: INFO: Received response from host: affinity-nodeport-timeout-7pch7
Jan  3 02:38:23.305: INFO: Received response from host: affinity-nodeport-timeout-7pch7
Jan  3 02:38:23.305: INFO: Received response from host: affinity-nodeport-timeout-7pch7
Jan  3 02:38:23.305: INFO: Received response from host: affinity-nodeport-timeout-7pch7
Jan  3 02:38:23.305: INFO: Received response from host: affinity-nodeport-timeout-7pch7
Jan  3 02:38:23.305: INFO: Received response from host: affinity-nodeport-timeout-7pch7
Jan  3 02:38:23.305: INFO: Received response from host: affinity-nodeport-timeout-7pch7
Jan  3 02:38:23.305: INFO: Received response from host: affinity-nodeport-timeout-7pch7
Jan  3 02:38:23.305: INFO: Received response from host: affinity-nodeport-timeout-7pch7
Jan  3 02:38:23.305: INFO: Received response from host: affinity-nodeport-timeout-7pch7
Jan  3 02:38:23.305: INFO: Received response from host: affinity-nodeport-timeout-7pch7
Jan  3 02:38:23.305: INFO: Received response from host: affinity-nodeport-timeout-7pch7
Jan  3 02:38:23.305: INFO: Received response from host: affinity-nodeport-timeout-7pch7
Jan  3 02:38:23.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-6976 exec execpod-affinityfwh67 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.7.5:32089/'
Jan  3 02:38:23.432: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.7.5:32089/\n"
Jan  3 02:38:23.432: INFO: stdout: "affinity-nodeport-timeout-7pch7"
Jan  3 02:38:43.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-6976 exec execpod-affinityfwh67 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.7.5:32089/'
Jan  3 02:38:43.573: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.7.5:32089/\n"
Jan  3 02:38:43.573: INFO: stdout: "affinity-nodeport-timeout-7pch7"
Jan  3 02:39:03.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-6976 exec execpod-affinityfwh67 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.7.5:32089/'
Jan  3 02:39:03.718: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.7.5:32089/\n"
Jan  3 02:39:03.718: INFO: stdout: "affinity-nodeport-timeout-zzs9w"
Jan  3 02:39:03.718: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-6976, will wait for the garbage collector to delete the pods 01/03/23 02:39:03.73
Jan  3 02:39:03.814: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 2.635212ms
Jan  3 02:39:03.915: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.436173ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  3 02:39:06.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6976" for this suite. 01/03/23 02:39:06.278
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","completed":172,"skipped":3053,"failed":0}
------------------------------
• [SLOW TEST] [51.982 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:38:14.298
    Jan  3 02:38:14.298: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename services 01/03/23 02:38:14.299
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:38:14.307
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:38:14.309
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2221
    STEP: creating service in namespace services-6976 01/03/23 02:38:14.311
    Jan  3 02:38:14.316: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-6976" to be "running and ready"
    Jan  3 02:38:14.325: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 8.668094ms
    Jan  3 02:38:14.325: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 02:38:16.334: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.017719317s
    Jan  3 02:38:16.334: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Jan  3 02:38:16.334: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Jan  3 02:38:16.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-6976 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Jan  3 02:38:16.483: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    Jan  3 02:38:16.483: INFO: stdout: "iptables"
    Jan  3 02:38:16.483: INFO: proxyMode: iptables
    Jan  3 02:38:16.505: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Jan  3 02:38:16.511: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-nodeport-timeout in namespace services-6976 01/03/23 02:38:16.511
    STEP: creating replication controller affinity-nodeport-timeout in namespace services-6976 01/03/23 02:38:16.534
    I0103 02:38:16.540006      23 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-6976, replica count: 3
    I0103 02:38:19.590821      23 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  3 02:38:19.600: INFO: Creating new exec pod
    Jan  3 02:38:19.604: INFO: Waiting up to 5m0s for pod "execpod-affinityfwh67" in namespace "services-6976" to be "running"
    Jan  3 02:38:19.609: INFO: Pod "execpod-affinityfwh67": Phase="Pending", Reason="", readiness=false. Elapsed: 4.589621ms
    Jan  3 02:38:21.611: INFO: Pod "execpod-affinityfwh67": Phase="Running", Reason="", readiness=true. Elapsed: 2.006835806s
    Jan  3 02:38:21.611: INFO: Pod "execpod-affinityfwh67" satisfied condition "running"
    Jan  3 02:38:22.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-6976 exec execpod-affinityfwh67 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
    Jan  3 02:38:22.751: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
    Jan  3 02:38:22.751: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  3 02:38:22.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-6976 exec execpod-affinityfwh67 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.139.7 80'
    Jan  3 02:38:22.871: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.139.7 80\nConnection to 10.96.139.7 80 port [tcp/http] succeeded!\n"
    Jan  3 02:38:22.871: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  3 02:38:22.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-6976 exec execpod-affinityfwh67 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.7.5 32089'
    Jan  3 02:38:22.983: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.7.5 32089\nConnection to 172.21.7.5 32089 port [tcp/*] succeeded!\n"
    Jan  3 02:38:22.983: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  3 02:38:22.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-6976 exec execpod-affinityfwh67 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.7.7 32089'
    Jan  3 02:38:23.102: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.7.7 32089\nConnection to 172.21.7.7 32089 port [tcp/*] succeeded!\n"
    Jan  3 02:38:23.102: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  3 02:38:23.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-6976 exec execpod-affinityfwh67 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.7.5:32089/ ; done'
    Jan  3 02:38:23.305: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:32089/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:32089/\n"
    Jan  3 02:38:23.305: INFO: stdout: "\naffinity-nodeport-timeout-7pch7\naffinity-nodeport-timeout-7pch7\naffinity-nodeport-timeout-7pch7\naffinity-nodeport-timeout-7pch7\naffinity-nodeport-timeout-7pch7\naffinity-nodeport-timeout-7pch7\naffinity-nodeport-timeout-7pch7\naffinity-nodeport-timeout-7pch7\naffinity-nodeport-timeout-7pch7\naffinity-nodeport-timeout-7pch7\naffinity-nodeport-timeout-7pch7\naffinity-nodeport-timeout-7pch7\naffinity-nodeport-timeout-7pch7\naffinity-nodeport-timeout-7pch7\naffinity-nodeport-timeout-7pch7\naffinity-nodeport-timeout-7pch7"
    Jan  3 02:38:23.305: INFO: Received response from host: affinity-nodeport-timeout-7pch7
    Jan  3 02:38:23.305: INFO: Received response from host: affinity-nodeport-timeout-7pch7
    Jan  3 02:38:23.305: INFO: Received response from host: affinity-nodeport-timeout-7pch7
    Jan  3 02:38:23.305: INFO: Received response from host: affinity-nodeport-timeout-7pch7
    Jan  3 02:38:23.305: INFO: Received response from host: affinity-nodeport-timeout-7pch7
    Jan  3 02:38:23.305: INFO: Received response from host: affinity-nodeport-timeout-7pch7
    Jan  3 02:38:23.305: INFO: Received response from host: affinity-nodeport-timeout-7pch7
    Jan  3 02:38:23.305: INFO: Received response from host: affinity-nodeport-timeout-7pch7
    Jan  3 02:38:23.305: INFO: Received response from host: affinity-nodeport-timeout-7pch7
    Jan  3 02:38:23.305: INFO: Received response from host: affinity-nodeport-timeout-7pch7
    Jan  3 02:38:23.305: INFO: Received response from host: affinity-nodeport-timeout-7pch7
    Jan  3 02:38:23.305: INFO: Received response from host: affinity-nodeport-timeout-7pch7
    Jan  3 02:38:23.305: INFO: Received response from host: affinity-nodeport-timeout-7pch7
    Jan  3 02:38:23.305: INFO: Received response from host: affinity-nodeport-timeout-7pch7
    Jan  3 02:38:23.305: INFO: Received response from host: affinity-nodeport-timeout-7pch7
    Jan  3 02:38:23.305: INFO: Received response from host: affinity-nodeport-timeout-7pch7
    Jan  3 02:38:23.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-6976 exec execpod-affinityfwh67 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.7.5:32089/'
    Jan  3 02:38:23.432: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.7.5:32089/\n"
    Jan  3 02:38:23.432: INFO: stdout: "affinity-nodeport-timeout-7pch7"
    Jan  3 02:38:43.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-6976 exec execpod-affinityfwh67 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.7.5:32089/'
    Jan  3 02:38:43.573: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.7.5:32089/\n"
    Jan  3 02:38:43.573: INFO: stdout: "affinity-nodeport-timeout-7pch7"
    Jan  3 02:39:03.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-6976 exec execpod-affinityfwh67 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.7.5:32089/'
    Jan  3 02:39:03.718: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.7.5:32089/\n"
    Jan  3 02:39:03.718: INFO: stdout: "affinity-nodeport-timeout-zzs9w"
    Jan  3 02:39:03.718: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-6976, will wait for the garbage collector to delete the pods 01/03/23 02:39:03.73
    Jan  3 02:39:03.814: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 2.635212ms
    Jan  3 02:39:03.915: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.436173ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  3 02:39:06.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6976" for this suite. 01/03/23 02:39:06.278
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:39:06.28
Jan  3 02:39:06.280: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename kubectl 01/03/23 02:39:06.281
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:39:06.319
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:39:06.321
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
STEP: creating a replication controller 01/03/23 02:39:06.325
Jan  3 02:39:06.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 create -f -'
Jan  3 02:39:07.255: INFO: stderr: ""
Jan  3 02:39:07.255: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/03/23 02:39:07.255
Jan  3 02:39:07.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan  3 02:39:07.324: INFO: stderr: ""
Jan  3 02:39:07.324: INFO: stdout: "update-demo-nautilus-7qwfg update-demo-nautilus-ssb7q "
Jan  3 02:39:07.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 get pods update-demo-nautilus-7qwfg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  3 02:39:07.383: INFO: stderr: ""
Jan  3 02:39:07.383: INFO: stdout: ""
Jan  3 02:39:07.383: INFO: update-demo-nautilus-7qwfg is created but not running
Jan  3 02:39:12.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan  3 02:39:12.441: INFO: stderr: ""
Jan  3 02:39:12.441: INFO: stdout: "update-demo-nautilus-7qwfg update-demo-nautilus-ssb7q "
Jan  3 02:39:12.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 get pods update-demo-nautilus-7qwfg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  3 02:39:12.497: INFO: stderr: ""
Jan  3 02:39:12.497: INFO: stdout: "true"
Jan  3 02:39:12.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 get pods update-demo-nautilus-7qwfg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan  3 02:39:12.551: INFO: stderr: ""
Jan  3 02:39:12.551: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan  3 02:39:12.551: INFO: validating pod update-demo-nautilus-7qwfg
Jan  3 02:39:12.554: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  3 02:39:12.554: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  3 02:39:12.554: INFO: update-demo-nautilus-7qwfg is verified up and running
Jan  3 02:39:12.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 get pods update-demo-nautilus-ssb7q -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  3 02:39:12.610: INFO: stderr: ""
Jan  3 02:39:12.610: INFO: stdout: ""
Jan  3 02:39:12.610: INFO: update-demo-nautilus-ssb7q is created but not running
Jan  3 02:39:17.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan  3 02:39:17.670: INFO: stderr: ""
Jan  3 02:39:17.670: INFO: stdout: "update-demo-nautilus-7qwfg update-demo-nautilus-ssb7q "
Jan  3 02:39:17.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 get pods update-demo-nautilus-7qwfg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  3 02:39:17.728: INFO: stderr: ""
Jan  3 02:39:17.728: INFO: stdout: "true"
Jan  3 02:39:17.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 get pods update-demo-nautilus-7qwfg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan  3 02:39:17.783: INFO: stderr: ""
Jan  3 02:39:17.783: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan  3 02:39:17.783: INFO: validating pod update-demo-nautilus-7qwfg
Jan  3 02:39:17.786: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  3 02:39:17.786: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  3 02:39:17.786: INFO: update-demo-nautilus-7qwfg is verified up and running
Jan  3 02:39:17.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 get pods update-demo-nautilus-ssb7q -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  3 02:39:17.841: INFO: stderr: ""
Jan  3 02:39:17.841: INFO: stdout: ""
Jan  3 02:39:17.841: INFO: update-demo-nautilus-ssb7q is created but not running
Jan  3 02:39:22.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan  3 02:39:22.901: INFO: stderr: ""
Jan  3 02:39:22.901: INFO: stdout: "update-demo-nautilus-7qwfg update-demo-nautilus-ssb7q "
Jan  3 02:39:22.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 get pods update-demo-nautilus-7qwfg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  3 02:39:22.956: INFO: stderr: ""
Jan  3 02:39:22.956: INFO: stdout: "true"
Jan  3 02:39:22.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 get pods update-demo-nautilus-7qwfg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan  3 02:39:23.012: INFO: stderr: ""
Jan  3 02:39:23.012: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan  3 02:39:23.012: INFO: validating pod update-demo-nautilus-7qwfg
Jan  3 02:39:23.014: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  3 02:39:23.014: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  3 02:39:23.014: INFO: update-demo-nautilus-7qwfg is verified up and running
Jan  3 02:39:23.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 get pods update-demo-nautilus-ssb7q -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  3 02:39:23.070: INFO: stderr: ""
Jan  3 02:39:23.070: INFO: stdout: "true"
Jan  3 02:39:23.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 get pods update-demo-nautilus-ssb7q -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan  3 02:39:23.126: INFO: stderr: ""
Jan  3 02:39:23.126: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan  3 02:39:23.126: INFO: validating pod update-demo-nautilus-ssb7q
Jan  3 02:39:23.129: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  3 02:39:23.129: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  3 02:39:23.129: INFO: update-demo-nautilus-ssb7q is verified up and running
STEP: scaling down the replication controller 01/03/23 02:39:23.129
Jan  3 02:39:23.131: INFO: scanned /root for discovery docs: <nil>
Jan  3 02:39:23.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jan  3 02:39:24.202: INFO: stderr: ""
Jan  3 02:39:24.202: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/03/23 02:39:24.202
Jan  3 02:39:24.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan  3 02:39:24.266: INFO: stderr: ""
Jan  3 02:39:24.266: INFO: stdout: "update-demo-nautilus-7qwfg update-demo-nautilus-ssb7q "
STEP: Replicas for name=update-demo: expected=1 actual=2 01/03/23 02:39:24.266
Jan  3 02:39:29.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan  3 02:39:29.326: INFO: stderr: ""
Jan  3 02:39:29.326: INFO: stdout: "update-demo-nautilus-7qwfg "
Jan  3 02:39:29.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 get pods update-demo-nautilus-7qwfg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  3 02:39:29.382: INFO: stderr: ""
Jan  3 02:39:29.382: INFO: stdout: "true"
Jan  3 02:39:29.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 get pods update-demo-nautilus-7qwfg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan  3 02:39:29.439: INFO: stderr: ""
Jan  3 02:39:29.439: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan  3 02:39:29.439: INFO: validating pod update-demo-nautilus-7qwfg
Jan  3 02:39:29.441: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  3 02:39:29.441: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  3 02:39:29.441: INFO: update-demo-nautilus-7qwfg is verified up and running
STEP: scaling up the replication controller 01/03/23 02:39:29.441
Jan  3 02:39:29.443: INFO: scanned /root for discovery docs: <nil>
Jan  3 02:39:29.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jan  3 02:39:30.514: INFO: stderr: ""
Jan  3 02:39:30.514: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/03/23 02:39:30.514
Jan  3 02:39:30.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan  3 02:39:30.571: INFO: stderr: ""
Jan  3 02:39:30.571: INFO: stdout: "update-demo-nautilus-7qwfg update-demo-nautilus-bbg2g "
Jan  3 02:39:30.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 get pods update-demo-nautilus-7qwfg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  3 02:39:30.628: INFO: stderr: ""
Jan  3 02:39:30.628: INFO: stdout: "true"
Jan  3 02:39:30.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 get pods update-demo-nautilus-7qwfg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan  3 02:39:30.685: INFO: stderr: ""
Jan  3 02:39:30.685: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan  3 02:39:30.685: INFO: validating pod update-demo-nautilus-7qwfg
Jan  3 02:39:30.688: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  3 02:39:30.688: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  3 02:39:30.688: INFO: update-demo-nautilus-7qwfg is verified up and running
Jan  3 02:39:30.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 get pods update-demo-nautilus-bbg2g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  3 02:39:30.743: INFO: stderr: ""
Jan  3 02:39:30.743: INFO: stdout: "true"
Jan  3 02:39:30.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 get pods update-demo-nautilus-bbg2g -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan  3 02:39:30.805: INFO: stderr: ""
Jan  3 02:39:30.805: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan  3 02:39:30.805: INFO: validating pod update-demo-nautilus-bbg2g
Jan  3 02:39:30.808: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  3 02:39:30.808: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  3 02:39:30.808: INFO: update-demo-nautilus-bbg2g is verified up and running
STEP: using delete to clean up resources 01/03/23 02:39:30.808
Jan  3 02:39:30.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 delete --grace-period=0 --force -f -'
Jan  3 02:39:30.865: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  3 02:39:30.865: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan  3 02:39:30.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 get rc,svc -l name=update-demo --no-headers'
Jan  3 02:39:30.931: INFO: stderr: "No resources found in kubectl-2560 namespace.\n"
Jan  3 02:39:30.931: INFO: stdout: ""
Jan  3 02:39:30.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan  3 02:39:30.991: INFO: stderr: ""
Jan  3 02:39:30.991: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan  3 02:39:30.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2560" for this suite. 01/03/23 02:39:30.994
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","completed":173,"skipped":3053,"failed":0}
------------------------------
• [SLOW TEST] [24.716 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:350

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:39:06.28
    Jan  3 02:39:06.280: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename kubectl 01/03/23 02:39:06.281
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:39:06.319
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:39:06.321
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:350
    STEP: creating a replication controller 01/03/23 02:39:06.325
    Jan  3 02:39:06.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 create -f -'
    Jan  3 02:39:07.255: INFO: stderr: ""
    Jan  3 02:39:07.255: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/03/23 02:39:07.255
    Jan  3 02:39:07.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan  3 02:39:07.324: INFO: stderr: ""
    Jan  3 02:39:07.324: INFO: stdout: "update-demo-nautilus-7qwfg update-demo-nautilus-ssb7q "
    Jan  3 02:39:07.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 get pods update-demo-nautilus-7qwfg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  3 02:39:07.383: INFO: stderr: ""
    Jan  3 02:39:07.383: INFO: stdout: ""
    Jan  3 02:39:07.383: INFO: update-demo-nautilus-7qwfg is created but not running
    Jan  3 02:39:12.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan  3 02:39:12.441: INFO: stderr: ""
    Jan  3 02:39:12.441: INFO: stdout: "update-demo-nautilus-7qwfg update-demo-nautilus-ssb7q "
    Jan  3 02:39:12.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 get pods update-demo-nautilus-7qwfg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  3 02:39:12.497: INFO: stderr: ""
    Jan  3 02:39:12.497: INFO: stdout: "true"
    Jan  3 02:39:12.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 get pods update-demo-nautilus-7qwfg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan  3 02:39:12.551: INFO: stderr: ""
    Jan  3 02:39:12.551: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan  3 02:39:12.551: INFO: validating pod update-demo-nautilus-7qwfg
    Jan  3 02:39:12.554: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan  3 02:39:12.554: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan  3 02:39:12.554: INFO: update-demo-nautilus-7qwfg is verified up and running
    Jan  3 02:39:12.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 get pods update-demo-nautilus-ssb7q -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  3 02:39:12.610: INFO: stderr: ""
    Jan  3 02:39:12.610: INFO: stdout: ""
    Jan  3 02:39:12.610: INFO: update-demo-nautilus-ssb7q is created but not running
    Jan  3 02:39:17.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan  3 02:39:17.670: INFO: stderr: ""
    Jan  3 02:39:17.670: INFO: stdout: "update-demo-nautilus-7qwfg update-demo-nautilus-ssb7q "
    Jan  3 02:39:17.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 get pods update-demo-nautilus-7qwfg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  3 02:39:17.728: INFO: stderr: ""
    Jan  3 02:39:17.728: INFO: stdout: "true"
    Jan  3 02:39:17.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 get pods update-demo-nautilus-7qwfg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan  3 02:39:17.783: INFO: stderr: ""
    Jan  3 02:39:17.783: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan  3 02:39:17.783: INFO: validating pod update-demo-nautilus-7qwfg
    Jan  3 02:39:17.786: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan  3 02:39:17.786: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan  3 02:39:17.786: INFO: update-demo-nautilus-7qwfg is verified up and running
    Jan  3 02:39:17.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 get pods update-demo-nautilus-ssb7q -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  3 02:39:17.841: INFO: stderr: ""
    Jan  3 02:39:17.841: INFO: stdout: ""
    Jan  3 02:39:17.841: INFO: update-demo-nautilus-ssb7q is created but not running
    Jan  3 02:39:22.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan  3 02:39:22.901: INFO: stderr: ""
    Jan  3 02:39:22.901: INFO: stdout: "update-demo-nautilus-7qwfg update-demo-nautilus-ssb7q "
    Jan  3 02:39:22.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 get pods update-demo-nautilus-7qwfg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  3 02:39:22.956: INFO: stderr: ""
    Jan  3 02:39:22.956: INFO: stdout: "true"
    Jan  3 02:39:22.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 get pods update-demo-nautilus-7qwfg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan  3 02:39:23.012: INFO: stderr: ""
    Jan  3 02:39:23.012: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan  3 02:39:23.012: INFO: validating pod update-demo-nautilus-7qwfg
    Jan  3 02:39:23.014: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan  3 02:39:23.014: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan  3 02:39:23.014: INFO: update-demo-nautilus-7qwfg is verified up and running
    Jan  3 02:39:23.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 get pods update-demo-nautilus-ssb7q -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  3 02:39:23.070: INFO: stderr: ""
    Jan  3 02:39:23.070: INFO: stdout: "true"
    Jan  3 02:39:23.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 get pods update-demo-nautilus-ssb7q -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan  3 02:39:23.126: INFO: stderr: ""
    Jan  3 02:39:23.126: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan  3 02:39:23.126: INFO: validating pod update-demo-nautilus-ssb7q
    Jan  3 02:39:23.129: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan  3 02:39:23.129: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan  3 02:39:23.129: INFO: update-demo-nautilus-ssb7q is verified up and running
    STEP: scaling down the replication controller 01/03/23 02:39:23.129
    Jan  3 02:39:23.131: INFO: scanned /root for discovery docs: <nil>
    Jan  3 02:39:23.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Jan  3 02:39:24.202: INFO: stderr: ""
    Jan  3 02:39:24.202: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/03/23 02:39:24.202
    Jan  3 02:39:24.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan  3 02:39:24.266: INFO: stderr: ""
    Jan  3 02:39:24.266: INFO: stdout: "update-demo-nautilus-7qwfg update-demo-nautilus-ssb7q "
    STEP: Replicas for name=update-demo: expected=1 actual=2 01/03/23 02:39:24.266
    Jan  3 02:39:29.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan  3 02:39:29.326: INFO: stderr: ""
    Jan  3 02:39:29.326: INFO: stdout: "update-demo-nautilus-7qwfg "
    Jan  3 02:39:29.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 get pods update-demo-nautilus-7qwfg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  3 02:39:29.382: INFO: stderr: ""
    Jan  3 02:39:29.382: INFO: stdout: "true"
    Jan  3 02:39:29.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 get pods update-demo-nautilus-7qwfg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan  3 02:39:29.439: INFO: stderr: ""
    Jan  3 02:39:29.439: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan  3 02:39:29.439: INFO: validating pod update-demo-nautilus-7qwfg
    Jan  3 02:39:29.441: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan  3 02:39:29.441: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan  3 02:39:29.441: INFO: update-demo-nautilus-7qwfg is verified up and running
    STEP: scaling up the replication controller 01/03/23 02:39:29.441
    Jan  3 02:39:29.443: INFO: scanned /root for discovery docs: <nil>
    Jan  3 02:39:29.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Jan  3 02:39:30.514: INFO: stderr: ""
    Jan  3 02:39:30.514: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/03/23 02:39:30.514
    Jan  3 02:39:30.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan  3 02:39:30.571: INFO: stderr: ""
    Jan  3 02:39:30.571: INFO: stdout: "update-demo-nautilus-7qwfg update-demo-nautilus-bbg2g "
    Jan  3 02:39:30.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 get pods update-demo-nautilus-7qwfg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  3 02:39:30.628: INFO: stderr: ""
    Jan  3 02:39:30.628: INFO: stdout: "true"
    Jan  3 02:39:30.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 get pods update-demo-nautilus-7qwfg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan  3 02:39:30.685: INFO: stderr: ""
    Jan  3 02:39:30.685: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan  3 02:39:30.685: INFO: validating pod update-demo-nautilus-7qwfg
    Jan  3 02:39:30.688: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan  3 02:39:30.688: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan  3 02:39:30.688: INFO: update-demo-nautilus-7qwfg is verified up and running
    Jan  3 02:39:30.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 get pods update-demo-nautilus-bbg2g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  3 02:39:30.743: INFO: stderr: ""
    Jan  3 02:39:30.743: INFO: stdout: "true"
    Jan  3 02:39:30.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 get pods update-demo-nautilus-bbg2g -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan  3 02:39:30.805: INFO: stderr: ""
    Jan  3 02:39:30.805: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan  3 02:39:30.805: INFO: validating pod update-demo-nautilus-bbg2g
    Jan  3 02:39:30.808: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan  3 02:39:30.808: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan  3 02:39:30.808: INFO: update-demo-nautilus-bbg2g is verified up and running
    STEP: using delete to clean up resources 01/03/23 02:39:30.808
    Jan  3 02:39:30.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 delete --grace-period=0 --force -f -'
    Jan  3 02:39:30.865: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  3 02:39:30.865: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jan  3 02:39:30.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 get rc,svc -l name=update-demo --no-headers'
    Jan  3 02:39:30.931: INFO: stderr: "No resources found in kubectl-2560 namespace.\n"
    Jan  3 02:39:30.931: INFO: stdout: ""
    Jan  3 02:39:30.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-2560 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan  3 02:39:30.991: INFO: stderr: ""
    Jan  3 02:39:30.991: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan  3 02:39:30.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2560" for this suite. 01/03/23 02:39:30.994
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:39:30.998
Jan  3 02:39:30.999: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename downward-api 01/03/23 02:39:30.999
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:39:31.006
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:39:31.008
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
STEP: Creating a pod to test downward api env vars 01/03/23 02:39:31.012
Jan  3 02:39:31.030: INFO: Waiting up to 5m0s for pod "downward-api-455a40db-138b-4122-a274-7b718f550b9e" in namespace "downward-api-8365" to be "Succeeded or Failed"
Jan  3 02:39:31.035: INFO: Pod "downward-api-455a40db-138b-4122-a274-7b718f550b9e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.582382ms
Jan  3 02:39:33.037: INFO: Pod "downward-api-455a40db-138b-4122-a274-7b718f550b9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007129544s
Jan  3 02:39:35.037: INFO: Pod "downward-api-455a40db-138b-4122-a274-7b718f550b9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007075026s
STEP: Saw pod success 01/03/23 02:39:35.037
Jan  3 02:39:35.037: INFO: Pod "downward-api-455a40db-138b-4122-a274-7b718f550b9e" satisfied condition "Succeeded or Failed"
Jan  3 02:39:35.039: INFO: Trying to get logs from node cncf-master pod downward-api-455a40db-138b-4122-a274-7b718f550b9e container dapi-container: <nil>
STEP: delete the pod 01/03/23 02:39:35.049
Jan  3 02:39:35.064: INFO: Waiting for pod downward-api-455a40db-138b-4122-a274-7b718f550b9e to disappear
Jan  3 02:39:35.079: INFO: Pod downward-api-455a40db-138b-4122-a274-7b718f550b9e no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan  3 02:39:35.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8365" for this suite. 01/03/23 02:39:35.082
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","completed":174,"skipped":3096,"failed":0}
------------------------------
• [4.122 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:39:30.998
    Jan  3 02:39:30.999: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename downward-api 01/03/23 02:39:30.999
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:39:31.006
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:39:31.008
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:266
    STEP: Creating a pod to test downward api env vars 01/03/23 02:39:31.012
    Jan  3 02:39:31.030: INFO: Waiting up to 5m0s for pod "downward-api-455a40db-138b-4122-a274-7b718f550b9e" in namespace "downward-api-8365" to be "Succeeded or Failed"
    Jan  3 02:39:31.035: INFO: Pod "downward-api-455a40db-138b-4122-a274-7b718f550b9e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.582382ms
    Jan  3 02:39:33.037: INFO: Pod "downward-api-455a40db-138b-4122-a274-7b718f550b9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007129544s
    Jan  3 02:39:35.037: INFO: Pod "downward-api-455a40db-138b-4122-a274-7b718f550b9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007075026s
    STEP: Saw pod success 01/03/23 02:39:35.037
    Jan  3 02:39:35.037: INFO: Pod "downward-api-455a40db-138b-4122-a274-7b718f550b9e" satisfied condition "Succeeded or Failed"
    Jan  3 02:39:35.039: INFO: Trying to get logs from node cncf-master pod downward-api-455a40db-138b-4122-a274-7b718f550b9e container dapi-container: <nil>
    STEP: delete the pod 01/03/23 02:39:35.049
    Jan  3 02:39:35.064: INFO: Waiting for pod downward-api-455a40db-138b-4122-a274-7b718f550b9e to disappear
    Jan  3 02:39:35.079: INFO: Pod downward-api-455a40db-138b-4122-a274-7b718f550b9e no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan  3 02:39:35.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8365" for this suite. 01/03/23 02:39:35.082
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:39:35.124
Jan  3 02:39:35.124: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename downward-api 01/03/23 02:39:35.125
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:39:35.132
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:39:35.134
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
STEP: Creating a pod to test downward API volume plugin 01/03/23 02:39:35.137
Jan  3 02:39:35.150: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3539eeeb-f027-4b8a-a647-aa38cc193310" in namespace "downward-api-7188" to be "Succeeded or Failed"
Jan  3 02:39:35.155: INFO: Pod "downwardapi-volume-3539eeeb-f027-4b8a-a647-aa38cc193310": Phase="Pending", Reason="", readiness=false. Elapsed: 4.977756ms
Jan  3 02:39:37.159: INFO: Pod "downwardapi-volume-3539eeeb-f027-4b8a-a647-aa38cc193310": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008218711s
Jan  3 02:39:39.321: INFO: Pod "downwardapi-volume-3539eeeb-f027-4b8a-a647-aa38cc193310": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.170291816s
STEP: Saw pod success 01/03/23 02:39:39.321
Jan  3 02:39:39.321: INFO: Pod "downwardapi-volume-3539eeeb-f027-4b8a-a647-aa38cc193310" satisfied condition "Succeeded or Failed"
Jan  3 02:39:39.323: INFO: Trying to get logs from node cncf-master pod downwardapi-volume-3539eeeb-f027-4b8a-a647-aa38cc193310 container client-container: <nil>
STEP: delete the pod 01/03/23 02:39:39.326
Jan  3 02:39:39.722: INFO: Waiting for pod downwardapi-volume-3539eeeb-f027-4b8a-a647-aa38cc193310 to disappear
Jan  3 02:39:39.745: INFO: Pod downwardapi-volume-3539eeeb-f027-4b8a-a647-aa38cc193310 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan  3 02:39:39.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7188" for this suite. 01/03/23 02:39:39.836
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":175,"skipped":3125,"failed":0}
------------------------------
• [4.731 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:39:35.124
    Jan  3 02:39:35.124: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename downward-api 01/03/23 02:39:35.125
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:39:35.132
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:39:35.134
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:248
    STEP: Creating a pod to test downward API volume plugin 01/03/23 02:39:35.137
    Jan  3 02:39:35.150: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3539eeeb-f027-4b8a-a647-aa38cc193310" in namespace "downward-api-7188" to be "Succeeded or Failed"
    Jan  3 02:39:35.155: INFO: Pod "downwardapi-volume-3539eeeb-f027-4b8a-a647-aa38cc193310": Phase="Pending", Reason="", readiness=false. Elapsed: 4.977756ms
    Jan  3 02:39:37.159: INFO: Pod "downwardapi-volume-3539eeeb-f027-4b8a-a647-aa38cc193310": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008218711s
    Jan  3 02:39:39.321: INFO: Pod "downwardapi-volume-3539eeeb-f027-4b8a-a647-aa38cc193310": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.170291816s
    STEP: Saw pod success 01/03/23 02:39:39.321
    Jan  3 02:39:39.321: INFO: Pod "downwardapi-volume-3539eeeb-f027-4b8a-a647-aa38cc193310" satisfied condition "Succeeded or Failed"
    Jan  3 02:39:39.323: INFO: Trying to get logs from node cncf-master pod downwardapi-volume-3539eeeb-f027-4b8a-a647-aa38cc193310 container client-container: <nil>
    STEP: delete the pod 01/03/23 02:39:39.326
    Jan  3 02:39:39.722: INFO: Waiting for pod downwardapi-volume-3539eeeb-f027-4b8a-a647-aa38cc193310 to disappear
    Jan  3 02:39:39.745: INFO: Pod downwardapi-volume-3539eeeb-f027-4b8a-a647-aa38cc193310 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan  3 02:39:39.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7188" for this suite. 01/03/23 02:39:39.836
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:39:39.856
Jan  3 02:39:39.856: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename runtimeclass 01/03/23 02:39:39.857
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:39:39.87
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:39:39.872
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 01/03/23 02:39:39.874
STEP: getting /apis/node.k8s.io 01/03/23 02:39:39.876
STEP: getting /apis/node.k8s.io/v1 01/03/23 02:39:39.876
STEP: creating 01/03/23 02:39:39.877
STEP: watching 01/03/23 02:39:39.906
Jan  3 02:39:39.906: INFO: starting watch
STEP: getting 01/03/23 02:39:39.91
STEP: listing 01/03/23 02:39:39.916
STEP: patching 01/03/23 02:39:39.917
STEP: updating 01/03/23 02:39:39.92
Jan  3 02:39:39.977: INFO: waiting for watch events with expected annotations
STEP: deleting 01/03/23 02:39:39.977
STEP: deleting a collection 01/03/23 02:39:39.992
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan  3 02:39:40.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-3759" for this suite. 01/03/23 02:39:40.004
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","completed":176,"skipped":3126,"failed":0}
------------------------------
• [0.151 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:39:39.856
    Jan  3 02:39:39.856: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename runtimeclass 01/03/23 02:39:39.857
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:39:39.87
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:39:39.872
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 01/03/23 02:39:39.874
    STEP: getting /apis/node.k8s.io 01/03/23 02:39:39.876
    STEP: getting /apis/node.k8s.io/v1 01/03/23 02:39:39.876
    STEP: creating 01/03/23 02:39:39.877
    STEP: watching 01/03/23 02:39:39.906
    Jan  3 02:39:39.906: INFO: starting watch
    STEP: getting 01/03/23 02:39:39.91
    STEP: listing 01/03/23 02:39:39.916
    STEP: patching 01/03/23 02:39:39.917
    STEP: updating 01/03/23 02:39:39.92
    Jan  3 02:39:39.977: INFO: waiting for watch events with expected annotations
    STEP: deleting 01/03/23 02:39:39.977
    STEP: deleting a collection 01/03/23 02:39:39.992
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan  3 02:39:40.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-3759" for this suite. 01/03/23 02:39:40.004
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:39:40.007
Jan  3 02:39:40.008: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename projected 01/03/23 02:39:40.008
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:39:40.019
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:39:40.021
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
STEP: Creating configMap with name projected-configmap-test-volume-map-25143bd4-adb9-4142-9305-d9ab85feb2d5 01/03/23 02:39:40.023
STEP: Creating a pod to test consume configMaps 01/03/23 02:39:40.037
Jan  3 02:39:40.045: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-447a19ce-1960-4195-a2dd-992219426135" in namespace "projected-319" to be "Succeeded or Failed"
Jan  3 02:39:40.060: INFO: Pod "pod-projected-configmaps-447a19ce-1960-4195-a2dd-992219426135": Phase="Pending", Reason="", readiness=false. Elapsed: 15.49285ms
Jan  3 02:39:42.063: INFO: Pod "pod-projected-configmaps-447a19ce-1960-4195-a2dd-992219426135": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018516978s
Jan  3 02:39:44.064: INFO: Pod "pod-projected-configmaps-447a19ce-1960-4195-a2dd-992219426135": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019554024s
STEP: Saw pod success 01/03/23 02:39:44.064
Jan  3 02:39:44.065: INFO: Pod "pod-projected-configmaps-447a19ce-1960-4195-a2dd-992219426135" satisfied condition "Succeeded or Failed"
Jan  3 02:39:44.066: INFO: Trying to get logs from node cncf-master pod pod-projected-configmaps-447a19ce-1960-4195-a2dd-992219426135 container agnhost-container: <nil>
STEP: delete the pod 01/03/23 02:39:44.07
Jan  3 02:39:44.088: INFO: Waiting for pod pod-projected-configmaps-447a19ce-1960-4195-a2dd-992219426135 to disappear
Jan  3 02:39:44.094: INFO: Pod pod-projected-configmaps-447a19ce-1960-4195-a2dd-992219426135 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan  3 02:39:44.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-319" for this suite. 01/03/23 02:39:44.096
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":177,"skipped":3153,"failed":0}
------------------------------
• [4.091 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:39:40.007
    Jan  3 02:39:40.008: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename projected 01/03/23 02:39:40.008
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:39:40.019
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:39:40.021
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:98
    STEP: Creating configMap with name projected-configmap-test-volume-map-25143bd4-adb9-4142-9305-d9ab85feb2d5 01/03/23 02:39:40.023
    STEP: Creating a pod to test consume configMaps 01/03/23 02:39:40.037
    Jan  3 02:39:40.045: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-447a19ce-1960-4195-a2dd-992219426135" in namespace "projected-319" to be "Succeeded or Failed"
    Jan  3 02:39:40.060: INFO: Pod "pod-projected-configmaps-447a19ce-1960-4195-a2dd-992219426135": Phase="Pending", Reason="", readiness=false. Elapsed: 15.49285ms
    Jan  3 02:39:42.063: INFO: Pod "pod-projected-configmaps-447a19ce-1960-4195-a2dd-992219426135": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018516978s
    Jan  3 02:39:44.064: INFO: Pod "pod-projected-configmaps-447a19ce-1960-4195-a2dd-992219426135": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019554024s
    STEP: Saw pod success 01/03/23 02:39:44.064
    Jan  3 02:39:44.065: INFO: Pod "pod-projected-configmaps-447a19ce-1960-4195-a2dd-992219426135" satisfied condition "Succeeded or Failed"
    Jan  3 02:39:44.066: INFO: Trying to get logs from node cncf-master pod pod-projected-configmaps-447a19ce-1960-4195-a2dd-992219426135 container agnhost-container: <nil>
    STEP: delete the pod 01/03/23 02:39:44.07
    Jan  3 02:39:44.088: INFO: Waiting for pod pod-projected-configmaps-447a19ce-1960-4195-a2dd-992219426135 to disappear
    Jan  3 02:39:44.094: INFO: Pod pod-projected-configmaps-447a19ce-1960-4195-a2dd-992219426135 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan  3 02:39:44.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-319" for this suite. 01/03/23 02:39:44.096
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:39:44.099
Jan  3 02:39:44.100: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename custom-resource-definition 01/03/23 02:39:44.1
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:39:44.111
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:39:44.113
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Jan  3 02:39:44.115: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  3 02:39:45.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2410" for this suite. 01/03/23 02:39:45.177
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","completed":178,"skipped":3165,"failed":0}
------------------------------
• [1.080 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:39:44.099
    Jan  3 02:39:44.100: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename custom-resource-definition 01/03/23 02:39:44.1
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:39:44.111
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:39:44.113
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Jan  3 02:39:44.115: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  3 02:39:45.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-2410" for this suite. 01/03/23 02:39:45.177
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:39:45.18
Jan  3 02:39:45.180: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename watch 01/03/23 02:39:45.181
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:39:45.214
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:39:45.216
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 01/03/23 02:39:45.217
STEP: creating a new configmap 01/03/23 02:39:45.218
STEP: modifying the configmap once 01/03/23 02:39:45.229
STEP: closing the watch once it receives two notifications 01/03/23 02:39:45.244
Jan  3 02:39:45.244: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3253  1077ca81-98e7-48f6-8cd3-6f58f4ea2adc 998058 0 2023-01-03 02:39:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[createdTime:2023-01-03T11:39:45.21994584+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T11:39:45.21994584+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update v1 2023-01-03 02:39:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  3 02:39:45.245: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3253  1077ca81-98e7-48f6-8cd3-6f58f4ea2adc 998059 0 2023-01-03 02:39:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[createdTime:2023-01-03T11:39:45.21994584+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T11:39:45.21994584+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update v1 2023-01-03 02:39:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 01/03/23 02:39:45.245
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 01/03/23 02:39:45.25
STEP: deleting the configmap 01/03/23 02:39:45.251
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 01/03/23 02:39:45.254
Jan  3 02:39:45.254: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3253  1077ca81-98e7-48f6-8cd3-6f58f4ea2adc 998060 0 2023-01-03 02:39:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[createdTime:2023-01-03T11:39:45.21994584+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T11:39:45.21994584+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update v1 2023-01-03 02:39:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  3 02:39:45.255: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3253  1077ca81-98e7-48f6-8cd3-6f58f4ea2adc 998061 0 2023-01-03 02:39:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[createdTime:2023-01-03T11:39:45.21994584+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T11:39:45.21994584+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update v1 2023-01-03 02:39:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan  3 02:39:45.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3253" for this suite. 01/03/23 02:39:45.26
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","completed":179,"skipped":3176,"failed":0}
------------------------------
• [0.082 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:39:45.18
    Jan  3 02:39:45.180: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename watch 01/03/23 02:39:45.181
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:39:45.214
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:39:45.216
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 01/03/23 02:39:45.217
    STEP: creating a new configmap 01/03/23 02:39:45.218
    STEP: modifying the configmap once 01/03/23 02:39:45.229
    STEP: closing the watch once it receives two notifications 01/03/23 02:39:45.244
    Jan  3 02:39:45.244: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3253  1077ca81-98e7-48f6-8cd3-6f58f4ea2adc 998058 0 2023-01-03 02:39:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[createdTime:2023-01-03T11:39:45.21994584+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T11:39:45.21994584+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update v1 2023-01-03 02:39:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  3 02:39:45.245: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3253  1077ca81-98e7-48f6-8cd3-6f58f4ea2adc 998059 0 2023-01-03 02:39:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[createdTime:2023-01-03T11:39:45.21994584+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T11:39:45.21994584+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update v1 2023-01-03 02:39:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 01/03/23 02:39:45.245
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 01/03/23 02:39:45.25
    STEP: deleting the configmap 01/03/23 02:39:45.251
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 01/03/23 02:39:45.254
    Jan  3 02:39:45.254: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3253  1077ca81-98e7-48f6-8cd3-6f58f4ea2adc 998060 0 2023-01-03 02:39:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[createdTime:2023-01-03T11:39:45.21994584+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T11:39:45.21994584+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update v1 2023-01-03 02:39:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  3 02:39:45.255: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3253  1077ca81-98e7-48f6-8cd3-6f58f4ea2adc 998061 0 2023-01-03 02:39:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[createdTime:2023-01-03T11:39:45.21994584+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T11:39:45.21994584+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update v1 2023-01-03 02:39:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan  3 02:39:45.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-3253" for this suite. 01/03/23 02:39:45.26
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:39:45.263
Jan  3 02:39:45.263: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename var-expansion 01/03/23 02:39:45.264
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:39:45.282
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:39:45.284
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
Jan  3 02:39:45.325: INFO: Waiting up to 2m0s for pod "var-expansion-f0927b38-b857-4d8e-9dee-883b0cd899b9" in namespace "var-expansion-6120" to be "container 0 failed with reason CreateContainerConfigError"
Jan  3 02:39:45.343: INFO: Pod "var-expansion-f0927b38-b857-4d8e-9dee-883b0cd899b9": Phase="Pending", Reason="", readiness=false. Elapsed: 18.053198ms
Jan  3 02:39:47.346: INFO: Pod "var-expansion-f0927b38-b857-4d8e-9dee-883b0cd899b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020869621s
Jan  3 02:39:47.346: INFO: Pod "var-expansion-f0927b38-b857-4d8e-9dee-883b0cd899b9" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jan  3 02:39:47.346: INFO: Deleting pod "var-expansion-f0927b38-b857-4d8e-9dee-883b0cd899b9" in namespace "var-expansion-6120"
Jan  3 02:39:47.349: INFO: Wait up to 5m0s for pod "var-expansion-f0927b38-b857-4d8e-9dee-883b0cd899b9" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan  3 02:39:51.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6120" for this suite. 01/03/23 02:39:51.356
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","completed":180,"skipped":3189,"failed":0}
------------------------------
• [SLOW TEST] [6.096 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:39:45.263
    Jan  3 02:39:45.263: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename var-expansion 01/03/23 02:39:45.264
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:39:45.282
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:39:45.284
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:185
    Jan  3 02:39:45.325: INFO: Waiting up to 2m0s for pod "var-expansion-f0927b38-b857-4d8e-9dee-883b0cd899b9" in namespace "var-expansion-6120" to be "container 0 failed with reason CreateContainerConfigError"
    Jan  3 02:39:45.343: INFO: Pod "var-expansion-f0927b38-b857-4d8e-9dee-883b0cd899b9": Phase="Pending", Reason="", readiness=false. Elapsed: 18.053198ms
    Jan  3 02:39:47.346: INFO: Pod "var-expansion-f0927b38-b857-4d8e-9dee-883b0cd899b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020869621s
    Jan  3 02:39:47.346: INFO: Pod "var-expansion-f0927b38-b857-4d8e-9dee-883b0cd899b9" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jan  3 02:39:47.346: INFO: Deleting pod "var-expansion-f0927b38-b857-4d8e-9dee-883b0cd899b9" in namespace "var-expansion-6120"
    Jan  3 02:39:47.349: INFO: Wait up to 5m0s for pod "var-expansion-f0927b38-b857-4d8e-9dee-883b0cd899b9" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan  3 02:39:51.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-6120" for this suite. 01/03/23 02:39:51.356
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:39:51.359
Jan  3 02:39:51.359: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename dns 01/03/23 02:39:51.36
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:39:51.374
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:39:51.376
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 01/03/23 02:39:51.378
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4829.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4829.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4829.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4829.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4829.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4829.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4829.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4829.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4829.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4829.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 63.180.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.180.63_udp@PTR;check="$$(dig +tcp +noall +answer +search 63.180.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.180.63_tcp@PTR;sleep 1; done
 01/03/23 02:39:51.449
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4829.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4829.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4829.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4829.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4829.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4829.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4829.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4829.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4829.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4829.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 63.180.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.180.63_udp@PTR;check="$$(dig +tcp +noall +answer +search 63.180.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.180.63_tcp@PTR;sleep 1; done
 01/03/23 02:39:51.449
STEP: creating a pod to probe DNS 01/03/23 02:39:51.449
STEP: submitting the pod to kubernetes 01/03/23 02:39:51.449
Jan  3 02:39:51.477: INFO: Waiting up to 15m0s for pod "dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc" in namespace "dns-4829" to be "running"
Jan  3 02:39:51.500: INFO: Pod "dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc": Phase="Pending", Reason="", readiness=false. Elapsed: 22.82644ms
Jan  3 02:39:53.503: INFO: Pod "dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025898607s
Jan  3 02:39:55.503: INFO: Pod "dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc": Phase="Running", Reason="", readiness=true. Elapsed: 4.025220949s
Jan  3 02:39:55.503: INFO: Pod "dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc" satisfied condition "running"
STEP: retrieving the pod 01/03/23 02:39:55.503
STEP: looking for the results for each expected name from probers 01/03/23 02:39:55.505
Jan  3 02:39:55.507: INFO: Unable to read wheezy_udp@dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
Jan  3 02:39:55.509: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
Jan  3 02:39:55.511: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
Jan  3 02:39:55.512: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
Jan  3 02:39:55.521: INFO: Unable to read jessie_udp@dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
Jan  3 02:39:55.523: INFO: Unable to read jessie_tcp@dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
Jan  3 02:39:55.525: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
Jan  3 02:39:55.526: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
Jan  3 02:39:55.533: INFO: Lookups using dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc failed for: [wheezy_udp@dns-test-service.dns-4829.svc.cluster.local wheezy_tcp@dns-test-service.dns-4829.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local jessie_udp@dns-test-service.dns-4829.svc.cluster.local jessie_tcp@dns-test-service.dns-4829.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local]

Jan  3 02:40:00.538: INFO: Unable to read wheezy_udp@dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
Jan  3 02:40:00.540: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
Jan  3 02:40:00.541: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
Jan  3 02:40:00.543: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
Jan  3 02:40:00.552: INFO: Unable to read jessie_udp@dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
Jan  3 02:40:00.553: INFO: Unable to read jessie_tcp@dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
Jan  3 02:40:00.555: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
Jan  3 02:40:00.557: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
Jan  3 02:40:00.563: INFO: Lookups using dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc failed for: [wheezy_udp@dns-test-service.dns-4829.svc.cluster.local wheezy_tcp@dns-test-service.dns-4829.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local jessie_udp@dns-test-service.dns-4829.svc.cluster.local jessie_tcp@dns-test-service.dns-4829.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local]

Jan  3 02:40:05.539: INFO: Unable to read wheezy_udp@dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
Jan  3 02:40:05.541: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
Jan  3 02:40:05.543: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
Jan  3 02:40:05.545: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
Jan  3 02:40:05.553: INFO: Unable to read jessie_udp@dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
Jan  3 02:40:05.555: INFO: Unable to read jessie_tcp@dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
Jan  3 02:40:05.557: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
Jan  3 02:40:05.558: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
Jan  3 02:40:05.565: INFO: Lookups using dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc failed for: [wheezy_udp@dns-test-service.dns-4829.svc.cluster.local wheezy_tcp@dns-test-service.dns-4829.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local jessie_udp@dns-test-service.dns-4829.svc.cluster.local jessie_tcp@dns-test-service.dns-4829.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local]

Jan  3 02:40:10.536: INFO: Unable to read wheezy_udp@dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
Jan  3 02:40:10.538: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
Jan  3 02:40:10.540: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
Jan  3 02:40:10.542: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
Jan  3 02:40:10.550: INFO: Unable to read jessie_udp@dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
Jan  3 02:40:10.552: INFO: Unable to read jessie_tcp@dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
Jan  3 02:40:10.553: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
Jan  3 02:40:10.555: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
Jan  3 02:40:10.562: INFO: Lookups using dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc failed for: [wheezy_udp@dns-test-service.dns-4829.svc.cluster.local wheezy_tcp@dns-test-service.dns-4829.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local jessie_udp@dns-test-service.dns-4829.svc.cluster.local jessie_tcp@dns-test-service.dns-4829.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local]

Jan  3 02:40:15.536: INFO: Unable to read wheezy_udp@dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
Jan  3 02:40:15.537: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
Jan  3 02:40:15.539: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
Jan  3 02:40:15.541: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
Jan  3 02:40:15.549: INFO: Unable to read jessie_udp@dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
Jan  3 02:40:15.550: INFO: Unable to read jessie_tcp@dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
Jan  3 02:40:15.552: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
Jan  3 02:40:15.554: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
Jan  3 02:40:15.560: INFO: Lookups using dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc failed for: [wheezy_udp@dns-test-service.dns-4829.svc.cluster.local wheezy_tcp@dns-test-service.dns-4829.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local jessie_udp@dns-test-service.dns-4829.svc.cluster.local jessie_tcp@dns-test-service.dns-4829.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local]

Jan  3 02:40:20.539: INFO: Unable to read wheezy_udp@dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
Jan  3 02:40:20.541: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
Jan  3 02:40:20.543: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
Jan  3 02:40:20.544: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
Jan  3 02:40:20.553: INFO: Unable to read jessie_udp@dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
Jan  3 02:40:20.554: INFO: Unable to read jessie_tcp@dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
Jan  3 02:40:20.556: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
Jan  3 02:40:20.558: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
Jan  3 02:40:20.565: INFO: Lookups using dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc failed for: [wheezy_udp@dns-test-service.dns-4829.svc.cluster.local wheezy_tcp@dns-test-service.dns-4829.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local jessie_udp@dns-test-service.dns-4829.svc.cluster.local jessie_tcp@dns-test-service.dns-4829.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local]

Jan  3 02:40:25.568: INFO: DNS probes using dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc succeeded

STEP: deleting the pod 01/03/23 02:40:25.569
STEP: deleting the test service 01/03/23 02:40:25.584
STEP: deleting the test headless service 01/03/23 02:40:25.652
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan  3 02:40:25.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4829" for this suite. 01/03/23 02:40:25.676
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","completed":181,"skipped":3192,"failed":0}
------------------------------
• [SLOW TEST] [34.321 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:39:51.359
    Jan  3 02:39:51.359: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename dns 01/03/23 02:39:51.36
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:39:51.374
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:39:51.376
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 01/03/23 02:39:51.378
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4829.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4829.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4829.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4829.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4829.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4829.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4829.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4829.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4829.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4829.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 63.180.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.180.63_udp@PTR;check="$$(dig +tcp +noall +answer +search 63.180.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.180.63_tcp@PTR;sleep 1; done
     01/03/23 02:39:51.449
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4829.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4829.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4829.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4829.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4829.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4829.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4829.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4829.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4829.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4829.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 63.180.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.180.63_udp@PTR;check="$$(dig +tcp +noall +answer +search 63.180.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.180.63_tcp@PTR;sleep 1; done
     01/03/23 02:39:51.449
    STEP: creating a pod to probe DNS 01/03/23 02:39:51.449
    STEP: submitting the pod to kubernetes 01/03/23 02:39:51.449
    Jan  3 02:39:51.477: INFO: Waiting up to 15m0s for pod "dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc" in namespace "dns-4829" to be "running"
    Jan  3 02:39:51.500: INFO: Pod "dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc": Phase="Pending", Reason="", readiness=false. Elapsed: 22.82644ms
    Jan  3 02:39:53.503: INFO: Pod "dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025898607s
    Jan  3 02:39:55.503: INFO: Pod "dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc": Phase="Running", Reason="", readiness=true. Elapsed: 4.025220949s
    Jan  3 02:39:55.503: INFO: Pod "dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc" satisfied condition "running"
    STEP: retrieving the pod 01/03/23 02:39:55.503
    STEP: looking for the results for each expected name from probers 01/03/23 02:39:55.505
    Jan  3 02:39:55.507: INFO: Unable to read wheezy_udp@dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
    Jan  3 02:39:55.509: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
    Jan  3 02:39:55.511: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
    Jan  3 02:39:55.512: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
    Jan  3 02:39:55.521: INFO: Unable to read jessie_udp@dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
    Jan  3 02:39:55.523: INFO: Unable to read jessie_tcp@dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
    Jan  3 02:39:55.525: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
    Jan  3 02:39:55.526: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
    Jan  3 02:39:55.533: INFO: Lookups using dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc failed for: [wheezy_udp@dns-test-service.dns-4829.svc.cluster.local wheezy_tcp@dns-test-service.dns-4829.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local jessie_udp@dns-test-service.dns-4829.svc.cluster.local jessie_tcp@dns-test-service.dns-4829.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local]

    Jan  3 02:40:00.538: INFO: Unable to read wheezy_udp@dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
    Jan  3 02:40:00.540: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
    Jan  3 02:40:00.541: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
    Jan  3 02:40:00.543: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
    Jan  3 02:40:00.552: INFO: Unable to read jessie_udp@dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
    Jan  3 02:40:00.553: INFO: Unable to read jessie_tcp@dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
    Jan  3 02:40:00.555: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
    Jan  3 02:40:00.557: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
    Jan  3 02:40:00.563: INFO: Lookups using dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc failed for: [wheezy_udp@dns-test-service.dns-4829.svc.cluster.local wheezy_tcp@dns-test-service.dns-4829.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local jessie_udp@dns-test-service.dns-4829.svc.cluster.local jessie_tcp@dns-test-service.dns-4829.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local]

    Jan  3 02:40:05.539: INFO: Unable to read wheezy_udp@dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
    Jan  3 02:40:05.541: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
    Jan  3 02:40:05.543: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
    Jan  3 02:40:05.545: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
    Jan  3 02:40:05.553: INFO: Unable to read jessie_udp@dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
    Jan  3 02:40:05.555: INFO: Unable to read jessie_tcp@dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
    Jan  3 02:40:05.557: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
    Jan  3 02:40:05.558: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
    Jan  3 02:40:05.565: INFO: Lookups using dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc failed for: [wheezy_udp@dns-test-service.dns-4829.svc.cluster.local wheezy_tcp@dns-test-service.dns-4829.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local jessie_udp@dns-test-service.dns-4829.svc.cluster.local jessie_tcp@dns-test-service.dns-4829.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local]

    Jan  3 02:40:10.536: INFO: Unable to read wheezy_udp@dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
    Jan  3 02:40:10.538: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
    Jan  3 02:40:10.540: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
    Jan  3 02:40:10.542: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
    Jan  3 02:40:10.550: INFO: Unable to read jessie_udp@dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
    Jan  3 02:40:10.552: INFO: Unable to read jessie_tcp@dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
    Jan  3 02:40:10.553: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
    Jan  3 02:40:10.555: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
    Jan  3 02:40:10.562: INFO: Lookups using dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc failed for: [wheezy_udp@dns-test-service.dns-4829.svc.cluster.local wheezy_tcp@dns-test-service.dns-4829.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local jessie_udp@dns-test-service.dns-4829.svc.cluster.local jessie_tcp@dns-test-service.dns-4829.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local]

    Jan  3 02:40:15.536: INFO: Unable to read wheezy_udp@dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
    Jan  3 02:40:15.537: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
    Jan  3 02:40:15.539: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
    Jan  3 02:40:15.541: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
    Jan  3 02:40:15.549: INFO: Unable to read jessie_udp@dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
    Jan  3 02:40:15.550: INFO: Unable to read jessie_tcp@dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
    Jan  3 02:40:15.552: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
    Jan  3 02:40:15.554: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
    Jan  3 02:40:15.560: INFO: Lookups using dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc failed for: [wheezy_udp@dns-test-service.dns-4829.svc.cluster.local wheezy_tcp@dns-test-service.dns-4829.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local jessie_udp@dns-test-service.dns-4829.svc.cluster.local jessie_tcp@dns-test-service.dns-4829.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local]

    Jan  3 02:40:20.539: INFO: Unable to read wheezy_udp@dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
    Jan  3 02:40:20.541: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
    Jan  3 02:40:20.543: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
    Jan  3 02:40:20.544: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
    Jan  3 02:40:20.553: INFO: Unable to read jessie_udp@dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
    Jan  3 02:40:20.554: INFO: Unable to read jessie_tcp@dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
    Jan  3 02:40:20.556: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
    Jan  3 02:40:20.558: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local from pod dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc: the server could not find the requested resource (get pods dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc)
    Jan  3 02:40:20.565: INFO: Lookups using dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc failed for: [wheezy_udp@dns-test-service.dns-4829.svc.cluster.local wheezy_tcp@dns-test-service.dns-4829.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local jessie_udp@dns-test-service.dns-4829.svc.cluster.local jessie_tcp@dns-test-service.dns-4829.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4829.svc.cluster.local]

    Jan  3 02:40:25.568: INFO: DNS probes using dns-4829/dns-test-a8edeee1-e027-4cb7-835a-34e53aa9cdfc succeeded

    STEP: deleting the pod 01/03/23 02:40:25.569
    STEP: deleting the test service 01/03/23 02:40:25.584
    STEP: deleting the test headless service 01/03/23 02:40:25.652
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan  3 02:40:25.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-4829" for this suite. 01/03/23 02:40:25.676
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:40:25.681
Jan  3 02:40:25.681: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename emptydir 01/03/23 02:40:25.682
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:40:25.696
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:40:25.701
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
STEP: Creating a pod to test emptydir 0777 on node default medium 01/03/23 02:40:25.707
Jan  3 02:40:25.716: INFO: Waiting up to 5m0s for pod "pod-ead8cea6-e159-40d6-8796-27c51e33f130" in namespace "emptydir-9240" to be "Succeeded or Failed"
Jan  3 02:40:25.732: INFO: Pod "pod-ead8cea6-e159-40d6-8796-27c51e33f130": Phase="Pending", Reason="", readiness=false. Elapsed: 16.574772ms
Jan  3 02:40:27.735: INFO: Pod "pod-ead8cea6-e159-40d6-8796-27c51e33f130": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019788211s
Jan  3 02:40:29.735: INFO: Pod "pod-ead8cea6-e159-40d6-8796-27c51e33f130": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019740167s
STEP: Saw pod success 01/03/23 02:40:29.735
Jan  3 02:40:29.736: INFO: Pod "pod-ead8cea6-e159-40d6-8796-27c51e33f130" satisfied condition "Succeeded or Failed"
Jan  3 02:40:29.737: INFO: Trying to get logs from node cncf-master pod pod-ead8cea6-e159-40d6-8796-27c51e33f130 container test-container: <nil>
STEP: delete the pod 01/03/23 02:40:29.741
Jan  3 02:40:29.759: INFO: Waiting for pod pod-ead8cea6-e159-40d6-8796-27c51e33f130 to disappear
Jan  3 02:40:29.764: INFO: Pod pod-ead8cea6-e159-40d6-8796-27c51e33f130 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan  3 02:40:29.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9240" for this suite. 01/03/23 02:40:29.767
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":182,"skipped":3193,"failed":0}
------------------------------
• [4.089 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:40:25.681
    Jan  3 02:40:25.681: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename emptydir 01/03/23 02:40:25.682
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:40:25.696
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:40:25.701
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:216
    STEP: Creating a pod to test emptydir 0777 on node default medium 01/03/23 02:40:25.707
    Jan  3 02:40:25.716: INFO: Waiting up to 5m0s for pod "pod-ead8cea6-e159-40d6-8796-27c51e33f130" in namespace "emptydir-9240" to be "Succeeded or Failed"
    Jan  3 02:40:25.732: INFO: Pod "pod-ead8cea6-e159-40d6-8796-27c51e33f130": Phase="Pending", Reason="", readiness=false. Elapsed: 16.574772ms
    Jan  3 02:40:27.735: INFO: Pod "pod-ead8cea6-e159-40d6-8796-27c51e33f130": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019788211s
    Jan  3 02:40:29.735: INFO: Pod "pod-ead8cea6-e159-40d6-8796-27c51e33f130": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019740167s
    STEP: Saw pod success 01/03/23 02:40:29.735
    Jan  3 02:40:29.736: INFO: Pod "pod-ead8cea6-e159-40d6-8796-27c51e33f130" satisfied condition "Succeeded or Failed"
    Jan  3 02:40:29.737: INFO: Trying to get logs from node cncf-master pod pod-ead8cea6-e159-40d6-8796-27c51e33f130 container test-container: <nil>
    STEP: delete the pod 01/03/23 02:40:29.741
    Jan  3 02:40:29.759: INFO: Waiting for pod pod-ead8cea6-e159-40d6-8796-27c51e33f130 to disappear
    Jan  3 02:40:29.764: INFO: Pod pod-ead8cea6-e159-40d6-8796-27c51e33f130 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan  3 02:40:29.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9240" for this suite. 01/03/23 02:40:29.767
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:40:29.77
Jan  3 02:40:29.771: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename replication-controller 01/03/23 02:40:29.771
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:40:29.782
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:40:29.784
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
STEP: creating a ReplicationController 01/03/23 02:40:29.799
STEP: waiting for RC to be added 01/03/23 02:40:29.802
STEP: waiting for available Replicas 01/03/23 02:40:29.803
STEP: patching ReplicationController 01/03/23 02:40:30.745
STEP: waiting for RC to be modified 01/03/23 02:40:30.754
STEP: patching ReplicationController status 01/03/23 02:40:30.754
STEP: waiting for RC to be modified 01/03/23 02:40:30.791
STEP: waiting for available Replicas 01/03/23 02:40:30.791
STEP: fetching ReplicationController status 01/03/23 02:40:30.794
STEP: patching ReplicationController scale 01/03/23 02:40:30.817
STEP: waiting for RC to be modified 01/03/23 02:40:30.822
STEP: waiting for ReplicationController's scale to be the max amount 01/03/23 02:40:30.822
STEP: fetching ReplicationController; ensuring that it's patched 01/03/23 02:40:31.943
STEP: updating ReplicationController status 01/03/23 02:40:31.962
STEP: waiting for RC to be modified 01/03/23 02:40:31.964
STEP: listing all ReplicationControllers 01/03/23 02:40:31.965
STEP: checking that ReplicationController has expected values 01/03/23 02:40:31.971
STEP: deleting ReplicationControllers by collection 01/03/23 02:40:31.971
STEP: waiting for ReplicationController to have a DELETED watchEvent 01/03/23 02:40:31.991
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan  3 02:40:32.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1437" for this suite. 01/03/23 02:40:32.077
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","completed":183,"skipped":3212,"failed":0}
------------------------------
• [2.309 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:40:29.77
    Jan  3 02:40:29.771: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename replication-controller 01/03/23 02:40:29.771
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:40:29.782
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:40:29.784
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:109
    STEP: creating a ReplicationController 01/03/23 02:40:29.799
    STEP: waiting for RC to be added 01/03/23 02:40:29.802
    STEP: waiting for available Replicas 01/03/23 02:40:29.803
    STEP: patching ReplicationController 01/03/23 02:40:30.745
    STEP: waiting for RC to be modified 01/03/23 02:40:30.754
    STEP: patching ReplicationController status 01/03/23 02:40:30.754
    STEP: waiting for RC to be modified 01/03/23 02:40:30.791
    STEP: waiting for available Replicas 01/03/23 02:40:30.791
    STEP: fetching ReplicationController status 01/03/23 02:40:30.794
    STEP: patching ReplicationController scale 01/03/23 02:40:30.817
    STEP: waiting for RC to be modified 01/03/23 02:40:30.822
    STEP: waiting for ReplicationController's scale to be the max amount 01/03/23 02:40:30.822
    STEP: fetching ReplicationController; ensuring that it's patched 01/03/23 02:40:31.943
    STEP: updating ReplicationController status 01/03/23 02:40:31.962
    STEP: waiting for RC to be modified 01/03/23 02:40:31.964
    STEP: listing all ReplicationControllers 01/03/23 02:40:31.965
    STEP: checking that ReplicationController has expected values 01/03/23 02:40:31.971
    STEP: deleting ReplicationControllers by collection 01/03/23 02:40:31.971
    STEP: waiting for ReplicationController to have a DELETED watchEvent 01/03/23 02:40:31.991
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan  3 02:40:32.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-1437" for this suite. 01/03/23 02:40:32.077
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:40:32.08
Jan  3 02:40:32.080: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename subpath 01/03/23 02:40:32.081
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:40:32.091
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:40:32.093
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/03/23 02:40:32.095
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-7r6q 01/03/23 02:40:32.114
STEP: Creating a pod to test atomic-volume-subpath 01/03/23 02:40:32.114
Jan  3 02:40:32.123: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-7r6q" in namespace "subpath-1661" to be "Succeeded or Failed"
Jan  3 02:40:32.139: INFO: Pod "pod-subpath-test-projected-7r6q": Phase="Pending", Reason="", readiness=false. Elapsed: 15.633049ms
Jan  3 02:40:34.141: INFO: Pod "pod-subpath-test-projected-7r6q": Phase="Running", Reason="", readiness=true. Elapsed: 2.018287309s
Jan  3 02:40:36.141: INFO: Pod "pod-subpath-test-projected-7r6q": Phase="Running", Reason="", readiness=true. Elapsed: 4.018236584s
Jan  3 02:40:38.141: INFO: Pod "pod-subpath-test-projected-7r6q": Phase="Running", Reason="", readiness=true. Elapsed: 6.017831213s
Jan  3 02:40:40.141: INFO: Pod "pod-subpath-test-projected-7r6q": Phase="Running", Reason="", readiness=true. Elapsed: 8.017837299s
Jan  3 02:40:42.141: INFO: Pod "pod-subpath-test-projected-7r6q": Phase="Running", Reason="", readiness=true. Elapsed: 10.018020894s
Jan  3 02:40:44.141: INFO: Pod "pod-subpath-test-projected-7r6q": Phase="Running", Reason="", readiness=true. Elapsed: 12.01808003s
Jan  3 02:40:46.141: INFO: Pod "pod-subpath-test-projected-7r6q": Phase="Running", Reason="", readiness=true. Elapsed: 14.018363053s
Jan  3 02:40:48.143: INFO: Pod "pod-subpath-test-projected-7r6q": Phase="Running", Reason="", readiness=true. Elapsed: 16.019798725s
Jan  3 02:40:50.142: INFO: Pod "pod-subpath-test-projected-7r6q": Phase="Running", Reason="", readiness=true. Elapsed: 18.01939528s
Jan  3 02:40:52.141: INFO: Pod "pod-subpath-test-projected-7r6q": Phase="Running", Reason="", readiness=true. Elapsed: 20.018463663s
Jan  3 02:40:54.142: INFO: Pod "pod-subpath-test-projected-7r6q": Phase="Running", Reason="", readiness=false. Elapsed: 22.019128074s
Jan  3 02:40:56.141: INFO: Pod "pod-subpath-test-projected-7r6q": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.018223376s
STEP: Saw pod success 01/03/23 02:40:56.141
Jan  3 02:40:56.141: INFO: Pod "pod-subpath-test-projected-7r6q" satisfied condition "Succeeded or Failed"
Jan  3 02:40:56.143: INFO: Trying to get logs from node cncf-master pod pod-subpath-test-projected-7r6q container test-container-subpath-projected-7r6q: <nil>
STEP: delete the pod 01/03/23 02:40:56.147
Jan  3 02:40:56.161: INFO: Waiting for pod pod-subpath-test-projected-7r6q to disappear
Jan  3 02:40:56.166: INFO: Pod pod-subpath-test-projected-7r6q no longer exists
STEP: Deleting pod pod-subpath-test-projected-7r6q 01/03/23 02:40:56.166
Jan  3 02:40:56.166: INFO: Deleting pod "pod-subpath-test-projected-7r6q" in namespace "subpath-1661"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan  3 02:40:56.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1661" for this suite. 01/03/23 02:40:56.171
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]","completed":184,"skipped":3225,"failed":0}
------------------------------
• [SLOW TEST] [24.093 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:40:32.08
    Jan  3 02:40:32.080: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename subpath 01/03/23 02:40:32.081
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:40:32.091
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:40:32.093
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/03/23 02:40:32.095
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-7r6q 01/03/23 02:40:32.114
    STEP: Creating a pod to test atomic-volume-subpath 01/03/23 02:40:32.114
    Jan  3 02:40:32.123: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-7r6q" in namespace "subpath-1661" to be "Succeeded or Failed"
    Jan  3 02:40:32.139: INFO: Pod "pod-subpath-test-projected-7r6q": Phase="Pending", Reason="", readiness=false. Elapsed: 15.633049ms
    Jan  3 02:40:34.141: INFO: Pod "pod-subpath-test-projected-7r6q": Phase="Running", Reason="", readiness=true. Elapsed: 2.018287309s
    Jan  3 02:40:36.141: INFO: Pod "pod-subpath-test-projected-7r6q": Phase="Running", Reason="", readiness=true. Elapsed: 4.018236584s
    Jan  3 02:40:38.141: INFO: Pod "pod-subpath-test-projected-7r6q": Phase="Running", Reason="", readiness=true. Elapsed: 6.017831213s
    Jan  3 02:40:40.141: INFO: Pod "pod-subpath-test-projected-7r6q": Phase="Running", Reason="", readiness=true. Elapsed: 8.017837299s
    Jan  3 02:40:42.141: INFO: Pod "pod-subpath-test-projected-7r6q": Phase="Running", Reason="", readiness=true. Elapsed: 10.018020894s
    Jan  3 02:40:44.141: INFO: Pod "pod-subpath-test-projected-7r6q": Phase="Running", Reason="", readiness=true. Elapsed: 12.01808003s
    Jan  3 02:40:46.141: INFO: Pod "pod-subpath-test-projected-7r6q": Phase="Running", Reason="", readiness=true. Elapsed: 14.018363053s
    Jan  3 02:40:48.143: INFO: Pod "pod-subpath-test-projected-7r6q": Phase="Running", Reason="", readiness=true. Elapsed: 16.019798725s
    Jan  3 02:40:50.142: INFO: Pod "pod-subpath-test-projected-7r6q": Phase="Running", Reason="", readiness=true. Elapsed: 18.01939528s
    Jan  3 02:40:52.141: INFO: Pod "pod-subpath-test-projected-7r6q": Phase="Running", Reason="", readiness=true. Elapsed: 20.018463663s
    Jan  3 02:40:54.142: INFO: Pod "pod-subpath-test-projected-7r6q": Phase="Running", Reason="", readiness=false. Elapsed: 22.019128074s
    Jan  3 02:40:56.141: INFO: Pod "pod-subpath-test-projected-7r6q": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.018223376s
    STEP: Saw pod success 01/03/23 02:40:56.141
    Jan  3 02:40:56.141: INFO: Pod "pod-subpath-test-projected-7r6q" satisfied condition "Succeeded or Failed"
    Jan  3 02:40:56.143: INFO: Trying to get logs from node cncf-master pod pod-subpath-test-projected-7r6q container test-container-subpath-projected-7r6q: <nil>
    STEP: delete the pod 01/03/23 02:40:56.147
    Jan  3 02:40:56.161: INFO: Waiting for pod pod-subpath-test-projected-7r6q to disappear
    Jan  3 02:40:56.166: INFO: Pod pod-subpath-test-projected-7r6q no longer exists
    STEP: Deleting pod pod-subpath-test-projected-7r6q 01/03/23 02:40:56.166
    Jan  3 02:40:56.166: INFO: Deleting pod "pod-subpath-test-projected-7r6q" in namespace "subpath-1661"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan  3 02:40:56.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-1661" for this suite. 01/03/23 02:40:56.171
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:40:56.174
Jan  3 02:40:56.174: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename projected 01/03/23 02:40:56.175
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:40:56.195
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:40:56.197
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
STEP: Creating configMap with name projected-configmap-test-volume-6459e17b-0d0f-4d09-8f49-931608f67db6 01/03/23 02:40:56.199
STEP: Creating a pod to test consume configMaps 01/03/23 02:40:56.202
Jan  3 02:40:56.218: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d9f6249b-5293-4675-af08-024f53162648" in namespace "projected-4002" to be "Succeeded or Failed"
Jan  3 02:40:56.256: INFO: Pod "pod-projected-configmaps-d9f6249b-5293-4675-af08-024f53162648": Phase="Pending", Reason="", readiness=false. Elapsed: 37.372975ms
Jan  3 02:40:58.263: INFO: Pod "pod-projected-configmaps-d9f6249b-5293-4675-af08-024f53162648": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044758547s
Jan  3 02:41:00.259: INFO: Pod "pod-projected-configmaps-d9f6249b-5293-4675-af08-024f53162648": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040720899s
STEP: Saw pod success 01/03/23 02:41:00.259
Jan  3 02:41:00.259: INFO: Pod "pod-projected-configmaps-d9f6249b-5293-4675-af08-024f53162648" satisfied condition "Succeeded or Failed"
Jan  3 02:41:00.261: INFO: Trying to get logs from node cncf-master pod pod-projected-configmaps-d9f6249b-5293-4675-af08-024f53162648 container agnhost-container: <nil>
STEP: delete the pod 01/03/23 02:41:00.264
Jan  3 02:41:00.280: INFO: Waiting for pod pod-projected-configmaps-d9f6249b-5293-4675-af08-024f53162648 to disappear
Jan  3 02:41:00.287: INFO: Pod pod-projected-configmaps-d9f6249b-5293-4675-af08-024f53162648 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan  3 02:41:00.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4002" for this suite. 01/03/23 02:41:00.29
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":185,"skipped":3258,"failed":0}
------------------------------
• [4.118 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:40:56.174
    Jan  3 02:40:56.174: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename projected 01/03/23 02:40:56.175
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:40:56.195
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:40:56.197
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:46
    STEP: Creating configMap with name projected-configmap-test-volume-6459e17b-0d0f-4d09-8f49-931608f67db6 01/03/23 02:40:56.199
    STEP: Creating a pod to test consume configMaps 01/03/23 02:40:56.202
    Jan  3 02:40:56.218: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d9f6249b-5293-4675-af08-024f53162648" in namespace "projected-4002" to be "Succeeded or Failed"
    Jan  3 02:40:56.256: INFO: Pod "pod-projected-configmaps-d9f6249b-5293-4675-af08-024f53162648": Phase="Pending", Reason="", readiness=false. Elapsed: 37.372975ms
    Jan  3 02:40:58.263: INFO: Pod "pod-projected-configmaps-d9f6249b-5293-4675-af08-024f53162648": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044758547s
    Jan  3 02:41:00.259: INFO: Pod "pod-projected-configmaps-d9f6249b-5293-4675-af08-024f53162648": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040720899s
    STEP: Saw pod success 01/03/23 02:41:00.259
    Jan  3 02:41:00.259: INFO: Pod "pod-projected-configmaps-d9f6249b-5293-4675-af08-024f53162648" satisfied condition "Succeeded or Failed"
    Jan  3 02:41:00.261: INFO: Trying to get logs from node cncf-master pod pod-projected-configmaps-d9f6249b-5293-4675-af08-024f53162648 container agnhost-container: <nil>
    STEP: delete the pod 01/03/23 02:41:00.264
    Jan  3 02:41:00.280: INFO: Waiting for pod pod-projected-configmaps-d9f6249b-5293-4675-af08-024f53162648 to disappear
    Jan  3 02:41:00.287: INFO: Pod pod-projected-configmaps-d9f6249b-5293-4675-af08-024f53162648 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan  3 02:41:00.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4002" for this suite. 01/03/23 02:41:00.29
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:41:00.293
Jan  3 02:41:00.293: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename webhook 01/03/23 02:41:00.293
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:41:00.33
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:41:00.332
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/03/23 02:41:00.35
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/23 02:41:00.624
STEP: Deploying the webhook pod 01/03/23 02:41:00.628
STEP: Wait for the deployment to be ready 01/03/23 02:41:00.684
Jan  3 02:41:00.715: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/03/23 02:41:02.766
STEP: Verifying the service has paired with the endpoint 01/03/23 02:41:02.78
Jan  3 02:41:03.780: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
Jan  3 02:41:03.782: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8246-crds.webhook.example.com via the AdmissionRegistration API 01/03/23 02:41:04.29
STEP: Creating a custom resource that should be mutated by the webhook 01/03/23 02:41:04.313
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  3 02:41:06.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9866" for this suite. 01/03/23 02:41:06.916
STEP: Destroying namespace "webhook-9866-markers" for this suite. 01/03/23 02:41:06.919
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","completed":186,"skipped":3280,"failed":0}
------------------------------
• [SLOW TEST] [6.688 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:41:00.293
    Jan  3 02:41:00.293: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename webhook 01/03/23 02:41:00.293
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:41:00.33
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:41:00.332
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/03/23 02:41:00.35
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/23 02:41:00.624
    STEP: Deploying the webhook pod 01/03/23 02:41:00.628
    STEP: Wait for the deployment to be ready 01/03/23 02:41:00.684
    Jan  3 02:41:00.715: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/03/23 02:41:02.766
    STEP: Verifying the service has paired with the endpoint 01/03/23 02:41:02.78
    Jan  3 02:41:03.780: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:340
    Jan  3 02:41:03.782: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8246-crds.webhook.example.com via the AdmissionRegistration API 01/03/23 02:41:04.29
    STEP: Creating a custom resource that should be mutated by the webhook 01/03/23 02:41:04.313
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  3 02:41:06.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9866" for this suite. 01/03/23 02:41:06.916
    STEP: Destroying namespace "webhook-9866-markers" for this suite. 01/03/23 02:41:06.919
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:41:06.981
Jan  3 02:41:06.981: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename webhook 01/03/23 02:41:06.982
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:41:07.009
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:41:07.01
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/03/23 02:41:07.066
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/23 02:41:07.668
STEP: Deploying the webhook pod 01/03/23 02:41:07.672
STEP: Wait for the deployment to be ready 01/03/23 02:41:07.73
Jan  3 02:41:07.733: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/03/23 02:41:09.745
STEP: Verifying the service has paired with the endpoint 01/03/23 02:41:09.76
Jan  3 02:41:10.760: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
STEP: Listing all of the created validation webhooks 01/03/23 02:41:10.817
STEP: Creating a configMap that should be mutated 01/03/23 02:41:10.831
STEP: Deleting the collection of validation webhooks 01/03/23 02:41:10.875
STEP: Creating a configMap that should not be mutated 01/03/23 02:41:10.943
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  3 02:41:10.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1266" for this suite. 01/03/23 02:41:10.964
STEP: Destroying namespace "webhook-1266-markers" for this suite. 01/03/23 02:41:10.967
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","completed":187,"skipped":3295,"failed":0}
------------------------------
• [4.063 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:41:06.981
    Jan  3 02:41:06.981: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename webhook 01/03/23 02:41:06.982
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:41:07.009
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:41:07.01
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/03/23 02:41:07.066
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/23 02:41:07.668
    STEP: Deploying the webhook pod 01/03/23 02:41:07.672
    STEP: Wait for the deployment to be ready 01/03/23 02:41:07.73
    Jan  3 02:41:07.733: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/03/23 02:41:09.745
    STEP: Verifying the service has paired with the endpoint 01/03/23 02:41:09.76
    Jan  3 02:41:10.760: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:655
    STEP: Listing all of the created validation webhooks 01/03/23 02:41:10.817
    STEP: Creating a configMap that should be mutated 01/03/23 02:41:10.831
    STEP: Deleting the collection of validation webhooks 01/03/23 02:41:10.875
    STEP: Creating a configMap that should not be mutated 01/03/23 02:41:10.943
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  3 02:41:10.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1266" for this suite. 01/03/23 02:41:10.964
    STEP: Destroying namespace "webhook-1266-markers" for this suite. 01/03/23 02:41:10.967
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:41:11.046
Jan  3 02:41:11.046: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename proxy 01/03/23 02:41:11.047
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:41:11.061
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:41:11.062
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Jan  3 02:41:11.064: INFO: Creating pod...
Jan  3 02:41:11.126: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-5293" to be "running"
Jan  3 02:41:11.137: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 11.28217ms
Jan  3 02:41:13.140: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.014421495s
Jan  3 02:41:13.140: INFO: Pod "agnhost" satisfied condition "running"
Jan  3 02:41:13.140: INFO: Creating service...
Jan  3 02:41:13.157: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5293/pods/agnhost/proxy/some/path/with/DELETE
Jan  3 02:41:13.165: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan  3 02:41:13.165: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5293/pods/agnhost/proxy/some/path/with/GET
Jan  3 02:41:13.180: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan  3 02:41:13.180: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5293/pods/agnhost/proxy/some/path/with/HEAD
Jan  3 02:41:13.182: INFO: http.Client request:HEAD | StatusCode:200
Jan  3 02:41:13.182: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5293/pods/agnhost/proxy/some/path/with/OPTIONS
Jan  3 02:41:13.183: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan  3 02:41:13.183: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5293/pods/agnhost/proxy/some/path/with/PATCH
Jan  3 02:41:13.185: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan  3 02:41:13.185: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5293/pods/agnhost/proxy/some/path/with/POST
Jan  3 02:41:13.186: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan  3 02:41:13.186: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5293/pods/agnhost/proxy/some/path/with/PUT
Jan  3 02:41:13.188: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan  3 02:41:13.188: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5293/services/test-service/proxy/some/path/with/DELETE
Jan  3 02:41:13.190: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan  3 02:41:13.190: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5293/services/test-service/proxy/some/path/with/GET
Jan  3 02:41:13.192: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan  3 02:41:13.192: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5293/services/test-service/proxy/some/path/with/HEAD
Jan  3 02:41:13.193: INFO: http.Client request:HEAD | StatusCode:200
Jan  3 02:41:13.193: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5293/services/test-service/proxy/some/path/with/OPTIONS
Jan  3 02:41:13.195: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan  3 02:41:13.195: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5293/services/test-service/proxy/some/path/with/PATCH
Jan  3 02:41:13.216: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan  3 02:41:13.216: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5293/services/test-service/proxy/some/path/with/POST
Jan  3 02:41:13.218: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan  3 02:41:13.218: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5293/services/test-service/proxy/some/path/with/PUT
Jan  3 02:41:13.220: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Jan  3 02:41:13.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5293" for this suite. 01/03/23 02:41:13.223
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","completed":188,"skipped":3336,"failed":0}
------------------------------
• [2.180 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:41:11.046
    Jan  3 02:41:11.046: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename proxy 01/03/23 02:41:11.047
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:41:11.061
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:41:11.062
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Jan  3 02:41:11.064: INFO: Creating pod...
    Jan  3 02:41:11.126: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-5293" to be "running"
    Jan  3 02:41:11.137: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 11.28217ms
    Jan  3 02:41:13.140: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.014421495s
    Jan  3 02:41:13.140: INFO: Pod "agnhost" satisfied condition "running"
    Jan  3 02:41:13.140: INFO: Creating service...
    Jan  3 02:41:13.157: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5293/pods/agnhost/proxy/some/path/with/DELETE
    Jan  3 02:41:13.165: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan  3 02:41:13.165: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5293/pods/agnhost/proxy/some/path/with/GET
    Jan  3 02:41:13.180: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jan  3 02:41:13.180: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5293/pods/agnhost/proxy/some/path/with/HEAD
    Jan  3 02:41:13.182: INFO: http.Client request:HEAD | StatusCode:200
    Jan  3 02:41:13.182: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5293/pods/agnhost/proxy/some/path/with/OPTIONS
    Jan  3 02:41:13.183: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan  3 02:41:13.183: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5293/pods/agnhost/proxy/some/path/with/PATCH
    Jan  3 02:41:13.185: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan  3 02:41:13.185: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5293/pods/agnhost/proxy/some/path/with/POST
    Jan  3 02:41:13.186: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan  3 02:41:13.186: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5293/pods/agnhost/proxy/some/path/with/PUT
    Jan  3 02:41:13.188: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan  3 02:41:13.188: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5293/services/test-service/proxy/some/path/with/DELETE
    Jan  3 02:41:13.190: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan  3 02:41:13.190: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5293/services/test-service/proxy/some/path/with/GET
    Jan  3 02:41:13.192: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jan  3 02:41:13.192: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5293/services/test-service/proxy/some/path/with/HEAD
    Jan  3 02:41:13.193: INFO: http.Client request:HEAD | StatusCode:200
    Jan  3 02:41:13.193: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5293/services/test-service/proxy/some/path/with/OPTIONS
    Jan  3 02:41:13.195: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan  3 02:41:13.195: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5293/services/test-service/proxy/some/path/with/PATCH
    Jan  3 02:41:13.216: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan  3 02:41:13.216: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5293/services/test-service/proxy/some/path/with/POST
    Jan  3 02:41:13.218: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan  3 02:41:13.218: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5293/services/test-service/proxy/some/path/with/PUT
    Jan  3 02:41:13.220: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Jan  3 02:41:13.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-5293" for this suite. 01/03/23 02:41:13.223
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:41:13.226
Jan  3 02:41:13.226: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename hostport 01/03/23 02:41:13.227
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:41:13.25
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:41:13.251
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 01/03/23 02:41:13.261
Jan  3 02:41:13.266: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-7794" to be "running and ready"
Jan  3 02:41:13.273: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.217739ms
Jan  3 02:41:13.273: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 02:41:15.275: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.008689299s
Jan  3 02:41:15.275: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan  3 02:41:15.275: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 172.21.7.13 on the node which pod1 resides and expect scheduled 01/03/23 02:41:15.275
Jan  3 02:41:15.280: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-7794" to be "running and ready"
Jan  3 02:41:15.284: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.612198ms
Jan  3 02:41:15.284: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 02:41:17.287: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.007178756s
Jan  3 02:41:17.287: INFO: The phase of Pod pod2 is Running (Ready = false)
Jan  3 02:41:19.287: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.007606418s
Jan  3 02:41:19.287: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan  3 02:41:19.287: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 172.21.7.13 but use UDP protocol on the node which pod2 resides 01/03/23 02:41:19.287
Jan  3 02:41:19.292: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-7794" to be "running and ready"
Jan  3 02:41:19.296: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.981153ms
Jan  3 02:41:19.296: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 02:41:21.301: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.009083122s
Jan  3 02:41:21.301: INFO: The phase of Pod pod3 is Running (Ready = true)
Jan  3 02:41:21.301: INFO: Pod "pod3" satisfied condition "running and ready"
Jan  3 02:41:21.305: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-7794" to be "running and ready"
Jan  3 02:41:21.318: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 13.088647ms
Jan  3 02:41:21.318: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jan  3 02:41:23.321: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.015504192s
Jan  3 02:41:23.321: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Jan  3 02:41:23.321: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 01/03/23 02:41:23.341
Jan  3 02:41:23.341: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.21.7.13 http://127.0.0.1:54323/hostname] Namespace:hostport-7794 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 02:41:23.341: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
Jan  3 02:41:23.341: INFO: ExecWithOptions: Clientset creation
Jan  3 02:41:23.342: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-7794/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+172.21.7.13+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.21.7.13, port: 54323 01/03/23 02:41:23.649
Jan  3 02:41:23.649: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.21.7.13:54323/hostname] Namespace:hostport-7794 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 02:41:23.649: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
Jan  3 02:41:23.649: INFO: ExecWithOptions: Clientset creation
Jan  3 02:41:23.649: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-7794/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F172.21.7.13%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.21.7.13, port: 54323 UDP 01/03/23 02:41:23.718
Jan  3 02:41:23.718: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 172.21.7.13 54323] Namespace:hostport-7794 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 02:41:23.718: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
Jan  3 02:41:23.718: INFO: ExecWithOptions: Clientset creation
Jan  3 02:41:23.718: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-7794/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+172.21.7.13+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/framework.go:187
Jan  3 02:41:28.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-7794" for this suite. 01/03/23 02:41:28.773
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","completed":189,"skipped":3341,"failed":0}
------------------------------
• [SLOW TEST] [15.550 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:41:13.226
    Jan  3 02:41:13.226: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename hostport 01/03/23 02:41:13.227
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:41:13.25
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:41:13.251
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 01/03/23 02:41:13.261
    Jan  3 02:41:13.266: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-7794" to be "running and ready"
    Jan  3 02:41:13.273: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.217739ms
    Jan  3 02:41:13.273: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 02:41:15.275: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.008689299s
    Jan  3 02:41:15.275: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan  3 02:41:15.275: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 172.21.7.13 on the node which pod1 resides and expect scheduled 01/03/23 02:41:15.275
    Jan  3 02:41:15.280: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-7794" to be "running and ready"
    Jan  3 02:41:15.284: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.612198ms
    Jan  3 02:41:15.284: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 02:41:17.287: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.007178756s
    Jan  3 02:41:17.287: INFO: The phase of Pod pod2 is Running (Ready = false)
    Jan  3 02:41:19.287: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.007606418s
    Jan  3 02:41:19.287: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan  3 02:41:19.287: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 172.21.7.13 but use UDP protocol on the node which pod2 resides 01/03/23 02:41:19.287
    Jan  3 02:41:19.292: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-7794" to be "running and ready"
    Jan  3 02:41:19.296: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.981153ms
    Jan  3 02:41:19.296: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 02:41:21.301: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.009083122s
    Jan  3 02:41:21.301: INFO: The phase of Pod pod3 is Running (Ready = true)
    Jan  3 02:41:21.301: INFO: Pod "pod3" satisfied condition "running and ready"
    Jan  3 02:41:21.305: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-7794" to be "running and ready"
    Jan  3 02:41:21.318: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 13.088647ms
    Jan  3 02:41:21.318: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 02:41:23.321: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.015504192s
    Jan  3 02:41:23.321: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Jan  3 02:41:23.321: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 01/03/23 02:41:23.341
    Jan  3 02:41:23.341: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.21.7.13 http://127.0.0.1:54323/hostname] Namespace:hostport-7794 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 02:41:23.341: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    Jan  3 02:41:23.341: INFO: ExecWithOptions: Clientset creation
    Jan  3 02:41:23.342: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-7794/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+172.21.7.13+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.21.7.13, port: 54323 01/03/23 02:41:23.649
    Jan  3 02:41:23.649: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.21.7.13:54323/hostname] Namespace:hostport-7794 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 02:41:23.649: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    Jan  3 02:41:23.649: INFO: ExecWithOptions: Clientset creation
    Jan  3 02:41:23.649: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-7794/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F172.21.7.13%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.21.7.13, port: 54323 UDP 01/03/23 02:41:23.718
    Jan  3 02:41:23.718: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 172.21.7.13 54323] Namespace:hostport-7794 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 02:41:23.718: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    Jan  3 02:41:23.718: INFO: ExecWithOptions: Clientset creation
    Jan  3 02:41:23.718: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-7794/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+172.21.7.13+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/framework.go:187
    Jan  3 02:41:28.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "hostport-7794" for this suite. 01/03/23 02:41:28.773
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:41:28.777
Jan  3 02:41:28.777: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename prestop 01/03/23 02:41:28.778
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:41:28.791
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:41:28.793
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-1834 01/03/23 02:41:28.795
STEP: Waiting for pods to come up. 01/03/23 02:41:28.821
Jan  3 02:41:28.821: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-1834" to be "running"
Jan  3 02:41:28.823: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 1.620088ms
Jan  3 02:41:30.849: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.028446465s
Jan  3 02:41:30.849: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-1834 01/03/23 02:41:30.851
Jan  3 02:41:30.857: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-1834" to be "running"
Jan  3 02:41:30.863: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 5.699074ms
Jan  3 02:41:32.872: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015300257s
Jan  3 02:41:34.878: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 4.021107434s
Jan  3 02:41:34.878: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 01/03/23 02:41:34.878
Jan  3 02:41:39.889: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 01/03/23 02:41:39.889
[AfterEach] [sig-node] PreStop
  test/e2e/framework/framework.go:187
Jan  3 02:41:39.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-1834" for this suite. 01/03/23 02:41:39.961
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","completed":190,"skipped":3351,"failed":0}
------------------------------
• [SLOW TEST] [11.187 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:41:28.777
    Jan  3 02:41:28.777: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename prestop 01/03/23 02:41:28.778
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:41:28.791
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:41:28.793
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-1834 01/03/23 02:41:28.795
    STEP: Waiting for pods to come up. 01/03/23 02:41:28.821
    Jan  3 02:41:28.821: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-1834" to be "running"
    Jan  3 02:41:28.823: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 1.620088ms
    Jan  3 02:41:30.849: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.028446465s
    Jan  3 02:41:30.849: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-1834 01/03/23 02:41:30.851
    Jan  3 02:41:30.857: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-1834" to be "running"
    Jan  3 02:41:30.863: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 5.699074ms
    Jan  3 02:41:32.872: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015300257s
    Jan  3 02:41:34.878: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 4.021107434s
    Jan  3 02:41:34.878: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 01/03/23 02:41:34.878
    Jan  3 02:41:39.889: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 01/03/23 02:41:39.889
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/framework.go:187
    Jan  3 02:41:39.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "prestop-1834" for this suite. 01/03/23 02:41:39.961
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:41:39.967
Jan  3 02:41:39.967: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename podtemplate 01/03/23 02:41:39.967
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:41:39.978
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:41:39.979
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 01/03/23 02:41:39.981
Jan  3 02:41:39.993: INFO: created test-podtemplate-1
Jan  3 02:41:39.996: INFO: created test-podtemplate-2
Jan  3 02:41:40.010: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 01/03/23 02:41:40.01
STEP: delete collection of pod templates 01/03/23 02:41:40.012
Jan  3 02:41:40.012: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 01/03/23 02:41:40.036
Jan  3 02:41:40.036: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Jan  3 02:41:40.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-7721" for this suite. 01/03/23 02:41:40.043
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","completed":191,"skipped":3407,"failed":0}
------------------------------
• [0.079 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:41:39.967
    Jan  3 02:41:39.967: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename podtemplate 01/03/23 02:41:39.967
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:41:39.978
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:41:39.979
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 01/03/23 02:41:39.981
    Jan  3 02:41:39.993: INFO: created test-podtemplate-1
    Jan  3 02:41:39.996: INFO: created test-podtemplate-2
    Jan  3 02:41:40.010: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 01/03/23 02:41:40.01
    STEP: delete collection of pod templates 01/03/23 02:41:40.012
    Jan  3 02:41:40.012: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 01/03/23 02:41:40.036
    Jan  3 02:41:40.036: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Jan  3 02:41:40.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-7721" for this suite. 01/03/23 02:41:40.043
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:41:40.047
Jan  3 02:41:40.047: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename daemonsets 01/03/23 02:41:40.048
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:41:40.101
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:41:40.104
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
Jan  3 02:41:40.125: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 01/03/23 02:41:40.129
Jan  3 02:41:40.132: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  3 02:41:40.132: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 01/03/23 02:41:40.132
Jan  3 02:41:40.175: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  3 02:41:40.175: INFO: Node cncf-node1 is running 0 daemon pod, expected 1
Jan  3 02:41:41.177: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan  3 02:41:41.177: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 01/03/23 02:41:41.179
Jan  3 02:41:41.205: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan  3 02:41:41.205: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Jan  3 02:41:42.207: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  3 02:41:42.207: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 01/03/23 02:41:42.207
Jan  3 02:41:42.213: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  3 02:41:42.213: INFO: Node cncf-node1 is running 0 daemon pod, expected 1
Jan  3 02:41:43.215: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  3 02:41:43.215: INFO: Node cncf-node1 is running 0 daemon pod, expected 1
Jan  3 02:41:44.226: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  3 02:41:44.226: INFO: Node cncf-node1 is running 0 daemon pod, expected 1
Jan  3 02:41:45.219: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan  3 02:41:45.219: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/03/23 02:41:45.222
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8967, will wait for the garbage collector to delete the pods 01/03/23 02:41:45.222
Jan  3 02:41:45.277: INFO: Deleting DaemonSet.extensions daemon-set took: 2.364392ms
Jan  3 02:41:45.378: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.007548ms
Jan  3 02:41:48.283: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  3 02:41:48.283: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan  3 02:41:48.285: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"999252"},"items":null}

Jan  3 02:41:48.286: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"999252"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan  3 02:41:48.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8967" for this suite. 01/03/23 02:41:48.315
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","completed":192,"skipped":3442,"failed":0}
------------------------------
• [SLOW TEST] [8.270 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:41:40.047
    Jan  3 02:41:40.047: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename daemonsets 01/03/23 02:41:40.048
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:41:40.101
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:41:40.104
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:193
    Jan  3 02:41:40.125: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 01/03/23 02:41:40.129
    Jan  3 02:41:40.132: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  3 02:41:40.132: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 01/03/23 02:41:40.132
    Jan  3 02:41:40.175: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  3 02:41:40.175: INFO: Node cncf-node1 is running 0 daemon pod, expected 1
    Jan  3 02:41:41.177: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan  3 02:41:41.177: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 01/03/23 02:41:41.179
    Jan  3 02:41:41.205: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan  3 02:41:41.205: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Jan  3 02:41:42.207: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  3 02:41:42.207: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 01/03/23 02:41:42.207
    Jan  3 02:41:42.213: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  3 02:41:42.213: INFO: Node cncf-node1 is running 0 daemon pod, expected 1
    Jan  3 02:41:43.215: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  3 02:41:43.215: INFO: Node cncf-node1 is running 0 daemon pod, expected 1
    Jan  3 02:41:44.226: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  3 02:41:44.226: INFO: Node cncf-node1 is running 0 daemon pod, expected 1
    Jan  3 02:41:45.219: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan  3 02:41:45.219: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/03/23 02:41:45.222
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8967, will wait for the garbage collector to delete the pods 01/03/23 02:41:45.222
    Jan  3 02:41:45.277: INFO: Deleting DaemonSet.extensions daemon-set took: 2.364392ms
    Jan  3 02:41:45.378: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.007548ms
    Jan  3 02:41:48.283: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  3 02:41:48.283: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan  3 02:41:48.285: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"999252"},"items":null}

    Jan  3 02:41:48.286: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"999252"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan  3 02:41:48.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-8967" for this suite. 01/03/23 02:41:48.315
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:41:48.318
Jan  3 02:41:48.318: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename container-probe 01/03/23 02:41:48.319
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:41:48.328
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:41:48.33
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
STEP: Creating pod busybox-b6ea8a7c-597c-42b6-be72-324cadf8424d in namespace container-probe-4582 01/03/23 02:41:48.332
Jan  3 02:41:48.349: INFO: Waiting up to 5m0s for pod "busybox-b6ea8a7c-597c-42b6-be72-324cadf8424d" in namespace "container-probe-4582" to be "not pending"
Jan  3 02:41:48.351: INFO: Pod "busybox-b6ea8a7c-597c-42b6-be72-324cadf8424d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.827275ms
Jan  3 02:41:50.353: INFO: Pod "busybox-b6ea8a7c-597c-42b6-be72-324cadf8424d": Phase="Running", Reason="", readiness=true. Elapsed: 2.003964882s
Jan  3 02:41:50.353: INFO: Pod "busybox-b6ea8a7c-597c-42b6-be72-324cadf8424d" satisfied condition "not pending"
Jan  3 02:41:50.353: INFO: Started pod busybox-b6ea8a7c-597c-42b6-be72-324cadf8424d in namespace container-probe-4582
STEP: checking the pod's current state and verifying that restartCount is present 01/03/23 02:41:50.353
Jan  3 02:41:50.355: INFO: Initial restart count of pod busybox-b6ea8a7c-597c-42b6-be72-324cadf8424d is 0
Jan  3 02:42:40.433: INFO: Restart count of pod container-probe-4582/busybox-b6ea8a7c-597c-42b6-be72-324cadf8424d is now 1 (50.078325085s elapsed)
STEP: deleting the pod 01/03/23 02:42:40.433
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan  3 02:42:40.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4582" for this suite. 01/03/23 02:42:40.462
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":193,"skipped":3452,"failed":0}
------------------------------
• [SLOW TEST] [52.147 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:41:48.318
    Jan  3 02:41:48.318: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename container-probe 01/03/23 02:41:48.319
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:41:48.328
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:41:48.33
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:131
    STEP: Creating pod busybox-b6ea8a7c-597c-42b6-be72-324cadf8424d in namespace container-probe-4582 01/03/23 02:41:48.332
    Jan  3 02:41:48.349: INFO: Waiting up to 5m0s for pod "busybox-b6ea8a7c-597c-42b6-be72-324cadf8424d" in namespace "container-probe-4582" to be "not pending"
    Jan  3 02:41:48.351: INFO: Pod "busybox-b6ea8a7c-597c-42b6-be72-324cadf8424d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.827275ms
    Jan  3 02:41:50.353: INFO: Pod "busybox-b6ea8a7c-597c-42b6-be72-324cadf8424d": Phase="Running", Reason="", readiness=true. Elapsed: 2.003964882s
    Jan  3 02:41:50.353: INFO: Pod "busybox-b6ea8a7c-597c-42b6-be72-324cadf8424d" satisfied condition "not pending"
    Jan  3 02:41:50.353: INFO: Started pod busybox-b6ea8a7c-597c-42b6-be72-324cadf8424d in namespace container-probe-4582
    STEP: checking the pod's current state and verifying that restartCount is present 01/03/23 02:41:50.353
    Jan  3 02:41:50.355: INFO: Initial restart count of pod busybox-b6ea8a7c-597c-42b6-be72-324cadf8424d is 0
    Jan  3 02:42:40.433: INFO: Restart count of pod container-probe-4582/busybox-b6ea8a7c-597c-42b6-be72-324cadf8424d is now 1 (50.078325085s elapsed)
    STEP: deleting the pod 01/03/23 02:42:40.433
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan  3 02:42:40.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-4582" for this suite. 01/03/23 02:42:40.462
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:42:40.465
Jan  3 02:42:40.465: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename cronjob 01/03/23 02:42:40.466
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:42:40.491
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:42:40.493
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 01/03/23 02:42:40.496
STEP: Ensuring no jobs are scheduled 01/03/23 02:42:40.5
STEP: Ensuring no job exists by listing jobs explicitly 01/03/23 02:47:40.505
STEP: Removing cronjob 01/03/23 02:47:40.506
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan  3 02:47:40.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-8501" for this suite. 01/03/23 02:47:40.515
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","completed":194,"skipped":3452,"failed":0}
------------------------------
• [SLOW TEST] [300.069 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:42:40.465
    Jan  3 02:42:40.465: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename cronjob 01/03/23 02:42:40.466
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:42:40.491
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:42:40.493
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 01/03/23 02:42:40.496
    STEP: Ensuring no jobs are scheduled 01/03/23 02:42:40.5
    STEP: Ensuring no job exists by listing jobs explicitly 01/03/23 02:47:40.505
    STEP: Removing cronjob 01/03/23 02:47:40.506
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan  3 02:47:40.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-8501" for this suite. 01/03/23 02:47:40.515
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:47:40.535
Jan  3 02:47:40.535: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename projected 01/03/23 02:47:40.536
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:47:40.559
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:47:40.561
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
STEP: Creating a pod to test downward API volume plugin 01/03/23 02:47:40.563
Jan  3 02:47:40.570: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2164b7f2-1d18-4e1c-991f-b3f0a36f62e0" in namespace "projected-3097" to be "Succeeded or Failed"
Jan  3 02:47:40.576: INFO: Pod "downwardapi-volume-2164b7f2-1d18-4e1c-991f-b3f0a36f62e0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.735981ms
Jan  3 02:47:42.580: INFO: Pod "downwardapi-volume-2164b7f2-1d18-4e1c-991f-b3f0a36f62e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010162394s
Jan  3 02:47:44.579: INFO: Pod "downwardapi-volume-2164b7f2-1d18-4e1c-991f-b3f0a36f62e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009791914s
STEP: Saw pod success 01/03/23 02:47:44.579
Jan  3 02:47:44.580: INFO: Pod "downwardapi-volume-2164b7f2-1d18-4e1c-991f-b3f0a36f62e0" satisfied condition "Succeeded or Failed"
Jan  3 02:47:44.581: INFO: Trying to get logs from node cncf-master pod downwardapi-volume-2164b7f2-1d18-4e1c-991f-b3f0a36f62e0 container client-container: <nil>
STEP: delete the pod 01/03/23 02:47:44.592
Jan  3 02:47:44.612: INFO: Waiting for pod downwardapi-volume-2164b7f2-1d18-4e1c-991f-b3f0a36f62e0 to disappear
Jan  3 02:47:44.617: INFO: Pod downwardapi-volume-2164b7f2-1d18-4e1c-991f-b3f0a36f62e0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan  3 02:47:44.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3097" for this suite. 01/03/23 02:47:44.62
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":195,"skipped":3458,"failed":0}
------------------------------
• [4.088 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:47:40.535
    Jan  3 02:47:40.535: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename projected 01/03/23 02:47:40.536
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:47:40.559
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:47:40.561
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:248
    STEP: Creating a pod to test downward API volume plugin 01/03/23 02:47:40.563
    Jan  3 02:47:40.570: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2164b7f2-1d18-4e1c-991f-b3f0a36f62e0" in namespace "projected-3097" to be "Succeeded or Failed"
    Jan  3 02:47:40.576: INFO: Pod "downwardapi-volume-2164b7f2-1d18-4e1c-991f-b3f0a36f62e0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.735981ms
    Jan  3 02:47:42.580: INFO: Pod "downwardapi-volume-2164b7f2-1d18-4e1c-991f-b3f0a36f62e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010162394s
    Jan  3 02:47:44.579: INFO: Pod "downwardapi-volume-2164b7f2-1d18-4e1c-991f-b3f0a36f62e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009791914s
    STEP: Saw pod success 01/03/23 02:47:44.579
    Jan  3 02:47:44.580: INFO: Pod "downwardapi-volume-2164b7f2-1d18-4e1c-991f-b3f0a36f62e0" satisfied condition "Succeeded or Failed"
    Jan  3 02:47:44.581: INFO: Trying to get logs from node cncf-master pod downwardapi-volume-2164b7f2-1d18-4e1c-991f-b3f0a36f62e0 container client-container: <nil>
    STEP: delete the pod 01/03/23 02:47:44.592
    Jan  3 02:47:44.612: INFO: Waiting for pod downwardapi-volume-2164b7f2-1d18-4e1c-991f-b3f0a36f62e0 to disappear
    Jan  3 02:47:44.617: INFO: Pod downwardapi-volume-2164b7f2-1d18-4e1c-991f-b3f0a36f62e0 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan  3 02:47:44.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3097" for this suite. 01/03/23 02:47:44.62
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:47:44.624
Jan  3 02:47:44.624: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename namespaces 01/03/23 02:47:44.625
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:47:44.647
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:47:44.649
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
STEP: Creating a test namespace 01/03/23 02:47:44.651
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:47:44.67
STEP: Creating a service in the namespace 01/03/23 02:47:44.672
STEP: Deleting the namespace 01/03/23 02:47:44.693
STEP: Waiting for the namespace to be removed. 01/03/23 02:47:44.727
STEP: Recreating the namespace 01/03/23 02:47:50.729
STEP: Verifying there is no service in the namespace 01/03/23 02:47:50.737
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jan  3 02:47:50.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3499" for this suite. 01/03/23 02:47:50.759
STEP: Destroying namespace "nsdeletetest-8199" for this suite. 01/03/23 02:47:50.761
Jan  3 02:47:50.766: INFO: Namespace nsdeletetest-8199 was already deleted
STEP: Destroying namespace "nsdeletetest-9296" for this suite. 01/03/23 02:47:50.766
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","completed":196,"skipped":3498,"failed":0}
------------------------------
• [SLOW TEST] [6.144 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:47:44.624
    Jan  3 02:47:44.624: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename namespaces 01/03/23 02:47:44.625
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:47:44.647
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:47:44.649
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:250
    STEP: Creating a test namespace 01/03/23 02:47:44.651
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:47:44.67
    STEP: Creating a service in the namespace 01/03/23 02:47:44.672
    STEP: Deleting the namespace 01/03/23 02:47:44.693
    STEP: Waiting for the namespace to be removed. 01/03/23 02:47:44.727
    STEP: Recreating the namespace 01/03/23 02:47:50.729
    STEP: Verifying there is no service in the namespace 01/03/23 02:47:50.737
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jan  3 02:47:50.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-3499" for this suite. 01/03/23 02:47:50.759
    STEP: Destroying namespace "nsdeletetest-8199" for this suite. 01/03/23 02:47:50.761
    Jan  3 02:47:50.766: INFO: Namespace nsdeletetest-8199 was already deleted
    STEP: Destroying namespace "nsdeletetest-9296" for this suite. 01/03/23 02:47:50.766
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:47:50.769
Jan  3 02:47:50.769: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename container-lifecycle-hook 01/03/23 02:47:50.769
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:47:50.777
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:47:50.779
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 01/03/23 02:47:50.797
Jan  3 02:47:50.803: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-256" to be "running and ready"
Jan  3 02:47:50.847: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 44.642401ms
Jan  3 02:47:50.847: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan  3 02:47:52.851: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.047864104s
Jan  3 02:47:52.851: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan  3 02:47:52.851: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
STEP: create the pod with lifecycle hook 01/03/23 02:47:52.852
Jan  3 02:47:52.857: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-256" to be "running and ready"
Jan  3 02:47:52.888: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 31.424454ms
Jan  3 02:47:52.888: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan  3 02:47:54.891: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.034276864s
Jan  3 02:47:54.891: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Jan  3 02:47:54.891: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 01/03/23 02:47:54.893
STEP: delete the pod with lifecycle hook 01/03/23 02:47:54.904
Jan  3 02:47:54.907: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan  3 02:47:54.912: INFO: Pod pod-with-poststart-exec-hook still exists
Jan  3 02:47:56.913: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan  3 02:47:56.915: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jan  3 02:47:56.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-256" for this suite. 01/03/23 02:47:56.918
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","completed":197,"skipped":3498,"failed":0}
------------------------------
• [SLOW TEST] [6.152 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:47:50.769
    Jan  3 02:47:50.769: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/03/23 02:47:50.769
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:47:50.777
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:47:50.779
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 01/03/23 02:47:50.797
    Jan  3 02:47:50.803: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-256" to be "running and ready"
    Jan  3 02:47:50.847: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 44.642401ms
    Jan  3 02:47:50.847: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 02:47:52.851: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.047864104s
    Jan  3 02:47:52.851: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan  3 02:47:52.851: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:97
    STEP: create the pod with lifecycle hook 01/03/23 02:47:52.852
    Jan  3 02:47:52.857: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-256" to be "running and ready"
    Jan  3 02:47:52.888: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 31.424454ms
    Jan  3 02:47:52.888: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 02:47:54.891: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.034276864s
    Jan  3 02:47:54.891: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Jan  3 02:47:54.891: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 01/03/23 02:47:54.893
    STEP: delete the pod with lifecycle hook 01/03/23 02:47:54.904
    Jan  3 02:47:54.907: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan  3 02:47:54.912: INFO: Pod pod-with-poststart-exec-hook still exists
    Jan  3 02:47:56.913: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan  3 02:47:56.915: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jan  3 02:47:56.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-256" for this suite. 01/03/23 02:47:56.918
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:47:56.927
Jan  3 02:47:56.927: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename secrets 01/03/23 02:47:56.928
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:47:56.938
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:47:56.94
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
STEP: Creating secret with name secret-test-0e15dfac-5657-4c7f-8e81-b6b1f7733b7d 01/03/23 02:47:56.942
STEP: Creating a pod to test consume secrets 01/03/23 02:47:56.945
Jan  3 02:47:56.963: INFO: Waiting up to 5m0s for pod "pod-secrets-0e0ac167-e139-4e0f-942d-098e7376f006" in namespace "secrets-3008" to be "Succeeded or Failed"
Jan  3 02:47:56.967: INFO: Pod "pod-secrets-0e0ac167-e139-4e0f-942d-098e7376f006": Phase="Pending", Reason="", readiness=false. Elapsed: 4.266774ms
Jan  3 02:47:58.970: INFO: Pod "pod-secrets-0e0ac167-e139-4e0f-942d-098e7376f006": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007322726s
Jan  3 02:48:00.969: INFO: Pod "pod-secrets-0e0ac167-e139-4e0f-942d-098e7376f006": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00670104s
STEP: Saw pod success 01/03/23 02:48:00.969
Jan  3 02:48:00.969: INFO: Pod "pod-secrets-0e0ac167-e139-4e0f-942d-098e7376f006" satisfied condition "Succeeded or Failed"
Jan  3 02:48:00.971: INFO: Trying to get logs from node cncf-master pod pod-secrets-0e0ac167-e139-4e0f-942d-098e7376f006 container secret-volume-test: <nil>
STEP: delete the pod 01/03/23 02:48:00.974
Jan  3 02:48:00.990: INFO: Waiting for pod pod-secrets-0e0ac167-e139-4e0f-942d-098e7376f006 to disappear
Jan  3 02:48:00.995: INFO: Pod pod-secrets-0e0ac167-e139-4e0f-942d-098e7376f006 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan  3 02:48:00.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3008" for this suite. 01/03/23 02:48:00.998
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","completed":198,"skipped":3595,"failed":0}
------------------------------
• [4.074 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:47:56.927
    Jan  3 02:47:56.927: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename secrets 01/03/23 02:47:56.928
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:47:56.938
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:47:56.94
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:46
    STEP: Creating secret with name secret-test-0e15dfac-5657-4c7f-8e81-b6b1f7733b7d 01/03/23 02:47:56.942
    STEP: Creating a pod to test consume secrets 01/03/23 02:47:56.945
    Jan  3 02:47:56.963: INFO: Waiting up to 5m0s for pod "pod-secrets-0e0ac167-e139-4e0f-942d-098e7376f006" in namespace "secrets-3008" to be "Succeeded or Failed"
    Jan  3 02:47:56.967: INFO: Pod "pod-secrets-0e0ac167-e139-4e0f-942d-098e7376f006": Phase="Pending", Reason="", readiness=false. Elapsed: 4.266774ms
    Jan  3 02:47:58.970: INFO: Pod "pod-secrets-0e0ac167-e139-4e0f-942d-098e7376f006": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007322726s
    Jan  3 02:48:00.969: INFO: Pod "pod-secrets-0e0ac167-e139-4e0f-942d-098e7376f006": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00670104s
    STEP: Saw pod success 01/03/23 02:48:00.969
    Jan  3 02:48:00.969: INFO: Pod "pod-secrets-0e0ac167-e139-4e0f-942d-098e7376f006" satisfied condition "Succeeded or Failed"
    Jan  3 02:48:00.971: INFO: Trying to get logs from node cncf-master pod pod-secrets-0e0ac167-e139-4e0f-942d-098e7376f006 container secret-volume-test: <nil>
    STEP: delete the pod 01/03/23 02:48:00.974
    Jan  3 02:48:00.990: INFO: Waiting for pod pod-secrets-0e0ac167-e139-4e0f-942d-098e7376f006 to disappear
    Jan  3 02:48:00.995: INFO: Pod pod-secrets-0e0ac167-e139-4e0f-942d-098e7376f006 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan  3 02:48:00.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-3008" for this suite. 01/03/23 02:48:00.998
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:48:01.002
Jan  3 02:48:01.002: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename container-lifecycle-hook 01/03/23 02:48:01.003
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:48:01.019
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:48:01.02
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 01/03/23 02:48:01.036
Jan  3 02:48:01.041: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6679" to be "running and ready"
Jan  3 02:48:01.047: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.149331ms
Jan  3 02:48:01.047: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan  3 02:48:03.050: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.009266055s
Jan  3 02:48:03.050: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan  3 02:48:03.050: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
STEP: create the pod with lifecycle hook 01/03/23 02:48:03.052
Jan  3 02:48:03.056: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-6679" to be "running and ready"
Jan  3 02:48:03.059: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.591723ms
Jan  3 02:48:03.059: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan  3 02:48:05.061: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.004914939s
Jan  3 02:48:05.061: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Jan  3 02:48:05.061: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 01/03/23 02:48:05.063
Jan  3 02:48:05.066: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan  3 02:48:05.093: INFO: Pod pod-with-prestop-http-hook still exists
Jan  3 02:48:07.094: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan  3 02:48:07.116: INFO: Pod pod-with-prestop-http-hook still exists
Jan  3 02:48:09.094: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan  3 02:48:09.096: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 01/03/23 02:48:09.096
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jan  3 02:48:09.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6679" for this suite. 01/03/23 02:48:09.102
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","completed":199,"skipped":3622,"failed":0}
------------------------------
• [SLOW TEST] [8.102 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:48:01.002
    Jan  3 02:48:01.002: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/03/23 02:48:01.003
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:48:01.019
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:48:01.02
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 01/03/23 02:48:01.036
    Jan  3 02:48:01.041: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6679" to be "running and ready"
    Jan  3 02:48:01.047: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.149331ms
    Jan  3 02:48:01.047: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 02:48:03.050: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.009266055s
    Jan  3 02:48:03.050: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan  3 02:48:03.050: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:152
    STEP: create the pod with lifecycle hook 01/03/23 02:48:03.052
    Jan  3 02:48:03.056: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-6679" to be "running and ready"
    Jan  3 02:48:03.059: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.591723ms
    Jan  3 02:48:03.059: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 02:48:05.061: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.004914939s
    Jan  3 02:48:05.061: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Jan  3 02:48:05.061: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 01/03/23 02:48:05.063
    Jan  3 02:48:05.066: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan  3 02:48:05.093: INFO: Pod pod-with-prestop-http-hook still exists
    Jan  3 02:48:07.094: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan  3 02:48:07.116: INFO: Pod pod-with-prestop-http-hook still exists
    Jan  3 02:48:09.094: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan  3 02:48:09.096: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 01/03/23 02:48:09.096
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jan  3 02:48:09.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-6679" for this suite. 01/03/23 02:48:09.102
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:48:09.105
Jan  3 02:48:09.105: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename ingressclass 01/03/23 02:48:09.106
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:48:09.15
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:48:09.152
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 01/03/23 02:48:09.154
STEP: getting /apis/networking.k8s.io 01/03/23 02:48:09.155
STEP: getting /apis/networking.k8s.iov1 01/03/23 02:48:09.156
STEP: creating 01/03/23 02:48:09.157
STEP: getting 01/03/23 02:48:09.178
STEP: listing 01/03/23 02:48:09.185
STEP: watching 01/03/23 02:48:09.187
Jan  3 02:48:09.187: INFO: starting watch
STEP: patching 01/03/23 02:48:09.187
STEP: updating 01/03/23 02:48:09.19
Jan  3 02:48:09.192: INFO: waiting for watch events with expected annotations
Jan  3 02:48:09.192: INFO: saw patched and updated annotations
STEP: deleting 01/03/23 02:48:09.192
STEP: deleting a collection 01/03/23 02:48:09.203
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:187
Jan  3 02:48:09.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-1162" for this suite. 01/03/23 02:48:09.216
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","completed":200,"skipped":3637,"failed":0}
------------------------------
• [0.114 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:48:09.105
    Jan  3 02:48:09.105: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename ingressclass 01/03/23 02:48:09.106
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:48:09.15
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:48:09.152
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 01/03/23 02:48:09.154
    STEP: getting /apis/networking.k8s.io 01/03/23 02:48:09.155
    STEP: getting /apis/networking.k8s.iov1 01/03/23 02:48:09.156
    STEP: creating 01/03/23 02:48:09.157
    STEP: getting 01/03/23 02:48:09.178
    STEP: listing 01/03/23 02:48:09.185
    STEP: watching 01/03/23 02:48:09.187
    Jan  3 02:48:09.187: INFO: starting watch
    STEP: patching 01/03/23 02:48:09.187
    STEP: updating 01/03/23 02:48:09.19
    Jan  3 02:48:09.192: INFO: waiting for watch events with expected annotations
    Jan  3 02:48:09.192: INFO: saw patched and updated annotations
    STEP: deleting 01/03/23 02:48:09.192
    STEP: deleting a collection 01/03/23 02:48:09.203
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:187
    Jan  3 02:48:09.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingressclass-1162" for this suite. 01/03/23 02:48:09.216
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:48:09.22
Jan  3 02:48:09.220: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename kubectl 01/03/23 02:48:09.221
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:48:09.236
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:48:09.239
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
STEP: validating api versions 01/03/23 02:48:09.24
Jan  3 02:48:09.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-8219 api-versions'
Jan  3 02:48:09.290: INFO: stderr: ""
Jan  3 02:48:09.290: INFO: stdout: "acme.cert-manager.io/v1\nacme.cert-manager.io/v1alpha2\nacme.cert-manager.io/v1alpha3\nacme.cert-manager.io/v1beta1\nadmissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\nceph.rook.io/v1\ncert-manager.io/v1\ncert-manager.io/v1alpha2\ncert-manager.io/v1alpha3\ncert-manager.io/v1beta1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nhypercloud.tmaxcloud.com/v1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nobjectbucket.io/v1alpha1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntmax.io/v1\ntraefik.containo.us/v1alpha1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan  3 02:48:09.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8219" for this suite. 01/03/23 02:48:09.295
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","completed":201,"skipped":3670,"failed":0}
------------------------------
• [0.077 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:816
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:48:09.22
    Jan  3 02:48:09.220: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename kubectl 01/03/23 02:48:09.221
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:48:09.236
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:48:09.239
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:822
    STEP: validating api versions 01/03/23 02:48:09.24
    Jan  3 02:48:09.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-8219 api-versions'
    Jan  3 02:48:09.290: INFO: stderr: ""
    Jan  3 02:48:09.290: INFO: stdout: "acme.cert-manager.io/v1\nacme.cert-manager.io/v1alpha2\nacme.cert-manager.io/v1alpha3\nacme.cert-manager.io/v1beta1\nadmissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\nceph.rook.io/v1\ncert-manager.io/v1\ncert-manager.io/v1alpha2\ncert-manager.io/v1alpha3\ncert-manager.io/v1beta1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nhypercloud.tmaxcloud.com/v1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nobjectbucket.io/v1alpha1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntmax.io/v1\ntraefik.containo.us/v1alpha1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan  3 02:48:09.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8219" for this suite. 01/03/23 02:48:09.295
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:48:09.298
Jan  3 02:48:09.298: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename subpath 01/03/23 02:48:09.3
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:48:09.307
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:48:09.309
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/03/23 02:48:09.311
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-bntr 01/03/23 02:48:09.334
STEP: Creating a pod to test atomic-volume-subpath 01/03/23 02:48:09.334
Jan  3 02:48:09.343: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-bntr" in namespace "subpath-9424" to be "Succeeded or Failed"
Jan  3 02:48:09.355: INFO: Pod "pod-subpath-test-secret-bntr": Phase="Pending", Reason="", readiness=false. Elapsed: 12.146511ms
Jan  3 02:48:11.358: INFO: Pod "pod-subpath-test-secret-bntr": Phase="Running", Reason="", readiness=true. Elapsed: 2.015134961s
Jan  3 02:48:13.359: INFO: Pod "pod-subpath-test-secret-bntr": Phase="Running", Reason="", readiness=true. Elapsed: 4.016095492s
Jan  3 02:48:15.357: INFO: Pod "pod-subpath-test-secret-bntr": Phase="Running", Reason="", readiness=true. Elapsed: 6.014454488s
Jan  3 02:48:17.359: INFO: Pod "pod-subpath-test-secret-bntr": Phase="Running", Reason="", readiness=true. Elapsed: 8.015895407s
Jan  3 02:48:19.361: INFO: Pod "pod-subpath-test-secret-bntr": Phase="Running", Reason="", readiness=true. Elapsed: 10.018649392s
Jan  3 02:48:21.358: INFO: Pod "pod-subpath-test-secret-bntr": Phase="Running", Reason="", readiness=true. Elapsed: 12.014884313s
Jan  3 02:48:23.357: INFO: Pod "pod-subpath-test-secret-bntr": Phase="Running", Reason="", readiness=true. Elapsed: 14.014753456s
Jan  3 02:48:25.358: INFO: Pod "pod-subpath-test-secret-bntr": Phase="Running", Reason="", readiness=true. Elapsed: 16.015810378s
Jan  3 02:48:27.358: INFO: Pod "pod-subpath-test-secret-bntr": Phase="Running", Reason="", readiness=true. Elapsed: 18.014945893s
Jan  3 02:48:29.378: INFO: Pod "pod-subpath-test-secret-bntr": Phase="Running", Reason="", readiness=true. Elapsed: 20.035606651s
Jan  3 02:48:31.357: INFO: Pod "pod-subpath-test-secret-bntr": Phase="Running", Reason="", readiness=false. Elapsed: 22.014817226s
Jan  3 02:48:33.359: INFO: Pod "pod-subpath-test-secret-bntr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.016076914s
STEP: Saw pod success 01/03/23 02:48:33.359
Jan  3 02:48:33.359: INFO: Pod "pod-subpath-test-secret-bntr" satisfied condition "Succeeded or Failed"
Jan  3 02:48:33.361: INFO: Trying to get logs from node cncf-master pod pod-subpath-test-secret-bntr container test-container-subpath-secret-bntr: <nil>
STEP: delete the pod 01/03/23 02:48:33.365
Jan  3 02:48:33.386: INFO: Waiting for pod pod-subpath-test-secret-bntr to disappear
Jan  3 02:48:33.396: INFO: Pod pod-subpath-test-secret-bntr no longer exists
STEP: Deleting pod pod-subpath-test-secret-bntr 01/03/23 02:48:33.396
Jan  3 02:48:33.396: INFO: Deleting pod "pod-subpath-test-secret-bntr" in namespace "subpath-9424"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan  3 02:48:33.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9424" for this suite. 01/03/23 02:48:33.401
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]","completed":202,"skipped":3680,"failed":0}
------------------------------
• [SLOW TEST] [24.105 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:48:09.298
    Jan  3 02:48:09.298: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename subpath 01/03/23 02:48:09.3
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:48:09.307
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:48:09.309
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/03/23 02:48:09.311
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-bntr 01/03/23 02:48:09.334
    STEP: Creating a pod to test atomic-volume-subpath 01/03/23 02:48:09.334
    Jan  3 02:48:09.343: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-bntr" in namespace "subpath-9424" to be "Succeeded or Failed"
    Jan  3 02:48:09.355: INFO: Pod "pod-subpath-test-secret-bntr": Phase="Pending", Reason="", readiness=false. Elapsed: 12.146511ms
    Jan  3 02:48:11.358: INFO: Pod "pod-subpath-test-secret-bntr": Phase="Running", Reason="", readiness=true. Elapsed: 2.015134961s
    Jan  3 02:48:13.359: INFO: Pod "pod-subpath-test-secret-bntr": Phase="Running", Reason="", readiness=true. Elapsed: 4.016095492s
    Jan  3 02:48:15.357: INFO: Pod "pod-subpath-test-secret-bntr": Phase="Running", Reason="", readiness=true. Elapsed: 6.014454488s
    Jan  3 02:48:17.359: INFO: Pod "pod-subpath-test-secret-bntr": Phase="Running", Reason="", readiness=true. Elapsed: 8.015895407s
    Jan  3 02:48:19.361: INFO: Pod "pod-subpath-test-secret-bntr": Phase="Running", Reason="", readiness=true. Elapsed: 10.018649392s
    Jan  3 02:48:21.358: INFO: Pod "pod-subpath-test-secret-bntr": Phase="Running", Reason="", readiness=true. Elapsed: 12.014884313s
    Jan  3 02:48:23.357: INFO: Pod "pod-subpath-test-secret-bntr": Phase="Running", Reason="", readiness=true. Elapsed: 14.014753456s
    Jan  3 02:48:25.358: INFO: Pod "pod-subpath-test-secret-bntr": Phase="Running", Reason="", readiness=true. Elapsed: 16.015810378s
    Jan  3 02:48:27.358: INFO: Pod "pod-subpath-test-secret-bntr": Phase="Running", Reason="", readiness=true. Elapsed: 18.014945893s
    Jan  3 02:48:29.378: INFO: Pod "pod-subpath-test-secret-bntr": Phase="Running", Reason="", readiness=true. Elapsed: 20.035606651s
    Jan  3 02:48:31.357: INFO: Pod "pod-subpath-test-secret-bntr": Phase="Running", Reason="", readiness=false. Elapsed: 22.014817226s
    Jan  3 02:48:33.359: INFO: Pod "pod-subpath-test-secret-bntr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.016076914s
    STEP: Saw pod success 01/03/23 02:48:33.359
    Jan  3 02:48:33.359: INFO: Pod "pod-subpath-test-secret-bntr" satisfied condition "Succeeded or Failed"
    Jan  3 02:48:33.361: INFO: Trying to get logs from node cncf-master pod pod-subpath-test-secret-bntr container test-container-subpath-secret-bntr: <nil>
    STEP: delete the pod 01/03/23 02:48:33.365
    Jan  3 02:48:33.386: INFO: Waiting for pod pod-subpath-test-secret-bntr to disappear
    Jan  3 02:48:33.396: INFO: Pod pod-subpath-test-secret-bntr no longer exists
    STEP: Deleting pod pod-subpath-test-secret-bntr 01/03/23 02:48:33.396
    Jan  3 02:48:33.396: INFO: Deleting pod "pod-subpath-test-secret-bntr" in namespace "subpath-9424"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan  3 02:48:33.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-9424" for this suite. 01/03/23 02:48:33.401
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:48:33.404
Jan  3 02:48:33.404: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename configmap 01/03/23 02:48:33.405
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:48:33.414
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:48:33.416
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
STEP: Creating configMap configmap-3447/configmap-test-d2fcca34-2f56-480c-88ae-15b933bcfbd8 01/03/23 02:48:33.418
STEP: Creating a pod to test consume configMaps 01/03/23 02:48:33.43
Jan  3 02:48:33.440: INFO: Waiting up to 5m0s for pod "pod-configmaps-8e8ba54b-80dd-429d-90cc-af842371dfc9" in namespace "configmap-3447" to be "Succeeded or Failed"
Jan  3 02:48:33.443: INFO: Pod "pod-configmaps-8e8ba54b-80dd-429d-90cc-af842371dfc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.880019ms
Jan  3 02:48:35.446: INFO: Pod "pod-configmaps-8e8ba54b-80dd-429d-90cc-af842371dfc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005913279s
Jan  3 02:48:37.446: INFO: Pod "pod-configmaps-8e8ba54b-80dd-429d-90cc-af842371dfc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006015105s
STEP: Saw pod success 01/03/23 02:48:37.446
Jan  3 02:48:37.446: INFO: Pod "pod-configmaps-8e8ba54b-80dd-429d-90cc-af842371dfc9" satisfied condition "Succeeded or Failed"
Jan  3 02:48:37.448: INFO: Trying to get logs from node cncf-master pod pod-configmaps-8e8ba54b-80dd-429d-90cc-af842371dfc9 container env-test: <nil>
STEP: delete the pod 01/03/23 02:48:37.451
Jan  3 02:48:37.501: INFO: Waiting for pod pod-configmaps-8e8ba54b-80dd-429d-90cc-af842371dfc9 to disappear
Jan  3 02:48:37.506: INFO: Pod pod-configmaps-8e8ba54b-80dd-429d-90cc-af842371dfc9 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jan  3 02:48:37.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3447" for this suite. 01/03/23 02:48:37.509
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","completed":203,"skipped":3692,"failed":0}
------------------------------
• [4.107 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:48:33.404
    Jan  3 02:48:33.404: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename configmap 01/03/23 02:48:33.405
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:48:33.414
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:48:33.416
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:92
    STEP: Creating configMap configmap-3447/configmap-test-d2fcca34-2f56-480c-88ae-15b933bcfbd8 01/03/23 02:48:33.418
    STEP: Creating a pod to test consume configMaps 01/03/23 02:48:33.43
    Jan  3 02:48:33.440: INFO: Waiting up to 5m0s for pod "pod-configmaps-8e8ba54b-80dd-429d-90cc-af842371dfc9" in namespace "configmap-3447" to be "Succeeded or Failed"
    Jan  3 02:48:33.443: INFO: Pod "pod-configmaps-8e8ba54b-80dd-429d-90cc-af842371dfc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.880019ms
    Jan  3 02:48:35.446: INFO: Pod "pod-configmaps-8e8ba54b-80dd-429d-90cc-af842371dfc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005913279s
    Jan  3 02:48:37.446: INFO: Pod "pod-configmaps-8e8ba54b-80dd-429d-90cc-af842371dfc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006015105s
    STEP: Saw pod success 01/03/23 02:48:37.446
    Jan  3 02:48:37.446: INFO: Pod "pod-configmaps-8e8ba54b-80dd-429d-90cc-af842371dfc9" satisfied condition "Succeeded or Failed"
    Jan  3 02:48:37.448: INFO: Trying to get logs from node cncf-master pod pod-configmaps-8e8ba54b-80dd-429d-90cc-af842371dfc9 container env-test: <nil>
    STEP: delete the pod 01/03/23 02:48:37.451
    Jan  3 02:48:37.501: INFO: Waiting for pod pod-configmaps-8e8ba54b-80dd-429d-90cc-af842371dfc9 to disappear
    Jan  3 02:48:37.506: INFO: Pod pod-configmaps-8e8ba54b-80dd-429d-90cc-af842371dfc9 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jan  3 02:48:37.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-3447" for this suite. 01/03/23 02:48:37.509
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:48:37.512
Jan  3 02:48:37.512: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename resourcequota 01/03/23 02:48:37.513
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:48:37.536
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:48:37.538
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
STEP: Creating a ResourceQuota with best effort scope 01/03/23 02:48:37.54
STEP: Ensuring ResourceQuota status is calculated 01/03/23 02:48:37.543
STEP: Creating a ResourceQuota with not best effort scope 01/03/23 02:48:39.546
STEP: Ensuring ResourceQuota status is calculated 01/03/23 02:48:39.55
STEP: Creating a best-effort pod 01/03/23 02:48:41.553
STEP: Ensuring resource quota with best effort scope captures the pod usage 01/03/23 02:48:41.564
STEP: Ensuring resource quota with not best effort ignored the pod usage 01/03/23 02:48:43.567
STEP: Deleting the pod 01/03/23 02:48:45.57
STEP: Ensuring resource quota status released the pod usage 01/03/23 02:48:45.611
STEP: Creating a not best-effort pod 01/03/23 02:48:47.613
STEP: Ensuring resource quota with not best effort scope captures the pod usage 01/03/23 02:48:47.627
STEP: Ensuring resource quota with best effort scope ignored the pod usage 01/03/23 02:48:49.63
STEP: Deleting the pod 01/03/23 02:48:51.633
STEP: Ensuring resource quota status released the pod usage 01/03/23 02:48:51.698
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan  3 02:48:53.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7851" for this suite. 01/03/23 02:48:53.704
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","completed":204,"skipped":3692,"failed":0}
------------------------------
• [SLOW TEST] [16.195 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:48:37.512
    Jan  3 02:48:37.512: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename resourcequota 01/03/23 02:48:37.513
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:48:37.536
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:48:37.538
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:793
    STEP: Creating a ResourceQuota with best effort scope 01/03/23 02:48:37.54
    STEP: Ensuring ResourceQuota status is calculated 01/03/23 02:48:37.543
    STEP: Creating a ResourceQuota with not best effort scope 01/03/23 02:48:39.546
    STEP: Ensuring ResourceQuota status is calculated 01/03/23 02:48:39.55
    STEP: Creating a best-effort pod 01/03/23 02:48:41.553
    STEP: Ensuring resource quota with best effort scope captures the pod usage 01/03/23 02:48:41.564
    STEP: Ensuring resource quota with not best effort ignored the pod usage 01/03/23 02:48:43.567
    STEP: Deleting the pod 01/03/23 02:48:45.57
    STEP: Ensuring resource quota status released the pod usage 01/03/23 02:48:45.611
    STEP: Creating a not best-effort pod 01/03/23 02:48:47.613
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 01/03/23 02:48:47.627
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 01/03/23 02:48:49.63
    STEP: Deleting the pod 01/03/23 02:48:51.633
    STEP: Ensuring resource quota status released the pod usage 01/03/23 02:48:51.698
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan  3 02:48:53.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-7851" for this suite. 01/03/23 02:48:53.704
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:48:53.707
Jan  3 02:48:53.707: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename container-runtime 01/03/23 02:48:53.708
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:48:53.741
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:48:53.743
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
STEP: create the container 01/03/23 02:48:53.745
STEP: wait for the container to reach Succeeded 01/03/23 02:48:53.786
STEP: get the container status 01/03/23 02:48:56.799
STEP: the container should be terminated 01/03/23 02:48:56.801
STEP: the termination message should be set 01/03/23 02:48:56.801
Jan  3 02:48:56.801: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 01/03/23 02:48:56.801
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan  3 02:48:57.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-182" for this suite. 01/03/23 02:48:57.082
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":205,"skipped":3702,"failed":0}
------------------------------
• [3.377 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:48:53.707
    Jan  3 02:48:53.707: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename container-runtime 01/03/23 02:48:53.708
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:48:53.741
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:48:53.743
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231
    STEP: create the container 01/03/23 02:48:53.745
    STEP: wait for the container to reach Succeeded 01/03/23 02:48:53.786
    STEP: get the container status 01/03/23 02:48:56.799
    STEP: the container should be terminated 01/03/23 02:48:56.801
    STEP: the termination message should be set 01/03/23 02:48:56.801
    Jan  3 02:48:56.801: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 01/03/23 02:48:56.801
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan  3 02:48:57.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-182" for this suite. 01/03/23 02:48:57.082
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:48:57.085
Jan  3 02:48:57.085: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename downward-api 01/03/23 02:48:57.086
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:48:57.127
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:48:57.129
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
STEP: Creating a pod to test downward API volume plugin 01/03/23 02:48:57.131
Jan  3 02:48:57.137: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2c6c82f9-714a-496d-86b4-0e771372c0d6" in namespace "downward-api-2613" to be "Succeeded or Failed"
Jan  3 02:48:57.150: INFO: Pod "downwardapi-volume-2c6c82f9-714a-496d-86b4-0e771372c0d6": Phase="Pending", Reason="", readiness=false. Elapsed: 13.051218ms
Jan  3 02:48:59.152: INFO: Pod "downwardapi-volume-2c6c82f9-714a-496d-86b4-0e771372c0d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015074529s
Jan  3 02:49:01.153: INFO: Pod "downwardapi-volume-2c6c82f9-714a-496d-86b4-0e771372c0d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015828908s
STEP: Saw pod success 01/03/23 02:49:01.153
Jan  3 02:49:01.153: INFO: Pod "downwardapi-volume-2c6c82f9-714a-496d-86b4-0e771372c0d6" satisfied condition "Succeeded or Failed"
Jan  3 02:49:01.155: INFO: Trying to get logs from node cncf-master pod downwardapi-volume-2c6c82f9-714a-496d-86b4-0e771372c0d6 container client-container: <nil>
STEP: delete the pod 01/03/23 02:49:01.158
Jan  3 02:49:01.177: INFO: Waiting for pod downwardapi-volume-2c6c82f9-714a-496d-86b4-0e771372c0d6 to disappear
Jan  3 02:49:01.182: INFO: Pod downwardapi-volume-2c6c82f9-714a-496d-86b4-0e771372c0d6 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan  3 02:49:01.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2613" for this suite. 01/03/23 02:49:01.185
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","completed":206,"skipped":3702,"failed":0}
------------------------------
• [4.103 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:48:57.085
    Jan  3 02:48:57.085: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename downward-api 01/03/23 02:48:57.086
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:48:57.127
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:48:57.129
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:234
    STEP: Creating a pod to test downward API volume plugin 01/03/23 02:48:57.131
    Jan  3 02:48:57.137: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2c6c82f9-714a-496d-86b4-0e771372c0d6" in namespace "downward-api-2613" to be "Succeeded or Failed"
    Jan  3 02:48:57.150: INFO: Pod "downwardapi-volume-2c6c82f9-714a-496d-86b4-0e771372c0d6": Phase="Pending", Reason="", readiness=false. Elapsed: 13.051218ms
    Jan  3 02:48:59.152: INFO: Pod "downwardapi-volume-2c6c82f9-714a-496d-86b4-0e771372c0d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015074529s
    Jan  3 02:49:01.153: INFO: Pod "downwardapi-volume-2c6c82f9-714a-496d-86b4-0e771372c0d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015828908s
    STEP: Saw pod success 01/03/23 02:49:01.153
    Jan  3 02:49:01.153: INFO: Pod "downwardapi-volume-2c6c82f9-714a-496d-86b4-0e771372c0d6" satisfied condition "Succeeded or Failed"
    Jan  3 02:49:01.155: INFO: Trying to get logs from node cncf-master pod downwardapi-volume-2c6c82f9-714a-496d-86b4-0e771372c0d6 container client-container: <nil>
    STEP: delete the pod 01/03/23 02:49:01.158
    Jan  3 02:49:01.177: INFO: Waiting for pod downwardapi-volume-2c6c82f9-714a-496d-86b4-0e771372c0d6 to disappear
    Jan  3 02:49:01.182: INFO: Pod downwardapi-volume-2c6c82f9-714a-496d-86b4-0e771372c0d6 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan  3 02:49:01.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2613" for this suite. 01/03/23 02:49:01.185
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:49:01.188
Jan  3 02:49:01.188: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename svcaccounts 01/03/23 02:49:01.189
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:49:01.216
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:49:01.218
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
STEP: creating a ServiceAccount 01/03/23 02:49:01.22
STEP: watching for the ServiceAccount to be added 01/03/23 02:49:01.239
STEP: patching the ServiceAccount 01/03/23 02:49:01.24
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 01/03/23 02:49:01.243
STEP: deleting the ServiceAccount 01/03/23 02:49:01.245
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan  3 02:49:01.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2375" for this suite. 01/03/23 02:49:01.275
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","completed":207,"skipped":3711,"failed":0}
------------------------------
• [0.089 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:49:01.188
    Jan  3 02:49:01.188: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename svcaccounts 01/03/23 02:49:01.189
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:49:01.216
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:49:01.218
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:646
    STEP: creating a ServiceAccount 01/03/23 02:49:01.22
    STEP: watching for the ServiceAccount to be added 01/03/23 02:49:01.239
    STEP: patching the ServiceAccount 01/03/23 02:49:01.24
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 01/03/23 02:49:01.243
    STEP: deleting the ServiceAccount 01/03/23 02:49:01.245
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan  3 02:49:01.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-2375" for this suite. 01/03/23 02:49:01.275
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:49:01.277
Jan  3 02:49:01.277: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename pods 01/03/23 02:49:01.278
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:49:01.285
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:49:01.286
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
Jan  3 02:49:01.291: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: creating the pod 01/03/23 02:49:01.291
STEP: submitting the pod to kubernetes 01/03/23 02:49:01.291
Jan  3 02:49:01.309: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-4a14d525-3ff4-4a78-a30d-acb50a74c468" in namespace "pods-3890" to be "running and ready"
Jan  3 02:49:01.354: INFO: Pod "pod-exec-websocket-4a14d525-3ff4-4a78-a30d-acb50a74c468": Phase="Pending", Reason="", readiness=false. Elapsed: 45.690042ms
Jan  3 02:49:01.355: INFO: The phase of Pod pod-exec-websocket-4a14d525-3ff4-4a78-a30d-acb50a74c468 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 02:49:03.358: INFO: Pod "pod-exec-websocket-4a14d525-3ff4-4a78-a30d-acb50a74c468": Phase="Running", Reason="", readiness=true. Elapsed: 2.048892153s
Jan  3 02:49:03.358: INFO: The phase of Pod pod-exec-websocket-4a14d525-3ff4-4a78-a30d-acb50a74c468 is Running (Ready = true)
Jan  3 02:49:03.358: INFO: Pod "pod-exec-websocket-4a14d525-3ff4-4a78-a30d-acb50a74c468" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan  3 02:49:03.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3890" for this suite. 01/03/23 02:49:03.476
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","completed":208,"skipped":3715,"failed":0}
------------------------------
• [2.201 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:49:01.277
    Jan  3 02:49:01.277: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename pods 01/03/23 02:49:01.278
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:49:01.285
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:49:01.286
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:535
    Jan  3 02:49:01.291: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: creating the pod 01/03/23 02:49:01.291
    STEP: submitting the pod to kubernetes 01/03/23 02:49:01.291
    Jan  3 02:49:01.309: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-4a14d525-3ff4-4a78-a30d-acb50a74c468" in namespace "pods-3890" to be "running and ready"
    Jan  3 02:49:01.354: INFO: Pod "pod-exec-websocket-4a14d525-3ff4-4a78-a30d-acb50a74c468": Phase="Pending", Reason="", readiness=false. Elapsed: 45.690042ms
    Jan  3 02:49:01.355: INFO: The phase of Pod pod-exec-websocket-4a14d525-3ff4-4a78-a30d-acb50a74c468 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 02:49:03.358: INFO: Pod "pod-exec-websocket-4a14d525-3ff4-4a78-a30d-acb50a74c468": Phase="Running", Reason="", readiness=true. Elapsed: 2.048892153s
    Jan  3 02:49:03.358: INFO: The phase of Pod pod-exec-websocket-4a14d525-3ff4-4a78-a30d-acb50a74c468 is Running (Ready = true)
    Jan  3 02:49:03.358: INFO: Pod "pod-exec-websocket-4a14d525-3ff4-4a78-a30d-acb50a74c468" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan  3 02:49:03.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-3890" for this suite. 01/03/23 02:49:03.476
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:49:03.479
Jan  3 02:49:03.479: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename disruption 01/03/23 02:49:03.48
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:49:03.491
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:49:03.493
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
STEP: creating the pdb 01/03/23 02:49:03.495
STEP: Waiting for the pdb to be processed 01/03/23 02:49:03.537
STEP: updating the pdb 01/03/23 02:49:05.542
STEP: Waiting for the pdb to be processed 01/03/23 02:49:05.547
STEP: patching the pdb 01/03/23 02:49:07.55
STEP: Waiting for the pdb to be processed 01/03/23 02:49:07.555
STEP: Waiting for the pdb to be deleted 01/03/23 02:49:09.571
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan  3 02:49:09.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9707" for this suite. 01/03/23 02:49:09.58
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","completed":209,"skipped":3716,"failed":0}
------------------------------
• [SLOW TEST] [6.104 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:49:03.479
    Jan  3 02:49:03.479: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename disruption 01/03/23 02:49:03.48
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:49:03.491
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:49:03.493
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:107
    STEP: creating the pdb 01/03/23 02:49:03.495
    STEP: Waiting for the pdb to be processed 01/03/23 02:49:03.537
    STEP: updating the pdb 01/03/23 02:49:05.542
    STEP: Waiting for the pdb to be processed 01/03/23 02:49:05.547
    STEP: patching the pdb 01/03/23 02:49:07.55
    STEP: Waiting for the pdb to be processed 01/03/23 02:49:07.555
    STEP: Waiting for the pdb to be deleted 01/03/23 02:49:09.571
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan  3 02:49:09.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-9707" for this suite. 01/03/23 02:49:09.58
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:49:09.585
Jan  3 02:49:09.585: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename container-probe 01/03/23 02:49:09.586
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:49:09.606
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:49:09.608
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
STEP: Creating pod test-webserver-f2ee424a-b38f-428e-91f4-8db99b9aed36 in namespace container-probe-9717 01/03/23 02:49:09.61
Jan  3 02:49:09.626: INFO: Waiting up to 5m0s for pod "test-webserver-f2ee424a-b38f-428e-91f4-8db99b9aed36" in namespace "container-probe-9717" to be "not pending"
Jan  3 02:49:09.628: INFO: Pod "test-webserver-f2ee424a-b38f-428e-91f4-8db99b9aed36": Phase="Pending", Reason="", readiness=false. Elapsed: 1.90746ms
Jan  3 02:49:11.631: INFO: Pod "test-webserver-f2ee424a-b38f-428e-91f4-8db99b9aed36": Phase="Running", Reason="", readiness=true. Elapsed: 2.004657593s
Jan  3 02:49:11.631: INFO: Pod "test-webserver-f2ee424a-b38f-428e-91f4-8db99b9aed36" satisfied condition "not pending"
Jan  3 02:49:11.631: INFO: Started pod test-webserver-f2ee424a-b38f-428e-91f4-8db99b9aed36 in namespace container-probe-9717
STEP: checking the pod's current state and verifying that restartCount is present 01/03/23 02:49:11.631
Jan  3 02:49:11.633: INFO: Initial restart count of pod test-webserver-f2ee424a-b38f-428e-91f4-8db99b9aed36 is 0
STEP: deleting the pod 01/03/23 02:53:12.081
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan  3 02:53:12.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9717" for this suite. 01/03/23 02:53:12.131
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":210,"skipped":3769,"failed":0}
------------------------------
• [SLOW TEST] [242.549 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:49:09.585
    Jan  3 02:49:09.585: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename container-probe 01/03/23 02:49:09.586
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:49:09.606
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:49:09.608
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:211
    STEP: Creating pod test-webserver-f2ee424a-b38f-428e-91f4-8db99b9aed36 in namespace container-probe-9717 01/03/23 02:49:09.61
    Jan  3 02:49:09.626: INFO: Waiting up to 5m0s for pod "test-webserver-f2ee424a-b38f-428e-91f4-8db99b9aed36" in namespace "container-probe-9717" to be "not pending"
    Jan  3 02:49:09.628: INFO: Pod "test-webserver-f2ee424a-b38f-428e-91f4-8db99b9aed36": Phase="Pending", Reason="", readiness=false. Elapsed: 1.90746ms
    Jan  3 02:49:11.631: INFO: Pod "test-webserver-f2ee424a-b38f-428e-91f4-8db99b9aed36": Phase="Running", Reason="", readiness=true. Elapsed: 2.004657593s
    Jan  3 02:49:11.631: INFO: Pod "test-webserver-f2ee424a-b38f-428e-91f4-8db99b9aed36" satisfied condition "not pending"
    Jan  3 02:49:11.631: INFO: Started pod test-webserver-f2ee424a-b38f-428e-91f4-8db99b9aed36 in namespace container-probe-9717
    STEP: checking the pod's current state and verifying that restartCount is present 01/03/23 02:49:11.631
    Jan  3 02:49:11.633: INFO: Initial restart count of pod test-webserver-f2ee424a-b38f-428e-91f4-8db99b9aed36 is 0
    STEP: deleting the pod 01/03/23 02:53:12.081
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan  3 02:53:12.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-9717" for this suite. 01/03/23 02:53:12.131
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:53:12.134
Jan  3 02:53:12.134: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename emptydir 01/03/23 02:53:12.135
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:53:12.158
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:53:12.16
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
STEP: Creating a pod to test emptydir 0666 on node default medium 01/03/23 02:53:12.161
Jan  3 02:53:12.212: INFO: Waiting up to 5m0s for pod "pod-1c546870-1e03-43e3-9edb-ed2fe37efd71" in namespace "emptydir-8647" to be "Succeeded or Failed"
Jan  3 02:53:12.214: INFO: Pod "pod-1c546870-1e03-43e3-9edb-ed2fe37efd71": Phase="Pending", Reason="", readiness=false. Elapsed: 1.578894ms
Jan  3 02:53:14.217: INFO: Pod "pod-1c546870-1e03-43e3-9edb-ed2fe37efd71": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004671433s
Jan  3 02:53:16.216: INFO: Pod "pod-1c546870-1e03-43e3-9edb-ed2fe37efd71": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003593928s
STEP: Saw pod success 01/03/23 02:53:16.216
Jan  3 02:53:16.216: INFO: Pod "pod-1c546870-1e03-43e3-9edb-ed2fe37efd71" satisfied condition "Succeeded or Failed"
Jan  3 02:53:16.218: INFO: Trying to get logs from node cncf-master pod pod-1c546870-1e03-43e3-9edb-ed2fe37efd71 container test-container: <nil>
STEP: delete the pod 01/03/23 02:53:16.228
Jan  3 02:53:16.277: INFO: Waiting for pod pod-1c546870-1e03-43e3-9edb-ed2fe37efd71 to disappear
Jan  3 02:53:16.282: INFO: Pod pod-1c546870-1e03-43e3-9edb-ed2fe37efd71 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan  3 02:53:16.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8647" for this suite. 01/03/23 02:53:16.285
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":211,"skipped":3778,"failed":0}
------------------------------
• [4.153 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:53:12.134
    Jan  3 02:53:12.134: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename emptydir 01/03/23 02:53:12.135
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:53:12.158
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:53:12.16
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:206
    STEP: Creating a pod to test emptydir 0666 on node default medium 01/03/23 02:53:12.161
    Jan  3 02:53:12.212: INFO: Waiting up to 5m0s for pod "pod-1c546870-1e03-43e3-9edb-ed2fe37efd71" in namespace "emptydir-8647" to be "Succeeded or Failed"
    Jan  3 02:53:12.214: INFO: Pod "pod-1c546870-1e03-43e3-9edb-ed2fe37efd71": Phase="Pending", Reason="", readiness=false. Elapsed: 1.578894ms
    Jan  3 02:53:14.217: INFO: Pod "pod-1c546870-1e03-43e3-9edb-ed2fe37efd71": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004671433s
    Jan  3 02:53:16.216: INFO: Pod "pod-1c546870-1e03-43e3-9edb-ed2fe37efd71": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003593928s
    STEP: Saw pod success 01/03/23 02:53:16.216
    Jan  3 02:53:16.216: INFO: Pod "pod-1c546870-1e03-43e3-9edb-ed2fe37efd71" satisfied condition "Succeeded or Failed"
    Jan  3 02:53:16.218: INFO: Trying to get logs from node cncf-master pod pod-1c546870-1e03-43e3-9edb-ed2fe37efd71 container test-container: <nil>
    STEP: delete the pod 01/03/23 02:53:16.228
    Jan  3 02:53:16.277: INFO: Waiting for pod pod-1c546870-1e03-43e3-9edb-ed2fe37efd71 to disappear
    Jan  3 02:53:16.282: INFO: Pod pod-1c546870-1e03-43e3-9edb-ed2fe37efd71 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan  3 02:53:16.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8647" for this suite. 01/03/23 02:53:16.285
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:53:16.288
Jan  3 02:53:16.288: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename disruption 01/03/23 02:53:16.289
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:53:16.315
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:53:16.317
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
STEP: Waiting for the pdb to be processed 01/03/23 02:53:16.322
STEP: Updating PodDisruptionBudget status 01/03/23 02:53:18.331
STEP: Waiting for all pods to be running 01/03/23 02:53:18.337
Jan  3 02:53:18.345: INFO: running pods: 0 < 1
STEP: locating a running pod 01/03/23 02:53:20.348
STEP: Waiting for the pdb to be processed 01/03/23 02:53:20.354
STEP: Patching PodDisruptionBudget status 01/03/23 02:53:20.391
STEP: Waiting for the pdb to be processed 01/03/23 02:53:20.395
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan  3 02:53:20.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-7644" for this suite. 01/03/23 02:53:20.415
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","completed":212,"skipped":3779,"failed":0}
------------------------------
• [4.130 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:53:16.288
    Jan  3 02:53:16.288: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename disruption 01/03/23 02:53:16.289
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:53:16.315
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:53:16.317
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:163
    STEP: Waiting for the pdb to be processed 01/03/23 02:53:16.322
    STEP: Updating PodDisruptionBudget status 01/03/23 02:53:18.331
    STEP: Waiting for all pods to be running 01/03/23 02:53:18.337
    Jan  3 02:53:18.345: INFO: running pods: 0 < 1
    STEP: locating a running pod 01/03/23 02:53:20.348
    STEP: Waiting for the pdb to be processed 01/03/23 02:53:20.354
    STEP: Patching PodDisruptionBudget status 01/03/23 02:53:20.391
    STEP: Waiting for the pdb to be processed 01/03/23 02:53:20.395
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan  3 02:53:20.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-7644" for this suite. 01/03/23 02:53:20.415
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:53:20.417
Jan  3 02:53:20.417: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename webhook 01/03/23 02:53:20.418
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:53:20.425
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:53:20.427
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/03/23 02:53:20.454
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/23 02:53:21.048
STEP: Deploying the webhook pod 01/03/23 02:53:21.064
STEP: Wait for the deployment to be ready 01/03/23 02:53:21.088
Jan  3 02:53:21.112: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/03/23 02:53:23.124
STEP: Verifying the service has paired with the endpoint 01/03/23 02:53:23.142
Jan  3 02:53:24.143: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 01/03/23 02:53:24.145
STEP: create a configmap that should be updated by the webhook 01/03/23 02:53:24.156
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  3 02:53:24.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4444" for this suite. 01/03/23 02:53:24.181
STEP: Destroying namespace "webhook-4444-markers" for this suite. 01/03/23 02:53:24.184
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","completed":213,"skipped":3779,"failed":0}
------------------------------
• [3.849 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:53:20.417
    Jan  3 02:53:20.417: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename webhook 01/03/23 02:53:20.418
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:53:20.425
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:53:20.427
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/03/23 02:53:20.454
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/23 02:53:21.048
    STEP: Deploying the webhook pod 01/03/23 02:53:21.064
    STEP: Wait for the deployment to be ready 01/03/23 02:53:21.088
    Jan  3 02:53:21.112: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/03/23 02:53:23.124
    STEP: Verifying the service has paired with the endpoint 01/03/23 02:53:23.142
    Jan  3 02:53:24.143: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:251
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 01/03/23 02:53:24.145
    STEP: create a configmap that should be updated by the webhook 01/03/23 02:53:24.156
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  3 02:53:24.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4444" for this suite. 01/03/23 02:53:24.181
    STEP: Destroying namespace "webhook-4444-markers" for this suite. 01/03/23 02:53:24.184
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:53:24.267
Jan  3 02:53:24.267: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename statefulset 01/03/23 02:53:24.268
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:53:24.289
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:53:24.291
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-5498 01/03/23 02:53:24.295
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
STEP: Creating statefulset ss in namespace statefulset-5498 01/03/23 02:53:24.312
Jan  3 02:53:24.331: INFO: Found 0 stateful pods, waiting for 1
Jan  3 02:53:34.334: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 01/03/23 02:53:34.337
STEP: Getting /status 01/03/23 02:53:34.344
Jan  3 02:53:34.354: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 01/03/23 02:53:34.354
Jan  3 02:53:34.358: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 01/03/23 02:53:34.358
Jan  3 02:53:34.360: INFO: Observed &StatefulSet event: ADDED
Jan  3 02:53:34.360: INFO: Found Statefulset ss in namespace statefulset-5498 with labels: map[e2e:testing] annotations: map[createdTime:2023-01-03T11:53:24.314311949+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T11:53:24.314311949+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan  3 02:53:34.360: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 01/03/23 02:53:34.36
Jan  3 02:53:34.360: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan  3 02:53:34.364: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 01/03/23 02:53:34.364
Jan  3 02:53:34.366: INFO: Observed &StatefulSet event: ADDED
Jan  3 02:53:34.366: INFO: Observed Statefulset ss in namespace statefulset-5498 with annotations: map[createdTime:2023-01-03T11:53:24.314311949+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T11:53:24.314311949+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan  3 02:53:34.366: INFO: Observed &StatefulSet event: MODIFIED
Jan  3 02:53:34.366: INFO: Found Statefulset ss in namespace statefulset-5498 with labels: map[e2e:testing] annotations: map[createdTime:2023-01-03T11:53:24.314311949+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T11:53:24.314311949+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan  3 02:53:34.366: INFO: Deleting all statefulset in ns statefulset-5498
Jan  3 02:53:34.384: INFO: Scaling statefulset ss to 0
Jan  3 02:53:44.407: INFO: Waiting for statefulset status.replicas updated to 0
Jan  3 02:53:44.409: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan  3 02:53:44.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5498" for this suite. 01/03/23 02:53:44.42
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","completed":214,"skipped":3784,"failed":0}
------------------------------
• [SLOW TEST] [20.161 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:975

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:53:24.267
    Jan  3 02:53:24.267: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename statefulset 01/03/23 02:53:24.268
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:53:24.289
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:53:24.291
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-5498 01/03/23 02:53:24.295
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:975
    STEP: Creating statefulset ss in namespace statefulset-5498 01/03/23 02:53:24.312
    Jan  3 02:53:24.331: INFO: Found 0 stateful pods, waiting for 1
    Jan  3 02:53:34.334: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 01/03/23 02:53:34.337
    STEP: Getting /status 01/03/23 02:53:34.344
    Jan  3 02:53:34.354: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 01/03/23 02:53:34.354
    Jan  3 02:53:34.358: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 01/03/23 02:53:34.358
    Jan  3 02:53:34.360: INFO: Observed &StatefulSet event: ADDED
    Jan  3 02:53:34.360: INFO: Found Statefulset ss in namespace statefulset-5498 with labels: map[e2e:testing] annotations: map[createdTime:2023-01-03T11:53:24.314311949+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T11:53:24.314311949+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan  3 02:53:34.360: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 01/03/23 02:53:34.36
    Jan  3 02:53:34.360: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan  3 02:53:34.364: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 01/03/23 02:53:34.364
    Jan  3 02:53:34.366: INFO: Observed &StatefulSet event: ADDED
    Jan  3 02:53:34.366: INFO: Observed Statefulset ss in namespace statefulset-5498 with annotations: map[createdTime:2023-01-03T11:53:24.314311949+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T11:53:24.314311949+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan  3 02:53:34.366: INFO: Observed &StatefulSet event: MODIFIED
    Jan  3 02:53:34.366: INFO: Found Statefulset ss in namespace statefulset-5498 with labels: map[e2e:testing] annotations: map[createdTime:2023-01-03T11:53:24.314311949+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T11:53:24.314311949+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan  3 02:53:34.366: INFO: Deleting all statefulset in ns statefulset-5498
    Jan  3 02:53:34.384: INFO: Scaling statefulset ss to 0
    Jan  3 02:53:44.407: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  3 02:53:44.409: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan  3 02:53:44.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-5498" for this suite. 01/03/23 02:53:44.42
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:53:44.429
Jan  3 02:53:44.429: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename endpointslice 01/03/23 02:53:44.43
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:53:44.48
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:53:44.482
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
Jan  3 02:53:44.489: INFO: Endpoints addresses: [172.21.7.5] , ports: [6443]
Jan  3 02:53:44.489: INFO: EndpointSlices addresses: [172.21.7.5] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jan  3 02:53:44.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-3351" for this suite. 01/03/23 02:53:44.491
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","completed":215,"skipped":3788,"failed":0}
------------------------------
• [0.065 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:53:44.429
    Jan  3 02:53:44.429: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename endpointslice 01/03/23 02:53:44.43
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:53:44.48
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:53:44.482
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:65
    Jan  3 02:53:44.489: INFO: Endpoints addresses: [172.21.7.5] , ports: [6443]
    Jan  3 02:53:44.489: INFO: EndpointSlices addresses: [172.21.7.5] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jan  3 02:53:44.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-3351" for this suite. 01/03/23 02:53:44.491
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:53:44.495
Jan  3 02:53:44.495: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename configmap 01/03/23 02:53:44.495
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:53:44.518
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:53:44.52
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
STEP: Creating configMap with name configmap-test-volume-ee837576-157f-4875-b25c-e9ff9adcb90c 01/03/23 02:53:44.521
STEP: Creating a pod to test consume configMaps 01/03/23 02:53:44.525
Jan  3 02:53:44.543: INFO: Waiting up to 5m0s for pod "pod-configmaps-18f13640-c97a-4a99-a87b-2a6e5d5f95bf" in namespace "configmap-9180" to be "Succeeded or Failed"
Jan  3 02:53:44.548: INFO: Pod "pod-configmaps-18f13640-c97a-4a99-a87b-2a6e5d5f95bf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.11369ms
Jan  3 02:53:46.550: INFO: Pod "pod-configmaps-18f13640-c97a-4a99-a87b-2a6e5d5f95bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006934925s
Jan  3 02:53:48.551: INFO: Pod "pod-configmaps-18f13640-c97a-4a99-a87b-2a6e5d5f95bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007678217s
STEP: Saw pod success 01/03/23 02:53:48.551
Jan  3 02:53:48.551: INFO: Pod "pod-configmaps-18f13640-c97a-4a99-a87b-2a6e5d5f95bf" satisfied condition "Succeeded or Failed"
Jan  3 02:53:48.571: INFO: Trying to get logs from node cncf-master pod pod-configmaps-18f13640-c97a-4a99-a87b-2a6e5d5f95bf container agnhost-container: <nil>
STEP: delete the pod 01/03/23 02:53:48.575
Jan  3 02:53:48.594: INFO: Waiting for pod pod-configmaps-18f13640-c97a-4a99-a87b-2a6e5d5f95bf to disappear
Jan  3 02:53:48.599: INFO: Pod pod-configmaps-18f13640-c97a-4a99-a87b-2a6e5d5f95bf no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan  3 02:53:48.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9180" for this suite. 01/03/23 02:53:48.602
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":216,"skipped":3816,"failed":0}
------------------------------
• [4.109 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:53:44.495
    Jan  3 02:53:44.495: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename configmap 01/03/23 02:53:44.495
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:53:44.518
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:53:44.52
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:73
    STEP: Creating configMap with name configmap-test-volume-ee837576-157f-4875-b25c-e9ff9adcb90c 01/03/23 02:53:44.521
    STEP: Creating a pod to test consume configMaps 01/03/23 02:53:44.525
    Jan  3 02:53:44.543: INFO: Waiting up to 5m0s for pod "pod-configmaps-18f13640-c97a-4a99-a87b-2a6e5d5f95bf" in namespace "configmap-9180" to be "Succeeded or Failed"
    Jan  3 02:53:44.548: INFO: Pod "pod-configmaps-18f13640-c97a-4a99-a87b-2a6e5d5f95bf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.11369ms
    Jan  3 02:53:46.550: INFO: Pod "pod-configmaps-18f13640-c97a-4a99-a87b-2a6e5d5f95bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006934925s
    Jan  3 02:53:48.551: INFO: Pod "pod-configmaps-18f13640-c97a-4a99-a87b-2a6e5d5f95bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007678217s
    STEP: Saw pod success 01/03/23 02:53:48.551
    Jan  3 02:53:48.551: INFO: Pod "pod-configmaps-18f13640-c97a-4a99-a87b-2a6e5d5f95bf" satisfied condition "Succeeded or Failed"
    Jan  3 02:53:48.571: INFO: Trying to get logs from node cncf-master pod pod-configmaps-18f13640-c97a-4a99-a87b-2a6e5d5f95bf container agnhost-container: <nil>
    STEP: delete the pod 01/03/23 02:53:48.575
    Jan  3 02:53:48.594: INFO: Waiting for pod pod-configmaps-18f13640-c97a-4a99-a87b-2a6e5d5f95bf to disappear
    Jan  3 02:53:48.599: INFO: Pod pod-configmaps-18f13640-c97a-4a99-a87b-2a6e5d5f95bf no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan  3 02:53:48.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9180" for this suite. 01/03/23 02:53:48.602
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:53:48.607
Jan  3 02:53:48.607: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename webhook 01/03/23 02:53:48.608
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:53:48.616
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:53:48.618
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/03/23 02:53:48.64
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/23 02:53:49.257
STEP: Deploying the webhook pod 01/03/23 02:53:49.261
STEP: Wait for the deployment to be ready 01/03/23 02:53:49.286
Jan  3 02:53:49.310: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/03/23 02:53:51.315
STEP: Verifying the service has paired with the endpoint 01/03/23 02:53:51.328
Jan  3 02:53:52.329: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
STEP: Creating a mutating webhook configuration 01/03/23 02:53:52.338
STEP: Updating a mutating webhook configuration's rules to not include the create operation 01/03/23 02:53:52.358
STEP: Creating a configMap that should not be mutated 01/03/23 02:53:52.375
STEP: Patching a mutating webhook configuration's rules to include the create operation 01/03/23 02:53:52.401
STEP: Creating a configMap that should be mutated 01/03/23 02:53:52.406
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  3 02:53:52.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4282" for this suite. 01/03/23 02:53:52.436
STEP: Destroying namespace "webhook-4282-markers" for this suite. 01/03/23 02:53:52.463
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","completed":217,"skipped":3886,"failed":0}
------------------------------
• [3.922 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:53:48.607
    Jan  3 02:53:48.607: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename webhook 01/03/23 02:53:48.608
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:53:48.616
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:53:48.618
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/03/23 02:53:48.64
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/23 02:53:49.257
    STEP: Deploying the webhook pod 01/03/23 02:53:49.261
    STEP: Wait for the deployment to be ready 01/03/23 02:53:49.286
    Jan  3 02:53:49.310: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/03/23 02:53:51.315
    STEP: Verifying the service has paired with the endpoint 01/03/23 02:53:51.328
    Jan  3 02:53:52.329: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:507
    STEP: Creating a mutating webhook configuration 01/03/23 02:53:52.338
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 01/03/23 02:53:52.358
    STEP: Creating a configMap that should not be mutated 01/03/23 02:53:52.375
    STEP: Patching a mutating webhook configuration's rules to include the create operation 01/03/23 02:53:52.401
    STEP: Creating a configMap that should be mutated 01/03/23 02:53:52.406
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  3 02:53:52.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4282" for this suite. 01/03/23 02:53:52.436
    STEP: Destroying namespace "webhook-4282-markers" for this suite. 01/03/23 02:53:52.463
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:53:52.53
Jan  3 02:53:52.530: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename kubectl 01/03/23 02:53:52.531
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:53:52.549
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:53:52.551
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
STEP: create deployment with httpd image 01/03/23 02:53:52.552
Jan  3 02:53:52.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-3037 create -f -'
Jan  3 02:53:53.980: INFO: stderr: ""
Jan  3 02:53:53.980: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 01/03/23 02:53:53.98
Jan  3 02:53:53.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-3037 diff -f -'
Jan  3 02:53:54.211: INFO: rc: 1
Jan  3 02:53:54.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-3037 delete -f -'
Jan  3 02:53:54.271: INFO: stderr: ""
Jan  3 02:53:54.271: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan  3 02:53:54.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3037" for this suite. 01/03/23 02:53:54.276
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","completed":218,"skipped":3900,"failed":0}
------------------------------
• [1.761 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:923
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:929

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:53:52.53
    Jan  3 02:53:52.530: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename kubectl 01/03/23 02:53:52.531
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:53:52.549
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:53:52.551
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:929
    STEP: create deployment with httpd image 01/03/23 02:53:52.552
    Jan  3 02:53:52.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-3037 create -f -'
    Jan  3 02:53:53.980: INFO: stderr: ""
    Jan  3 02:53:53.980: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 01/03/23 02:53:53.98
    Jan  3 02:53:53.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-3037 diff -f -'
    Jan  3 02:53:54.211: INFO: rc: 1
    Jan  3 02:53:54.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-3037 delete -f -'
    Jan  3 02:53:54.271: INFO: stderr: ""
    Jan  3 02:53:54.271: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan  3 02:53:54.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3037" for this suite. 01/03/23 02:53:54.276
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:53:54.291
Jan  3 02:53:54.291: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename pods 01/03/23 02:53:54.292
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:53:54.337
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:53:54.339
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
STEP: Create set of pods 01/03/23 02:53:54.341
Jan  3 02:53:54.347: INFO: created test-pod-1
Jan  3 02:53:54.364: INFO: created test-pod-2
Jan  3 02:53:54.381: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 01/03/23 02:53:54.381
Jan  3 02:53:54.382: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-5685' to be running and ready
Jan  3 02:53:54.418: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan  3 02:53:54.418: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan  3 02:53:54.418: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan  3 02:53:54.418: INFO: 0 / 3 pods in namespace 'pods-5685' are running and ready (0 seconds elapsed)
Jan  3 02:53:54.418: INFO: expected 0 pod replicas in namespace 'pods-5685', 0 are Running and Ready.
Jan  3 02:53:54.418: INFO: POD         NODE         PHASE    GRACE  CONDITIONS
Jan  3 02:53:54.418: INFO: test-pod-1  cncf-master  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 02:53:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-03 02:53:54 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-03 02:53:54 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 02:53:54 +0000 UTC  }]
Jan  3 02:53:54.418: INFO: test-pod-2  cncf-master  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 02:53:54 +0000 UTC  }]
Jan  3 02:53:54.418: INFO: test-pod-3  cncf-master  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 02:53:54 +0000 UTC  }]
Jan  3 02:53:54.418: INFO: 
Jan  3 02:53:56.424: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan  3 02:53:56.424: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan  3 02:53:56.424: INFO: 1 / 3 pods in namespace 'pods-5685' are running and ready (2 seconds elapsed)
Jan  3 02:53:56.424: INFO: expected 0 pod replicas in namespace 'pods-5685', 0 are Running and Ready.
Jan  3 02:53:56.424: INFO: POD         NODE         PHASE    GRACE  CONDITIONS
Jan  3 02:53:56.424: INFO: test-pod-1  cncf-master  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 02:53:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-03 02:53:54 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-03 02:53:54 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 02:53:54 +0000 UTC  }]
Jan  3 02:53:56.424: INFO: test-pod-3  cncf-master  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 02:53:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-03 02:53:54 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-03 02:53:54 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 02:53:54 +0000 UTC  }]
Jan  3 02:53:56.424: INFO: 
Jan  3 02:53:58.425: INFO: 3 / 3 pods in namespace 'pods-5685' are running and ready (4 seconds elapsed)
Jan  3 02:53:58.425: INFO: expected 0 pod replicas in namespace 'pods-5685', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 01/03/23 02:53:58.493
Jan  3 02:53:58.532: INFO: Pod quantity 3 is different from expected quantity 0
Jan  3 02:53:59.549: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan  3 02:54:00.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5685" for this suite. 01/03/23 02:54:00.537
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","completed":219,"skipped":3905,"failed":0}
------------------------------
• [SLOW TEST] [6.249 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:53:54.291
    Jan  3 02:53:54.291: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename pods 01/03/23 02:53:54.292
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:53:54.337
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:53:54.339
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:844
    STEP: Create set of pods 01/03/23 02:53:54.341
    Jan  3 02:53:54.347: INFO: created test-pod-1
    Jan  3 02:53:54.364: INFO: created test-pod-2
    Jan  3 02:53:54.381: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 01/03/23 02:53:54.381
    Jan  3 02:53:54.382: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-5685' to be running and ready
    Jan  3 02:53:54.418: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan  3 02:53:54.418: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan  3 02:53:54.418: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan  3 02:53:54.418: INFO: 0 / 3 pods in namespace 'pods-5685' are running and ready (0 seconds elapsed)
    Jan  3 02:53:54.418: INFO: expected 0 pod replicas in namespace 'pods-5685', 0 are Running and Ready.
    Jan  3 02:53:54.418: INFO: POD         NODE         PHASE    GRACE  CONDITIONS
    Jan  3 02:53:54.418: INFO: test-pod-1  cncf-master  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 02:53:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-03 02:53:54 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-03 02:53:54 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 02:53:54 +0000 UTC  }]
    Jan  3 02:53:54.418: INFO: test-pod-2  cncf-master  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 02:53:54 +0000 UTC  }]
    Jan  3 02:53:54.418: INFO: test-pod-3  cncf-master  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 02:53:54 +0000 UTC  }]
    Jan  3 02:53:54.418: INFO: 
    Jan  3 02:53:56.424: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan  3 02:53:56.424: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan  3 02:53:56.424: INFO: 1 / 3 pods in namespace 'pods-5685' are running and ready (2 seconds elapsed)
    Jan  3 02:53:56.424: INFO: expected 0 pod replicas in namespace 'pods-5685', 0 are Running and Ready.
    Jan  3 02:53:56.424: INFO: POD         NODE         PHASE    GRACE  CONDITIONS
    Jan  3 02:53:56.424: INFO: test-pod-1  cncf-master  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 02:53:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-03 02:53:54 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-03 02:53:54 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 02:53:54 +0000 UTC  }]
    Jan  3 02:53:56.424: INFO: test-pod-3  cncf-master  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 02:53:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-03 02:53:54 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-03 02:53:54 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-03 02:53:54 +0000 UTC  }]
    Jan  3 02:53:56.424: INFO: 
    Jan  3 02:53:58.425: INFO: 3 / 3 pods in namespace 'pods-5685' are running and ready (4 seconds elapsed)
    Jan  3 02:53:58.425: INFO: expected 0 pod replicas in namespace 'pods-5685', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 01/03/23 02:53:58.493
    Jan  3 02:53:58.532: INFO: Pod quantity 3 is different from expected quantity 0
    Jan  3 02:53:59.549: INFO: Pod quantity 3 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan  3 02:54:00.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-5685" for this suite. 01/03/23 02:54:00.537
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:54:00.542
Jan  3 02:54:00.542: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename services 01/03/23 02:54:00.543
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:54:00.617
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:54:00.619
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
STEP: creating a collection of services 01/03/23 02:54:00.621
Jan  3 02:54:00.621: INFO: Creating e2e-svc-a-47vp9
Jan  3 02:54:00.643: INFO: Creating e2e-svc-b-vppgj
Jan  3 02:54:00.666: INFO: Creating e2e-svc-c-fv274
STEP: deleting service collection 01/03/23 02:54:00.694
Jan  3 02:54:00.769: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  3 02:54:00.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8112" for this suite. 01/03/23 02:54:00.772
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","completed":220,"skipped":3957,"failed":0}
------------------------------
• [0.232 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:54:00.542
    Jan  3 02:54:00.542: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename services 01/03/23 02:54:00.543
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:54:00.617
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:54:00.619
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3641
    STEP: creating a collection of services 01/03/23 02:54:00.621
    Jan  3 02:54:00.621: INFO: Creating e2e-svc-a-47vp9
    Jan  3 02:54:00.643: INFO: Creating e2e-svc-b-vppgj
    Jan  3 02:54:00.666: INFO: Creating e2e-svc-c-fv274
    STEP: deleting service collection 01/03/23 02:54:00.694
    Jan  3 02:54:00.769: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  3 02:54:00.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8112" for this suite. 01/03/23 02:54:00.772
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:54:00.774
Jan  3 02:54:00.775: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename projected 01/03/23 02:54:00.775
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:54:00.785
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:54:00.787
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
STEP: Creating a pod to test downward API volume plugin 01/03/23 02:54:00.792
Jan  3 02:54:00.807: INFO: Waiting up to 5m0s for pod "downwardapi-volume-15f20563-d775-400a-be52-040b7591bf6b" in namespace "projected-8812" to be "Succeeded or Failed"
Jan  3 02:54:00.814: INFO: Pod "downwardapi-volume-15f20563-d775-400a-be52-040b7591bf6b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.942377ms
Jan  3 02:54:02.816: INFO: Pod "downwardapi-volume-15f20563-d775-400a-be52-040b7591bf6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009392706s
Jan  3 02:54:04.816: INFO: Pod "downwardapi-volume-15f20563-d775-400a-be52-040b7591bf6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009540659s
STEP: Saw pod success 01/03/23 02:54:04.816
Jan  3 02:54:04.816: INFO: Pod "downwardapi-volume-15f20563-d775-400a-be52-040b7591bf6b" satisfied condition "Succeeded or Failed"
Jan  3 02:54:04.818: INFO: Trying to get logs from node cncf-master pod downwardapi-volume-15f20563-d775-400a-be52-040b7591bf6b container client-container: <nil>
STEP: delete the pod 01/03/23 02:54:04.821
Jan  3 02:54:04.863: INFO: Waiting for pod downwardapi-volume-15f20563-d775-400a-be52-040b7591bf6b to disappear
Jan  3 02:54:04.865: INFO: Pod downwardapi-volume-15f20563-d775-400a-be52-040b7591bf6b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan  3 02:54:04.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8812" for this suite. 01/03/23 02:54:04.869
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":221,"skipped":3960,"failed":0}
------------------------------
• [4.098 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:54:00.774
    Jan  3 02:54:00.775: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename projected 01/03/23 02:54:00.775
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:54:00.785
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:54:00.787
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:67
    STEP: Creating a pod to test downward API volume plugin 01/03/23 02:54:00.792
    Jan  3 02:54:00.807: INFO: Waiting up to 5m0s for pod "downwardapi-volume-15f20563-d775-400a-be52-040b7591bf6b" in namespace "projected-8812" to be "Succeeded or Failed"
    Jan  3 02:54:00.814: INFO: Pod "downwardapi-volume-15f20563-d775-400a-be52-040b7591bf6b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.942377ms
    Jan  3 02:54:02.816: INFO: Pod "downwardapi-volume-15f20563-d775-400a-be52-040b7591bf6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009392706s
    Jan  3 02:54:04.816: INFO: Pod "downwardapi-volume-15f20563-d775-400a-be52-040b7591bf6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009540659s
    STEP: Saw pod success 01/03/23 02:54:04.816
    Jan  3 02:54:04.816: INFO: Pod "downwardapi-volume-15f20563-d775-400a-be52-040b7591bf6b" satisfied condition "Succeeded or Failed"
    Jan  3 02:54:04.818: INFO: Trying to get logs from node cncf-master pod downwardapi-volume-15f20563-d775-400a-be52-040b7591bf6b container client-container: <nil>
    STEP: delete the pod 01/03/23 02:54:04.821
    Jan  3 02:54:04.863: INFO: Waiting for pod downwardapi-volume-15f20563-d775-400a-be52-040b7591bf6b to disappear
    Jan  3 02:54:04.865: INFO: Pod downwardapi-volume-15f20563-d775-400a-be52-040b7591bf6b no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan  3 02:54:04.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8812" for this suite. 01/03/23 02:54:04.869
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:54:04.873
Jan  3 02:54:04.874: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename proxy 01/03/23 02:54:04.874
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:54:04.892
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:54:04.895
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Jan  3 02:54:04.897: INFO: Creating pod...
Jan  3 02:54:04.903: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-1424" to be "running"
Jan  3 02:54:04.915: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 12.495094ms
Jan  3 02:54:06.918: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.015349628s
Jan  3 02:54:06.918: INFO: Pod "agnhost" satisfied condition "running"
Jan  3 02:54:06.918: INFO: Creating service...
Jan  3 02:54:06.935: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1424/pods/agnhost/proxy?method=DELETE
Jan  3 02:54:06.941: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan  3 02:54:06.941: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1424/pods/agnhost/proxy?method=OPTIONS
Jan  3 02:54:06.983: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan  3 02:54:06.983: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1424/pods/agnhost/proxy?method=PATCH
Jan  3 02:54:06.984: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan  3 02:54:06.984: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1424/pods/agnhost/proxy?method=POST
Jan  3 02:54:06.986: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan  3 02:54:06.986: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1424/pods/agnhost/proxy?method=PUT
Jan  3 02:54:06.987: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan  3 02:54:06.987: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1424/services/e2e-proxy-test-service/proxy?method=DELETE
Jan  3 02:54:06.989: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan  3 02:54:06.989: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1424/services/e2e-proxy-test-service/proxy?method=OPTIONS
Jan  3 02:54:06.991: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan  3 02:54:06.991: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1424/services/e2e-proxy-test-service/proxy?method=PATCH
Jan  3 02:54:06.993: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan  3 02:54:06.993: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1424/services/e2e-proxy-test-service/proxy?method=POST
Jan  3 02:54:06.995: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan  3 02:54:06.995: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1424/services/e2e-proxy-test-service/proxy?method=PUT
Jan  3 02:54:06.997: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan  3 02:54:06.997: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1424/pods/agnhost/proxy?method=GET
Jan  3 02:54:06.998: INFO: http.Client request:GET StatusCode:301
Jan  3 02:54:06.999: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1424/services/e2e-proxy-test-service/proxy?method=GET
Jan  3 02:54:07.000: INFO: http.Client request:GET StatusCode:301
Jan  3 02:54:07.000: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1424/pods/agnhost/proxy?method=HEAD
Jan  3 02:54:07.001: INFO: http.Client request:HEAD StatusCode:301
Jan  3 02:54:07.001: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1424/services/e2e-proxy-test-service/proxy?method=HEAD
Jan  3 02:54:07.003: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Jan  3 02:54:07.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1424" for this suite. 01/03/23 02:54:07.006
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]","completed":222,"skipped":3980,"failed":0}
------------------------------
• [2.135 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:54:04.873
    Jan  3 02:54:04.874: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename proxy 01/03/23 02:54:04.874
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:54:04.892
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:54:04.895
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Jan  3 02:54:04.897: INFO: Creating pod...
    Jan  3 02:54:04.903: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-1424" to be "running"
    Jan  3 02:54:04.915: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 12.495094ms
    Jan  3 02:54:06.918: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.015349628s
    Jan  3 02:54:06.918: INFO: Pod "agnhost" satisfied condition "running"
    Jan  3 02:54:06.918: INFO: Creating service...
    Jan  3 02:54:06.935: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1424/pods/agnhost/proxy?method=DELETE
    Jan  3 02:54:06.941: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan  3 02:54:06.941: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1424/pods/agnhost/proxy?method=OPTIONS
    Jan  3 02:54:06.983: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan  3 02:54:06.983: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1424/pods/agnhost/proxy?method=PATCH
    Jan  3 02:54:06.984: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan  3 02:54:06.984: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1424/pods/agnhost/proxy?method=POST
    Jan  3 02:54:06.986: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan  3 02:54:06.986: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1424/pods/agnhost/proxy?method=PUT
    Jan  3 02:54:06.987: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan  3 02:54:06.987: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1424/services/e2e-proxy-test-service/proxy?method=DELETE
    Jan  3 02:54:06.989: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan  3 02:54:06.989: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1424/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Jan  3 02:54:06.991: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan  3 02:54:06.991: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1424/services/e2e-proxy-test-service/proxy?method=PATCH
    Jan  3 02:54:06.993: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan  3 02:54:06.993: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1424/services/e2e-proxy-test-service/proxy?method=POST
    Jan  3 02:54:06.995: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan  3 02:54:06.995: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1424/services/e2e-proxy-test-service/proxy?method=PUT
    Jan  3 02:54:06.997: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan  3 02:54:06.997: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1424/pods/agnhost/proxy?method=GET
    Jan  3 02:54:06.998: INFO: http.Client request:GET StatusCode:301
    Jan  3 02:54:06.999: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1424/services/e2e-proxy-test-service/proxy?method=GET
    Jan  3 02:54:07.000: INFO: http.Client request:GET StatusCode:301
    Jan  3 02:54:07.000: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1424/pods/agnhost/proxy?method=HEAD
    Jan  3 02:54:07.001: INFO: http.Client request:HEAD StatusCode:301
    Jan  3 02:54:07.001: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1424/services/e2e-proxy-test-service/proxy?method=HEAD
    Jan  3 02:54:07.003: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Jan  3 02:54:07.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-1424" for this suite. 01/03/23 02:54:07.006
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:54:07.01
Jan  3 02:54:07.010: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename configmap 01/03/23 02:54:07.01
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:54:07.026
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:54:07.028
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
STEP: Creating configMap with name configmap-test-volume-8cdee17b-913f-4db6-a630-5e36e5abfce4 01/03/23 02:54:07.029
STEP: Creating a pod to test consume configMaps 01/03/23 02:54:07.037
Jan  3 02:54:07.046: INFO: Waiting up to 5m0s for pod "pod-configmaps-083bf8b4-35fe-441a-9746-ea2c9cd39875" in namespace "configmap-9020" to be "Succeeded or Failed"
Jan  3 02:54:07.049: INFO: Pod "pod-configmaps-083bf8b4-35fe-441a-9746-ea2c9cd39875": Phase="Pending", Reason="", readiness=false. Elapsed: 2.837133ms
Jan  3 02:54:09.051: INFO: Pod "pod-configmaps-083bf8b4-35fe-441a-9746-ea2c9cd39875": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005383539s
Jan  3 02:54:11.052: INFO: Pod "pod-configmaps-083bf8b4-35fe-441a-9746-ea2c9cd39875": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005847016s
STEP: Saw pod success 01/03/23 02:54:11.052
Jan  3 02:54:11.052: INFO: Pod "pod-configmaps-083bf8b4-35fe-441a-9746-ea2c9cd39875" satisfied condition "Succeeded or Failed"
Jan  3 02:54:11.054: INFO: Trying to get logs from node cncf-master pod pod-configmaps-083bf8b4-35fe-441a-9746-ea2c9cd39875 container agnhost-container: <nil>
STEP: delete the pod 01/03/23 02:54:11.057
Jan  3 02:54:11.089: INFO: Waiting for pod pod-configmaps-083bf8b4-35fe-441a-9746-ea2c9cd39875 to disappear
Jan  3 02:54:11.095: INFO: Pod pod-configmaps-083bf8b4-35fe-441a-9746-ea2c9cd39875 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan  3 02:54:11.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9020" for this suite. 01/03/23 02:54:11.121
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":223,"skipped":4014,"failed":0}
------------------------------
• [4.114 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:54:07.01
    Jan  3 02:54:07.010: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename configmap 01/03/23 02:54:07.01
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:54:07.026
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:54:07.028
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:46
    STEP: Creating configMap with name configmap-test-volume-8cdee17b-913f-4db6-a630-5e36e5abfce4 01/03/23 02:54:07.029
    STEP: Creating a pod to test consume configMaps 01/03/23 02:54:07.037
    Jan  3 02:54:07.046: INFO: Waiting up to 5m0s for pod "pod-configmaps-083bf8b4-35fe-441a-9746-ea2c9cd39875" in namespace "configmap-9020" to be "Succeeded or Failed"
    Jan  3 02:54:07.049: INFO: Pod "pod-configmaps-083bf8b4-35fe-441a-9746-ea2c9cd39875": Phase="Pending", Reason="", readiness=false. Elapsed: 2.837133ms
    Jan  3 02:54:09.051: INFO: Pod "pod-configmaps-083bf8b4-35fe-441a-9746-ea2c9cd39875": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005383539s
    Jan  3 02:54:11.052: INFO: Pod "pod-configmaps-083bf8b4-35fe-441a-9746-ea2c9cd39875": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005847016s
    STEP: Saw pod success 01/03/23 02:54:11.052
    Jan  3 02:54:11.052: INFO: Pod "pod-configmaps-083bf8b4-35fe-441a-9746-ea2c9cd39875" satisfied condition "Succeeded or Failed"
    Jan  3 02:54:11.054: INFO: Trying to get logs from node cncf-master pod pod-configmaps-083bf8b4-35fe-441a-9746-ea2c9cd39875 container agnhost-container: <nil>
    STEP: delete the pod 01/03/23 02:54:11.057
    Jan  3 02:54:11.089: INFO: Waiting for pod pod-configmaps-083bf8b4-35fe-441a-9746-ea2c9cd39875 to disappear
    Jan  3 02:54:11.095: INFO: Pod pod-configmaps-083bf8b4-35fe-441a-9746-ea2c9cd39875 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan  3 02:54:11.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9020" for this suite. 01/03/23 02:54:11.121
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:54:11.124
Jan  3 02:54:11.124: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename projected 01/03/23 02:54:11.125
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:54:11.135
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:54:11.137
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
STEP: Creating projection with configMap that has name projected-configmap-test-upd-816ebb46-7ba6-4037-ba08-de93eaf380aa 01/03/23 02:54:11.15
STEP: Creating the pod 01/03/23 02:54:11.153
Jan  3 02:54:11.161: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5aeb6fe9-82cb-4fab-9dd0-7de14a495ae5" in namespace "projected-1175" to be "running and ready"
Jan  3 02:54:11.172: INFO: Pod "pod-projected-configmaps-5aeb6fe9-82cb-4fab-9dd0-7de14a495ae5": Phase="Pending", Reason="", readiness=false. Elapsed: 11.163206ms
Jan  3 02:54:11.172: INFO: The phase of Pod pod-projected-configmaps-5aeb6fe9-82cb-4fab-9dd0-7de14a495ae5 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 02:54:13.175: INFO: Pod "pod-projected-configmaps-5aeb6fe9-82cb-4fab-9dd0-7de14a495ae5": Phase="Running", Reason="", readiness=true. Elapsed: 2.014105753s
Jan  3 02:54:13.175: INFO: The phase of Pod pod-projected-configmaps-5aeb6fe9-82cb-4fab-9dd0-7de14a495ae5 is Running (Ready = true)
Jan  3 02:54:13.175: INFO: Pod "pod-projected-configmaps-5aeb6fe9-82cb-4fab-9dd0-7de14a495ae5" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-816ebb46-7ba6-4037-ba08-de93eaf380aa 01/03/23 02:54:13.18
STEP: waiting to observe update in volume 01/03/23 02:54:13.182
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan  3 02:54:15.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1175" for this suite. 01/03/23 02:54:15.197
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":224,"skipped":4018,"failed":0}
------------------------------
• [4.104 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:54:11.124
    Jan  3 02:54:11.124: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename projected 01/03/23 02:54:11.125
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:54:11.135
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:54:11.137
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:123
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-816ebb46-7ba6-4037-ba08-de93eaf380aa 01/03/23 02:54:11.15
    STEP: Creating the pod 01/03/23 02:54:11.153
    Jan  3 02:54:11.161: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5aeb6fe9-82cb-4fab-9dd0-7de14a495ae5" in namespace "projected-1175" to be "running and ready"
    Jan  3 02:54:11.172: INFO: Pod "pod-projected-configmaps-5aeb6fe9-82cb-4fab-9dd0-7de14a495ae5": Phase="Pending", Reason="", readiness=false. Elapsed: 11.163206ms
    Jan  3 02:54:11.172: INFO: The phase of Pod pod-projected-configmaps-5aeb6fe9-82cb-4fab-9dd0-7de14a495ae5 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 02:54:13.175: INFO: Pod "pod-projected-configmaps-5aeb6fe9-82cb-4fab-9dd0-7de14a495ae5": Phase="Running", Reason="", readiness=true. Elapsed: 2.014105753s
    Jan  3 02:54:13.175: INFO: The phase of Pod pod-projected-configmaps-5aeb6fe9-82cb-4fab-9dd0-7de14a495ae5 is Running (Ready = true)
    Jan  3 02:54:13.175: INFO: Pod "pod-projected-configmaps-5aeb6fe9-82cb-4fab-9dd0-7de14a495ae5" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-816ebb46-7ba6-4037-ba08-de93eaf380aa 01/03/23 02:54:13.18
    STEP: waiting to observe update in volume 01/03/23 02:54:13.182
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan  3 02:54:15.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1175" for this suite. 01/03/23 02:54:15.197
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:54:15.228
Jan  3 02:54:15.228: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename webhook 01/03/23 02:54:15.229
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:54:15.245
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:54:15.247
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/03/23 02:54:15.257
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/23 02:54:15.65
STEP: Deploying the webhook pod 01/03/23 02:54:15.655
STEP: Wait for the deployment to be ready 01/03/23 02:54:15.703
Jan  3 02:54:15.706: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/03/23 02:54:17.713
STEP: Verifying the service has paired with the endpoint 01/03/23 02:54:17.729
Jan  3 02:54:18.729: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
STEP: Creating a validating webhook configuration 01/03/23 02:54:18.731
STEP: Creating a configMap that does not comply to the validation webhook rules 01/03/23 02:54:18.741
STEP: Updating a validating webhook configuration's rules to not include the create operation 01/03/23 02:54:18.747
STEP: Creating a configMap that does not comply to the validation webhook rules 01/03/23 02:54:18.762
STEP: Patching a validating webhook configuration's rules to include the create operation 01/03/23 02:54:18.801
STEP: Creating a configMap that does not comply to the validation webhook rules 01/03/23 02:54:18.805
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  3 02:54:18.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4895" for this suite. 01/03/23 02:54:18.829
STEP: Destroying namespace "webhook-4895-markers" for this suite. 01/03/23 02:54:18.834
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","completed":225,"skipped":4018,"failed":0}
------------------------------
• [3.709 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:54:15.228
    Jan  3 02:54:15.228: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename webhook 01/03/23 02:54:15.229
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:54:15.245
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:54:15.247
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/03/23 02:54:15.257
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/23 02:54:15.65
    STEP: Deploying the webhook pod 01/03/23 02:54:15.655
    STEP: Wait for the deployment to be ready 01/03/23 02:54:15.703
    Jan  3 02:54:15.706: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/03/23 02:54:17.713
    STEP: Verifying the service has paired with the endpoint 01/03/23 02:54:17.729
    Jan  3 02:54:18.729: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:412
    STEP: Creating a validating webhook configuration 01/03/23 02:54:18.731
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/03/23 02:54:18.741
    STEP: Updating a validating webhook configuration's rules to not include the create operation 01/03/23 02:54:18.747
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/03/23 02:54:18.762
    STEP: Patching a validating webhook configuration's rules to include the create operation 01/03/23 02:54:18.801
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/03/23 02:54:18.805
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  3 02:54:18.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4895" for this suite. 01/03/23 02:54:18.829
    STEP: Destroying namespace "webhook-4895-markers" for this suite. 01/03/23 02:54:18.834
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:54:18.937
Jan  3 02:54:18.937: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename job 01/03/23 02:54:18.938
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:54:18.954
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:54:18.956
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
STEP: Creating a job 01/03/23 02:54:18.958
STEP: Ensuring active pods == parallelism 01/03/23 02:54:18.969
STEP: Orphaning one of the Job's Pods 01/03/23 02:54:20.972
Jan  3 02:54:21.481: INFO: Successfully updated pod "adopt-release-4lqqj"
STEP: Checking that the Job readopts the Pod 01/03/23 02:54:21.481
Jan  3 02:54:21.481: INFO: Waiting up to 15m0s for pod "adopt-release-4lqqj" in namespace "job-5812" to be "adopted"
Jan  3 02:54:21.493: INFO: Pod "adopt-release-4lqqj": Phase="Running", Reason="", readiness=true. Elapsed: 11.660944ms
Jan  3 02:54:23.496: INFO: Pod "adopt-release-4lqqj": Phase="Running", Reason="", readiness=true. Elapsed: 2.014976979s
Jan  3 02:54:23.496: INFO: Pod "adopt-release-4lqqj" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 01/03/23 02:54:23.496
Jan  3 02:54:24.006: INFO: Successfully updated pod "adopt-release-4lqqj"
STEP: Checking that the Job releases the Pod 01/03/23 02:54:24.006
Jan  3 02:54:24.006: INFO: Waiting up to 15m0s for pod "adopt-release-4lqqj" in namespace "job-5812" to be "released"
Jan  3 02:54:24.020: INFO: Pod "adopt-release-4lqqj": Phase="Running", Reason="", readiness=true. Elapsed: 14.263308ms
Jan  3 02:54:26.022: INFO: Pod "adopt-release-4lqqj": Phase="Running", Reason="", readiness=true. Elapsed: 2.016236385s
Jan  3 02:54:26.022: INFO: Pod "adopt-release-4lqqj" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan  3 02:54:26.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5812" for this suite. 01/03/23 02:54:26.025
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","completed":226,"skipped":4018,"failed":0}
------------------------------
• [SLOW TEST] [7.090 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:54:18.937
    Jan  3 02:54:18.937: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename job 01/03/23 02:54:18.938
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:54:18.954
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:54:18.956
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:335
    STEP: Creating a job 01/03/23 02:54:18.958
    STEP: Ensuring active pods == parallelism 01/03/23 02:54:18.969
    STEP: Orphaning one of the Job's Pods 01/03/23 02:54:20.972
    Jan  3 02:54:21.481: INFO: Successfully updated pod "adopt-release-4lqqj"
    STEP: Checking that the Job readopts the Pod 01/03/23 02:54:21.481
    Jan  3 02:54:21.481: INFO: Waiting up to 15m0s for pod "adopt-release-4lqqj" in namespace "job-5812" to be "adopted"
    Jan  3 02:54:21.493: INFO: Pod "adopt-release-4lqqj": Phase="Running", Reason="", readiness=true. Elapsed: 11.660944ms
    Jan  3 02:54:23.496: INFO: Pod "adopt-release-4lqqj": Phase="Running", Reason="", readiness=true. Elapsed: 2.014976979s
    Jan  3 02:54:23.496: INFO: Pod "adopt-release-4lqqj" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 01/03/23 02:54:23.496
    Jan  3 02:54:24.006: INFO: Successfully updated pod "adopt-release-4lqqj"
    STEP: Checking that the Job releases the Pod 01/03/23 02:54:24.006
    Jan  3 02:54:24.006: INFO: Waiting up to 15m0s for pod "adopt-release-4lqqj" in namespace "job-5812" to be "released"
    Jan  3 02:54:24.020: INFO: Pod "adopt-release-4lqqj": Phase="Running", Reason="", readiness=true. Elapsed: 14.263308ms
    Jan  3 02:54:26.022: INFO: Pod "adopt-release-4lqqj": Phase="Running", Reason="", readiness=true. Elapsed: 2.016236385s
    Jan  3 02:54:26.022: INFO: Pod "adopt-release-4lqqj" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan  3 02:54:26.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-5812" for this suite. 01/03/23 02:54:26.025
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:54:26.028
Jan  3 02:54:26.028: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename webhook 01/03/23 02:54:26.028
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:54:26.046
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:54:26.048
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/03/23 02:54:26.086
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/23 02:54:26.771
STEP: Deploying the webhook pod 01/03/23 02:54:26.775
STEP: Wait for the deployment to be ready 01/03/23 02:54:26.799
Jan  3 02:54:26.851: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/03/23 02:54:28.857
STEP: Verifying the service has paired with the endpoint 01/03/23 02:54:28.876
Jan  3 02:54:29.876: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
Jan  3 02:54:29.878: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4933-crds.webhook.example.com via the AdmissionRegistration API 01/03/23 02:54:30.391
STEP: Creating a custom resource while v1 is storage version 01/03/23 02:54:30.401
STEP: Patching Custom Resource Definition to set v2 as storage 01/03/23 02:54:32.448
STEP: Patching the custom resource while v2 is storage version 01/03/23 02:54:32.461
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  3 02:54:33.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5008" for this suite. 01/03/23 02:54:33.078
STEP: Destroying namespace "webhook-5008-markers" for this suite. 01/03/23 02:54:33.114
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","completed":227,"skipped":4018,"failed":0}
------------------------------
• [SLOW TEST] [7.232 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:54:26.028
    Jan  3 02:54:26.028: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename webhook 01/03/23 02:54:26.028
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:54:26.046
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:54:26.048
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/03/23 02:54:26.086
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/23 02:54:26.771
    STEP: Deploying the webhook pod 01/03/23 02:54:26.775
    STEP: Wait for the deployment to be ready 01/03/23 02:54:26.799
    Jan  3 02:54:26.851: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/03/23 02:54:28.857
    STEP: Verifying the service has paired with the endpoint 01/03/23 02:54:28.876
    Jan  3 02:54:29.876: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:322
    Jan  3 02:54:29.878: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4933-crds.webhook.example.com via the AdmissionRegistration API 01/03/23 02:54:30.391
    STEP: Creating a custom resource while v1 is storage version 01/03/23 02:54:30.401
    STEP: Patching Custom Resource Definition to set v2 as storage 01/03/23 02:54:32.448
    STEP: Patching the custom resource while v2 is storage version 01/03/23 02:54:32.461
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  3 02:54:33.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5008" for this suite. 01/03/23 02:54:33.078
    STEP: Destroying namespace "webhook-5008-markers" for this suite. 01/03/23 02:54:33.114
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:54:33.26
Jan  3 02:54:33.260: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename configmap 01/03/23 02:54:33.261
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:54:33.308
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:54:33.31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
STEP: Creating configMap with name configmap-test-upd-13899890-0792-4c6a-93f4-6cd205265970 01/03/23 02:54:33.316
STEP: Creating the pod 01/03/23 02:54:33.335
Jan  3 02:54:33.342: INFO: Waiting up to 5m0s for pod "pod-configmaps-723a16e7-bf1b-4de1-9bc3-f141608658e3" in namespace "configmap-7739" to be "running and ready"
Jan  3 02:54:33.351: INFO: Pod "pod-configmaps-723a16e7-bf1b-4de1-9bc3-f141608658e3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.313396ms
Jan  3 02:54:33.351: INFO: The phase of Pod pod-configmaps-723a16e7-bf1b-4de1-9bc3-f141608658e3 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 02:54:35.359: INFO: Pod "pod-configmaps-723a16e7-bf1b-4de1-9bc3-f141608658e3": Phase="Running", Reason="", readiness=true. Elapsed: 2.016559117s
Jan  3 02:54:35.359: INFO: The phase of Pod pod-configmaps-723a16e7-bf1b-4de1-9bc3-f141608658e3 is Running (Ready = true)
Jan  3 02:54:35.359: INFO: Pod "pod-configmaps-723a16e7-bf1b-4de1-9bc3-f141608658e3" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-13899890-0792-4c6a-93f4-6cd205265970 01/03/23 02:54:35.364
STEP: waiting to observe update in volume 01/03/23 02:54:35.366
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan  3 02:54:37.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7739" for this suite. 01/03/23 02:54:37.383
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":228,"skipped":4028,"failed":0}
------------------------------
• [4.126 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:54:33.26
    Jan  3 02:54:33.260: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename configmap 01/03/23 02:54:33.261
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:54:33.308
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:54:33.31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:123
    STEP: Creating configMap with name configmap-test-upd-13899890-0792-4c6a-93f4-6cd205265970 01/03/23 02:54:33.316
    STEP: Creating the pod 01/03/23 02:54:33.335
    Jan  3 02:54:33.342: INFO: Waiting up to 5m0s for pod "pod-configmaps-723a16e7-bf1b-4de1-9bc3-f141608658e3" in namespace "configmap-7739" to be "running and ready"
    Jan  3 02:54:33.351: INFO: Pod "pod-configmaps-723a16e7-bf1b-4de1-9bc3-f141608658e3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.313396ms
    Jan  3 02:54:33.351: INFO: The phase of Pod pod-configmaps-723a16e7-bf1b-4de1-9bc3-f141608658e3 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 02:54:35.359: INFO: Pod "pod-configmaps-723a16e7-bf1b-4de1-9bc3-f141608658e3": Phase="Running", Reason="", readiness=true. Elapsed: 2.016559117s
    Jan  3 02:54:35.359: INFO: The phase of Pod pod-configmaps-723a16e7-bf1b-4de1-9bc3-f141608658e3 is Running (Ready = true)
    Jan  3 02:54:35.359: INFO: Pod "pod-configmaps-723a16e7-bf1b-4de1-9bc3-f141608658e3" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-13899890-0792-4c6a-93f4-6cd205265970 01/03/23 02:54:35.364
    STEP: waiting to observe update in volume 01/03/23 02:54:35.366
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan  3 02:54:37.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7739" for this suite. 01/03/23 02:54:37.383
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:54:37.386
Jan  3 02:54:37.386: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename sched-preemption 01/03/23 02:54:37.386
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:54:37.406
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:54:37.408
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan  3 02:54:37.425: INFO: Waiting up to 1m0s for all nodes to be ready
Jan  3 02:55:37.476: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
STEP: Create pods that use 4/5 of node resources. 01/03/23 02:55:37.478
Jan  3 02:55:37.520: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan  3 02:55:37.526: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan  3 02:55:37.549: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan  3 02:55:37.566: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Jan  3 02:55:37.583: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Jan  3 02:55:37.601: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 01/03/23 02:55:37.601
Jan  3 02:55:37.601: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-2973" to be "running"
Jan  3 02:55:37.606: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 5.507ms
Jan  3 02:55:39.609: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008324866s
Jan  3 02:55:41.609: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008313772s
Jan  3 02:55:43.610: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.009536548s
Jan  3 02:55:43.610: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jan  3 02:55:43.610: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-2973" to be "running"
Jan  3 02:55:43.612: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.988814ms
Jan  3 02:55:43.612: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jan  3 02:55:43.612: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-2973" to be "running"
Jan  3 02:55:43.614: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 1.74768ms
Jan  3 02:55:45.617: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004323918s
Jan  3 02:55:47.618: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005292978s
Jan  3 02:55:49.618: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.005388506s
Jan  3 02:55:49.618: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jan  3 02:55:49.618: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-2973" to be "running"
Jan  3 02:55:49.619: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.446258ms
Jan  3 02:55:49.619: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Jan  3 02:55:49.619: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-2973" to be "running"
Jan  3 02:55:49.621: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.589434ms
Jan  3 02:55:49.621: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Jan  3 02:55:49.621: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-2973" to be "running"
Jan  3 02:55:49.622: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.517099ms
Jan  3 02:55:49.622: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 01/03/23 02:55:49.622
Jan  3 02:55:49.627: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-2973" to be "running"
Jan  3 02:55:49.639: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.887898ms
Jan  3 02:55:51.683: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056751083s
Jan  3 02:55:53.642: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.015743647s
Jan  3 02:55:53.642: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jan  3 02:55:53.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-2973" for this suite. 01/03/23 02:55:53.656
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","completed":229,"skipped":4030,"failed":0}
------------------------------
• [SLOW TEST] [76.315 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:54:37.386
    Jan  3 02:54:37.386: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename sched-preemption 01/03/23 02:54:37.386
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:54:37.406
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:54:37.408
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jan  3 02:54:37.425: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan  3 02:55:37.476: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:125
    STEP: Create pods that use 4/5 of node resources. 01/03/23 02:55:37.478
    Jan  3 02:55:37.520: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jan  3 02:55:37.526: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jan  3 02:55:37.549: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jan  3 02:55:37.566: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Jan  3 02:55:37.583: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Jan  3 02:55:37.601: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 01/03/23 02:55:37.601
    Jan  3 02:55:37.601: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-2973" to be "running"
    Jan  3 02:55:37.606: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 5.507ms
    Jan  3 02:55:39.609: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008324866s
    Jan  3 02:55:41.609: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008313772s
    Jan  3 02:55:43.610: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.009536548s
    Jan  3 02:55:43.610: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jan  3 02:55:43.610: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-2973" to be "running"
    Jan  3 02:55:43.612: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.988814ms
    Jan  3 02:55:43.612: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan  3 02:55:43.612: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-2973" to be "running"
    Jan  3 02:55:43.614: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 1.74768ms
    Jan  3 02:55:45.617: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004323918s
    Jan  3 02:55:47.618: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005292978s
    Jan  3 02:55:49.618: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.005388506s
    Jan  3 02:55:49.618: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan  3 02:55:49.618: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-2973" to be "running"
    Jan  3 02:55:49.619: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.446258ms
    Jan  3 02:55:49.619: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan  3 02:55:49.619: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-2973" to be "running"
    Jan  3 02:55:49.621: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.589434ms
    Jan  3 02:55:49.621: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan  3 02:55:49.621: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-2973" to be "running"
    Jan  3 02:55:49.622: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.517099ms
    Jan  3 02:55:49.622: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 01/03/23 02:55:49.622
    Jan  3 02:55:49.627: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-2973" to be "running"
    Jan  3 02:55:49.639: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.887898ms
    Jan  3 02:55:51.683: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056751083s
    Jan  3 02:55:53.642: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.015743647s
    Jan  3 02:55:53.642: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jan  3 02:55:53.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-2973" for this suite. 01/03/23 02:55:53.656
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:55:53.702
Jan  3 02:55:53.702: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename downward-api 01/03/23 02:55:53.703
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:55:53.711
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:55:53.712
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
STEP: Creating the pod 01/03/23 02:55:53.714
Jan  3 02:55:53.761: INFO: Waiting up to 5m0s for pod "annotationupdate264e6985-928b-48ec-94c6-5dcb1324e5b6" in namespace "downward-api-350" to be "running and ready"
Jan  3 02:55:53.780: INFO: Pod "annotationupdate264e6985-928b-48ec-94c6-5dcb1324e5b6": Phase="Pending", Reason="", readiness=false. Elapsed: 19.66496ms
Jan  3 02:55:53.780: INFO: The phase of Pod annotationupdate264e6985-928b-48ec-94c6-5dcb1324e5b6 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 02:55:55.783: INFO: Pod "annotationupdate264e6985-928b-48ec-94c6-5dcb1324e5b6": Phase="Running", Reason="", readiness=true. Elapsed: 2.022698693s
Jan  3 02:55:55.783: INFO: The phase of Pod annotationupdate264e6985-928b-48ec-94c6-5dcb1324e5b6 is Running (Ready = true)
Jan  3 02:55:55.783: INFO: Pod "annotationupdate264e6985-928b-48ec-94c6-5dcb1324e5b6" satisfied condition "running and ready"
Jan  3 02:55:56.296: INFO: Successfully updated pod "annotationupdate264e6985-928b-48ec-94c6-5dcb1324e5b6"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan  3 02:56:00.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-350" for this suite. 01/03/23 02:56:00.328
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","completed":230,"skipped":4051,"failed":0}
------------------------------
• [SLOW TEST] [6.628 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:55:53.702
    Jan  3 02:55:53.702: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename downward-api 01/03/23 02:55:53.703
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:55:53.711
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:55:53.712
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:161
    STEP: Creating the pod 01/03/23 02:55:53.714
    Jan  3 02:55:53.761: INFO: Waiting up to 5m0s for pod "annotationupdate264e6985-928b-48ec-94c6-5dcb1324e5b6" in namespace "downward-api-350" to be "running and ready"
    Jan  3 02:55:53.780: INFO: Pod "annotationupdate264e6985-928b-48ec-94c6-5dcb1324e5b6": Phase="Pending", Reason="", readiness=false. Elapsed: 19.66496ms
    Jan  3 02:55:53.780: INFO: The phase of Pod annotationupdate264e6985-928b-48ec-94c6-5dcb1324e5b6 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 02:55:55.783: INFO: Pod "annotationupdate264e6985-928b-48ec-94c6-5dcb1324e5b6": Phase="Running", Reason="", readiness=true. Elapsed: 2.022698693s
    Jan  3 02:55:55.783: INFO: The phase of Pod annotationupdate264e6985-928b-48ec-94c6-5dcb1324e5b6 is Running (Ready = true)
    Jan  3 02:55:55.783: INFO: Pod "annotationupdate264e6985-928b-48ec-94c6-5dcb1324e5b6" satisfied condition "running and ready"
    Jan  3 02:55:56.296: INFO: Successfully updated pod "annotationupdate264e6985-928b-48ec-94c6-5dcb1324e5b6"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan  3 02:56:00.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-350" for this suite. 01/03/23 02:56:00.328
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:56:00.331
Jan  3 02:56:00.331: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename events 01/03/23 02:56:00.331
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:56:00.345
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:56:00.346
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 01/03/23 02:56:00.348
STEP: listing all events in all namespaces 01/03/23 02:56:00.36
STEP: patching the test event 01/03/23 02:56:00.37
STEP: fetching the test event 01/03/23 02:56:00.373
STEP: updating the test event 01/03/23 02:56:00.379
STEP: getting the test event 01/03/23 02:56:00.383
STEP: deleting the test event 01/03/23 02:56:00.385
STEP: listing all events in all namespaces 01/03/23 02:56:00.387
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Jan  3 02:56:00.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5729" for this suite. 01/03/23 02:56:00.396
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-instrumentation] Events should manage the lifecycle of an event [Conformance]","completed":231,"skipped":4068,"failed":0}
------------------------------
• [0.067 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:56:00.331
    Jan  3 02:56:00.331: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename events 01/03/23 02:56:00.331
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:56:00.345
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:56:00.346
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 01/03/23 02:56:00.348
    STEP: listing all events in all namespaces 01/03/23 02:56:00.36
    STEP: patching the test event 01/03/23 02:56:00.37
    STEP: fetching the test event 01/03/23 02:56:00.373
    STEP: updating the test event 01/03/23 02:56:00.379
    STEP: getting the test event 01/03/23 02:56:00.383
    STEP: deleting the test event 01/03/23 02:56:00.385
    STEP: listing all events in all namespaces 01/03/23 02:56:00.387
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Jan  3 02:56:00.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-5729" for this suite. 01/03/23 02:56:00.396
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:56:00.4
Jan  3 02:56:00.401: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename deployment 01/03/23 02:56:00.401
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:56:00.449
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:56:00.45
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Jan  3 02:56:00.452: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jan  3 02:56:00.463: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan  3 02:56:05.466: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/03/23 02:56:05.466
Jan  3 02:56:05.466: INFO: Creating deployment "test-rolling-update-deployment"
Jan  3 02:56:05.470: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jan  3 02:56:05.485: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jan  3 02:56:07.490: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jan  3 02:56:07.497: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan  3 02:56:07.502: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-5733  af3b3ebf-c159-4310-a1e8-415291a7dabb 1004198 1 2023-01-03 02:56:05 +0000 UTC <nil> <nil> map[name:sample-pod] map[createdTime:2023-01-03T11:56:05.468317538+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:3546343826724305833 updatedTime:2023-01-03T11:56:05.468317538+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update apps/v1 2023-01-03 02:56:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-03 02:56:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00357d868 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-03 02:56:05 +0000 UTC,LastTransitionTime:2023-01-03 02:56:05 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-01-03 02:56:06 +0000 UTC,LastTransitionTime:2023-01-03 02:56:05 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan  3 02:56:07.504: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-5733  7d7419f0-ca9b-484c-a431-62df12addc40 1004188 1 2023-01-03 02:56:05 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[createdTime:2023-01-03T11:56:05.468317538+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833 updatedTime:2023-01-03T11:56:05.468317538+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [{apps/v1 Deployment test-rolling-update-deployment af3b3ebf-c159-4310-a1e8-415291a7dabb 0xc002a165c7 0xc002a165c8}] [] [{kube-controller-manager Update apps/v1 2023-01-03 02:56:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:createdTime":{},"f:creator":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{},"f:updatedTime":{},"f:updater":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af3b3ebf-c159-4310-a1e8-415291a7dabb\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-03 02:56:06 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002a16678 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan  3 02:56:07.504: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jan  3 02:56:07.504: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-5733  160ba28b-20f8-42ab-975a-2fd470107c05 1004197 2 2023-01-03 02:56:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[createdTime:2023-01-03T11:56:00.45403118+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832 updatedTime:2023-01-03T11:56:00.45403118+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [{apps/v1 Deployment test-rolling-update-deployment af3b3ebf-c159-4310-a1e8-415291a7dabb 0xc002a16467 0xc002a16468}] [] [{e2e.test Update apps/v1 2023-01-03 02:56:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-03 02:56:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af3b3ebf-c159-4310-a1e8-415291a7dabb\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-03 02:56:06 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002a16528 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan  3 02:56:07.506: INFO: Pod "test-rolling-update-deployment-78f575d8ff-b5kwx" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-b5kwx test-rolling-update-deployment-78f575d8ff- deployment-5733  10b27aa4-0c47-4a73-b243-29fab75205d2 1004187 0 2023-01-03 02:56:05 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cni.projectcalico.org/containerID:801228ddc19369f2cd894ac5b934648b78379617573c22e0b13cd7618216dbcd cni.projectcalico.org/podIP:10.244.246.113/32 cni.projectcalico.org/podIPs:10.244.246.113/32 createdTime:2023-01-03T11:56:05.495481587+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T11:56:05.495481587+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff 7d7419f0-ca9b-484c-a431-62df12addc40 0xc003c199c7 0xc003c199c8}] [] [{kube-controller-manager Update v1 2023-01-03 02:56:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7d7419f0-ca9b-484c-a431-62df12addc40\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-03 02:56:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-03 02:56:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.246.113\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-67nzv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-67nzv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-master,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:56:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:56:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:56:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:56:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.5,PodIP:10.244.246.113,StartTime:2023-01-03 02:56:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-03 02:56:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787,ContainerID:cri-o://2269acc6bb843035e42f67a1fbd72fb7a9713c168264cde1670e35f308440dc1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.246.113,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan  3 02:56:07.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5733" for this suite. 01/03/23 02:56:07.509
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","completed":232,"skipped":4083,"failed":0}
------------------------------
• [SLOW TEST] [7.111 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:56:00.4
    Jan  3 02:56:00.401: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename deployment 01/03/23 02:56:00.401
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:56:00.449
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:56:00.45
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Jan  3 02:56:00.452: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Jan  3 02:56:00.463: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan  3 02:56:05.466: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/03/23 02:56:05.466
    Jan  3 02:56:05.466: INFO: Creating deployment "test-rolling-update-deployment"
    Jan  3 02:56:05.470: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Jan  3 02:56:05.485: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Jan  3 02:56:07.490: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Jan  3 02:56:07.497: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan  3 02:56:07.502: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-5733  af3b3ebf-c159-4310-a1e8-415291a7dabb 1004198 1 2023-01-03 02:56:05 +0000 UTC <nil> <nil> map[name:sample-pod] map[createdTime:2023-01-03T11:56:05.468317538+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:3546343826724305833 updatedTime:2023-01-03T11:56:05.468317538+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update apps/v1 2023-01-03 02:56:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-03 02:56:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00357d868 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-03 02:56:05 +0000 UTC,LastTransitionTime:2023-01-03 02:56:05 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-01-03 02:56:06 +0000 UTC,LastTransitionTime:2023-01-03 02:56:05 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan  3 02:56:07.504: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-5733  7d7419f0-ca9b-484c-a431-62df12addc40 1004188 1 2023-01-03 02:56:05 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[createdTime:2023-01-03T11:56:05.468317538+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833 updatedTime:2023-01-03T11:56:05.468317538+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [{apps/v1 Deployment test-rolling-update-deployment af3b3ebf-c159-4310-a1e8-415291a7dabb 0xc002a165c7 0xc002a165c8}] [] [{kube-controller-manager Update apps/v1 2023-01-03 02:56:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:createdTime":{},"f:creator":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{},"f:updatedTime":{},"f:updater":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af3b3ebf-c159-4310-a1e8-415291a7dabb\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-03 02:56:06 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002a16678 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan  3 02:56:07.504: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Jan  3 02:56:07.504: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-5733  160ba28b-20f8-42ab-975a-2fd470107c05 1004197 2 2023-01-03 02:56:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[createdTime:2023-01-03T11:56:00.45403118+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832 updatedTime:2023-01-03T11:56:00.45403118+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [{apps/v1 Deployment test-rolling-update-deployment af3b3ebf-c159-4310-a1e8-415291a7dabb 0xc002a16467 0xc002a16468}] [] [{e2e.test Update apps/v1 2023-01-03 02:56:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-03 02:56:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af3b3ebf-c159-4310-a1e8-415291a7dabb\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-03 02:56:06 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002a16528 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan  3 02:56:07.506: INFO: Pod "test-rolling-update-deployment-78f575d8ff-b5kwx" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-b5kwx test-rolling-update-deployment-78f575d8ff- deployment-5733  10b27aa4-0c47-4a73-b243-29fab75205d2 1004187 0 2023-01-03 02:56:05 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cni.projectcalico.org/containerID:801228ddc19369f2cd894ac5b934648b78379617573c22e0b13cd7618216dbcd cni.projectcalico.org/podIP:10.244.246.113/32 cni.projectcalico.org/podIPs:10.244.246.113/32 createdTime:2023-01-03T11:56:05.495481587+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T11:56:05.495481587+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff 7d7419f0-ca9b-484c-a431-62df12addc40 0xc003c199c7 0xc003c199c8}] [] [{kube-controller-manager Update v1 2023-01-03 02:56:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7d7419f0-ca9b-484c-a431-62df12addc40\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-03 02:56:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-03 02:56:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.246.113\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-67nzv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-67nzv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-master,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:56:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:56:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:56:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:56:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.5,PodIP:10.244.246.113,StartTime:2023-01-03 02:56:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-03 02:56:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787,ContainerID:cri-o://2269acc6bb843035e42f67a1fbd72fb7a9713c168264cde1670e35f308440dc1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.246.113,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan  3 02:56:07.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-5733" for this suite. 01/03/23 02:56:07.509
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:56:07.512
Jan  3 02:56:07.512: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename projected 01/03/23 02:56:07.513
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:56:07.523
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:56:07.525
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
STEP: Creating projection with secret that has name projected-secret-test-441b7688-e9ba-40db-ae40-f54db0a7ef11 01/03/23 02:56:07.526
STEP: Creating a pod to test consume secrets 01/03/23 02:56:07.537
Jan  3 02:56:07.543: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6b04e68e-1d87-4572-9a96-31b19a77507d" in namespace "projected-2453" to be "Succeeded or Failed"
Jan  3 02:56:07.554: INFO: Pod "pod-projected-secrets-6b04e68e-1d87-4572-9a96-31b19a77507d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.310839ms
Jan  3 02:56:09.557: INFO: Pod "pod-projected-secrets-6b04e68e-1d87-4572-9a96-31b19a77507d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013955483s
Jan  3 02:56:11.569: INFO: Pod "pod-projected-secrets-6b04e68e-1d87-4572-9a96-31b19a77507d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02546165s
STEP: Saw pod success 01/03/23 02:56:11.569
Jan  3 02:56:11.569: INFO: Pod "pod-projected-secrets-6b04e68e-1d87-4572-9a96-31b19a77507d" satisfied condition "Succeeded or Failed"
Jan  3 02:56:11.570: INFO: Trying to get logs from node cncf-master pod pod-projected-secrets-6b04e68e-1d87-4572-9a96-31b19a77507d container projected-secret-volume-test: <nil>
STEP: delete the pod 01/03/23 02:56:11.573
Jan  3 02:56:11.592: INFO: Waiting for pod pod-projected-secrets-6b04e68e-1d87-4572-9a96-31b19a77507d to disappear
Jan  3 02:56:11.598: INFO: Pod pod-projected-secrets-6b04e68e-1d87-4572-9a96-31b19a77507d no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan  3 02:56:11.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2453" for this suite. 01/03/23 02:56:11.601
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":233,"skipped":4083,"failed":0}
------------------------------
• [4.092 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:56:07.512
    Jan  3 02:56:07.512: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename projected 01/03/23 02:56:07.513
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:56:07.523
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:56:07.525
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:66
    STEP: Creating projection with secret that has name projected-secret-test-441b7688-e9ba-40db-ae40-f54db0a7ef11 01/03/23 02:56:07.526
    STEP: Creating a pod to test consume secrets 01/03/23 02:56:07.537
    Jan  3 02:56:07.543: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6b04e68e-1d87-4572-9a96-31b19a77507d" in namespace "projected-2453" to be "Succeeded or Failed"
    Jan  3 02:56:07.554: INFO: Pod "pod-projected-secrets-6b04e68e-1d87-4572-9a96-31b19a77507d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.310839ms
    Jan  3 02:56:09.557: INFO: Pod "pod-projected-secrets-6b04e68e-1d87-4572-9a96-31b19a77507d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013955483s
    Jan  3 02:56:11.569: INFO: Pod "pod-projected-secrets-6b04e68e-1d87-4572-9a96-31b19a77507d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02546165s
    STEP: Saw pod success 01/03/23 02:56:11.569
    Jan  3 02:56:11.569: INFO: Pod "pod-projected-secrets-6b04e68e-1d87-4572-9a96-31b19a77507d" satisfied condition "Succeeded or Failed"
    Jan  3 02:56:11.570: INFO: Trying to get logs from node cncf-master pod pod-projected-secrets-6b04e68e-1d87-4572-9a96-31b19a77507d container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/03/23 02:56:11.573
    Jan  3 02:56:11.592: INFO: Waiting for pod pod-projected-secrets-6b04e68e-1d87-4572-9a96-31b19a77507d to disappear
    Jan  3 02:56:11.598: INFO: Pod pod-projected-secrets-6b04e68e-1d87-4572-9a96-31b19a77507d no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan  3 02:56:11.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2453" for this suite. 01/03/23 02:56:11.601
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:56:11.606
Jan  3 02:56:11.606: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename configmap 01/03/23 02:56:11.607
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:56:11.625
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:56:11.627
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
STEP: Creating configMap with name configmap-test-volume-2d655030-8d20-4a71-a5a2-132257f24f14 01/03/23 02:56:11.629
STEP: Creating a pod to test consume configMaps 01/03/23 02:56:11.632
Jan  3 02:56:11.651: INFO: Waiting up to 5m0s for pod "pod-configmaps-910713da-3c93-4cbe-99a3-181ddccf3e93" in namespace "configmap-6106" to be "Succeeded or Failed"
Jan  3 02:56:11.655: INFO: Pod "pod-configmaps-910713da-3c93-4cbe-99a3-181ddccf3e93": Phase="Pending", Reason="", readiness=false. Elapsed: 3.679259ms
Jan  3 02:56:13.658: INFO: Pod "pod-configmaps-910713da-3c93-4cbe-99a3-181ddccf3e93": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006782247s
Jan  3 02:56:15.658: INFO: Pod "pod-configmaps-910713da-3c93-4cbe-99a3-181ddccf3e93": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0067925s
Jan  3 02:56:17.657: INFO: Pod "pod-configmaps-910713da-3c93-4cbe-99a3-181ddccf3e93": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00605678s
STEP: Saw pod success 01/03/23 02:56:17.657
Jan  3 02:56:17.657: INFO: Pod "pod-configmaps-910713da-3c93-4cbe-99a3-181ddccf3e93" satisfied condition "Succeeded or Failed"
Jan  3 02:56:17.659: INFO: Trying to get logs from node cncf-master pod pod-configmaps-910713da-3c93-4cbe-99a3-181ddccf3e93 container configmap-volume-test: <nil>
STEP: delete the pod 01/03/23 02:56:17.663
Jan  3 02:56:17.713: INFO: Waiting for pod pod-configmaps-910713da-3c93-4cbe-99a3-181ddccf3e93 to disappear
Jan  3 02:56:17.718: INFO: Pod pod-configmaps-910713da-3c93-4cbe-99a3-181ddccf3e93 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan  3 02:56:17.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6106" for this suite. 01/03/23 02:56:17.721
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":234,"skipped":4120,"failed":0}
------------------------------
• [SLOW TEST] [6.118 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:56:11.606
    Jan  3 02:56:11.606: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename configmap 01/03/23 02:56:11.607
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:56:11.625
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:56:11.627
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:422
    STEP: Creating configMap with name configmap-test-volume-2d655030-8d20-4a71-a5a2-132257f24f14 01/03/23 02:56:11.629
    STEP: Creating a pod to test consume configMaps 01/03/23 02:56:11.632
    Jan  3 02:56:11.651: INFO: Waiting up to 5m0s for pod "pod-configmaps-910713da-3c93-4cbe-99a3-181ddccf3e93" in namespace "configmap-6106" to be "Succeeded or Failed"
    Jan  3 02:56:11.655: INFO: Pod "pod-configmaps-910713da-3c93-4cbe-99a3-181ddccf3e93": Phase="Pending", Reason="", readiness=false. Elapsed: 3.679259ms
    Jan  3 02:56:13.658: INFO: Pod "pod-configmaps-910713da-3c93-4cbe-99a3-181ddccf3e93": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006782247s
    Jan  3 02:56:15.658: INFO: Pod "pod-configmaps-910713da-3c93-4cbe-99a3-181ddccf3e93": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0067925s
    Jan  3 02:56:17.657: INFO: Pod "pod-configmaps-910713da-3c93-4cbe-99a3-181ddccf3e93": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00605678s
    STEP: Saw pod success 01/03/23 02:56:17.657
    Jan  3 02:56:17.657: INFO: Pod "pod-configmaps-910713da-3c93-4cbe-99a3-181ddccf3e93" satisfied condition "Succeeded or Failed"
    Jan  3 02:56:17.659: INFO: Trying to get logs from node cncf-master pod pod-configmaps-910713da-3c93-4cbe-99a3-181ddccf3e93 container configmap-volume-test: <nil>
    STEP: delete the pod 01/03/23 02:56:17.663
    Jan  3 02:56:17.713: INFO: Waiting for pod pod-configmaps-910713da-3c93-4cbe-99a3-181ddccf3e93 to disappear
    Jan  3 02:56:17.718: INFO: Pod pod-configmaps-910713da-3c93-4cbe-99a3-181ddccf3e93 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan  3 02:56:17.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6106" for this suite. 01/03/23 02:56:17.721
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:56:17.727
Jan  3 02:56:17.727: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename services 01/03/23 02:56:17.727
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:56:17.738
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:56:17.74
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
STEP: creating service endpoint-test2 in namespace services-9399 01/03/23 02:56:17.742
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9399 to expose endpoints map[] 01/03/23 02:56:17.755
Jan  3 02:56:17.758: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Jan  3 02:56:18.763: INFO: successfully validated that service endpoint-test2 in namespace services-9399 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-9399 01/03/23 02:56:18.763
Jan  3 02:56:18.769: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-9399" to be "running and ready"
Jan  3 02:56:18.772: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.214004ms
Jan  3 02:56:18.772: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 02:56:20.775: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006164947s
Jan  3 02:56:20.775: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan  3 02:56:20.775: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9399 to expose endpoints map[pod1:[80]] 01/03/23 02:56:20.777
Jan  3 02:56:20.782: INFO: successfully validated that service endpoint-test2 in namespace services-9399 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 01/03/23 02:56:20.782
Jan  3 02:56:20.782: INFO: Creating new exec pod
Jan  3 02:56:20.786: INFO: Waiting up to 5m0s for pod "execpodbqxvn" in namespace "services-9399" to be "running"
Jan  3 02:56:20.799: INFO: Pod "execpodbqxvn": Phase="Pending", Reason="", readiness=false. Elapsed: 12.555411ms
Jan  3 02:56:22.805: INFO: Pod "execpodbqxvn": Phase="Running", Reason="", readiness=true. Elapsed: 2.018684057s
Jan  3 02:56:22.805: INFO: Pod "execpodbqxvn" satisfied condition "running"
Jan  3 02:56:23.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-9399 exec execpodbqxvn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan  3 02:56:23.937: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan  3 02:56:23.937: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  3 02:56:23.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-9399 exec execpodbqxvn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.232.79 80'
Jan  3 02:56:24.063: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.232.79 80\nConnection to 10.96.232.79 80 port [tcp/http] succeeded!\n"
Jan  3 02:56:24.063: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-9399 01/03/23 02:56:24.063
Jan  3 02:56:24.068: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-9399" to be "running and ready"
Jan  3 02:56:24.080: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 11.681705ms
Jan  3 02:56:24.080: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 02:56:26.083: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.014284451s
Jan  3 02:56:26.083: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan  3 02:56:26.083: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9399 to expose endpoints map[pod1:[80] pod2:[80]] 01/03/23 02:56:26.084
Jan  3 02:56:26.091: INFO: successfully validated that service endpoint-test2 in namespace services-9399 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 01/03/23 02:56:26.091
Jan  3 02:56:27.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-9399 exec execpodbqxvn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan  3 02:56:27.228: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan  3 02:56:27.228: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  3 02:56:27.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-9399 exec execpodbqxvn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.232.79 80'
Jan  3 02:56:27.371: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.232.79 80\nConnection to 10.96.232.79 80 port [tcp/http] succeeded!\n"
Jan  3 02:56:27.371: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-9399 01/03/23 02:56:27.371
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9399 to expose endpoints map[pod2:[80]] 01/03/23 02:56:27.4
Jan  3 02:56:27.412: INFO: successfully validated that service endpoint-test2 in namespace services-9399 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 01/03/23 02:56:27.413
Jan  3 02:56:28.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-9399 exec execpodbqxvn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan  3 02:56:28.560: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan  3 02:56:28.560: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  3 02:56:28.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-9399 exec execpodbqxvn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.232.79 80'
Jan  3 02:56:28.704: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.232.79 80\nConnection to 10.96.232.79 80 port [tcp/http] succeeded!\n"
Jan  3 02:56:28.704: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-9399 01/03/23 02:56:28.704
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9399 to expose endpoints map[] 01/03/23 02:56:28.727
Jan  3 02:56:28.736: INFO: successfully validated that service endpoint-test2 in namespace services-9399 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  3 02:56:28.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9399" for this suite. 01/03/23 02:56:28.785
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","completed":235,"skipped":4166,"failed":0}
------------------------------
• [SLOW TEST] [11.065 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:56:17.727
    Jan  3 02:56:17.727: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename services 01/03/23 02:56:17.727
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:56:17.738
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:56:17.74
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:791
    STEP: creating service endpoint-test2 in namespace services-9399 01/03/23 02:56:17.742
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9399 to expose endpoints map[] 01/03/23 02:56:17.755
    Jan  3 02:56:17.758: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
    Jan  3 02:56:18.763: INFO: successfully validated that service endpoint-test2 in namespace services-9399 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-9399 01/03/23 02:56:18.763
    Jan  3 02:56:18.769: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-9399" to be "running and ready"
    Jan  3 02:56:18.772: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.214004ms
    Jan  3 02:56:18.772: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 02:56:20.775: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006164947s
    Jan  3 02:56:20.775: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan  3 02:56:20.775: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9399 to expose endpoints map[pod1:[80]] 01/03/23 02:56:20.777
    Jan  3 02:56:20.782: INFO: successfully validated that service endpoint-test2 in namespace services-9399 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 01/03/23 02:56:20.782
    Jan  3 02:56:20.782: INFO: Creating new exec pod
    Jan  3 02:56:20.786: INFO: Waiting up to 5m0s for pod "execpodbqxvn" in namespace "services-9399" to be "running"
    Jan  3 02:56:20.799: INFO: Pod "execpodbqxvn": Phase="Pending", Reason="", readiness=false. Elapsed: 12.555411ms
    Jan  3 02:56:22.805: INFO: Pod "execpodbqxvn": Phase="Running", Reason="", readiness=true. Elapsed: 2.018684057s
    Jan  3 02:56:22.805: INFO: Pod "execpodbqxvn" satisfied condition "running"
    Jan  3 02:56:23.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-9399 exec execpodbqxvn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Jan  3 02:56:23.937: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan  3 02:56:23.937: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  3 02:56:23.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-9399 exec execpodbqxvn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.232.79 80'
    Jan  3 02:56:24.063: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.232.79 80\nConnection to 10.96.232.79 80 port [tcp/http] succeeded!\n"
    Jan  3 02:56:24.063: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Creating pod pod2 in namespace services-9399 01/03/23 02:56:24.063
    Jan  3 02:56:24.068: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-9399" to be "running and ready"
    Jan  3 02:56:24.080: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 11.681705ms
    Jan  3 02:56:24.080: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 02:56:26.083: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.014284451s
    Jan  3 02:56:26.083: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan  3 02:56:26.083: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9399 to expose endpoints map[pod1:[80] pod2:[80]] 01/03/23 02:56:26.084
    Jan  3 02:56:26.091: INFO: successfully validated that service endpoint-test2 in namespace services-9399 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 01/03/23 02:56:26.091
    Jan  3 02:56:27.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-9399 exec execpodbqxvn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Jan  3 02:56:27.228: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan  3 02:56:27.228: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  3 02:56:27.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-9399 exec execpodbqxvn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.232.79 80'
    Jan  3 02:56:27.371: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.232.79 80\nConnection to 10.96.232.79 80 port [tcp/http] succeeded!\n"
    Jan  3 02:56:27.371: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-9399 01/03/23 02:56:27.371
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9399 to expose endpoints map[pod2:[80]] 01/03/23 02:56:27.4
    Jan  3 02:56:27.412: INFO: successfully validated that service endpoint-test2 in namespace services-9399 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 01/03/23 02:56:27.413
    Jan  3 02:56:28.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-9399 exec execpodbqxvn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Jan  3 02:56:28.560: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan  3 02:56:28.560: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  3 02:56:28.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-9399 exec execpodbqxvn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.232.79 80'
    Jan  3 02:56:28.704: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.232.79 80\nConnection to 10.96.232.79 80 port [tcp/http] succeeded!\n"
    Jan  3 02:56:28.704: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod2 in namespace services-9399 01/03/23 02:56:28.704
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9399 to expose endpoints map[] 01/03/23 02:56:28.727
    Jan  3 02:56:28.736: INFO: successfully validated that service endpoint-test2 in namespace services-9399 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  3 02:56:28.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9399" for this suite. 01/03/23 02:56:28.785
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:56:28.792
Jan  3 02:56:28.792: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename webhook 01/03/23 02:56:28.793
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:56:28.828
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:56:28.829
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/03/23 02:56:28.857
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/23 02:56:29.137
STEP: Deploying the webhook pod 01/03/23 02:56:29.142
STEP: Wait for the deployment to be ready 01/03/23 02:56:29.185
Jan  3 02:56:29.200: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/03/23 02:56:31.205
STEP: Verifying the service has paired with the endpoint 01/03/23 02:56:31.26
Jan  3 02:56:32.260: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
STEP: fetching the /apis discovery document 01/03/23 02:56:32.262
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 01/03/23 02:56:32.263
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 01/03/23 02:56:32.263
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 01/03/23 02:56:32.263
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 01/03/23 02:56:32.264
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 01/03/23 02:56:32.264
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 01/03/23 02:56:32.265
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  3 02:56:32.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6887" for this suite. 01/03/23 02:56:32.267
STEP: Destroying namespace "webhook-6887-markers" for this suite. 01/03/23 02:56:32.27
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","completed":236,"skipped":4176,"failed":0}
------------------------------
• [3.563 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:56:28.792
    Jan  3 02:56:28.792: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename webhook 01/03/23 02:56:28.793
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:56:28.828
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:56:28.829
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/03/23 02:56:28.857
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/23 02:56:29.137
    STEP: Deploying the webhook pod 01/03/23 02:56:29.142
    STEP: Wait for the deployment to be ready 01/03/23 02:56:29.185
    Jan  3 02:56:29.200: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/03/23 02:56:31.205
    STEP: Verifying the service has paired with the endpoint 01/03/23 02:56:31.26
    Jan  3 02:56:32.260: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:116
    STEP: fetching the /apis discovery document 01/03/23 02:56:32.262
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 01/03/23 02:56:32.263
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 01/03/23 02:56:32.263
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 01/03/23 02:56:32.263
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 01/03/23 02:56:32.264
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 01/03/23 02:56:32.264
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 01/03/23 02:56:32.265
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  3 02:56:32.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6887" for this suite. 01/03/23 02:56:32.267
    STEP: Destroying namespace "webhook-6887-markers" for this suite. 01/03/23 02:56:32.27
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:56:32.355
Jan  3 02:56:32.356: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename deployment 01/03/23 02:56:32.356
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:56:32.389
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:56:32.391
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Jan  3 02:56:32.393: INFO: Creating simple deployment test-new-deployment
Jan  3 02:56:32.421: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource 01/03/23 02:56:34.452
STEP: updating a scale subresource 01/03/23 02:56:34.454
STEP: verifying the deployment Spec.Replicas was modified 01/03/23 02:56:34.457
STEP: Patch a scale subresource 01/03/23 02:56:34.469
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan  3 02:56:34.520: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-3686  e6f1c551-4897-4011-a5f2-b1d3a16065fc 1004574 3 2023-01-03 02:56:32 +0000 UTC <nil> <nil> map[name:httpd] map[createdTime:2023-01-03T11:56:32.394549352+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:56:32.394549352+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-01-03 02:56:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-03 02:56:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007df90a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-03 02:56:33 +0000 UTC,LastTransitionTime:2023-01-03 02:56:33 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-01-03 02:56:33 +0000 UTC,LastTransitionTime:2023-01-03 02:56:32 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan  3 02:56:34.543: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-3686  ccb48146-e287-4462-ac21-2d371ea44a2e 1004582 3 2023-01-03 02:56:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[createdTime:2023-01-03T11:56:32.394549352+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:56:32.394549352+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [{apps/v1 Deployment test-new-deployment e6f1c551-4897-4011-a5f2-b1d3a16065fc 0xc0078f4ec7 0xc0078f4ec8}] [] [{kube-controller-manager Update apps/v1 2023-01-03 02:56:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:createdTime":{},"f:creator":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{},"f:updatedTime":{},"f:updater":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e6f1c551-4897-4011-a5f2-b1d3a16065fc\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-03 02:56:34 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0078f4f58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan  3 02:56:34.592: INFO: Pod "test-new-deployment-845c8977d9-mqrfj" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-mqrfj test-new-deployment-845c8977d9- deployment-3686  6ed70870-0f91-4ad3-acbd-d5272cc9b8d5 1004585 0 2023-01-03 02:56:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[createdTime:2023-01-03T11:56:34.470490358+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T11:56:34.470490358+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 ccb48146-e287-4462-ac21-2d371ea44a2e 0xc00405ef37 0xc00405ef38}] [] [{kube-controller-manager Update v1 2023-01-03 02:56:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ccb48146-e287-4462-ac21-2d371ea44a2e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-03 02:56:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nr55x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nr55x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:56:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:56:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:56:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:56:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.7,PodIP:,StartTime:2023-01-03 02:56:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 02:56:34.592: INFO: Pod "test-new-deployment-845c8977d9-qbssb" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-qbssb test-new-deployment-845c8977d9- deployment-3686  1841038e-170a-4f16-8950-4f3e59c9bbae 1004586 0 2023-01-03 02:56:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[createdTime:2023-01-03T11:56:34.533570377+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T11:56:34.533570377+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 ccb48146-e287-4462-ac21-2d371ea44a2e 0xc00405f167 0xc00405f168}] [] [{kube-controller-manager Update v1 2023-01-03 02:56:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ccb48146-e287-4462-ac21-2d371ea44a2e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jb2pg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jb2pg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  3 02:56:34.592: INFO: Pod "test-new-deployment-845c8977d9-wq2fr" is available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-wq2fr test-new-deployment-845c8977d9- deployment-3686  bdfebb03-1f72-423e-9a09-829e4f79d66d 1004541 0 2023-01-03 02:56:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:845c9d48884f8b3cf7f7fc8ad534b555cb688b9b7a52daa284f67d23ef41ed8f cni.projectcalico.org/podIP:10.244.246.118/32 cni.projectcalico.org/podIPs:10.244.246.118/32 createdTime:2023-01-03T11:56:32.438849619+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T11:56:32.438849619+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 ccb48146-e287-4462-ac21-2d371ea44a2e 0xc00405f2ee 0xc00405f2ef}] [] [{calico Update v1 2023-01-03 02:56:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-03 02:56:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ccb48146-e287-4462-ac21-2d371ea44a2e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-03 02:56:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.246.118\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q5jsq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q5jsq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-master,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:56:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:56:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:56:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:56:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.5,PodIP:10.244.246.118,StartTime:2023-01-03 02:56:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-03 02:56:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://28c4d26f98353af433cfda4008a263931fc53749b53ea23eabe0bfe6c84bf748,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.246.118,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan  3 02:56:34.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3686" for this suite. 01/03/23 02:56:34.613
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","completed":237,"skipped":4177,"failed":0}
------------------------------
• [2.291 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:56:32.355
    Jan  3 02:56:32.356: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename deployment 01/03/23 02:56:32.356
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:56:32.389
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:56:32.391
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Jan  3 02:56:32.393: INFO: Creating simple deployment test-new-deployment
    Jan  3 02:56:32.421: INFO: deployment "test-new-deployment" doesn't have the required revision set
    STEP: getting scale subresource 01/03/23 02:56:34.452
    STEP: updating a scale subresource 01/03/23 02:56:34.454
    STEP: verifying the deployment Spec.Replicas was modified 01/03/23 02:56:34.457
    STEP: Patch a scale subresource 01/03/23 02:56:34.469
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan  3 02:56:34.520: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-3686  e6f1c551-4897-4011-a5f2-b1d3a16065fc 1004574 3 2023-01-03 02:56:32 +0000 UTC <nil> <nil> map[name:httpd] map[createdTime:2023-01-03T11:56:32.394549352+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:56:32.394549352+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-01-03 02:56:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-03 02:56:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007df90a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-03 02:56:33 +0000 UTC,LastTransitionTime:2023-01-03 02:56:33 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-01-03 02:56:33 +0000 UTC,LastTransitionTime:2023-01-03 02:56:32 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan  3 02:56:34.543: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-3686  ccb48146-e287-4462-ac21-2d371ea44a2e 1004582 3 2023-01-03 02:56:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[createdTime:2023-01-03T11:56:32.394549352+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:56:32.394549352+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [{apps/v1 Deployment test-new-deployment e6f1c551-4897-4011-a5f2-b1d3a16065fc 0xc0078f4ec7 0xc0078f4ec8}] [] [{kube-controller-manager Update apps/v1 2023-01-03 02:56:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:createdTime":{},"f:creator":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{},"f:updatedTime":{},"f:updater":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e6f1c551-4897-4011-a5f2-b1d3a16065fc\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-03 02:56:34 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0078f4f58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan  3 02:56:34.592: INFO: Pod "test-new-deployment-845c8977d9-mqrfj" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-mqrfj test-new-deployment-845c8977d9- deployment-3686  6ed70870-0f91-4ad3-acbd-d5272cc9b8d5 1004585 0 2023-01-03 02:56:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[createdTime:2023-01-03T11:56:34.470490358+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T11:56:34.470490358+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 ccb48146-e287-4462-ac21-2d371ea44a2e 0xc00405ef37 0xc00405ef38}] [] [{kube-controller-manager Update v1 2023-01-03 02:56:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ccb48146-e287-4462-ac21-2d371ea44a2e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-03 02:56:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nr55x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nr55x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:56:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:56:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:56:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:56:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.7,PodIP:,StartTime:2023-01-03 02:56:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 02:56:34.592: INFO: Pod "test-new-deployment-845c8977d9-qbssb" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-qbssb test-new-deployment-845c8977d9- deployment-3686  1841038e-170a-4f16-8950-4f3e59c9bbae 1004586 0 2023-01-03 02:56:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[createdTime:2023-01-03T11:56:34.533570377+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T11:56:34.533570377+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 ccb48146-e287-4462-ac21-2d371ea44a2e 0xc00405f167 0xc00405f168}] [] [{kube-controller-manager Update v1 2023-01-03 02:56:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ccb48146-e287-4462-ac21-2d371ea44a2e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jb2pg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jb2pg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  3 02:56:34.592: INFO: Pod "test-new-deployment-845c8977d9-wq2fr" is available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-wq2fr test-new-deployment-845c8977d9- deployment-3686  bdfebb03-1f72-423e-9a09-829e4f79d66d 1004541 0 2023-01-03 02:56:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:845c9d48884f8b3cf7f7fc8ad534b555cb688b9b7a52daa284f67d23ef41ed8f cni.projectcalico.org/podIP:10.244.246.118/32 cni.projectcalico.org/podIPs:10.244.246.118/32 createdTime:2023-01-03T11:56:32.438849619+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T11:56:32.438849619+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 ccb48146-e287-4462-ac21-2d371ea44a2e 0xc00405f2ee 0xc00405f2ef}] [] [{calico Update v1 2023-01-03 02:56:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-03 02:56:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ccb48146-e287-4462-ac21-2d371ea44a2e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-03 02:56:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.246.118\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q5jsq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q5jsq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-master,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:56:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:56:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:56:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:56:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.5,PodIP:10.244.246.118,StartTime:2023-01-03 02:56:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-03 02:56:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://28c4d26f98353af433cfda4008a263931fc53749b53ea23eabe0bfe6c84bf748,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.246.118,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan  3 02:56:34.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-3686" for this suite. 01/03/23 02:56:34.613
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:56:34.647
Jan  3 02:56:34.647: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename tables 01/03/23 02:56:34.647
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:56:34.664
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:56:34.666
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:187
Jan  3 02:56:34.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-653" for this suite. 01/03/23 02:56:34.681
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","completed":238,"skipped":4181,"failed":0}
------------------------------
• [0.037 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:56:34.647
    Jan  3 02:56:34.647: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename tables 01/03/23 02:56:34.647
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:56:34.664
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:56:34.666
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:187
    Jan  3 02:56:34.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "tables-653" for this suite. 01/03/23 02:56:34.681
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:56:34.687
Jan  3 02:56:34.687: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename emptydir 01/03/23 02:56:34.688
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:56:34.732
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:56:34.734
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
STEP: Creating a pod to test emptydir 0777 on node default medium 01/03/23 02:56:34.736
Jan  3 02:56:34.742: INFO: Waiting up to 5m0s for pod "pod-fd42efaa-d0d6-4ecc-a786-10a4b18c1459" in namespace "emptydir-2950" to be "Succeeded or Failed"
Jan  3 02:56:34.755: INFO: Pod "pod-fd42efaa-d0d6-4ecc-a786-10a4b18c1459": Phase="Pending", Reason="", readiness=false. Elapsed: 12.917588ms
Jan  3 02:56:36.757: INFO: Pod "pod-fd42efaa-d0d6-4ecc-a786-10a4b18c1459": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015171916s
Jan  3 02:56:38.762: INFO: Pod "pod-fd42efaa-d0d6-4ecc-a786-10a4b18c1459": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020627626s
Jan  3 02:56:40.758: INFO: Pod "pod-fd42efaa-d0d6-4ecc-a786-10a4b18c1459": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016607885s
STEP: Saw pod success 01/03/23 02:56:40.758
Jan  3 02:56:40.758: INFO: Pod "pod-fd42efaa-d0d6-4ecc-a786-10a4b18c1459" satisfied condition "Succeeded or Failed"
Jan  3 02:56:40.760: INFO: Trying to get logs from node cncf-master pod pod-fd42efaa-d0d6-4ecc-a786-10a4b18c1459 container test-container: <nil>
STEP: delete the pod 01/03/23 02:56:40.763
Jan  3 02:56:40.808: INFO: Waiting for pod pod-fd42efaa-d0d6-4ecc-a786-10a4b18c1459 to disappear
Jan  3 02:56:40.813: INFO: Pod pod-fd42efaa-d0d6-4ecc-a786-10a4b18c1459 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan  3 02:56:40.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2950" for this suite. 01/03/23 02:56:40.816
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":239,"skipped":4293,"failed":0}
------------------------------
• [SLOW TEST] [6.131 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:56:34.687
    Jan  3 02:56:34.687: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename emptydir 01/03/23 02:56:34.688
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:56:34.732
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:56:34.734
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:186
    STEP: Creating a pod to test emptydir 0777 on node default medium 01/03/23 02:56:34.736
    Jan  3 02:56:34.742: INFO: Waiting up to 5m0s for pod "pod-fd42efaa-d0d6-4ecc-a786-10a4b18c1459" in namespace "emptydir-2950" to be "Succeeded or Failed"
    Jan  3 02:56:34.755: INFO: Pod "pod-fd42efaa-d0d6-4ecc-a786-10a4b18c1459": Phase="Pending", Reason="", readiness=false. Elapsed: 12.917588ms
    Jan  3 02:56:36.757: INFO: Pod "pod-fd42efaa-d0d6-4ecc-a786-10a4b18c1459": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015171916s
    Jan  3 02:56:38.762: INFO: Pod "pod-fd42efaa-d0d6-4ecc-a786-10a4b18c1459": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020627626s
    Jan  3 02:56:40.758: INFO: Pod "pod-fd42efaa-d0d6-4ecc-a786-10a4b18c1459": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016607885s
    STEP: Saw pod success 01/03/23 02:56:40.758
    Jan  3 02:56:40.758: INFO: Pod "pod-fd42efaa-d0d6-4ecc-a786-10a4b18c1459" satisfied condition "Succeeded or Failed"
    Jan  3 02:56:40.760: INFO: Trying to get logs from node cncf-master pod pod-fd42efaa-d0d6-4ecc-a786-10a4b18c1459 container test-container: <nil>
    STEP: delete the pod 01/03/23 02:56:40.763
    Jan  3 02:56:40.808: INFO: Waiting for pod pod-fd42efaa-d0d6-4ecc-a786-10a4b18c1459 to disappear
    Jan  3 02:56:40.813: INFO: Pod pod-fd42efaa-d0d6-4ecc-a786-10a4b18c1459 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan  3 02:56:40.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2950" for this suite. 01/03/23 02:56:40.816
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:56:40.82
Jan  3 02:56:40.821: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename secrets 01/03/23 02:56:40.821
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:56:40.844
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:56:40.845
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
STEP: Creating secret with name secret-test-6c7874bd-6f12-4891-a391-fbf39ddc4814 01/03/23 02:56:40.847
STEP: Creating a pod to test consume secrets 01/03/23 02:56:40.851
Jan  3 02:56:40.857: INFO: Waiting up to 5m0s for pod "pod-secrets-9de79e4f-125e-44a4-bee6-7a335e7adbb6" in namespace "secrets-8610" to be "Succeeded or Failed"
Jan  3 02:56:40.873: INFO: Pod "pod-secrets-9de79e4f-125e-44a4-bee6-7a335e7adbb6": Phase="Pending", Reason="", readiness=false. Elapsed: 16.119214ms
Jan  3 02:56:42.875: INFO: Pod "pod-secrets-9de79e4f-125e-44a4-bee6-7a335e7adbb6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018191855s
Jan  3 02:56:44.875: INFO: Pod "pod-secrets-9de79e4f-125e-44a4-bee6-7a335e7adbb6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018400698s
Jan  3 02:56:46.875: INFO: Pod "pod-secrets-9de79e4f-125e-44a4-bee6-7a335e7adbb6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018455815s
STEP: Saw pod success 01/03/23 02:56:46.875
Jan  3 02:56:46.875: INFO: Pod "pod-secrets-9de79e4f-125e-44a4-bee6-7a335e7adbb6" satisfied condition "Succeeded or Failed"
Jan  3 02:56:46.877: INFO: Trying to get logs from node cncf-master pod pod-secrets-9de79e4f-125e-44a4-bee6-7a335e7adbb6 container secret-volume-test: <nil>
STEP: delete the pod 01/03/23 02:56:46.88
Jan  3 02:56:46.906: INFO: Waiting for pod pod-secrets-9de79e4f-125e-44a4-bee6-7a335e7adbb6 to disappear
Jan  3 02:56:46.911: INFO: Pod pod-secrets-9de79e4f-125e-44a4-bee6-7a335e7adbb6 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan  3 02:56:46.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8610" for this suite. 01/03/23 02:56:46.914
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":240,"skipped":4357,"failed":0}
------------------------------
• [SLOW TEST] [6.096 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:56:40.82
    Jan  3 02:56:40.821: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename secrets 01/03/23 02:56:40.821
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:56:40.844
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:56:40.845
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:67
    STEP: Creating secret with name secret-test-6c7874bd-6f12-4891-a391-fbf39ddc4814 01/03/23 02:56:40.847
    STEP: Creating a pod to test consume secrets 01/03/23 02:56:40.851
    Jan  3 02:56:40.857: INFO: Waiting up to 5m0s for pod "pod-secrets-9de79e4f-125e-44a4-bee6-7a335e7adbb6" in namespace "secrets-8610" to be "Succeeded or Failed"
    Jan  3 02:56:40.873: INFO: Pod "pod-secrets-9de79e4f-125e-44a4-bee6-7a335e7adbb6": Phase="Pending", Reason="", readiness=false. Elapsed: 16.119214ms
    Jan  3 02:56:42.875: INFO: Pod "pod-secrets-9de79e4f-125e-44a4-bee6-7a335e7adbb6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018191855s
    Jan  3 02:56:44.875: INFO: Pod "pod-secrets-9de79e4f-125e-44a4-bee6-7a335e7adbb6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018400698s
    Jan  3 02:56:46.875: INFO: Pod "pod-secrets-9de79e4f-125e-44a4-bee6-7a335e7adbb6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018455815s
    STEP: Saw pod success 01/03/23 02:56:46.875
    Jan  3 02:56:46.875: INFO: Pod "pod-secrets-9de79e4f-125e-44a4-bee6-7a335e7adbb6" satisfied condition "Succeeded or Failed"
    Jan  3 02:56:46.877: INFO: Trying to get logs from node cncf-master pod pod-secrets-9de79e4f-125e-44a4-bee6-7a335e7adbb6 container secret-volume-test: <nil>
    STEP: delete the pod 01/03/23 02:56:46.88
    Jan  3 02:56:46.906: INFO: Waiting for pod pod-secrets-9de79e4f-125e-44a4-bee6-7a335e7adbb6 to disappear
    Jan  3 02:56:46.911: INFO: Pod pod-secrets-9de79e4f-125e-44a4-bee6-7a335e7adbb6 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan  3 02:56:46.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8610" for this suite. 01/03/23 02:56:46.914
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:56:46.917
Jan  3 02:56:46.917: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename daemonsets 01/03/23 02:56:46.918
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:56:46.957
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:56:46.959
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
STEP: Creating simple DaemonSet "daemon-set" 01/03/23 02:56:46.991
STEP: Check that daemon pods launch on every node of the cluster. 01/03/23 02:56:46.995
Jan  3 02:56:47.019: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  3 02:56:47.019: INFO: Node cncf-master is running 0 daemon pod, expected 1
Jan  3 02:56:48.030: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  3 02:56:48.030: INFO: Node cncf-master is running 0 daemon pod, expected 1
Jan  3 02:56:49.042: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan  3 02:56:49.042: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets 01/03/23 02:56:49.043
STEP: DeleteCollection of the DaemonSets 01/03/23 02:56:49.045
STEP: Verify that ReplicaSets have been deleted 01/03/23 02:56:49.048
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
Jan  3 02:56:49.061: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1004815"},"items":null}

Jan  3 02:56:49.080: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1004815"},"items":[{"metadata":{"name":"daemon-set-2vl92","generateName":"daemon-set-","namespace":"daemonsets-3886","uid":"a82837f7-6ea8-4f41-8043-8bc09e9c82b7","resourceVersion":"1004814","creationTimestamp":"2023-01-03T02:56:47Z","deletionTimestamp":"2023-01-03T02:57:19Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"d3809408fa816ef3f5c19b3634f5be3eba81c69846b74c2eeab79d6673e5c1a3","cni.projectcalico.org/podIP":"10.244.89.116/32","cni.projectcalico.org/podIPs":"10.244.89.116/32","createdTime":"2023-01-03T11:56:47.020553189+09:00","creator":"system:serviceaccount:kube-system:daemon-set-controller","updatedTime":"2023-01-03T11:56:47.020553189+09:00","updater":"system:serviceaccount:kube-system:daemon-set-controller"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"9e519175-91f1-4279-a823-fab792e18c89","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-03T02:56:47Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-03T02:56:47Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9e519175-91f1-4279-a823-fab792e18c89\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-03T02:56:48Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.89.116\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-z4vgs","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-z4vgs","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"cncf-node2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["cncf-node2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-03T02:56:47Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-03T02:56:48Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-03T02:56:48Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-03T02:56:47Z"}],"hostIP":"172.21.7.13","podIP":"10.244.89.116","podIPs":[{"ip":"10.244.89.116"}],"startTime":"2023-01-03T02:56:47Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-03T02:56:47Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://b8b574564bfb8c7d18fce618507304038e76dea9dae568ffbde3dc83fa71773c","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-95b5p","generateName":"daemon-set-","namespace":"daemonsets-3886","uid":"ad95da92-b279-47a4-bacc-7d5b66b2c5e0","resourceVersion":"1004813","creationTimestamp":"2023-01-03T02:56:47Z","deletionTimestamp":"2023-01-03T02:57:19Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"6b135631c3f4bf45b9da68c379ee2989d1b234f1f1f5c8b643c586c18aa73952","cni.projectcalico.org/podIP":"10.244.232.21/32","cni.projectcalico.org/podIPs":"10.244.232.21/32","createdTime":"2023-01-03T11:56:47.024543468+09:00","creator":"system:serviceaccount:kube-system:daemon-set-controller","updatedTime":"2023-01-03T11:56:47.024543468+09:00","updater":"system:serviceaccount:kube-system:daemon-set-controller"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"9e519175-91f1-4279-a823-fab792e18c89","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-03T02:56:47Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-03T02:56:47Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9e519175-91f1-4279-a823-fab792e18c89\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-03T02:56:48Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.232.21\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-ndxnx","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-ndxnx","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"cncf-node1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["cncf-node1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-03T02:56:47Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-03T02:56:48Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-03T02:56:48Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-03T02:56:47Z"}],"hostIP":"172.21.7.7","podIP":"10.244.232.21","podIPs":[{"ip":"10.244.232.21"}],"startTime":"2023-01-03T02:56:47Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-03T02:56:47Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://7a1c33660a26560d5b94b00105ef73ed54b5584d02a26c3878ebdfed7c97da99","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-bzzpl","generateName":"daemon-set-","namespace":"daemonsets-3886","uid":"3c9159c1-87b9-4834-aa84-94bbbd8b777f","resourceVersion":"1004815","creationTimestamp":"2023-01-03T02:56:47Z","deletionTimestamp":"2023-01-03T02:57:19Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"02b0be2b6efa091d08b8d5a0cac2103f770441622f6f8dc1a0439f1e5b5f02ce","cni.projectcalico.org/podIP":"10.244.246.120/32","cni.projectcalico.org/podIPs":"10.244.246.120/32","createdTime":"2023-01-03T11:56:47.025091428+09:00","creator":"system:serviceaccount:kube-system:daemon-set-controller","updatedTime":"2023-01-03T11:56:47.025091428+09:00","updater":"system:serviceaccount:kube-system:daemon-set-controller"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"9e519175-91f1-4279-a823-fab792e18c89","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-03T02:56:47Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-03T02:56:47Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9e519175-91f1-4279-a823-fab792e18c89\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-03T02:56:48Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.246.120\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-txk49","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-txk49","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"cncf-master","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["cncf-master"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-03T02:56:47Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-03T02:56:48Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-03T02:56:48Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-03T02:56:47Z"}],"hostIP":"172.21.7.5","podIP":"10.244.246.120","podIPs":[{"ip":"10.244.246.120"}],"startTime":"2023-01-03T02:56:47Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-03T02:56:47Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://deba6d3aef39102c79b1fc948bf1ce2ed22ac2ba3ae09e28ae74486092e3daca","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan  3 02:56:49.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3886" for this suite. 01/03/23 02:56:49.09
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","completed":241,"skipped":4377,"failed":0}
------------------------------
• [2.175 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:56:46.917
    Jan  3 02:56:46.917: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename daemonsets 01/03/23 02:56:46.918
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:56:46.957
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:56:46.959
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:822
    STEP: Creating simple DaemonSet "daemon-set" 01/03/23 02:56:46.991
    STEP: Check that daemon pods launch on every node of the cluster. 01/03/23 02:56:46.995
    Jan  3 02:56:47.019: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  3 02:56:47.019: INFO: Node cncf-master is running 0 daemon pod, expected 1
    Jan  3 02:56:48.030: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  3 02:56:48.030: INFO: Node cncf-master is running 0 daemon pod, expected 1
    Jan  3 02:56:49.042: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan  3 02:56:49.042: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: listing all DeamonSets 01/03/23 02:56:49.043
    STEP: DeleteCollection of the DaemonSets 01/03/23 02:56:49.045
    STEP: Verify that ReplicaSets have been deleted 01/03/23 02:56:49.048
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    Jan  3 02:56:49.061: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1004815"},"items":null}

    Jan  3 02:56:49.080: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1004815"},"items":[{"metadata":{"name":"daemon-set-2vl92","generateName":"daemon-set-","namespace":"daemonsets-3886","uid":"a82837f7-6ea8-4f41-8043-8bc09e9c82b7","resourceVersion":"1004814","creationTimestamp":"2023-01-03T02:56:47Z","deletionTimestamp":"2023-01-03T02:57:19Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"d3809408fa816ef3f5c19b3634f5be3eba81c69846b74c2eeab79d6673e5c1a3","cni.projectcalico.org/podIP":"10.244.89.116/32","cni.projectcalico.org/podIPs":"10.244.89.116/32","createdTime":"2023-01-03T11:56:47.020553189+09:00","creator":"system:serviceaccount:kube-system:daemon-set-controller","updatedTime":"2023-01-03T11:56:47.020553189+09:00","updater":"system:serviceaccount:kube-system:daemon-set-controller"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"9e519175-91f1-4279-a823-fab792e18c89","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-03T02:56:47Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-03T02:56:47Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9e519175-91f1-4279-a823-fab792e18c89\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-03T02:56:48Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.89.116\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-z4vgs","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-z4vgs","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"cncf-node2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["cncf-node2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-03T02:56:47Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-03T02:56:48Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-03T02:56:48Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-03T02:56:47Z"}],"hostIP":"172.21.7.13","podIP":"10.244.89.116","podIPs":[{"ip":"10.244.89.116"}],"startTime":"2023-01-03T02:56:47Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-03T02:56:47Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://b8b574564bfb8c7d18fce618507304038e76dea9dae568ffbde3dc83fa71773c","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-95b5p","generateName":"daemon-set-","namespace":"daemonsets-3886","uid":"ad95da92-b279-47a4-bacc-7d5b66b2c5e0","resourceVersion":"1004813","creationTimestamp":"2023-01-03T02:56:47Z","deletionTimestamp":"2023-01-03T02:57:19Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"6b135631c3f4bf45b9da68c379ee2989d1b234f1f1f5c8b643c586c18aa73952","cni.projectcalico.org/podIP":"10.244.232.21/32","cni.projectcalico.org/podIPs":"10.244.232.21/32","createdTime":"2023-01-03T11:56:47.024543468+09:00","creator":"system:serviceaccount:kube-system:daemon-set-controller","updatedTime":"2023-01-03T11:56:47.024543468+09:00","updater":"system:serviceaccount:kube-system:daemon-set-controller"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"9e519175-91f1-4279-a823-fab792e18c89","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-03T02:56:47Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-03T02:56:47Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9e519175-91f1-4279-a823-fab792e18c89\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-03T02:56:48Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.232.21\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-ndxnx","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-ndxnx","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"cncf-node1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["cncf-node1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-03T02:56:47Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-03T02:56:48Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-03T02:56:48Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-03T02:56:47Z"}],"hostIP":"172.21.7.7","podIP":"10.244.232.21","podIPs":[{"ip":"10.244.232.21"}],"startTime":"2023-01-03T02:56:47Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-03T02:56:47Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://7a1c33660a26560d5b94b00105ef73ed54b5584d02a26c3878ebdfed7c97da99","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-bzzpl","generateName":"daemon-set-","namespace":"daemonsets-3886","uid":"3c9159c1-87b9-4834-aa84-94bbbd8b777f","resourceVersion":"1004815","creationTimestamp":"2023-01-03T02:56:47Z","deletionTimestamp":"2023-01-03T02:57:19Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"02b0be2b6efa091d08b8d5a0cac2103f770441622f6f8dc1a0439f1e5b5f02ce","cni.projectcalico.org/podIP":"10.244.246.120/32","cni.projectcalico.org/podIPs":"10.244.246.120/32","createdTime":"2023-01-03T11:56:47.025091428+09:00","creator":"system:serviceaccount:kube-system:daemon-set-controller","updatedTime":"2023-01-03T11:56:47.025091428+09:00","updater":"system:serviceaccount:kube-system:daemon-set-controller"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"9e519175-91f1-4279-a823-fab792e18c89","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-03T02:56:47Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-03T02:56:47Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9e519175-91f1-4279-a823-fab792e18c89\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-03T02:56:48Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.246.120\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-txk49","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-txk49","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"cncf-master","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["cncf-master"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-03T02:56:47Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-03T02:56:48Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-03T02:56:48Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-03T02:56:47Z"}],"hostIP":"172.21.7.5","podIP":"10.244.246.120","podIPs":[{"ip":"10.244.246.120"}],"startTime":"2023-01-03T02:56:47Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-03T02:56:47Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://deba6d3aef39102c79b1fc948bf1ce2ed22ac2ba3ae09e28ae74486092e3daca","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan  3 02:56:49.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-3886" for this suite. 01/03/23 02:56:49.09
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:56:49.093
Jan  3 02:56:49.093: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename deployment 01/03/23 02:56:49.094
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:56:49.101
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:56:49.102
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Jan  3 02:56:49.106: INFO: Creating deployment "test-recreate-deployment"
Jan  3 02:56:49.125: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jan  3 02:56:49.188: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jan  3 02:56:51.201: INFO: Waiting deployment "test-recreate-deployment" to complete
Jan  3 02:56:51.210: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jan  3 02:56:51.214: INFO: Updating deployment test-recreate-deployment
Jan  3 02:56:51.214: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan  3 02:56:51.431: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-3117  1590e23b-3707-4965-9efe-ccf4d3d16928 1004888 2 2023-01-03 02:56:49 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[createdTime:2023-01-03T11:56:49.108107541+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:2 updatedTime:2023-01-03T11:56:49.108107541+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update apps/v1 2023-01-03 02:56:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-03 02:56:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007f1dd18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-03 02:56:51 +0000 UTC,LastTransitionTime:2023-01-03 02:56:51 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-01-03 02:56:51 +0000 UTC,LastTransitionTime:2023-01-03 02:56:49 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jan  3 02:56:51.433: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-3117  cd2313d3-3059-4748-b4b6-c07fcbc21093 1004887 1 2023-01-03 02:56:51 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[createdTime:2023-01-03T11:56:49.108107541+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2 updatedTime:2023-01-03T11:56:49.108107541+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [{apps/v1 Deployment test-recreate-deployment 1590e23b-3707-4965-9efe-ccf4d3d16928 0xc008d601fe 0xc008d601ff}] [] [{kube-controller-manager Update apps/v1 2023-01-03 02:56:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:createdTime":{},"f:creator":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{},"f:updatedTime":{},"f:updater":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1590e23b-3707-4965-9efe-ccf4d3d16928\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-03 02:56:51 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc008d602a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan  3 02:56:51.433: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jan  3 02:56:51.433: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-3117  87a1fac5-73bc-462a-8af7-03d01fe79cc7 1004873 2 2023-01-03 02:56:49 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[createdTime:2023-01-03T11:56:49.108107541+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:56:49.108107541+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [{apps/v1 Deployment test-recreate-deployment 1590e23b-3707-4965-9efe-ccf4d3d16928 0xc008d600b7 0xc008d600b8}] [] [{kube-controller-manager Update apps/v1 2023-01-03 02:56:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:createdTime":{},"f:creator":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{},"f:updatedTime":{},"f:updater":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1590e23b-3707-4965-9efe-ccf4d3d16928\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-03 02:56:51 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc008d60168 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan  3 02:56:51.435: INFO: Pod "test-recreate-deployment-9d58999df-x69fb" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-9d58999df-x69fb test-recreate-deployment-9d58999df- deployment-3117  83ec0676-b1b3-4d5e-88a9-b47db9da9d3e 1004885 0 2023-01-03 02:56:51 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[createdTime:2023-01-03T11:56:51.298946731+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T11:56:51.298946731+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df cd2313d3-3059-4748-b4b6-c07fcbc21093 0xc00338b11e 0xc00338b11f}] [] [{kube-controller-manager Update v1 2023-01-03 02:56:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cd2313d3-3059-4748-b4b6-c07fcbc21093\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-03 02:56:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dd7mm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dd7mm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-master,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:56:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:56:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:56:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:56:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.5,PodIP:,StartTime:2023-01-03 02:56:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan  3 02:56:51.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3117" for this suite. 01/03/23 02:56:51.438
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","completed":242,"skipped":4386,"failed":0}
------------------------------
• [2.361 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:56:49.093
    Jan  3 02:56:49.093: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename deployment 01/03/23 02:56:49.094
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:56:49.101
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:56:49.102
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Jan  3 02:56:49.106: INFO: Creating deployment "test-recreate-deployment"
    Jan  3 02:56:49.125: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Jan  3 02:56:49.188: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Jan  3 02:56:51.201: INFO: Waiting deployment "test-recreate-deployment" to complete
    Jan  3 02:56:51.210: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Jan  3 02:56:51.214: INFO: Updating deployment test-recreate-deployment
    Jan  3 02:56:51.214: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan  3 02:56:51.431: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-3117  1590e23b-3707-4965-9efe-ccf4d3d16928 1004888 2 2023-01-03 02:56:49 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[createdTime:2023-01-03T11:56:49.108107541+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:2 updatedTime:2023-01-03T11:56:49.108107541+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update apps/v1 2023-01-03 02:56:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-03 02:56:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007f1dd18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-03 02:56:51 +0000 UTC,LastTransitionTime:2023-01-03 02:56:51 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-01-03 02:56:51 +0000 UTC,LastTransitionTime:2023-01-03 02:56:49 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Jan  3 02:56:51.433: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-3117  cd2313d3-3059-4748-b4b6-c07fcbc21093 1004887 1 2023-01-03 02:56:51 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[createdTime:2023-01-03T11:56:49.108107541+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2 updatedTime:2023-01-03T11:56:49.108107541+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [{apps/v1 Deployment test-recreate-deployment 1590e23b-3707-4965-9efe-ccf4d3d16928 0xc008d601fe 0xc008d601ff}] [] [{kube-controller-manager Update apps/v1 2023-01-03 02:56:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:createdTime":{},"f:creator":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{},"f:updatedTime":{},"f:updater":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1590e23b-3707-4965-9efe-ccf4d3d16928\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-03 02:56:51 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc008d602a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan  3 02:56:51.433: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Jan  3 02:56:51.433: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-3117  87a1fac5-73bc-462a-8af7-03d01fe79cc7 1004873 2 2023-01-03 02:56:49 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[createdTime:2023-01-03T11:56:49.108107541+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T11:56:49.108107541+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [{apps/v1 Deployment test-recreate-deployment 1590e23b-3707-4965-9efe-ccf4d3d16928 0xc008d600b7 0xc008d600b8}] [] [{kube-controller-manager Update apps/v1 2023-01-03 02:56:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:createdTime":{},"f:creator":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{},"f:updatedTime":{},"f:updater":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1590e23b-3707-4965-9efe-ccf4d3d16928\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-03 02:56:51 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc008d60168 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan  3 02:56:51.435: INFO: Pod "test-recreate-deployment-9d58999df-x69fb" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-9d58999df-x69fb test-recreate-deployment-9d58999df- deployment-3117  83ec0676-b1b3-4d5e-88a9-b47db9da9d3e 1004885 0 2023-01-03 02:56:51 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[createdTime:2023-01-03T11:56:51.298946731+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T11:56:51.298946731+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df cd2313d3-3059-4748-b4b6-c07fcbc21093 0xc00338b11e 0xc00338b11f}] [] [{kube-controller-manager Update v1 2023-01-03 02:56:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cd2313d3-3059-4748-b4b6-c07fcbc21093\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-03 02:56:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dd7mm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dd7mm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-master,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:56:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:56:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:56:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 02:56:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.5,PodIP:,StartTime:2023-01-03 02:56:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan  3 02:56:51.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-3117" for this suite. 01/03/23 02:56:51.438
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:56:51.456
Jan  3 02:56:51.456: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename downward-api 01/03/23 02:56:51.457
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:56:51.476
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:56:51.478
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
STEP: Creating a pod to test downward API volume plugin 01/03/23 02:56:51.48
Jan  3 02:56:51.486: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e9e67a6d-1515-406b-af55-32dc52755412" in namespace "downward-api-8138" to be "Succeeded or Failed"
Jan  3 02:56:51.499: INFO: Pod "downwardapi-volume-e9e67a6d-1515-406b-af55-32dc52755412": Phase="Pending", Reason="", readiness=false. Elapsed: 13.311902ms
Jan  3 02:56:53.502: INFO: Pod "downwardapi-volume-e9e67a6d-1515-406b-af55-32dc52755412": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015867401s
Jan  3 02:56:55.503: INFO: Pod "downwardapi-volume-e9e67a6d-1515-406b-af55-32dc52755412": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017143795s
STEP: Saw pod success 01/03/23 02:56:55.503
Jan  3 02:56:55.503: INFO: Pod "downwardapi-volume-e9e67a6d-1515-406b-af55-32dc52755412" satisfied condition "Succeeded or Failed"
Jan  3 02:56:55.504: INFO: Trying to get logs from node cncf-master pod downwardapi-volume-e9e67a6d-1515-406b-af55-32dc52755412 container client-container: <nil>
STEP: delete the pod 01/03/23 02:56:55.508
Jan  3 02:56:55.537: INFO: Waiting for pod downwardapi-volume-e9e67a6d-1515-406b-af55-32dc52755412 to disappear
Jan  3 02:56:55.542: INFO: Pod downwardapi-volume-e9e67a6d-1515-406b-af55-32dc52755412 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan  3 02:56:55.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8138" for this suite. 01/03/23 02:56:55.546
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","completed":243,"skipped":4431,"failed":0}
------------------------------
• [4.092 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:56:51.456
    Jan  3 02:56:51.456: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename downward-api 01/03/23 02:56:51.457
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:56:51.476
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:56:51.478
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:220
    STEP: Creating a pod to test downward API volume plugin 01/03/23 02:56:51.48
    Jan  3 02:56:51.486: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e9e67a6d-1515-406b-af55-32dc52755412" in namespace "downward-api-8138" to be "Succeeded or Failed"
    Jan  3 02:56:51.499: INFO: Pod "downwardapi-volume-e9e67a6d-1515-406b-af55-32dc52755412": Phase="Pending", Reason="", readiness=false. Elapsed: 13.311902ms
    Jan  3 02:56:53.502: INFO: Pod "downwardapi-volume-e9e67a6d-1515-406b-af55-32dc52755412": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015867401s
    Jan  3 02:56:55.503: INFO: Pod "downwardapi-volume-e9e67a6d-1515-406b-af55-32dc52755412": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017143795s
    STEP: Saw pod success 01/03/23 02:56:55.503
    Jan  3 02:56:55.503: INFO: Pod "downwardapi-volume-e9e67a6d-1515-406b-af55-32dc52755412" satisfied condition "Succeeded or Failed"
    Jan  3 02:56:55.504: INFO: Trying to get logs from node cncf-master pod downwardapi-volume-e9e67a6d-1515-406b-af55-32dc52755412 container client-container: <nil>
    STEP: delete the pod 01/03/23 02:56:55.508
    Jan  3 02:56:55.537: INFO: Waiting for pod downwardapi-volume-e9e67a6d-1515-406b-af55-32dc52755412 to disappear
    Jan  3 02:56:55.542: INFO: Pod downwardapi-volume-e9e67a6d-1515-406b-af55-32dc52755412 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan  3 02:56:55.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8138" for this suite. 01/03/23 02:56:55.546
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:56:55.549
Jan  3 02:56:55.549: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename gc 01/03/23 02:56:55.55
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:56:55.571
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:56:55.573
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 01/03/23 02:56:55.59
STEP: delete the rc 01/03/23 02:57:00.717
STEP: wait for the rc to be deleted 01/03/23 02:57:00.727
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 01/03/23 02:57:05.759
STEP: Gathering metrics 01/03/23 02:57:35.775
Jan  3 02:57:35.787: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cncf-master" in namespace "kube-system" to be "running and ready"
Jan  3 02:57:35.792: INFO: Pod "kube-controller-manager-cncf-master": Phase="Running", Reason="", readiness=true. Elapsed: 5.02383ms
Jan  3 02:57:35.792: INFO: The phase of Pod kube-controller-manager-cncf-master is Running (Ready = true)
Jan  3 02:57:35.792: INFO: Pod "kube-controller-manager-cncf-master" satisfied condition "running and ready"
Jan  3 02:57:35.849: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan  3 02:57:35.850: INFO: Deleting pod "simpletest.rc-2bgnb" in namespace "gc-4455"
Jan  3 02:57:35.899: INFO: Deleting pod "simpletest.rc-2p8zn" in namespace "gc-4455"
Jan  3 02:57:35.930: INFO: Deleting pod "simpletest.rc-2xsc4" in namespace "gc-4455"
Jan  3 02:57:35.958: INFO: Deleting pod "simpletest.rc-2znjp" in namespace "gc-4455"
Jan  3 02:57:36.010: INFO: Deleting pod "simpletest.rc-4bfjc" in namespace "gc-4455"
Jan  3 02:57:36.038: INFO: Deleting pod "simpletest.rc-4h4pt" in namespace "gc-4455"
Jan  3 02:57:36.082: INFO: Deleting pod "simpletest.rc-4vbjg" in namespace "gc-4455"
Jan  3 02:57:36.150: INFO: Deleting pod "simpletest.rc-4zl4x" in namespace "gc-4455"
Jan  3 02:57:36.162: INFO: Deleting pod "simpletest.rc-56b2x" in namespace "gc-4455"
Jan  3 02:57:36.193: INFO: Deleting pod "simpletest.rc-56j8q" in namespace "gc-4455"
Jan  3 02:57:36.216: INFO: Deleting pod "simpletest.rc-5v4t4" in namespace "gc-4455"
Jan  3 02:57:36.253: INFO: Deleting pod "simpletest.rc-5xqzb" in namespace "gc-4455"
Jan  3 02:57:36.307: INFO: Deleting pod "simpletest.rc-5xtfm" in namespace "gc-4455"
Jan  3 02:57:36.340: INFO: Deleting pod "simpletest.rc-5zcdz" in namespace "gc-4455"
Jan  3 02:57:36.363: INFO: Deleting pod "simpletest.rc-6fh6q" in namespace "gc-4455"
Jan  3 02:57:36.382: INFO: Deleting pod "simpletest.rc-6m9cg" in namespace "gc-4455"
Jan  3 02:57:36.472: INFO: Deleting pod "simpletest.rc-6tr9z" in namespace "gc-4455"
Jan  3 02:57:36.512: INFO: Deleting pod "simpletest.rc-6v2vc" in namespace "gc-4455"
Jan  3 02:57:36.529: INFO: Deleting pod "simpletest.rc-7dvls" in namespace "gc-4455"
Jan  3 02:57:36.583: INFO: Deleting pod "simpletest.rc-7kc8q" in namespace "gc-4455"
Jan  3 02:57:36.615: INFO: Deleting pod "simpletest.rc-7n46p" in namespace "gc-4455"
Jan  3 02:57:36.632: INFO: Deleting pod "simpletest.rc-7qt7t" in namespace "gc-4455"
Jan  3 02:57:36.667: INFO: Deleting pod "simpletest.rc-7xw2b" in namespace "gc-4455"
Jan  3 02:57:36.717: INFO: Deleting pod "simpletest.rc-82lmg" in namespace "gc-4455"
Jan  3 02:57:36.743: INFO: Deleting pod "simpletest.rc-82p7n" in namespace "gc-4455"
Jan  3 02:57:36.794: INFO: Deleting pod "simpletest.rc-8mlnw" in namespace "gc-4455"
Jan  3 02:57:36.841: INFO: Deleting pod "simpletest.rc-8xj7p" in namespace "gc-4455"
Jan  3 02:57:36.866: INFO: Deleting pod "simpletest.rc-9djrb" in namespace "gc-4455"
Jan  3 02:57:36.886: INFO: Deleting pod "simpletest.rc-9fg86" in namespace "gc-4455"
Jan  3 02:57:36.909: INFO: Deleting pod "simpletest.rc-9lsrp" in namespace "gc-4455"
Jan  3 02:57:36.984: INFO: Deleting pod "simpletest.rc-9qqbs" in namespace "gc-4455"
Jan  3 02:57:37.006: INFO: Deleting pod "simpletest.rc-bdx5s" in namespace "gc-4455"
Jan  3 02:57:37.038: INFO: Deleting pod "simpletest.rc-bdzjr" in namespace "gc-4455"
Jan  3 02:57:37.061: INFO: Deleting pod "simpletest.rc-cgttg" in namespace "gc-4455"
Jan  3 02:57:37.111: INFO: Deleting pod "simpletest.rc-cp28h" in namespace "gc-4455"
Jan  3 02:57:37.138: INFO: Deleting pod "simpletest.rc-d8lbr" in namespace "gc-4455"
Jan  3 02:57:37.170: INFO: Deleting pod "simpletest.rc-d9r8l" in namespace "gc-4455"
Jan  3 02:57:37.205: INFO: Deleting pod "simpletest.rc-dctz7" in namespace "gc-4455"
Jan  3 02:57:37.279: INFO: Deleting pod "simpletest.rc-dhgtz" in namespace "gc-4455"
Jan  3 02:57:37.293: INFO: Deleting pod "simpletest.rc-dqrm4" in namespace "gc-4455"
Jan  3 02:57:37.325: INFO: Deleting pod "simpletest.rc-dwqgd" in namespace "gc-4455"
Jan  3 02:57:37.356: INFO: Deleting pod "simpletest.rc-f96jk" in namespace "gc-4455"
Jan  3 02:57:37.417: INFO: Deleting pod "simpletest.rc-ffvsb" in namespace "gc-4455"
Jan  3 02:57:37.533: INFO: Deleting pod "simpletest.rc-fr4x2" in namespace "gc-4455"
Jan  3 02:57:37.596: INFO: Deleting pod "simpletest.rc-ftkjf" in namespace "gc-4455"
Jan  3 02:57:37.664: INFO: Deleting pod "simpletest.rc-g89p4" in namespace "gc-4455"
Jan  3 02:57:37.682: INFO: Deleting pod "simpletest.rc-g8t9b" in namespace "gc-4455"
Jan  3 02:57:37.756: INFO: Deleting pod "simpletest.rc-g98l9" in namespace "gc-4455"
Jan  3 02:57:37.825: INFO: Deleting pod "simpletest.rc-gcd5v" in namespace "gc-4455"
Jan  3 02:57:37.859: INFO: Deleting pod "simpletest.rc-gdmkl" in namespace "gc-4455"
Jan  3 02:57:37.916: INFO: Deleting pod "simpletest.rc-gfwfr" in namespace "gc-4455"
Jan  3 02:57:37.947: INFO: Deleting pod "simpletest.rc-gj5rl" in namespace "gc-4455"
Jan  3 02:57:37.991: INFO: Deleting pod "simpletest.rc-gkgjk" in namespace "gc-4455"
Jan  3 02:57:38.048: INFO: Deleting pod "simpletest.rc-gknxn" in namespace "gc-4455"
Jan  3 02:57:38.077: INFO: Deleting pod "simpletest.rc-h6wsk" in namespace "gc-4455"
Jan  3 02:57:38.106: INFO: Deleting pod "simpletest.rc-hbs92" in namespace "gc-4455"
Jan  3 02:57:38.129: INFO: Deleting pod "simpletest.rc-hcbzc" in namespace "gc-4455"
Jan  3 02:57:38.203: INFO: Deleting pod "simpletest.rc-hg78q" in namespace "gc-4455"
Jan  3 02:57:38.278: INFO: Deleting pod "simpletest.rc-j22dl" in namespace "gc-4455"
Jan  3 02:57:38.380: INFO: Deleting pod "simpletest.rc-j46ms" in namespace "gc-4455"
Jan  3 02:57:38.415: INFO: Deleting pod "simpletest.rc-jkpcb" in namespace "gc-4455"
Jan  3 02:57:38.464: INFO: Deleting pod "simpletest.rc-jnggr" in namespace "gc-4455"
Jan  3 02:57:38.489: INFO: Deleting pod "simpletest.rc-jphwr" in namespace "gc-4455"
Jan  3 02:57:38.535: INFO: Deleting pod "simpletest.rc-jqm7h" in namespace "gc-4455"
Jan  3 02:57:38.564: INFO: Deleting pod "simpletest.rc-kchd2" in namespace "gc-4455"
Jan  3 02:57:38.650: INFO: Deleting pod "simpletest.rc-khvgj" in namespace "gc-4455"
Jan  3 02:57:38.738: INFO: Deleting pod "simpletest.rc-kkh5w" in namespace "gc-4455"
Jan  3 02:57:38.764: INFO: Deleting pod "simpletest.rc-kl6q8" in namespace "gc-4455"
Jan  3 02:57:38.933: INFO: Deleting pod "simpletest.rc-ld7kx" in namespace "gc-4455"
Jan  3 02:57:38.993: INFO: Deleting pod "simpletest.rc-lhpkg" in namespace "gc-4455"
Jan  3 02:57:39.073: INFO: Deleting pod "simpletest.rc-llbfj" in namespace "gc-4455"
Jan  3 02:57:39.096: INFO: Deleting pod "simpletest.rc-ltdjx" in namespace "gc-4455"
Jan  3 02:57:39.121: INFO: Deleting pod "simpletest.rc-mh95r" in namespace "gc-4455"
Jan  3 02:57:39.213: INFO: Deleting pod "simpletest.rc-msbsl" in namespace "gc-4455"
Jan  3 02:57:39.254: INFO: Deleting pod "simpletest.rc-n2x72" in namespace "gc-4455"
Jan  3 02:57:39.335: INFO: Deleting pod "simpletest.rc-n6mqk" in namespace "gc-4455"
Jan  3 02:57:39.363: INFO: Deleting pod "simpletest.rc-ncw2d" in namespace "gc-4455"
Jan  3 02:57:39.402: INFO: Deleting pod "simpletest.rc-nfb42" in namespace "gc-4455"
Jan  3 02:57:39.507: INFO: Deleting pod "simpletest.rc-nlmhh" in namespace "gc-4455"
Jan  3 02:57:39.630: INFO: Deleting pod "simpletest.rc-nmtln" in namespace "gc-4455"
Jan  3 02:57:39.656: INFO: Deleting pod "simpletest.rc-pjzs4" in namespace "gc-4455"
Jan  3 02:57:39.714: INFO: Deleting pod "simpletest.rc-r8kpc" in namespace "gc-4455"
Jan  3 02:57:39.806: INFO: Deleting pod "simpletest.rc-r99d9" in namespace "gc-4455"
Jan  3 02:57:39.834: INFO: Deleting pod "simpletest.rc-r9slw" in namespace "gc-4455"
Jan  3 02:57:39.868: INFO: Deleting pod "simpletest.rc-rkbcl" in namespace "gc-4455"
Jan  3 02:57:39.920: INFO: Deleting pod "simpletest.rc-rmfwr" in namespace "gc-4455"
Jan  3 02:57:39.955: INFO: Deleting pod "simpletest.rc-rs7s6" in namespace "gc-4455"
Jan  3 02:57:40.041: INFO: Deleting pod "simpletest.rc-rttvl" in namespace "gc-4455"
Jan  3 02:57:40.075: INFO: Deleting pod "simpletest.rc-sx5dx" in namespace "gc-4455"
Jan  3 02:57:40.115: INFO: Deleting pod "simpletest.rc-t6hq4" in namespace "gc-4455"
Jan  3 02:57:40.126: INFO: Deleting pod "simpletest.rc-t6qb5" in namespace "gc-4455"
Jan  3 02:57:40.184: INFO: Deleting pod "simpletest.rc-tk4f4" in namespace "gc-4455"
Jan  3 02:57:40.203: INFO: Deleting pod "simpletest.rc-tnpld" in namespace "gc-4455"
Jan  3 02:57:40.270: INFO: Deleting pod "simpletest.rc-v8gvp" in namespace "gc-4455"
Jan  3 02:57:40.366: INFO: Deleting pod "simpletest.rc-vvx7c" in namespace "gc-4455"
Jan  3 02:57:40.390: INFO: Deleting pod "simpletest.rc-w298r" in namespace "gc-4455"
Jan  3 02:57:40.425: INFO: Deleting pod "simpletest.rc-wf9rs" in namespace "gc-4455"
Jan  3 02:57:40.487: INFO: Deleting pod "simpletest.rc-wgtjg" in namespace "gc-4455"
Jan  3 02:57:40.522: INFO: Deleting pod "simpletest.rc-xcjlf" in namespace "gc-4455"
Jan  3 02:57:40.596: INFO: Deleting pod "simpletest.rc-z5t6k" in namespace "gc-4455"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan  3 02:57:40.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4455" for this suite. 01/03/23 02:57:40.665
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","completed":244,"skipped":4437,"failed":0}
------------------------------
• [SLOW TEST] [45.120 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:56:55.549
    Jan  3 02:56:55.549: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename gc 01/03/23 02:56:55.55
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:56:55.571
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:56:55.573
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 01/03/23 02:56:55.59
    STEP: delete the rc 01/03/23 02:57:00.717
    STEP: wait for the rc to be deleted 01/03/23 02:57:00.727
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 01/03/23 02:57:05.759
    STEP: Gathering metrics 01/03/23 02:57:35.775
    Jan  3 02:57:35.787: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cncf-master" in namespace "kube-system" to be "running and ready"
    Jan  3 02:57:35.792: INFO: Pod "kube-controller-manager-cncf-master": Phase="Running", Reason="", readiness=true. Elapsed: 5.02383ms
    Jan  3 02:57:35.792: INFO: The phase of Pod kube-controller-manager-cncf-master is Running (Ready = true)
    Jan  3 02:57:35.792: INFO: Pod "kube-controller-manager-cncf-master" satisfied condition "running and ready"
    Jan  3 02:57:35.849: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jan  3 02:57:35.850: INFO: Deleting pod "simpletest.rc-2bgnb" in namespace "gc-4455"
    Jan  3 02:57:35.899: INFO: Deleting pod "simpletest.rc-2p8zn" in namespace "gc-4455"
    Jan  3 02:57:35.930: INFO: Deleting pod "simpletest.rc-2xsc4" in namespace "gc-4455"
    Jan  3 02:57:35.958: INFO: Deleting pod "simpletest.rc-2znjp" in namespace "gc-4455"
    Jan  3 02:57:36.010: INFO: Deleting pod "simpletest.rc-4bfjc" in namespace "gc-4455"
    Jan  3 02:57:36.038: INFO: Deleting pod "simpletest.rc-4h4pt" in namespace "gc-4455"
    Jan  3 02:57:36.082: INFO: Deleting pod "simpletest.rc-4vbjg" in namespace "gc-4455"
    Jan  3 02:57:36.150: INFO: Deleting pod "simpletest.rc-4zl4x" in namespace "gc-4455"
    Jan  3 02:57:36.162: INFO: Deleting pod "simpletest.rc-56b2x" in namespace "gc-4455"
    Jan  3 02:57:36.193: INFO: Deleting pod "simpletest.rc-56j8q" in namespace "gc-4455"
    Jan  3 02:57:36.216: INFO: Deleting pod "simpletest.rc-5v4t4" in namespace "gc-4455"
    Jan  3 02:57:36.253: INFO: Deleting pod "simpletest.rc-5xqzb" in namespace "gc-4455"
    Jan  3 02:57:36.307: INFO: Deleting pod "simpletest.rc-5xtfm" in namespace "gc-4455"
    Jan  3 02:57:36.340: INFO: Deleting pod "simpletest.rc-5zcdz" in namespace "gc-4455"
    Jan  3 02:57:36.363: INFO: Deleting pod "simpletest.rc-6fh6q" in namespace "gc-4455"
    Jan  3 02:57:36.382: INFO: Deleting pod "simpletest.rc-6m9cg" in namespace "gc-4455"
    Jan  3 02:57:36.472: INFO: Deleting pod "simpletest.rc-6tr9z" in namespace "gc-4455"
    Jan  3 02:57:36.512: INFO: Deleting pod "simpletest.rc-6v2vc" in namespace "gc-4455"
    Jan  3 02:57:36.529: INFO: Deleting pod "simpletest.rc-7dvls" in namespace "gc-4455"
    Jan  3 02:57:36.583: INFO: Deleting pod "simpletest.rc-7kc8q" in namespace "gc-4455"
    Jan  3 02:57:36.615: INFO: Deleting pod "simpletest.rc-7n46p" in namespace "gc-4455"
    Jan  3 02:57:36.632: INFO: Deleting pod "simpletest.rc-7qt7t" in namespace "gc-4455"
    Jan  3 02:57:36.667: INFO: Deleting pod "simpletest.rc-7xw2b" in namespace "gc-4455"
    Jan  3 02:57:36.717: INFO: Deleting pod "simpletest.rc-82lmg" in namespace "gc-4455"
    Jan  3 02:57:36.743: INFO: Deleting pod "simpletest.rc-82p7n" in namespace "gc-4455"
    Jan  3 02:57:36.794: INFO: Deleting pod "simpletest.rc-8mlnw" in namespace "gc-4455"
    Jan  3 02:57:36.841: INFO: Deleting pod "simpletest.rc-8xj7p" in namespace "gc-4455"
    Jan  3 02:57:36.866: INFO: Deleting pod "simpletest.rc-9djrb" in namespace "gc-4455"
    Jan  3 02:57:36.886: INFO: Deleting pod "simpletest.rc-9fg86" in namespace "gc-4455"
    Jan  3 02:57:36.909: INFO: Deleting pod "simpletest.rc-9lsrp" in namespace "gc-4455"
    Jan  3 02:57:36.984: INFO: Deleting pod "simpletest.rc-9qqbs" in namespace "gc-4455"
    Jan  3 02:57:37.006: INFO: Deleting pod "simpletest.rc-bdx5s" in namespace "gc-4455"
    Jan  3 02:57:37.038: INFO: Deleting pod "simpletest.rc-bdzjr" in namespace "gc-4455"
    Jan  3 02:57:37.061: INFO: Deleting pod "simpletest.rc-cgttg" in namespace "gc-4455"
    Jan  3 02:57:37.111: INFO: Deleting pod "simpletest.rc-cp28h" in namespace "gc-4455"
    Jan  3 02:57:37.138: INFO: Deleting pod "simpletest.rc-d8lbr" in namespace "gc-4455"
    Jan  3 02:57:37.170: INFO: Deleting pod "simpletest.rc-d9r8l" in namespace "gc-4455"
    Jan  3 02:57:37.205: INFO: Deleting pod "simpletest.rc-dctz7" in namespace "gc-4455"
    Jan  3 02:57:37.279: INFO: Deleting pod "simpletest.rc-dhgtz" in namespace "gc-4455"
    Jan  3 02:57:37.293: INFO: Deleting pod "simpletest.rc-dqrm4" in namespace "gc-4455"
    Jan  3 02:57:37.325: INFO: Deleting pod "simpletest.rc-dwqgd" in namespace "gc-4455"
    Jan  3 02:57:37.356: INFO: Deleting pod "simpletest.rc-f96jk" in namespace "gc-4455"
    Jan  3 02:57:37.417: INFO: Deleting pod "simpletest.rc-ffvsb" in namespace "gc-4455"
    Jan  3 02:57:37.533: INFO: Deleting pod "simpletest.rc-fr4x2" in namespace "gc-4455"
    Jan  3 02:57:37.596: INFO: Deleting pod "simpletest.rc-ftkjf" in namespace "gc-4455"
    Jan  3 02:57:37.664: INFO: Deleting pod "simpletest.rc-g89p4" in namespace "gc-4455"
    Jan  3 02:57:37.682: INFO: Deleting pod "simpletest.rc-g8t9b" in namespace "gc-4455"
    Jan  3 02:57:37.756: INFO: Deleting pod "simpletest.rc-g98l9" in namespace "gc-4455"
    Jan  3 02:57:37.825: INFO: Deleting pod "simpletest.rc-gcd5v" in namespace "gc-4455"
    Jan  3 02:57:37.859: INFO: Deleting pod "simpletest.rc-gdmkl" in namespace "gc-4455"
    Jan  3 02:57:37.916: INFO: Deleting pod "simpletest.rc-gfwfr" in namespace "gc-4455"
    Jan  3 02:57:37.947: INFO: Deleting pod "simpletest.rc-gj5rl" in namespace "gc-4455"
    Jan  3 02:57:37.991: INFO: Deleting pod "simpletest.rc-gkgjk" in namespace "gc-4455"
    Jan  3 02:57:38.048: INFO: Deleting pod "simpletest.rc-gknxn" in namespace "gc-4455"
    Jan  3 02:57:38.077: INFO: Deleting pod "simpletest.rc-h6wsk" in namespace "gc-4455"
    Jan  3 02:57:38.106: INFO: Deleting pod "simpletest.rc-hbs92" in namespace "gc-4455"
    Jan  3 02:57:38.129: INFO: Deleting pod "simpletest.rc-hcbzc" in namespace "gc-4455"
    Jan  3 02:57:38.203: INFO: Deleting pod "simpletest.rc-hg78q" in namespace "gc-4455"
    Jan  3 02:57:38.278: INFO: Deleting pod "simpletest.rc-j22dl" in namespace "gc-4455"
    Jan  3 02:57:38.380: INFO: Deleting pod "simpletest.rc-j46ms" in namespace "gc-4455"
    Jan  3 02:57:38.415: INFO: Deleting pod "simpletest.rc-jkpcb" in namespace "gc-4455"
    Jan  3 02:57:38.464: INFO: Deleting pod "simpletest.rc-jnggr" in namespace "gc-4455"
    Jan  3 02:57:38.489: INFO: Deleting pod "simpletest.rc-jphwr" in namespace "gc-4455"
    Jan  3 02:57:38.535: INFO: Deleting pod "simpletest.rc-jqm7h" in namespace "gc-4455"
    Jan  3 02:57:38.564: INFO: Deleting pod "simpletest.rc-kchd2" in namespace "gc-4455"
    Jan  3 02:57:38.650: INFO: Deleting pod "simpletest.rc-khvgj" in namespace "gc-4455"
    Jan  3 02:57:38.738: INFO: Deleting pod "simpletest.rc-kkh5w" in namespace "gc-4455"
    Jan  3 02:57:38.764: INFO: Deleting pod "simpletest.rc-kl6q8" in namespace "gc-4455"
    Jan  3 02:57:38.933: INFO: Deleting pod "simpletest.rc-ld7kx" in namespace "gc-4455"
    Jan  3 02:57:38.993: INFO: Deleting pod "simpletest.rc-lhpkg" in namespace "gc-4455"
    Jan  3 02:57:39.073: INFO: Deleting pod "simpletest.rc-llbfj" in namespace "gc-4455"
    Jan  3 02:57:39.096: INFO: Deleting pod "simpletest.rc-ltdjx" in namespace "gc-4455"
    Jan  3 02:57:39.121: INFO: Deleting pod "simpletest.rc-mh95r" in namespace "gc-4455"
    Jan  3 02:57:39.213: INFO: Deleting pod "simpletest.rc-msbsl" in namespace "gc-4455"
    Jan  3 02:57:39.254: INFO: Deleting pod "simpletest.rc-n2x72" in namespace "gc-4455"
    Jan  3 02:57:39.335: INFO: Deleting pod "simpletest.rc-n6mqk" in namespace "gc-4455"
    Jan  3 02:57:39.363: INFO: Deleting pod "simpletest.rc-ncw2d" in namespace "gc-4455"
    Jan  3 02:57:39.402: INFO: Deleting pod "simpletest.rc-nfb42" in namespace "gc-4455"
    Jan  3 02:57:39.507: INFO: Deleting pod "simpletest.rc-nlmhh" in namespace "gc-4455"
    Jan  3 02:57:39.630: INFO: Deleting pod "simpletest.rc-nmtln" in namespace "gc-4455"
    Jan  3 02:57:39.656: INFO: Deleting pod "simpletest.rc-pjzs4" in namespace "gc-4455"
    Jan  3 02:57:39.714: INFO: Deleting pod "simpletest.rc-r8kpc" in namespace "gc-4455"
    Jan  3 02:57:39.806: INFO: Deleting pod "simpletest.rc-r99d9" in namespace "gc-4455"
    Jan  3 02:57:39.834: INFO: Deleting pod "simpletest.rc-r9slw" in namespace "gc-4455"
    Jan  3 02:57:39.868: INFO: Deleting pod "simpletest.rc-rkbcl" in namespace "gc-4455"
    Jan  3 02:57:39.920: INFO: Deleting pod "simpletest.rc-rmfwr" in namespace "gc-4455"
    Jan  3 02:57:39.955: INFO: Deleting pod "simpletest.rc-rs7s6" in namespace "gc-4455"
    Jan  3 02:57:40.041: INFO: Deleting pod "simpletest.rc-rttvl" in namespace "gc-4455"
    Jan  3 02:57:40.075: INFO: Deleting pod "simpletest.rc-sx5dx" in namespace "gc-4455"
    Jan  3 02:57:40.115: INFO: Deleting pod "simpletest.rc-t6hq4" in namespace "gc-4455"
    Jan  3 02:57:40.126: INFO: Deleting pod "simpletest.rc-t6qb5" in namespace "gc-4455"
    Jan  3 02:57:40.184: INFO: Deleting pod "simpletest.rc-tk4f4" in namespace "gc-4455"
    Jan  3 02:57:40.203: INFO: Deleting pod "simpletest.rc-tnpld" in namespace "gc-4455"
    Jan  3 02:57:40.270: INFO: Deleting pod "simpletest.rc-v8gvp" in namespace "gc-4455"
    Jan  3 02:57:40.366: INFO: Deleting pod "simpletest.rc-vvx7c" in namespace "gc-4455"
    Jan  3 02:57:40.390: INFO: Deleting pod "simpletest.rc-w298r" in namespace "gc-4455"
    Jan  3 02:57:40.425: INFO: Deleting pod "simpletest.rc-wf9rs" in namespace "gc-4455"
    Jan  3 02:57:40.487: INFO: Deleting pod "simpletest.rc-wgtjg" in namespace "gc-4455"
    Jan  3 02:57:40.522: INFO: Deleting pod "simpletest.rc-xcjlf" in namespace "gc-4455"
    Jan  3 02:57:40.596: INFO: Deleting pod "simpletest.rc-z5t6k" in namespace "gc-4455"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan  3 02:57:40.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-4455" for this suite. 01/03/23 02:57:40.665
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:57:40.671
Jan  3 02:57:40.671: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename runtimeclass 01/03/23 02:57:40.672
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:57:40.745
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:57:40.747
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan  3 02:57:40.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-3983" for this suite. 01/03/23 02:57:40.837
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]","completed":245,"skipped":4467,"failed":0}
------------------------------
• [0.200 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:57:40.671
    Jan  3 02:57:40.671: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename runtimeclass 01/03/23 02:57:40.672
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:57:40.745
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:57:40.747
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan  3 02:57:40.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-3983" for this suite. 01/03/23 02:57:40.837
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:57:40.871
Jan  3 02:57:40.871: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename secrets 01/03/23 02:57:40.872
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:57:40.895
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:57:40.897
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
STEP: Creating secret with name s-test-opt-del-9063de58-fd53-48a8-b799-c63a7102c297 01/03/23 02:57:40.906
STEP: Creating secret with name s-test-opt-upd-a38221ec-0745-4a18-b0e2-42a3dbf1d2f0 01/03/23 02:57:40.91
STEP: Creating the pod 01/03/23 02:57:40.936
Jan  3 02:57:40.967: INFO: Waiting up to 5m0s for pod "pod-secrets-fbd7ae1e-6f01-4d97-875e-ec4d6cbf48e7" in namespace "secrets-1820" to be "running and ready"
Jan  3 02:57:41.007: INFO: Pod "pod-secrets-fbd7ae1e-6f01-4d97-875e-ec4d6cbf48e7": Phase="Pending", Reason="", readiness=false. Elapsed: 39.637105ms
Jan  3 02:57:41.007: INFO: The phase of Pod pod-secrets-fbd7ae1e-6f01-4d97-875e-ec4d6cbf48e7 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 02:57:43.022: INFO: Pod "pod-secrets-fbd7ae1e-6f01-4d97-875e-ec4d6cbf48e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054357019s
Jan  3 02:57:43.022: INFO: The phase of Pod pod-secrets-fbd7ae1e-6f01-4d97-875e-ec4d6cbf48e7 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 02:57:45.009: INFO: Pod "pod-secrets-fbd7ae1e-6f01-4d97-875e-ec4d6cbf48e7": Phase="Running", Reason="", readiness=true. Elapsed: 4.042072974s
Jan  3 02:57:45.010: INFO: The phase of Pod pod-secrets-fbd7ae1e-6f01-4d97-875e-ec4d6cbf48e7 is Running (Ready = true)
Jan  3 02:57:45.010: INFO: Pod "pod-secrets-fbd7ae1e-6f01-4d97-875e-ec4d6cbf48e7" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-9063de58-fd53-48a8-b799-c63a7102c297 01/03/23 02:57:45.02
STEP: Updating secret s-test-opt-upd-a38221ec-0745-4a18-b0e2-42a3dbf1d2f0 01/03/23 02:57:45.023
STEP: Creating secret with name s-test-opt-create-548f0e67-7951-49b4-afbc-79d10966cc00 01/03/23 02:57:45.028
STEP: waiting to observe update in volume 01/03/23 02:57:45.033
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan  3 02:57:47.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1820" for this suite. 01/03/23 02:57:47.138
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":246,"skipped":4472,"failed":0}
------------------------------
• [SLOW TEST] [6.313 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:57:40.871
    Jan  3 02:57:40.871: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename secrets 01/03/23 02:57:40.872
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:57:40.895
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:57:40.897
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:204
    STEP: Creating secret with name s-test-opt-del-9063de58-fd53-48a8-b799-c63a7102c297 01/03/23 02:57:40.906
    STEP: Creating secret with name s-test-opt-upd-a38221ec-0745-4a18-b0e2-42a3dbf1d2f0 01/03/23 02:57:40.91
    STEP: Creating the pod 01/03/23 02:57:40.936
    Jan  3 02:57:40.967: INFO: Waiting up to 5m0s for pod "pod-secrets-fbd7ae1e-6f01-4d97-875e-ec4d6cbf48e7" in namespace "secrets-1820" to be "running and ready"
    Jan  3 02:57:41.007: INFO: Pod "pod-secrets-fbd7ae1e-6f01-4d97-875e-ec4d6cbf48e7": Phase="Pending", Reason="", readiness=false. Elapsed: 39.637105ms
    Jan  3 02:57:41.007: INFO: The phase of Pod pod-secrets-fbd7ae1e-6f01-4d97-875e-ec4d6cbf48e7 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 02:57:43.022: INFO: Pod "pod-secrets-fbd7ae1e-6f01-4d97-875e-ec4d6cbf48e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054357019s
    Jan  3 02:57:43.022: INFO: The phase of Pod pod-secrets-fbd7ae1e-6f01-4d97-875e-ec4d6cbf48e7 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 02:57:45.009: INFO: Pod "pod-secrets-fbd7ae1e-6f01-4d97-875e-ec4d6cbf48e7": Phase="Running", Reason="", readiness=true. Elapsed: 4.042072974s
    Jan  3 02:57:45.010: INFO: The phase of Pod pod-secrets-fbd7ae1e-6f01-4d97-875e-ec4d6cbf48e7 is Running (Ready = true)
    Jan  3 02:57:45.010: INFO: Pod "pod-secrets-fbd7ae1e-6f01-4d97-875e-ec4d6cbf48e7" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-9063de58-fd53-48a8-b799-c63a7102c297 01/03/23 02:57:45.02
    STEP: Updating secret s-test-opt-upd-a38221ec-0745-4a18-b0e2-42a3dbf1d2f0 01/03/23 02:57:45.023
    STEP: Creating secret with name s-test-opt-create-548f0e67-7951-49b4-afbc-79d10966cc00 01/03/23 02:57:45.028
    STEP: waiting to observe update in volume 01/03/23 02:57:45.033
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan  3 02:57:47.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-1820" for this suite. 01/03/23 02:57:47.138
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:57:47.186
Jan  3 02:57:47.186: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename container-probe 01/03/23 02:57:47.187
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:57:47.256
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:57:47.259
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
Jan  3 02:57:47.320: INFO: Waiting up to 5m0s for pod "test-webserver-38df161e-ad87-4cdf-8ba6-56a5251f2d26" in namespace "container-probe-7819" to be "running and ready"
Jan  3 02:57:47.360: INFO: Pod "test-webserver-38df161e-ad87-4cdf-8ba6-56a5251f2d26": Phase="Pending", Reason="", readiness=false. Elapsed: 40.112307ms
Jan  3 02:57:47.360: INFO: The phase of Pod test-webserver-38df161e-ad87-4cdf-8ba6-56a5251f2d26 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 02:57:49.371: INFO: Pod "test-webserver-38df161e-ad87-4cdf-8ba6-56a5251f2d26": Phase="Running", Reason="", readiness=false. Elapsed: 2.051873165s
Jan  3 02:57:49.371: INFO: The phase of Pod test-webserver-38df161e-ad87-4cdf-8ba6-56a5251f2d26 is Running (Ready = false)
Jan  3 02:57:51.395: INFO: Pod "test-webserver-38df161e-ad87-4cdf-8ba6-56a5251f2d26": Phase="Running", Reason="", readiness=false. Elapsed: 4.075742818s
Jan  3 02:57:51.395: INFO: The phase of Pod test-webserver-38df161e-ad87-4cdf-8ba6-56a5251f2d26 is Running (Ready = false)
Jan  3 02:57:53.363: INFO: Pod "test-webserver-38df161e-ad87-4cdf-8ba6-56a5251f2d26": Phase="Running", Reason="", readiness=false. Elapsed: 6.043011316s
Jan  3 02:57:53.363: INFO: The phase of Pod test-webserver-38df161e-ad87-4cdf-8ba6-56a5251f2d26 is Running (Ready = false)
Jan  3 02:57:55.363: INFO: Pod "test-webserver-38df161e-ad87-4cdf-8ba6-56a5251f2d26": Phase="Running", Reason="", readiness=false. Elapsed: 8.043800431s
Jan  3 02:57:55.363: INFO: The phase of Pod test-webserver-38df161e-ad87-4cdf-8ba6-56a5251f2d26 is Running (Ready = false)
Jan  3 02:57:57.362: INFO: Pod "test-webserver-38df161e-ad87-4cdf-8ba6-56a5251f2d26": Phase="Running", Reason="", readiness=false. Elapsed: 10.042811774s
Jan  3 02:57:57.362: INFO: The phase of Pod test-webserver-38df161e-ad87-4cdf-8ba6-56a5251f2d26 is Running (Ready = false)
Jan  3 02:57:59.363: INFO: Pod "test-webserver-38df161e-ad87-4cdf-8ba6-56a5251f2d26": Phase="Running", Reason="", readiness=false. Elapsed: 12.043560683s
Jan  3 02:57:59.363: INFO: The phase of Pod test-webserver-38df161e-ad87-4cdf-8ba6-56a5251f2d26 is Running (Ready = false)
Jan  3 02:58:01.363: INFO: Pod "test-webserver-38df161e-ad87-4cdf-8ba6-56a5251f2d26": Phase="Running", Reason="", readiness=false. Elapsed: 14.043608512s
Jan  3 02:58:01.363: INFO: The phase of Pod test-webserver-38df161e-ad87-4cdf-8ba6-56a5251f2d26 is Running (Ready = false)
Jan  3 02:58:03.364: INFO: Pod "test-webserver-38df161e-ad87-4cdf-8ba6-56a5251f2d26": Phase="Running", Reason="", readiness=false. Elapsed: 16.044038096s
Jan  3 02:58:03.364: INFO: The phase of Pod test-webserver-38df161e-ad87-4cdf-8ba6-56a5251f2d26 is Running (Ready = false)
Jan  3 02:58:05.364: INFO: Pod "test-webserver-38df161e-ad87-4cdf-8ba6-56a5251f2d26": Phase="Running", Reason="", readiness=false. Elapsed: 18.044095819s
Jan  3 02:58:05.364: INFO: The phase of Pod test-webserver-38df161e-ad87-4cdf-8ba6-56a5251f2d26 is Running (Ready = false)
Jan  3 02:58:07.363: INFO: Pod "test-webserver-38df161e-ad87-4cdf-8ba6-56a5251f2d26": Phase="Running", Reason="", readiness=false. Elapsed: 20.043723736s
Jan  3 02:58:07.363: INFO: The phase of Pod test-webserver-38df161e-ad87-4cdf-8ba6-56a5251f2d26 is Running (Ready = false)
Jan  3 02:58:09.363: INFO: Pod "test-webserver-38df161e-ad87-4cdf-8ba6-56a5251f2d26": Phase="Running", Reason="", readiness=true. Elapsed: 22.04332862s
Jan  3 02:58:09.363: INFO: The phase of Pod test-webserver-38df161e-ad87-4cdf-8ba6-56a5251f2d26 is Running (Ready = true)
Jan  3 02:58:09.363: INFO: Pod "test-webserver-38df161e-ad87-4cdf-8ba6-56a5251f2d26" satisfied condition "running and ready"
Jan  3 02:58:09.365: INFO: Container started at 2023-01-03 02:57:48 +0000 UTC, pod became ready at 2023-01-03 02:58:07 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan  3 02:58:09.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7819" for this suite. 01/03/23 02:58:09.368
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","completed":247,"skipped":4515,"failed":0}
------------------------------
• [SLOW TEST] [22.185 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:57:47.186
    Jan  3 02:57:47.186: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename container-probe 01/03/23 02:57:47.187
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:57:47.256
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:57:47.259
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:68
    Jan  3 02:57:47.320: INFO: Waiting up to 5m0s for pod "test-webserver-38df161e-ad87-4cdf-8ba6-56a5251f2d26" in namespace "container-probe-7819" to be "running and ready"
    Jan  3 02:57:47.360: INFO: Pod "test-webserver-38df161e-ad87-4cdf-8ba6-56a5251f2d26": Phase="Pending", Reason="", readiness=false. Elapsed: 40.112307ms
    Jan  3 02:57:47.360: INFO: The phase of Pod test-webserver-38df161e-ad87-4cdf-8ba6-56a5251f2d26 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 02:57:49.371: INFO: Pod "test-webserver-38df161e-ad87-4cdf-8ba6-56a5251f2d26": Phase="Running", Reason="", readiness=false. Elapsed: 2.051873165s
    Jan  3 02:57:49.371: INFO: The phase of Pod test-webserver-38df161e-ad87-4cdf-8ba6-56a5251f2d26 is Running (Ready = false)
    Jan  3 02:57:51.395: INFO: Pod "test-webserver-38df161e-ad87-4cdf-8ba6-56a5251f2d26": Phase="Running", Reason="", readiness=false. Elapsed: 4.075742818s
    Jan  3 02:57:51.395: INFO: The phase of Pod test-webserver-38df161e-ad87-4cdf-8ba6-56a5251f2d26 is Running (Ready = false)
    Jan  3 02:57:53.363: INFO: Pod "test-webserver-38df161e-ad87-4cdf-8ba6-56a5251f2d26": Phase="Running", Reason="", readiness=false. Elapsed: 6.043011316s
    Jan  3 02:57:53.363: INFO: The phase of Pod test-webserver-38df161e-ad87-4cdf-8ba6-56a5251f2d26 is Running (Ready = false)
    Jan  3 02:57:55.363: INFO: Pod "test-webserver-38df161e-ad87-4cdf-8ba6-56a5251f2d26": Phase="Running", Reason="", readiness=false. Elapsed: 8.043800431s
    Jan  3 02:57:55.363: INFO: The phase of Pod test-webserver-38df161e-ad87-4cdf-8ba6-56a5251f2d26 is Running (Ready = false)
    Jan  3 02:57:57.362: INFO: Pod "test-webserver-38df161e-ad87-4cdf-8ba6-56a5251f2d26": Phase="Running", Reason="", readiness=false. Elapsed: 10.042811774s
    Jan  3 02:57:57.362: INFO: The phase of Pod test-webserver-38df161e-ad87-4cdf-8ba6-56a5251f2d26 is Running (Ready = false)
    Jan  3 02:57:59.363: INFO: Pod "test-webserver-38df161e-ad87-4cdf-8ba6-56a5251f2d26": Phase="Running", Reason="", readiness=false. Elapsed: 12.043560683s
    Jan  3 02:57:59.363: INFO: The phase of Pod test-webserver-38df161e-ad87-4cdf-8ba6-56a5251f2d26 is Running (Ready = false)
    Jan  3 02:58:01.363: INFO: Pod "test-webserver-38df161e-ad87-4cdf-8ba6-56a5251f2d26": Phase="Running", Reason="", readiness=false. Elapsed: 14.043608512s
    Jan  3 02:58:01.363: INFO: The phase of Pod test-webserver-38df161e-ad87-4cdf-8ba6-56a5251f2d26 is Running (Ready = false)
    Jan  3 02:58:03.364: INFO: Pod "test-webserver-38df161e-ad87-4cdf-8ba6-56a5251f2d26": Phase="Running", Reason="", readiness=false. Elapsed: 16.044038096s
    Jan  3 02:58:03.364: INFO: The phase of Pod test-webserver-38df161e-ad87-4cdf-8ba6-56a5251f2d26 is Running (Ready = false)
    Jan  3 02:58:05.364: INFO: Pod "test-webserver-38df161e-ad87-4cdf-8ba6-56a5251f2d26": Phase="Running", Reason="", readiness=false. Elapsed: 18.044095819s
    Jan  3 02:58:05.364: INFO: The phase of Pod test-webserver-38df161e-ad87-4cdf-8ba6-56a5251f2d26 is Running (Ready = false)
    Jan  3 02:58:07.363: INFO: Pod "test-webserver-38df161e-ad87-4cdf-8ba6-56a5251f2d26": Phase="Running", Reason="", readiness=false. Elapsed: 20.043723736s
    Jan  3 02:58:07.363: INFO: The phase of Pod test-webserver-38df161e-ad87-4cdf-8ba6-56a5251f2d26 is Running (Ready = false)
    Jan  3 02:58:09.363: INFO: Pod "test-webserver-38df161e-ad87-4cdf-8ba6-56a5251f2d26": Phase="Running", Reason="", readiness=true. Elapsed: 22.04332862s
    Jan  3 02:58:09.363: INFO: The phase of Pod test-webserver-38df161e-ad87-4cdf-8ba6-56a5251f2d26 is Running (Ready = true)
    Jan  3 02:58:09.363: INFO: Pod "test-webserver-38df161e-ad87-4cdf-8ba6-56a5251f2d26" satisfied condition "running and ready"
    Jan  3 02:58:09.365: INFO: Container started at 2023-01-03 02:57:48 +0000 UTC, pod became ready at 2023-01-03 02:58:07 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan  3 02:58:09.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-7819" for this suite. 01/03/23 02:58:09.368
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:58:09.372
Jan  3 02:58:09.373: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename init-container 01/03/23 02:58:09.373
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:58:09.391
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:58:09.392
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
STEP: creating the pod 01/03/23 02:58:09.394
Jan  3 02:58:09.395: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jan  3 02:58:12.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7435" for this suite. 01/03/23 02:58:12.352
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","completed":248,"skipped":4546,"failed":0}
------------------------------
• [2.982 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:58:09.372
    Jan  3 02:58:09.373: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename init-container 01/03/23 02:58:09.373
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:58:09.391
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:58:09.392
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:254
    STEP: creating the pod 01/03/23 02:58:09.394
    Jan  3 02:58:09.395: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan  3 02:58:12.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-7435" for this suite. 01/03/23 02:58:12.352
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:58:12.356
Jan  3 02:58:12.356: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename podtemplate 01/03/23 02:58:12.356
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:58:12.374
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:58:12.375
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 01/03/23 02:58:12.377
STEP: Replace a pod template 01/03/23 02:58:12.391
Jan  3 02:58:12.399: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Jan  3 02:58:12.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-863" for this suite. 01/03/23 02:58:12.403
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] PodTemplates should replace a pod template [Conformance]","completed":249,"skipped":4557,"failed":0}
------------------------------
• [0.050 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:58:12.356
    Jan  3 02:58:12.356: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename podtemplate 01/03/23 02:58:12.356
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:58:12.374
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:58:12.375
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 01/03/23 02:58:12.377
    STEP: Replace a pod template 01/03/23 02:58:12.391
    Jan  3 02:58:12.399: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Jan  3 02:58:12.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-863" for this suite. 01/03/23 02:58:12.403
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:58:12.406
Jan  3 02:58:12.406: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename crd-publish-openapi 01/03/23 02:58:12.407
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:58:12.425
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:58:12.427
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
Jan  3 02:58:12.428: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/03/23 02:58:18.417
Jan  3 02:58:18.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-8848 --namespace=crd-publish-openapi-8848 create -f -'
Jan  3 02:58:19.394: INFO: stderr: ""
Jan  3 02:58:19.394: INFO: stdout: "e2e-test-crd-publish-openapi-2342-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan  3 02:58:19.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-8848 --namespace=crd-publish-openapi-8848 delete e2e-test-crd-publish-openapi-2342-crds test-cr'
Jan  3 02:58:19.456: INFO: stderr: ""
Jan  3 02:58:19.456: INFO: stdout: "e2e-test-crd-publish-openapi-2342-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jan  3 02:58:19.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-8848 --namespace=crd-publish-openapi-8848 apply -f -'
Jan  3 02:58:19.671: INFO: stderr: ""
Jan  3 02:58:19.671: INFO: stdout: "e2e-test-crd-publish-openapi-2342-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan  3 02:58:19.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-8848 --namespace=crd-publish-openapi-8848 delete e2e-test-crd-publish-openapi-2342-crds test-cr'
Jan  3 02:58:19.738: INFO: stderr: ""
Jan  3 02:58:19.738: INFO: stdout: "e2e-test-crd-publish-openapi-2342-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 01/03/23 02:58:19.738
Jan  3 02:58:19.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-8848 explain e2e-test-crd-publish-openapi-2342-crds'
Jan  3 02:58:19.949: INFO: stderr: ""
Jan  3 02:58:19.949: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2342-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  3 02:58:25.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8848" for this suite. 01/03/23 02:58:25.319
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","completed":250,"skipped":4564,"failed":0}
------------------------------
• [SLOW TEST] [12.915 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:58:12.406
    Jan  3 02:58:12.406: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename crd-publish-openapi 01/03/23 02:58:12.407
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:58:12.425
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:58:12.427
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:152
    Jan  3 02:58:12.428: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/03/23 02:58:18.417
    Jan  3 02:58:18.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-8848 --namespace=crd-publish-openapi-8848 create -f -'
    Jan  3 02:58:19.394: INFO: stderr: ""
    Jan  3 02:58:19.394: INFO: stdout: "e2e-test-crd-publish-openapi-2342-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jan  3 02:58:19.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-8848 --namespace=crd-publish-openapi-8848 delete e2e-test-crd-publish-openapi-2342-crds test-cr'
    Jan  3 02:58:19.456: INFO: stderr: ""
    Jan  3 02:58:19.456: INFO: stdout: "e2e-test-crd-publish-openapi-2342-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Jan  3 02:58:19.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-8848 --namespace=crd-publish-openapi-8848 apply -f -'
    Jan  3 02:58:19.671: INFO: stderr: ""
    Jan  3 02:58:19.671: INFO: stdout: "e2e-test-crd-publish-openapi-2342-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jan  3 02:58:19.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-8848 --namespace=crd-publish-openapi-8848 delete e2e-test-crd-publish-openapi-2342-crds test-cr'
    Jan  3 02:58:19.738: INFO: stderr: ""
    Jan  3 02:58:19.738: INFO: stdout: "e2e-test-crd-publish-openapi-2342-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 01/03/23 02:58:19.738
    Jan  3 02:58:19.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=crd-publish-openapi-8848 explain e2e-test-crd-publish-openapi-2342-crds'
    Jan  3 02:58:19.949: INFO: stderr: ""
    Jan  3 02:58:19.949: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2342-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  3 02:58:25.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-8848" for this suite. 01/03/23 02:58:25.319
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:58:25.322
Jan  3 02:58:25.322: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename containers 01/03/23 02:58:25.323
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:58:25.344
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:58:25.346
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
STEP: Creating a pod to test override all 01/03/23 02:58:25.347
Jan  3 02:58:25.352: INFO: Waiting up to 5m0s for pod "client-containers-32dce3a7-9578-406e-872e-5614694ef602" in namespace "containers-8779" to be "Succeeded or Failed"
Jan  3 02:58:25.389: INFO: Pod "client-containers-32dce3a7-9578-406e-872e-5614694ef602": Phase="Pending", Reason="", readiness=false. Elapsed: 36.322231ms
Jan  3 02:58:27.392: INFO: Pod "client-containers-32dce3a7-9578-406e-872e-5614694ef602": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039355594s
Jan  3 02:58:29.393: INFO: Pod "client-containers-32dce3a7-9578-406e-872e-5614694ef602": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040485298s
STEP: Saw pod success 01/03/23 02:58:29.393
Jan  3 02:58:29.393: INFO: Pod "client-containers-32dce3a7-9578-406e-872e-5614694ef602" satisfied condition "Succeeded or Failed"
Jan  3 02:58:29.394: INFO: Trying to get logs from node cncf-master pod client-containers-32dce3a7-9578-406e-872e-5614694ef602 container agnhost-container: <nil>
STEP: delete the pod 01/03/23 02:58:29.398
Jan  3 02:58:29.435: INFO: Waiting for pod client-containers-32dce3a7-9578-406e-872e-5614694ef602 to disappear
Jan  3 02:58:29.453: INFO: Pod client-containers-32dce3a7-9578-406e-872e-5614694ef602 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jan  3 02:58:29.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8779" for this suite. 01/03/23 02:58:29.456
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","completed":251,"skipped":4598,"failed":0}
------------------------------
• [4.136 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:58:25.322
    Jan  3 02:58:25.322: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename containers 01/03/23 02:58:25.323
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:58:25.344
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:58:25.346
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:86
    STEP: Creating a pod to test override all 01/03/23 02:58:25.347
    Jan  3 02:58:25.352: INFO: Waiting up to 5m0s for pod "client-containers-32dce3a7-9578-406e-872e-5614694ef602" in namespace "containers-8779" to be "Succeeded or Failed"
    Jan  3 02:58:25.389: INFO: Pod "client-containers-32dce3a7-9578-406e-872e-5614694ef602": Phase="Pending", Reason="", readiness=false. Elapsed: 36.322231ms
    Jan  3 02:58:27.392: INFO: Pod "client-containers-32dce3a7-9578-406e-872e-5614694ef602": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039355594s
    Jan  3 02:58:29.393: INFO: Pod "client-containers-32dce3a7-9578-406e-872e-5614694ef602": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040485298s
    STEP: Saw pod success 01/03/23 02:58:29.393
    Jan  3 02:58:29.393: INFO: Pod "client-containers-32dce3a7-9578-406e-872e-5614694ef602" satisfied condition "Succeeded or Failed"
    Jan  3 02:58:29.394: INFO: Trying to get logs from node cncf-master pod client-containers-32dce3a7-9578-406e-872e-5614694ef602 container agnhost-container: <nil>
    STEP: delete the pod 01/03/23 02:58:29.398
    Jan  3 02:58:29.435: INFO: Waiting for pod client-containers-32dce3a7-9578-406e-872e-5614694ef602 to disappear
    Jan  3 02:58:29.453: INFO: Pod client-containers-32dce3a7-9578-406e-872e-5614694ef602 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jan  3 02:58:29.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-8779" for this suite. 01/03/23 02:58:29.456
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:58:29.459
Jan  3 02:58:29.459: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename daemonsets 01/03/23 02:58:29.46
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:58:29.467
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:58:29.469
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
Jan  3 02:58:29.502: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 01/03/23 02:58:29.506
Jan  3 02:58:29.522: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  3 02:58:29.522: INFO: Node cncf-master is running 0 daemon pod, expected 1
Jan  3 02:58:30.536: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  3 02:58:30.536: INFO: Node cncf-master is running 0 daemon pod, expected 1
Jan  3 02:58:31.528: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan  3 02:58:31.528: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image. 01/03/23 02:58:31.534
STEP: Check that daemon pods images are updated. 01/03/23 02:58:31.542
Jan  3 02:58:31.559: INFO: Wrong image for pod: daemon-set-pd8dl. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan  3 02:58:31.559: INFO: Wrong image for pod: daemon-set-td6bg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan  3 02:58:31.559: INFO: Wrong image for pod: daemon-set-tjlj2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan  3 02:58:32.575: INFO: Wrong image for pod: daemon-set-td6bg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan  3 02:58:32.575: INFO: Wrong image for pod: daemon-set-tjlj2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan  3 02:58:33.573: INFO: Pod daemon-set-m9hzg is not available
Jan  3 02:58:33.573: INFO: Wrong image for pod: daemon-set-td6bg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan  3 02:58:33.573: INFO: Wrong image for pod: daemon-set-tjlj2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan  3 02:58:34.577: INFO: Wrong image for pod: daemon-set-td6bg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan  3 02:58:35.573: INFO: Pod daemon-set-d77kk is not available
Jan  3 02:58:35.573: INFO: Wrong image for pod: daemon-set-td6bg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan  3 02:58:38.574: INFO: Pod daemon-set-lwzt6 is not available
STEP: Check that daemon pods are still running on every node of the cluster. 01/03/23 02:58:38.576
Jan  3 02:58:38.581: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan  3 02:58:38.581: INFO: Node cncf-node1 is running 0 daemon pod, expected 1
Jan  3 02:58:39.587: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan  3 02:58:39.587: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/03/23 02:58:39.595
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7369, will wait for the garbage collector to delete the pods 01/03/23 02:58:39.595
Jan  3 02:58:39.650: INFO: Deleting DaemonSet.extensions daemon-set took: 2.734605ms
Jan  3 02:58:39.750: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.436718ms
Jan  3 02:58:41.765: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  3 02:58:41.765: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan  3 02:58:41.767: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1008074"},"items":null}

Jan  3 02:58:41.768: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1008074"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan  3 02:58:41.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7369" for this suite. 01/03/23 02:58:41.779
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","completed":252,"skipped":4620,"failed":0}
------------------------------
• [SLOW TEST] [12.322 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:58:29.459
    Jan  3 02:58:29.459: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename daemonsets 01/03/23 02:58:29.46
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:58:29.467
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:58:29.469
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:373
    Jan  3 02:58:29.502: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 01/03/23 02:58:29.506
    Jan  3 02:58:29.522: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  3 02:58:29.522: INFO: Node cncf-master is running 0 daemon pod, expected 1
    Jan  3 02:58:30.536: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  3 02:58:30.536: INFO: Node cncf-master is running 0 daemon pod, expected 1
    Jan  3 02:58:31.528: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan  3 02:58:31.528: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Update daemon pods image. 01/03/23 02:58:31.534
    STEP: Check that daemon pods images are updated. 01/03/23 02:58:31.542
    Jan  3 02:58:31.559: INFO: Wrong image for pod: daemon-set-pd8dl. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan  3 02:58:31.559: INFO: Wrong image for pod: daemon-set-td6bg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan  3 02:58:31.559: INFO: Wrong image for pod: daemon-set-tjlj2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan  3 02:58:32.575: INFO: Wrong image for pod: daemon-set-td6bg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan  3 02:58:32.575: INFO: Wrong image for pod: daemon-set-tjlj2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan  3 02:58:33.573: INFO: Pod daemon-set-m9hzg is not available
    Jan  3 02:58:33.573: INFO: Wrong image for pod: daemon-set-td6bg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan  3 02:58:33.573: INFO: Wrong image for pod: daemon-set-tjlj2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan  3 02:58:34.577: INFO: Wrong image for pod: daemon-set-td6bg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan  3 02:58:35.573: INFO: Pod daemon-set-d77kk is not available
    Jan  3 02:58:35.573: INFO: Wrong image for pod: daemon-set-td6bg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan  3 02:58:38.574: INFO: Pod daemon-set-lwzt6 is not available
    STEP: Check that daemon pods are still running on every node of the cluster. 01/03/23 02:58:38.576
    Jan  3 02:58:38.581: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan  3 02:58:38.581: INFO: Node cncf-node1 is running 0 daemon pod, expected 1
    Jan  3 02:58:39.587: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan  3 02:58:39.587: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/03/23 02:58:39.595
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7369, will wait for the garbage collector to delete the pods 01/03/23 02:58:39.595
    Jan  3 02:58:39.650: INFO: Deleting DaemonSet.extensions daemon-set took: 2.734605ms
    Jan  3 02:58:39.750: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.436718ms
    Jan  3 02:58:41.765: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  3 02:58:41.765: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan  3 02:58:41.767: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1008074"},"items":null}

    Jan  3 02:58:41.768: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1008074"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan  3 02:58:41.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-7369" for this suite. 01/03/23 02:58:41.779
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:58:41.782
Jan  3 02:58:41.782: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename dns 01/03/23 02:58:41.783
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:58:41.795
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:58:41.797
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 01/03/23 02:58:41.799
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4436 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4436;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4436 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4436;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4436.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4436.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4436.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4436.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4436.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4436.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4436.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4436.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4436.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4436.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4436.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4436.svc;check="$$(dig +notcp +noall +answer +search 34.88.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.88.34_udp@PTR;check="$$(dig +tcp +noall +answer +search 34.88.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.88.34_tcp@PTR;sleep 1; done
 01/03/23 02:58:41.843
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4436 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4436;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4436 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4436;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4436.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4436.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4436.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4436.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4436.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4436.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4436.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4436.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4436.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4436.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4436.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4436.svc;check="$$(dig +notcp +noall +answer +search 34.88.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.88.34_udp@PTR;check="$$(dig +tcp +noall +answer +search 34.88.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.88.34_tcp@PTR;sleep 1; done
 01/03/23 02:58:41.843
STEP: creating a pod to probe DNS 01/03/23 02:58:41.843
STEP: submitting the pod to kubernetes 01/03/23 02:58:41.843
Jan  3 02:58:41.897: INFO: Waiting up to 15m0s for pod "dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0" in namespace "dns-4436" to be "running"
Jan  3 02:58:41.899: INFO: Pod "dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.727355ms
Jan  3 02:58:43.901: INFO: Pod "dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0": Phase="Running", Reason="", readiness=true. Elapsed: 2.004055947s
Jan  3 02:58:43.901: INFO: Pod "dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0" satisfied condition "running"
STEP: retrieving the pod 01/03/23 02:58:43.901
STEP: looking for the results for each expected name from probers 01/03/23 02:58:43.903
Jan  3 02:58:43.905: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:58:43.907: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:58:43.909: INFO: Unable to read wheezy_udp@dns-test-service.dns-4436 from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:58:43.911: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4436 from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:58:43.912: INFO: Unable to read wheezy_udp@dns-test-service.dns-4436.svc from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:58:43.914: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4436.svc from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:58:43.927: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:58:43.929: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:58:43.931: INFO: Unable to read jessie_udp@dns-test-service.dns-4436 from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:58:43.932: INFO: Unable to read jessie_tcp@dns-test-service.dns-4436 from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:58:43.934: INFO: Unable to read jessie_udp@dns-test-service.dns-4436.svc from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:58:43.936: INFO: Unable to read jessie_tcp@dns-test-service.dns-4436.svc from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:58:43.946: INFO: Lookups using dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4436 wheezy_tcp@dns-test-service.dns-4436 wheezy_udp@dns-test-service.dns-4436.svc wheezy_tcp@dns-test-service.dns-4436.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4436 jessie_tcp@dns-test-service.dns-4436 jessie_udp@dns-test-service.dns-4436.svc jessie_tcp@dns-test-service.dns-4436.svc]

Jan  3 02:58:48.951: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:58:48.953: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:58:48.955: INFO: Unable to read wheezy_udp@dns-test-service.dns-4436 from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:58:48.957: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4436 from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:58:48.958: INFO: Unable to read wheezy_udp@dns-test-service.dns-4436.svc from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:58:48.960: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4436.svc from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:58:48.972: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:58:48.973: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:58:48.975: INFO: Unable to read jessie_udp@dns-test-service.dns-4436 from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:58:48.976: INFO: Unable to read jessie_tcp@dns-test-service.dns-4436 from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:58:48.978: INFO: Unable to read jessie_udp@dns-test-service.dns-4436.svc from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:58:48.980: INFO: Unable to read jessie_tcp@dns-test-service.dns-4436.svc from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:58:48.990: INFO: Lookups using dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4436 wheezy_tcp@dns-test-service.dns-4436 wheezy_udp@dns-test-service.dns-4436.svc wheezy_tcp@dns-test-service.dns-4436.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4436 jessie_tcp@dns-test-service.dns-4436 jessie_udp@dns-test-service.dns-4436.svc jessie_tcp@dns-test-service.dns-4436.svc]

Jan  3 02:58:53.951: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:58:53.953: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:58:53.955: INFO: Unable to read wheezy_udp@dns-test-service.dns-4436 from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:58:53.956: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4436 from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:58:53.958: INFO: Unable to read wheezy_udp@dns-test-service.dns-4436.svc from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:58:53.960: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4436.svc from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:58:53.973: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:58:53.974: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:58:53.976: INFO: Unable to read jessie_udp@dns-test-service.dns-4436 from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:58:53.978: INFO: Unable to read jessie_tcp@dns-test-service.dns-4436 from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:58:53.979: INFO: Unable to read jessie_udp@dns-test-service.dns-4436.svc from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:58:53.981: INFO: Unable to read jessie_tcp@dns-test-service.dns-4436.svc from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:58:53.991: INFO: Lookups using dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4436 wheezy_tcp@dns-test-service.dns-4436 wheezy_udp@dns-test-service.dns-4436.svc wheezy_tcp@dns-test-service.dns-4436.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4436 jessie_tcp@dns-test-service.dns-4436 jessie_udp@dns-test-service.dns-4436.svc jessie_tcp@dns-test-service.dns-4436.svc]

Jan  3 02:58:58.971: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:58:58.973: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:58:58.979: INFO: Unable to read wheezy_udp@dns-test-service.dns-4436 from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:58:58.985: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4436 from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:58:58.987: INFO: Unable to read wheezy_udp@dns-test-service.dns-4436.svc from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:58:58.989: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4436.svc from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:58:59.028: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:58:59.030: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:58:59.032: INFO: Unable to read jessie_udp@dns-test-service.dns-4436 from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:58:59.033: INFO: Unable to read jessie_tcp@dns-test-service.dns-4436 from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:58:59.035: INFO: Unable to read jessie_udp@dns-test-service.dns-4436.svc from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:58:59.037: INFO: Unable to read jessie_tcp@dns-test-service.dns-4436.svc from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:58:59.047: INFO: Lookups using dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4436 wheezy_tcp@dns-test-service.dns-4436 wheezy_udp@dns-test-service.dns-4436.svc wheezy_tcp@dns-test-service.dns-4436.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4436 jessie_tcp@dns-test-service.dns-4436 jessie_udp@dns-test-service.dns-4436.svc jessie_tcp@dns-test-service.dns-4436.svc]

Jan  3 02:59:03.951: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:59:03.953: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:59:03.955: INFO: Unable to read wheezy_udp@dns-test-service.dns-4436 from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:59:03.957: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4436 from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:59:03.959: INFO: Unable to read wheezy_udp@dns-test-service.dns-4436.svc from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:59:03.961: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4436.svc from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:59:03.973: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:59:03.975: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:59:03.976: INFO: Unable to read jessie_udp@dns-test-service.dns-4436 from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:59:03.978: INFO: Unable to read jessie_tcp@dns-test-service.dns-4436 from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:59:03.980: INFO: Unable to read jessie_udp@dns-test-service.dns-4436.svc from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:59:03.982: INFO: Unable to read jessie_tcp@dns-test-service.dns-4436.svc from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:59:03.992: INFO: Lookups using dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4436 wheezy_tcp@dns-test-service.dns-4436 wheezy_udp@dns-test-service.dns-4436.svc wheezy_tcp@dns-test-service.dns-4436.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4436 jessie_tcp@dns-test-service.dns-4436 jessie_udp@dns-test-service.dns-4436.svc jessie_tcp@dns-test-service.dns-4436.svc]

Jan  3 02:59:08.949: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:59:08.951: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:59:08.953: INFO: Unable to read wheezy_udp@dns-test-service.dns-4436 from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:59:08.955: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4436 from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:59:08.957: INFO: Unable to read wheezy_udp@dns-test-service.dns-4436.svc from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:59:08.959: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4436.svc from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:59:08.972: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:59:08.974: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:59:08.975: INFO: Unable to read jessie_udp@dns-test-service.dns-4436 from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:59:08.977: INFO: Unable to read jessie_tcp@dns-test-service.dns-4436 from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:59:08.979: INFO: Unable to read jessie_udp@dns-test-service.dns-4436.svc from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:59:08.981: INFO: Unable to read jessie_tcp@dns-test-service.dns-4436.svc from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:59:08.991: INFO: Lookups using dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4436 wheezy_tcp@dns-test-service.dns-4436 wheezy_udp@dns-test-service.dns-4436.svc wheezy_tcp@dns-test-service.dns-4436.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4436 jessie_tcp@dns-test-service.dns-4436 jessie_udp@dns-test-service.dns-4436.svc jessie_tcp@dns-test-service.dns-4436.svc]

Jan  3 02:59:13.950: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:59:13.952: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:59:13.954: INFO: Unable to read wheezy_udp@dns-test-service.dns-4436 from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:59:13.956: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4436 from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:59:13.957: INFO: Unable to read wheezy_udp@dns-test-service.dns-4436.svc from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:59:13.959: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4436.svc from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
Jan  3 02:59:13.989: INFO: Lookups using dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4436 wheezy_tcp@dns-test-service.dns-4436 wheezy_udp@dns-test-service.dns-4436.svc wheezy_tcp@dns-test-service.dns-4436.svc]

Jan  3 02:59:18.990: INFO: DNS probes using dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0 succeeded

STEP: deleting the pod 01/03/23 02:59:18.99
STEP: deleting the test service 01/03/23 02:59:19.051
STEP: deleting the test headless service 01/03/23 02:59:19.088
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan  3 02:59:19.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4436" for this suite. 01/03/23 02:59:19.195
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","completed":253,"skipped":4627,"failed":0}
------------------------------
• [SLOW TEST] [37.420 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:58:41.782
    Jan  3 02:58:41.782: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename dns 01/03/23 02:58:41.783
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:58:41.795
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:58:41.797
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 01/03/23 02:58:41.799
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4436 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4436;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4436 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4436;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4436.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4436.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4436.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4436.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4436.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4436.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4436.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4436.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4436.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4436.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4436.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4436.svc;check="$$(dig +notcp +noall +answer +search 34.88.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.88.34_udp@PTR;check="$$(dig +tcp +noall +answer +search 34.88.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.88.34_tcp@PTR;sleep 1; done
     01/03/23 02:58:41.843
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4436 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4436;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4436 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4436;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4436.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4436.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4436.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4436.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4436.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4436.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4436.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4436.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4436.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4436.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4436.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4436.svc;check="$$(dig +notcp +noall +answer +search 34.88.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.88.34_udp@PTR;check="$$(dig +tcp +noall +answer +search 34.88.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.88.34_tcp@PTR;sleep 1; done
     01/03/23 02:58:41.843
    STEP: creating a pod to probe DNS 01/03/23 02:58:41.843
    STEP: submitting the pod to kubernetes 01/03/23 02:58:41.843
    Jan  3 02:58:41.897: INFO: Waiting up to 15m0s for pod "dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0" in namespace "dns-4436" to be "running"
    Jan  3 02:58:41.899: INFO: Pod "dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.727355ms
    Jan  3 02:58:43.901: INFO: Pod "dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0": Phase="Running", Reason="", readiness=true. Elapsed: 2.004055947s
    Jan  3 02:58:43.901: INFO: Pod "dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0" satisfied condition "running"
    STEP: retrieving the pod 01/03/23 02:58:43.901
    STEP: looking for the results for each expected name from probers 01/03/23 02:58:43.903
    Jan  3 02:58:43.905: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:58:43.907: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:58:43.909: INFO: Unable to read wheezy_udp@dns-test-service.dns-4436 from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:58:43.911: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4436 from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:58:43.912: INFO: Unable to read wheezy_udp@dns-test-service.dns-4436.svc from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:58:43.914: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4436.svc from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:58:43.927: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:58:43.929: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:58:43.931: INFO: Unable to read jessie_udp@dns-test-service.dns-4436 from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:58:43.932: INFO: Unable to read jessie_tcp@dns-test-service.dns-4436 from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:58:43.934: INFO: Unable to read jessie_udp@dns-test-service.dns-4436.svc from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:58:43.936: INFO: Unable to read jessie_tcp@dns-test-service.dns-4436.svc from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:58:43.946: INFO: Lookups using dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4436 wheezy_tcp@dns-test-service.dns-4436 wheezy_udp@dns-test-service.dns-4436.svc wheezy_tcp@dns-test-service.dns-4436.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4436 jessie_tcp@dns-test-service.dns-4436 jessie_udp@dns-test-service.dns-4436.svc jessie_tcp@dns-test-service.dns-4436.svc]

    Jan  3 02:58:48.951: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:58:48.953: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:58:48.955: INFO: Unable to read wheezy_udp@dns-test-service.dns-4436 from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:58:48.957: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4436 from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:58:48.958: INFO: Unable to read wheezy_udp@dns-test-service.dns-4436.svc from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:58:48.960: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4436.svc from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:58:48.972: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:58:48.973: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:58:48.975: INFO: Unable to read jessie_udp@dns-test-service.dns-4436 from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:58:48.976: INFO: Unable to read jessie_tcp@dns-test-service.dns-4436 from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:58:48.978: INFO: Unable to read jessie_udp@dns-test-service.dns-4436.svc from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:58:48.980: INFO: Unable to read jessie_tcp@dns-test-service.dns-4436.svc from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:58:48.990: INFO: Lookups using dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4436 wheezy_tcp@dns-test-service.dns-4436 wheezy_udp@dns-test-service.dns-4436.svc wheezy_tcp@dns-test-service.dns-4436.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4436 jessie_tcp@dns-test-service.dns-4436 jessie_udp@dns-test-service.dns-4436.svc jessie_tcp@dns-test-service.dns-4436.svc]

    Jan  3 02:58:53.951: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:58:53.953: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:58:53.955: INFO: Unable to read wheezy_udp@dns-test-service.dns-4436 from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:58:53.956: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4436 from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:58:53.958: INFO: Unable to read wheezy_udp@dns-test-service.dns-4436.svc from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:58:53.960: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4436.svc from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:58:53.973: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:58:53.974: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:58:53.976: INFO: Unable to read jessie_udp@dns-test-service.dns-4436 from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:58:53.978: INFO: Unable to read jessie_tcp@dns-test-service.dns-4436 from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:58:53.979: INFO: Unable to read jessie_udp@dns-test-service.dns-4436.svc from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:58:53.981: INFO: Unable to read jessie_tcp@dns-test-service.dns-4436.svc from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:58:53.991: INFO: Lookups using dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4436 wheezy_tcp@dns-test-service.dns-4436 wheezy_udp@dns-test-service.dns-4436.svc wheezy_tcp@dns-test-service.dns-4436.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4436 jessie_tcp@dns-test-service.dns-4436 jessie_udp@dns-test-service.dns-4436.svc jessie_tcp@dns-test-service.dns-4436.svc]

    Jan  3 02:58:58.971: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:58:58.973: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:58:58.979: INFO: Unable to read wheezy_udp@dns-test-service.dns-4436 from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:58:58.985: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4436 from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:58:58.987: INFO: Unable to read wheezy_udp@dns-test-service.dns-4436.svc from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:58:58.989: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4436.svc from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:58:59.028: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:58:59.030: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:58:59.032: INFO: Unable to read jessie_udp@dns-test-service.dns-4436 from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:58:59.033: INFO: Unable to read jessie_tcp@dns-test-service.dns-4436 from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:58:59.035: INFO: Unable to read jessie_udp@dns-test-service.dns-4436.svc from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:58:59.037: INFO: Unable to read jessie_tcp@dns-test-service.dns-4436.svc from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:58:59.047: INFO: Lookups using dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4436 wheezy_tcp@dns-test-service.dns-4436 wheezy_udp@dns-test-service.dns-4436.svc wheezy_tcp@dns-test-service.dns-4436.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4436 jessie_tcp@dns-test-service.dns-4436 jessie_udp@dns-test-service.dns-4436.svc jessie_tcp@dns-test-service.dns-4436.svc]

    Jan  3 02:59:03.951: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:59:03.953: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:59:03.955: INFO: Unable to read wheezy_udp@dns-test-service.dns-4436 from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:59:03.957: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4436 from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:59:03.959: INFO: Unable to read wheezy_udp@dns-test-service.dns-4436.svc from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:59:03.961: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4436.svc from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:59:03.973: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:59:03.975: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:59:03.976: INFO: Unable to read jessie_udp@dns-test-service.dns-4436 from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:59:03.978: INFO: Unable to read jessie_tcp@dns-test-service.dns-4436 from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:59:03.980: INFO: Unable to read jessie_udp@dns-test-service.dns-4436.svc from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:59:03.982: INFO: Unable to read jessie_tcp@dns-test-service.dns-4436.svc from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:59:03.992: INFO: Lookups using dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4436 wheezy_tcp@dns-test-service.dns-4436 wheezy_udp@dns-test-service.dns-4436.svc wheezy_tcp@dns-test-service.dns-4436.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4436 jessie_tcp@dns-test-service.dns-4436 jessie_udp@dns-test-service.dns-4436.svc jessie_tcp@dns-test-service.dns-4436.svc]

    Jan  3 02:59:08.949: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:59:08.951: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:59:08.953: INFO: Unable to read wheezy_udp@dns-test-service.dns-4436 from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:59:08.955: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4436 from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:59:08.957: INFO: Unable to read wheezy_udp@dns-test-service.dns-4436.svc from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:59:08.959: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4436.svc from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:59:08.972: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:59:08.974: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:59:08.975: INFO: Unable to read jessie_udp@dns-test-service.dns-4436 from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:59:08.977: INFO: Unable to read jessie_tcp@dns-test-service.dns-4436 from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:59:08.979: INFO: Unable to read jessie_udp@dns-test-service.dns-4436.svc from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:59:08.981: INFO: Unable to read jessie_tcp@dns-test-service.dns-4436.svc from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:59:08.991: INFO: Lookups using dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4436 wheezy_tcp@dns-test-service.dns-4436 wheezy_udp@dns-test-service.dns-4436.svc wheezy_tcp@dns-test-service.dns-4436.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4436 jessie_tcp@dns-test-service.dns-4436 jessie_udp@dns-test-service.dns-4436.svc jessie_tcp@dns-test-service.dns-4436.svc]

    Jan  3 02:59:13.950: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:59:13.952: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:59:13.954: INFO: Unable to read wheezy_udp@dns-test-service.dns-4436 from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:59:13.956: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4436 from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:59:13.957: INFO: Unable to read wheezy_udp@dns-test-service.dns-4436.svc from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:59:13.959: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4436.svc from pod dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0: the server could not find the requested resource (get pods dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0)
    Jan  3 02:59:13.989: INFO: Lookups using dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4436 wheezy_tcp@dns-test-service.dns-4436 wheezy_udp@dns-test-service.dns-4436.svc wheezy_tcp@dns-test-service.dns-4436.svc]

    Jan  3 02:59:18.990: INFO: DNS probes using dns-4436/dns-test-ceb5643d-1ace-4fdc-a535-1da3900a51b0 succeeded

    STEP: deleting the pod 01/03/23 02:59:18.99
    STEP: deleting the test service 01/03/23 02:59:19.051
    STEP: deleting the test headless service 01/03/23 02:59:19.088
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan  3 02:59:19.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-4436" for this suite. 01/03/23 02:59:19.195
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:59:19.203
Jan  3 02:59:19.203: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename services 01/03/23 02:59:19.204
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:59:19.225
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:59:19.227
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
STEP: creating a Service 01/03/23 02:59:19.243
STEP: watching for the Service to be added 01/03/23 02:59:19.266
Jan  3 02:59:19.267: INFO: Found Service test-service-gp62p in namespace services-7408 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Jan  3 02:59:19.267: INFO: Service test-service-gp62p created
STEP: Getting /status 01/03/23 02:59:19.267
Jan  3 02:59:19.277: INFO: Service test-service-gp62p has LoadBalancer: {[]}
STEP: patching the ServiceStatus 01/03/23 02:59:19.277
STEP: watching for the Service to be patched 01/03/23 02:59:19.28
Jan  3 02:59:19.281: INFO: observed Service test-service-gp62p in namespace services-7408 with annotations: map[createdTime:2023-01-03T11:59:19.244762232+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T11:59:19.244762232+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & LoadBalancer: {[]}
Jan  3 02:59:19.281: INFO: Found Service test-service-gp62p in namespace services-7408 with annotations: map[createdTime:2023-01-03T11:59:19.244762232+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount patchedstatus:true updatedTime:2023-01-03T11:59:19.244762232+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & LoadBalancer: {[{203.0.113.1  []}]}
Jan  3 02:59:19.281: INFO: Service test-service-gp62p has service status patched
STEP: updating the ServiceStatus 01/03/23 02:59:19.281
Jan  3 02:59:19.285: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 01/03/23 02:59:19.285
Jan  3 02:59:19.286: INFO: Observed Service test-service-gp62p in namespace services-7408 with annotations: map[createdTime:2023-01-03T11:59:19.244762232+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T11:59:19.244762232+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {[]}
Jan  3 02:59:19.286: INFO: Observed event: &Service{ObjectMeta:{test-service-gp62p  services-7408  53a05618-b246-4ada-89a5-cebe4399509e 1008312 0 2023-01-03 02:59:19 +0000 UTC <nil> <nil> map[test-service-static:true] map[createdTime:2023-01-03T11:59:19.244762232+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount patchedstatus:true updatedTime:2023-01-03T11:59:19.244762232+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update v1 2023-01-03 02:59:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-01-03 02:59:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.96.94.184,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.96.94.184],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Jan  3 02:59:19.286: INFO: Found Service test-service-gp62p in namespace services-7408 with annotations: map[createdTime:2023-01-03T11:59:19.244762232+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount patchedstatus:true updatedTime:2023-01-03T11:59:19.244762232+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan  3 02:59:19.286: INFO: Service test-service-gp62p has service status updated
STEP: patching the service 01/03/23 02:59:19.286
STEP: watching for the Service to be patched 01/03/23 02:59:19.325
Jan  3 02:59:19.326: INFO: observed Service test-service-gp62p in namespace services-7408 with labels: map[test-service-static:true]
Jan  3 02:59:19.326: INFO: observed Service test-service-gp62p in namespace services-7408 with labels: map[test-service-static:true]
Jan  3 02:59:19.326: INFO: observed Service test-service-gp62p in namespace services-7408 with labels: map[test-service-static:true]
Jan  3 02:59:19.326: INFO: Found Service test-service-gp62p in namespace services-7408 with labels: map[test-service:patched test-service-static:true]
Jan  3 02:59:19.326: INFO: Service test-service-gp62p patched
STEP: deleting the service 01/03/23 02:59:19.326
STEP: watching for the Service to be deleted 01/03/23 02:59:19.335
Jan  3 02:59:19.336: INFO: Observed event: ADDED
Jan  3 02:59:19.336: INFO: Observed event: MODIFIED
Jan  3 02:59:19.336: INFO: Observed event: MODIFIED
Jan  3 02:59:19.336: INFO: Observed event: MODIFIED
Jan  3 02:59:19.336: INFO: Found Service test-service-gp62p in namespace services-7408 with labels: map[test-service:patched test-service-static:true] & annotations: map[createdTime:2023-01-03T11:59:19.244762232+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount patchedstatus:true updatedTime:2023-01-03T11:59:19.244762232+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount]
Jan  3 02:59:19.336: INFO: Service test-service-gp62p deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  3 02:59:19.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7408" for this suite. 01/03/23 02:59:19.358
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","completed":254,"skipped":4630,"failed":0}
------------------------------
• [0.160 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:59:19.203
    Jan  3 02:59:19.203: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename services 01/03/23 02:59:19.204
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:59:19.225
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:59:19.227
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3415
    STEP: creating a Service 01/03/23 02:59:19.243
    STEP: watching for the Service to be added 01/03/23 02:59:19.266
    Jan  3 02:59:19.267: INFO: Found Service test-service-gp62p in namespace services-7408 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Jan  3 02:59:19.267: INFO: Service test-service-gp62p created
    STEP: Getting /status 01/03/23 02:59:19.267
    Jan  3 02:59:19.277: INFO: Service test-service-gp62p has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 01/03/23 02:59:19.277
    STEP: watching for the Service to be patched 01/03/23 02:59:19.28
    Jan  3 02:59:19.281: INFO: observed Service test-service-gp62p in namespace services-7408 with annotations: map[createdTime:2023-01-03T11:59:19.244762232+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T11:59:19.244762232+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & LoadBalancer: {[]}
    Jan  3 02:59:19.281: INFO: Found Service test-service-gp62p in namespace services-7408 with annotations: map[createdTime:2023-01-03T11:59:19.244762232+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount patchedstatus:true updatedTime:2023-01-03T11:59:19.244762232+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & LoadBalancer: {[{203.0.113.1  []}]}
    Jan  3 02:59:19.281: INFO: Service test-service-gp62p has service status patched
    STEP: updating the ServiceStatus 01/03/23 02:59:19.281
    Jan  3 02:59:19.285: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 01/03/23 02:59:19.285
    Jan  3 02:59:19.286: INFO: Observed Service test-service-gp62p in namespace services-7408 with annotations: map[createdTime:2023-01-03T11:59:19.244762232+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T11:59:19.244762232+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {[]}
    Jan  3 02:59:19.286: INFO: Observed event: &Service{ObjectMeta:{test-service-gp62p  services-7408  53a05618-b246-4ada-89a5-cebe4399509e 1008312 0 2023-01-03 02:59:19 +0000 UTC <nil> <nil> map[test-service-static:true] map[createdTime:2023-01-03T11:59:19.244762232+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount patchedstatus:true updatedTime:2023-01-03T11:59:19.244762232+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update v1 2023-01-03 02:59:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-01-03 02:59:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.96.94.184,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.96.94.184],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Jan  3 02:59:19.286: INFO: Found Service test-service-gp62p in namespace services-7408 with annotations: map[createdTime:2023-01-03T11:59:19.244762232+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount patchedstatus:true updatedTime:2023-01-03T11:59:19.244762232+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan  3 02:59:19.286: INFO: Service test-service-gp62p has service status updated
    STEP: patching the service 01/03/23 02:59:19.286
    STEP: watching for the Service to be patched 01/03/23 02:59:19.325
    Jan  3 02:59:19.326: INFO: observed Service test-service-gp62p in namespace services-7408 with labels: map[test-service-static:true]
    Jan  3 02:59:19.326: INFO: observed Service test-service-gp62p in namespace services-7408 with labels: map[test-service-static:true]
    Jan  3 02:59:19.326: INFO: observed Service test-service-gp62p in namespace services-7408 with labels: map[test-service-static:true]
    Jan  3 02:59:19.326: INFO: Found Service test-service-gp62p in namespace services-7408 with labels: map[test-service:patched test-service-static:true]
    Jan  3 02:59:19.326: INFO: Service test-service-gp62p patched
    STEP: deleting the service 01/03/23 02:59:19.326
    STEP: watching for the Service to be deleted 01/03/23 02:59:19.335
    Jan  3 02:59:19.336: INFO: Observed event: ADDED
    Jan  3 02:59:19.336: INFO: Observed event: MODIFIED
    Jan  3 02:59:19.336: INFO: Observed event: MODIFIED
    Jan  3 02:59:19.336: INFO: Observed event: MODIFIED
    Jan  3 02:59:19.336: INFO: Found Service test-service-gp62p in namespace services-7408 with labels: map[test-service:patched test-service-static:true] & annotations: map[createdTime:2023-01-03T11:59:19.244762232+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount patchedstatus:true updatedTime:2023-01-03T11:59:19.244762232+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount]
    Jan  3 02:59:19.336: INFO: Service test-service-gp62p deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  3 02:59:19.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7408" for this suite. 01/03/23 02:59:19.358
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:59:19.365
Jan  3 02:59:19.365: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename server-version 01/03/23 02:59:19.365
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:59:19.374
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:59:19.375
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 01/03/23 02:59:19.377
STEP: Confirm major version 01/03/23 02:59:19.378
Jan  3 02:59:19.378: INFO: Major version: 1
STEP: Confirm minor version 01/03/23 02:59:19.378
Jan  3 02:59:19.378: INFO: cleanMinorVersion: 25
Jan  3 02:59:19.378: INFO: Minor version: 25
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:187
Jan  3 02:59:19.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-8001" for this suite. 01/03/23 02:59:19.392
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","completed":255,"skipped":4676,"failed":0}
------------------------------
• [0.030 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:59:19.365
    Jan  3 02:59:19.365: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename server-version 01/03/23 02:59:19.365
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:59:19.374
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:59:19.375
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 01/03/23 02:59:19.377
    STEP: Confirm major version 01/03/23 02:59:19.378
    Jan  3 02:59:19.378: INFO: Major version: 1
    STEP: Confirm minor version 01/03/23 02:59:19.378
    Jan  3 02:59:19.378: INFO: cleanMinorVersion: 25
    Jan  3 02:59:19.378: INFO: Minor version: 25
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:187
    Jan  3 02:59:19.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "server-version-8001" for this suite. 01/03/23 02:59:19.392
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:59:19.395
Jan  3 02:59:19.395: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename webhook 01/03/23 02:59:19.396
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:59:19.408
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:59:19.41
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/03/23 02:59:19.472
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/23 02:59:20.115
STEP: Deploying the webhook pod 01/03/23 02:59:20.12
STEP: Wait for the deployment to be ready 01/03/23 02:59:20.157
Jan  3 02:59:20.180: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/03/23 02:59:22.186
STEP: Verifying the service has paired with the endpoint 01/03/23 02:59:22.224
Jan  3 02:59:23.224: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
STEP: Registering the crd webhook via the AdmissionRegistration API 01/03/23 02:59:23.227
STEP: Creating a custom resource definition that should be denied by the webhook 01/03/23 02:59:23.237
Jan  3 02:59:23.237: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  3 02:59:23.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3285" for this suite. 01/03/23 02:59:23.256
STEP: Destroying namespace "webhook-3285-markers" for this suite. 01/03/23 02:59:23.258
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","completed":256,"skipped":4678,"failed":0}
------------------------------
• [3.964 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:59:19.395
    Jan  3 02:59:19.395: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename webhook 01/03/23 02:59:19.396
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:59:19.408
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:59:19.41
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/03/23 02:59:19.472
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/23 02:59:20.115
    STEP: Deploying the webhook pod 01/03/23 02:59:20.12
    STEP: Wait for the deployment to be ready 01/03/23 02:59:20.157
    Jan  3 02:59:20.180: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/03/23 02:59:22.186
    STEP: Verifying the service has paired with the endpoint 01/03/23 02:59:22.224
    Jan  3 02:59:23.224: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:307
    STEP: Registering the crd webhook via the AdmissionRegistration API 01/03/23 02:59:23.227
    STEP: Creating a custom resource definition that should be denied by the webhook 01/03/23 02:59:23.237
    Jan  3 02:59:23.237: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  3 02:59:23.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-3285" for this suite. 01/03/23 02:59:23.256
    STEP: Destroying namespace "webhook-3285-markers" for this suite. 01/03/23 02:59:23.258
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:59:23.36
Jan  3 02:59:23.360: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename conformance-tests 01/03/23 02:59:23.361
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:59:23.386
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:59:23.389
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 01/03/23 02:59:23.391
Jan  3 02:59:23.391: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:187
Jan  3 02:59:23.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "conformance-tests-3284" for this suite. 01/03/23 02:59:23.41
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]","completed":257,"skipped":4706,"failed":0}
------------------------------
• [0.053 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:59:23.36
    Jan  3 02:59:23.360: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename conformance-tests 01/03/23 02:59:23.361
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:59:23.386
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:59:23.389
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 01/03/23 02:59:23.391
    Jan  3 02:59:23.391: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:187
    Jan  3 02:59:23.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "conformance-tests-3284" for this suite. 01/03/23 02:59:23.41
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:59:23.415
Jan  3 02:59:23.415: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename webhook 01/03/23 02:59:23.415
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:59:23.432
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:59:23.434
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/03/23 02:59:23.453
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/23 02:59:23.86
STEP: Deploying the webhook pod 01/03/23 02:59:23.864
STEP: Wait for the deployment to be ready 01/03/23 02:59:23.904
Jan  3 02:59:23.908: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/03/23 02:59:25.914
STEP: Verifying the service has paired with the endpoint 01/03/23 02:59:25.929
Jan  3 02:59:26.929: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
STEP: Registering the webhook via the AdmissionRegistration API 01/03/23 02:59:26.931
Jan  3 02:59:26.992: INFO: Waiting for webhook configuration to be ready...
STEP: create a pod that should be denied by the webhook 01/03/23 02:59:27.1
STEP: create a pod that causes the webhook to hang 01/03/23 02:59:27.11
STEP: create a configmap that should be denied by the webhook 01/03/23 02:59:37.117
STEP: create a configmap that should be admitted by the webhook 01/03/23 02:59:37.129
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 01/03/23 02:59:37.137
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 01/03/23 02:59:37.205
STEP: create a namespace that bypass the webhook 01/03/23 02:59:37.208
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 01/03/23 02:59:37.213
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  3 02:59:37.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6918" for this suite. 01/03/23 02:59:37.286
STEP: Destroying namespace "webhook-6918-markers" for this suite. 01/03/23 02:59:37.288
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","completed":258,"skipped":4752,"failed":0}
------------------------------
• [SLOW TEST] [13.968 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:59:23.415
    Jan  3 02:59:23.415: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename webhook 01/03/23 02:59:23.415
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:59:23.432
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:59:23.434
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/03/23 02:59:23.453
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/23 02:59:23.86
    STEP: Deploying the webhook pod 01/03/23 02:59:23.864
    STEP: Wait for the deployment to be ready 01/03/23 02:59:23.904
    Jan  3 02:59:23.908: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/03/23 02:59:25.914
    STEP: Verifying the service has paired with the endpoint 01/03/23 02:59:25.929
    Jan  3 02:59:26.929: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:196
    STEP: Registering the webhook via the AdmissionRegistration API 01/03/23 02:59:26.931
    Jan  3 02:59:26.992: INFO: Waiting for webhook configuration to be ready...
    STEP: create a pod that should be denied by the webhook 01/03/23 02:59:27.1
    STEP: create a pod that causes the webhook to hang 01/03/23 02:59:27.11
    STEP: create a configmap that should be denied by the webhook 01/03/23 02:59:37.117
    STEP: create a configmap that should be admitted by the webhook 01/03/23 02:59:37.129
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 01/03/23 02:59:37.137
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 01/03/23 02:59:37.205
    STEP: create a namespace that bypass the webhook 01/03/23 02:59:37.208
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 01/03/23 02:59:37.213
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  3 02:59:37.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6918" for this suite. 01/03/23 02:59:37.286
    STEP: Destroying namespace "webhook-6918-markers" for this suite. 01/03/23 02:59:37.288
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:59:37.385
Jan  3 02:59:37.385: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename services 01/03/23 02:59:37.386
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:59:37.416
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:59:37.418
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
STEP: creating service multi-endpoint-test in namespace services-4376 01/03/23 02:59:37.42
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4376 to expose endpoints map[] 01/03/23 02:59:37.485
Jan  3 02:59:37.507: INFO: successfully validated that service multi-endpoint-test in namespace services-4376 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-4376 01/03/23 02:59:37.508
Jan  3 02:59:37.513: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-4376" to be "running and ready"
Jan  3 02:59:37.519: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006819ms
Jan  3 02:59:37.519: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 02:59:39.527: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.013747766s
Jan  3 02:59:39.527: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan  3 02:59:39.527: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4376 to expose endpoints map[pod1:[100]] 01/03/23 02:59:39.528
Jan  3 02:59:39.533: INFO: successfully validated that service multi-endpoint-test in namespace services-4376 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-4376 01/03/23 02:59:39.533
Jan  3 02:59:39.538: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-4376" to be "running and ready"
Jan  3 02:59:39.542: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.775247ms
Jan  3 02:59:39.543: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 02:59:41.545: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.007607734s
Jan  3 02:59:41.545: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan  3 02:59:41.545: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4376 to expose endpoints map[pod1:[100] pod2:[101]] 01/03/23 02:59:41.547
Jan  3 02:59:41.554: INFO: successfully validated that service multi-endpoint-test in namespace services-4376 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 01/03/23 02:59:41.554
Jan  3 02:59:41.554: INFO: Creating new exec pod
Jan  3 02:59:41.558: INFO: Waiting up to 5m0s for pod "execpodqqlwn" in namespace "services-4376" to be "running"
Jan  3 02:59:41.566: INFO: Pod "execpodqqlwn": Phase="Pending", Reason="", readiness=false. Elapsed: 7.08474ms
Jan  3 02:59:43.568: INFO: Pod "execpodqqlwn": Phase="Running", Reason="", readiness=true. Elapsed: 2.009587932s
Jan  3 02:59:43.568: INFO: Pod "execpodqqlwn" satisfied condition "running"
Jan  3 02:59:44.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-4376 exec execpodqqlwn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Jan  3 02:59:44.709: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Jan  3 02:59:44.709: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  3 02:59:44.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-4376 exec execpodqqlwn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.171.86 80'
Jan  3 02:59:44.855: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.171.86 80\nConnection to 10.96.171.86 80 port [tcp/http] succeeded!\n"
Jan  3 02:59:44.855: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  3 02:59:44.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-4376 exec execpodqqlwn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Jan  3 02:59:44.975: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Jan  3 02:59:44.975: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  3 02:59:44.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-4376 exec execpodqqlwn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.171.86 81'
Jan  3 02:59:45.107: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.171.86 81\nConnection to 10.96.171.86 81 port [tcp/*] succeeded!\n"
Jan  3 02:59:45.107: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-4376 01/03/23 02:59:45.107
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4376 to expose endpoints map[pod2:[101]] 01/03/23 02:59:45.164
Jan  3 02:59:45.180: INFO: successfully validated that service multi-endpoint-test in namespace services-4376 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-4376 01/03/23 02:59:45.18
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4376 to expose endpoints map[] 01/03/23 02:59:45.193
Jan  3 02:59:45.206: INFO: successfully validated that service multi-endpoint-test in namespace services-4376 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  3 02:59:45.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4376" for this suite. 01/03/23 02:59:45.251
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","completed":259,"skipped":4792,"failed":0}
------------------------------
• [SLOW TEST] [7.868 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:59:37.385
    Jan  3 02:59:37.385: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename services 01/03/23 02:59:37.386
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:59:37.416
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:59:37.418
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:852
    STEP: creating service multi-endpoint-test in namespace services-4376 01/03/23 02:59:37.42
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4376 to expose endpoints map[] 01/03/23 02:59:37.485
    Jan  3 02:59:37.507: INFO: successfully validated that service multi-endpoint-test in namespace services-4376 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-4376 01/03/23 02:59:37.508
    Jan  3 02:59:37.513: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-4376" to be "running and ready"
    Jan  3 02:59:37.519: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006819ms
    Jan  3 02:59:37.519: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 02:59:39.527: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.013747766s
    Jan  3 02:59:39.527: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan  3 02:59:39.527: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4376 to expose endpoints map[pod1:[100]] 01/03/23 02:59:39.528
    Jan  3 02:59:39.533: INFO: successfully validated that service multi-endpoint-test in namespace services-4376 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-4376 01/03/23 02:59:39.533
    Jan  3 02:59:39.538: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-4376" to be "running and ready"
    Jan  3 02:59:39.542: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.775247ms
    Jan  3 02:59:39.543: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 02:59:41.545: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.007607734s
    Jan  3 02:59:41.545: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan  3 02:59:41.545: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4376 to expose endpoints map[pod1:[100] pod2:[101]] 01/03/23 02:59:41.547
    Jan  3 02:59:41.554: INFO: successfully validated that service multi-endpoint-test in namespace services-4376 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 01/03/23 02:59:41.554
    Jan  3 02:59:41.554: INFO: Creating new exec pod
    Jan  3 02:59:41.558: INFO: Waiting up to 5m0s for pod "execpodqqlwn" in namespace "services-4376" to be "running"
    Jan  3 02:59:41.566: INFO: Pod "execpodqqlwn": Phase="Pending", Reason="", readiness=false. Elapsed: 7.08474ms
    Jan  3 02:59:43.568: INFO: Pod "execpodqqlwn": Phase="Running", Reason="", readiness=true. Elapsed: 2.009587932s
    Jan  3 02:59:43.568: INFO: Pod "execpodqqlwn" satisfied condition "running"
    Jan  3 02:59:44.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-4376 exec execpodqqlwn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
    Jan  3 02:59:44.709: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Jan  3 02:59:44.709: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  3 02:59:44.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-4376 exec execpodqqlwn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.171.86 80'
    Jan  3 02:59:44.855: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.171.86 80\nConnection to 10.96.171.86 80 port [tcp/http] succeeded!\n"
    Jan  3 02:59:44.855: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  3 02:59:44.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-4376 exec execpodqqlwn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
    Jan  3 02:59:44.975: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Jan  3 02:59:44.975: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  3 02:59:44.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-4376 exec execpodqqlwn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.171.86 81'
    Jan  3 02:59:45.107: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.171.86 81\nConnection to 10.96.171.86 81 port [tcp/*] succeeded!\n"
    Jan  3 02:59:45.107: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-4376 01/03/23 02:59:45.107
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4376 to expose endpoints map[pod2:[101]] 01/03/23 02:59:45.164
    Jan  3 02:59:45.180: INFO: successfully validated that service multi-endpoint-test in namespace services-4376 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-4376 01/03/23 02:59:45.18
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4376 to expose endpoints map[] 01/03/23 02:59:45.193
    Jan  3 02:59:45.206: INFO: successfully validated that service multi-endpoint-test in namespace services-4376 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  3 02:59:45.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4376" for this suite. 01/03/23 02:59:45.251
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:59:45.255
Jan  3 02:59:45.255: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename runtimeclass 01/03/23 02:59:45.256
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:59:45.263
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:59:45.265
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Jan  3 02:59:45.318: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-3059 to be scheduled
Jan  3 02:59:45.320: INFO: 1 pods are not scheduled: [runtimeclass-3059/test-runtimeclass-runtimeclass-3059-preconfigured-handler-fph6v(d1d20111-c66a-4cfd-8382-504ef3ec9747)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan  3 02:59:47.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-3059" for this suite. 01/03/23 02:59:47.333
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]","completed":260,"skipped":4827,"failed":0}
------------------------------
• [2.081 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:59:45.255
    Jan  3 02:59:45.255: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename runtimeclass 01/03/23 02:59:45.256
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:59:45.263
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:59:45.265
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Jan  3 02:59:45.318: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-3059 to be scheduled
    Jan  3 02:59:45.320: INFO: 1 pods are not scheduled: [runtimeclass-3059/test-runtimeclass-runtimeclass-3059-preconfigured-handler-fph6v(d1d20111-c66a-4cfd-8382-504ef3ec9747)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan  3 02:59:47.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-3059" for this suite. 01/03/23 02:59:47.333
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:59:47.336
Jan  3 02:59:47.336: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename webhook 01/03/23 02:59:47.337
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:59:47.361
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:59:47.363
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/03/23 02:59:47.385
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/23 02:59:47.895
STEP: Deploying the webhook pod 01/03/23 02:59:47.907
STEP: Wait for the deployment to be ready 01/03/23 02:59:47.925
Jan  3 02:59:47.948: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/03/23 02:59:49.954
STEP: Verifying the service has paired with the endpoint 01/03/23 02:59:49.991
Jan  3 02:59:50.992: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
Jan  3 02:59:50.994: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8608-crds.webhook.example.com via the AdmissionRegistration API 01/03/23 02:59:51.501
STEP: Creating a custom resource that should be mutated by the webhook 01/03/23 02:59:51.512
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  3 02:59:54.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8564" for this suite. 01/03/23 02:59:54.063
STEP: Destroying namespace "webhook-8564-markers" for this suite. 01/03/23 02:59:54.065
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","completed":261,"skipped":4831,"failed":0}
------------------------------
• [SLOW TEST] [6.809 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:59:47.336
    Jan  3 02:59:47.336: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename webhook 01/03/23 02:59:47.337
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:59:47.361
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:59:47.363
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/03/23 02:59:47.385
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/23 02:59:47.895
    STEP: Deploying the webhook pod 01/03/23 02:59:47.907
    STEP: Wait for the deployment to be ready 01/03/23 02:59:47.925
    Jan  3 02:59:47.948: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/03/23 02:59:49.954
    STEP: Verifying the service has paired with the endpoint 01/03/23 02:59:49.991
    Jan  3 02:59:50.992: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:290
    Jan  3 02:59:50.994: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8608-crds.webhook.example.com via the AdmissionRegistration API 01/03/23 02:59:51.501
    STEP: Creating a custom resource that should be mutated by the webhook 01/03/23 02:59:51.512
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  3 02:59:54.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8564" for this suite. 01/03/23 02:59:54.063
    STEP: Destroying namespace "webhook-8564-markers" for this suite. 01/03/23 02:59:54.065
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:59:54.147
Jan  3 02:59:54.147: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename discovery 01/03/23 02:59:54.148
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:59:54.193
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:59:54.195
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 01/03/23 02:59:54.198
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Jan  3 02:59:54.548: INFO: Checking APIGroup: apiregistration.k8s.io
Jan  3 02:59:54.548: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jan  3 02:59:54.548: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Jan  3 02:59:54.548: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jan  3 02:59:54.548: INFO: Checking APIGroup: apps
Jan  3 02:59:54.549: INFO: PreferredVersion.GroupVersion: apps/v1
Jan  3 02:59:54.549: INFO: Versions found [{apps/v1 v1}]
Jan  3 02:59:54.549: INFO: apps/v1 matches apps/v1
Jan  3 02:59:54.549: INFO: Checking APIGroup: events.k8s.io
Jan  3 02:59:54.550: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jan  3 02:59:54.550: INFO: Versions found [{events.k8s.io/v1 v1}]
Jan  3 02:59:54.550: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jan  3 02:59:54.550: INFO: Checking APIGroup: authentication.k8s.io
Jan  3 02:59:54.551: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jan  3 02:59:54.551: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Jan  3 02:59:54.551: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jan  3 02:59:54.551: INFO: Checking APIGroup: authorization.k8s.io
Jan  3 02:59:54.551: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jan  3 02:59:54.551: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Jan  3 02:59:54.551: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jan  3 02:59:54.551: INFO: Checking APIGroup: autoscaling
Jan  3 02:59:54.552: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Jan  3 02:59:54.552: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
Jan  3 02:59:54.552: INFO: autoscaling/v2 matches autoscaling/v2
Jan  3 02:59:54.552: INFO: Checking APIGroup: batch
Jan  3 02:59:54.553: INFO: PreferredVersion.GroupVersion: batch/v1
Jan  3 02:59:54.553: INFO: Versions found [{batch/v1 v1}]
Jan  3 02:59:54.553: INFO: batch/v1 matches batch/v1
Jan  3 02:59:54.553: INFO: Checking APIGroup: certificates.k8s.io
Jan  3 02:59:54.553: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jan  3 02:59:54.553: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Jan  3 02:59:54.553: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jan  3 02:59:54.553: INFO: Checking APIGroup: networking.k8s.io
Jan  3 02:59:54.554: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jan  3 02:59:54.554: INFO: Versions found [{networking.k8s.io/v1 v1}]
Jan  3 02:59:54.554: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jan  3 02:59:54.554: INFO: Checking APIGroup: policy
Jan  3 02:59:54.555: INFO: PreferredVersion.GroupVersion: policy/v1
Jan  3 02:59:54.555: INFO: Versions found [{policy/v1 v1}]
Jan  3 02:59:54.555: INFO: policy/v1 matches policy/v1
Jan  3 02:59:54.555: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jan  3 02:59:54.555: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jan  3 02:59:54.555: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Jan  3 02:59:54.555: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jan  3 02:59:54.555: INFO: Checking APIGroup: storage.k8s.io
Jan  3 02:59:54.556: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jan  3 02:59:54.556: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jan  3 02:59:54.556: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jan  3 02:59:54.556: INFO: Checking APIGroup: admissionregistration.k8s.io
Jan  3 02:59:54.557: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jan  3 02:59:54.557: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Jan  3 02:59:54.557: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jan  3 02:59:54.557: INFO: Checking APIGroup: apiextensions.k8s.io
Jan  3 02:59:54.557: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jan  3 02:59:54.557: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Jan  3 02:59:54.557: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jan  3 02:59:54.557: INFO: Checking APIGroup: scheduling.k8s.io
Jan  3 02:59:54.558: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jan  3 02:59:54.558: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Jan  3 02:59:54.558: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jan  3 02:59:54.558: INFO: Checking APIGroup: coordination.k8s.io
Jan  3 02:59:54.558: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jan  3 02:59:54.558: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Jan  3 02:59:54.558: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jan  3 02:59:54.558: INFO: Checking APIGroup: node.k8s.io
Jan  3 02:59:54.559: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Jan  3 02:59:54.559: INFO: Versions found [{node.k8s.io/v1 v1}]
Jan  3 02:59:54.559: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Jan  3 02:59:54.559: INFO: Checking APIGroup: discovery.k8s.io
Jan  3 02:59:54.560: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Jan  3 02:59:54.560: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Jan  3 02:59:54.560: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Jan  3 02:59:54.560: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Jan  3 02:59:54.560: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Jan  3 02:59:54.560: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Jan  3 02:59:54.560: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Jan  3 02:59:54.560: INFO: Checking APIGroup: acme.cert-manager.io
Jan  3 02:59:54.561: INFO: PreferredVersion.GroupVersion: acme.cert-manager.io/v1
Jan  3 02:59:54.561: INFO: Versions found [{acme.cert-manager.io/v1 v1} {acme.cert-manager.io/v1beta1 v1beta1} {acme.cert-manager.io/v1alpha3 v1alpha3} {acme.cert-manager.io/v1alpha2 v1alpha2}]
Jan  3 02:59:54.561: INFO: acme.cert-manager.io/v1 matches acme.cert-manager.io/v1
Jan  3 02:59:54.561: INFO: Checking APIGroup: ceph.rook.io
Jan  3 02:59:54.561: INFO: PreferredVersion.GroupVersion: ceph.rook.io/v1
Jan  3 02:59:54.561: INFO: Versions found [{ceph.rook.io/v1 v1}]
Jan  3 02:59:54.561: INFO: ceph.rook.io/v1 matches ceph.rook.io/v1
Jan  3 02:59:54.561: INFO: Checking APIGroup: cert-manager.io
Jan  3 02:59:54.562: INFO: PreferredVersion.GroupVersion: cert-manager.io/v1
Jan  3 02:59:54.562: INFO: Versions found [{cert-manager.io/v1 v1} {cert-manager.io/v1beta1 v1beta1} {cert-manager.io/v1alpha3 v1alpha3} {cert-manager.io/v1alpha2 v1alpha2}]
Jan  3 02:59:54.562: INFO: cert-manager.io/v1 matches cert-manager.io/v1
Jan  3 02:59:54.562: INFO: Checking APIGroup: crd.projectcalico.org
Jan  3 02:59:54.563: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Jan  3 02:59:54.563: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Jan  3 02:59:54.563: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Jan  3 02:59:54.563: INFO: Checking APIGroup: hypercloud.tmaxcloud.com
Jan  3 02:59:54.563: INFO: PreferredVersion.GroupVersion: hypercloud.tmaxcloud.com/v1
Jan  3 02:59:54.563: INFO: Versions found [{hypercloud.tmaxcloud.com/v1 v1}]
Jan  3 02:59:54.563: INFO: hypercloud.tmaxcloud.com/v1 matches hypercloud.tmaxcloud.com/v1
Jan  3 02:59:54.563: INFO: Checking APIGroup: snapshot.storage.k8s.io
Jan  3 02:59:54.564: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Jan  3 02:59:54.564: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1} {snapshot.storage.k8s.io/v1beta1 v1beta1}]
Jan  3 02:59:54.564: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
Jan  3 02:59:54.564: INFO: Checking APIGroup: tmax.io
Jan  3 02:59:54.564: INFO: PreferredVersion.GroupVersion: tmax.io/v1
Jan  3 02:59:54.564: INFO: Versions found [{tmax.io/v1 v1}]
Jan  3 02:59:54.564: INFO: tmax.io/v1 matches tmax.io/v1
Jan  3 02:59:54.564: INFO: Checking APIGroup: objectbucket.io
Jan  3 02:59:54.565: INFO: PreferredVersion.GroupVersion: objectbucket.io/v1alpha1
Jan  3 02:59:54.565: INFO: Versions found [{objectbucket.io/v1alpha1 v1alpha1}]
Jan  3 02:59:54.565: INFO: objectbucket.io/v1alpha1 matches objectbucket.io/v1alpha1
Jan  3 02:59:54.565: INFO: Checking APIGroup: traefik.containo.us
Jan  3 02:59:54.565: INFO: PreferredVersion.GroupVersion: traefik.containo.us/v1alpha1
Jan  3 02:59:54.565: INFO: Versions found [{traefik.containo.us/v1alpha1 v1alpha1}]
Jan  3 02:59:54.565: INFO: traefik.containo.us/v1alpha1 matches traefik.containo.us/v1alpha1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:187
Jan  3 02:59:54.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-8312" for this suite. 01/03/23 02:59:54.568
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","completed":262,"skipped":4885,"failed":0}
------------------------------
• [0.424 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:59:54.147
    Jan  3 02:59:54.147: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename discovery 01/03/23 02:59:54.148
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:59:54.193
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:59:54.195
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 01/03/23 02:59:54.198
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Jan  3 02:59:54.548: INFO: Checking APIGroup: apiregistration.k8s.io
    Jan  3 02:59:54.548: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Jan  3 02:59:54.548: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Jan  3 02:59:54.548: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Jan  3 02:59:54.548: INFO: Checking APIGroup: apps
    Jan  3 02:59:54.549: INFO: PreferredVersion.GroupVersion: apps/v1
    Jan  3 02:59:54.549: INFO: Versions found [{apps/v1 v1}]
    Jan  3 02:59:54.549: INFO: apps/v1 matches apps/v1
    Jan  3 02:59:54.549: INFO: Checking APIGroup: events.k8s.io
    Jan  3 02:59:54.550: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Jan  3 02:59:54.550: INFO: Versions found [{events.k8s.io/v1 v1}]
    Jan  3 02:59:54.550: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Jan  3 02:59:54.550: INFO: Checking APIGroup: authentication.k8s.io
    Jan  3 02:59:54.551: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Jan  3 02:59:54.551: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Jan  3 02:59:54.551: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Jan  3 02:59:54.551: INFO: Checking APIGroup: authorization.k8s.io
    Jan  3 02:59:54.551: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Jan  3 02:59:54.551: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Jan  3 02:59:54.551: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Jan  3 02:59:54.551: INFO: Checking APIGroup: autoscaling
    Jan  3 02:59:54.552: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Jan  3 02:59:54.552: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
    Jan  3 02:59:54.552: INFO: autoscaling/v2 matches autoscaling/v2
    Jan  3 02:59:54.552: INFO: Checking APIGroup: batch
    Jan  3 02:59:54.553: INFO: PreferredVersion.GroupVersion: batch/v1
    Jan  3 02:59:54.553: INFO: Versions found [{batch/v1 v1}]
    Jan  3 02:59:54.553: INFO: batch/v1 matches batch/v1
    Jan  3 02:59:54.553: INFO: Checking APIGroup: certificates.k8s.io
    Jan  3 02:59:54.553: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Jan  3 02:59:54.553: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Jan  3 02:59:54.553: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Jan  3 02:59:54.553: INFO: Checking APIGroup: networking.k8s.io
    Jan  3 02:59:54.554: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Jan  3 02:59:54.554: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Jan  3 02:59:54.554: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Jan  3 02:59:54.554: INFO: Checking APIGroup: policy
    Jan  3 02:59:54.555: INFO: PreferredVersion.GroupVersion: policy/v1
    Jan  3 02:59:54.555: INFO: Versions found [{policy/v1 v1}]
    Jan  3 02:59:54.555: INFO: policy/v1 matches policy/v1
    Jan  3 02:59:54.555: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Jan  3 02:59:54.555: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Jan  3 02:59:54.555: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Jan  3 02:59:54.555: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Jan  3 02:59:54.555: INFO: Checking APIGroup: storage.k8s.io
    Jan  3 02:59:54.556: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Jan  3 02:59:54.556: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Jan  3 02:59:54.556: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Jan  3 02:59:54.556: INFO: Checking APIGroup: admissionregistration.k8s.io
    Jan  3 02:59:54.557: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Jan  3 02:59:54.557: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Jan  3 02:59:54.557: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Jan  3 02:59:54.557: INFO: Checking APIGroup: apiextensions.k8s.io
    Jan  3 02:59:54.557: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Jan  3 02:59:54.557: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Jan  3 02:59:54.557: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Jan  3 02:59:54.557: INFO: Checking APIGroup: scheduling.k8s.io
    Jan  3 02:59:54.558: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Jan  3 02:59:54.558: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Jan  3 02:59:54.558: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Jan  3 02:59:54.558: INFO: Checking APIGroup: coordination.k8s.io
    Jan  3 02:59:54.558: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Jan  3 02:59:54.558: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Jan  3 02:59:54.558: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Jan  3 02:59:54.558: INFO: Checking APIGroup: node.k8s.io
    Jan  3 02:59:54.559: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Jan  3 02:59:54.559: INFO: Versions found [{node.k8s.io/v1 v1}]
    Jan  3 02:59:54.559: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Jan  3 02:59:54.559: INFO: Checking APIGroup: discovery.k8s.io
    Jan  3 02:59:54.560: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Jan  3 02:59:54.560: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Jan  3 02:59:54.560: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Jan  3 02:59:54.560: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Jan  3 02:59:54.560: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
    Jan  3 02:59:54.560: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
    Jan  3 02:59:54.560: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
    Jan  3 02:59:54.560: INFO: Checking APIGroup: acme.cert-manager.io
    Jan  3 02:59:54.561: INFO: PreferredVersion.GroupVersion: acme.cert-manager.io/v1
    Jan  3 02:59:54.561: INFO: Versions found [{acme.cert-manager.io/v1 v1} {acme.cert-manager.io/v1beta1 v1beta1} {acme.cert-manager.io/v1alpha3 v1alpha3} {acme.cert-manager.io/v1alpha2 v1alpha2}]
    Jan  3 02:59:54.561: INFO: acme.cert-manager.io/v1 matches acme.cert-manager.io/v1
    Jan  3 02:59:54.561: INFO: Checking APIGroup: ceph.rook.io
    Jan  3 02:59:54.561: INFO: PreferredVersion.GroupVersion: ceph.rook.io/v1
    Jan  3 02:59:54.561: INFO: Versions found [{ceph.rook.io/v1 v1}]
    Jan  3 02:59:54.561: INFO: ceph.rook.io/v1 matches ceph.rook.io/v1
    Jan  3 02:59:54.561: INFO: Checking APIGroup: cert-manager.io
    Jan  3 02:59:54.562: INFO: PreferredVersion.GroupVersion: cert-manager.io/v1
    Jan  3 02:59:54.562: INFO: Versions found [{cert-manager.io/v1 v1} {cert-manager.io/v1beta1 v1beta1} {cert-manager.io/v1alpha3 v1alpha3} {cert-manager.io/v1alpha2 v1alpha2}]
    Jan  3 02:59:54.562: INFO: cert-manager.io/v1 matches cert-manager.io/v1
    Jan  3 02:59:54.562: INFO: Checking APIGroup: crd.projectcalico.org
    Jan  3 02:59:54.563: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    Jan  3 02:59:54.563: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    Jan  3 02:59:54.563: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    Jan  3 02:59:54.563: INFO: Checking APIGroup: hypercloud.tmaxcloud.com
    Jan  3 02:59:54.563: INFO: PreferredVersion.GroupVersion: hypercloud.tmaxcloud.com/v1
    Jan  3 02:59:54.563: INFO: Versions found [{hypercloud.tmaxcloud.com/v1 v1}]
    Jan  3 02:59:54.563: INFO: hypercloud.tmaxcloud.com/v1 matches hypercloud.tmaxcloud.com/v1
    Jan  3 02:59:54.563: INFO: Checking APIGroup: snapshot.storage.k8s.io
    Jan  3 02:59:54.564: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
    Jan  3 02:59:54.564: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1} {snapshot.storage.k8s.io/v1beta1 v1beta1}]
    Jan  3 02:59:54.564: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
    Jan  3 02:59:54.564: INFO: Checking APIGroup: tmax.io
    Jan  3 02:59:54.564: INFO: PreferredVersion.GroupVersion: tmax.io/v1
    Jan  3 02:59:54.564: INFO: Versions found [{tmax.io/v1 v1}]
    Jan  3 02:59:54.564: INFO: tmax.io/v1 matches tmax.io/v1
    Jan  3 02:59:54.564: INFO: Checking APIGroup: objectbucket.io
    Jan  3 02:59:54.565: INFO: PreferredVersion.GroupVersion: objectbucket.io/v1alpha1
    Jan  3 02:59:54.565: INFO: Versions found [{objectbucket.io/v1alpha1 v1alpha1}]
    Jan  3 02:59:54.565: INFO: objectbucket.io/v1alpha1 matches objectbucket.io/v1alpha1
    Jan  3 02:59:54.565: INFO: Checking APIGroup: traefik.containo.us
    Jan  3 02:59:54.565: INFO: PreferredVersion.GroupVersion: traefik.containo.us/v1alpha1
    Jan  3 02:59:54.565: INFO: Versions found [{traefik.containo.us/v1alpha1 v1alpha1}]
    Jan  3 02:59:54.565: INFO: traefik.containo.us/v1alpha1 matches traefik.containo.us/v1alpha1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:187
    Jan  3 02:59:54.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "discovery-8312" for this suite. 01/03/23 02:59:54.568
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:59:54.571
Jan  3 02:59:54.571: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename pods 01/03/23 02:59:54.572
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:59:54.619
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:59:54.621
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
STEP: creating pod 01/03/23 02:59:54.623
Jan  3 02:59:54.629: INFO: Waiting up to 5m0s for pod "pod-hostip-39757cf2-d58c-49ea-a578-f2cccc160de4" in namespace "pods-8850" to be "running and ready"
Jan  3 02:59:54.661: INFO: Pod "pod-hostip-39757cf2-d58c-49ea-a578-f2cccc160de4": Phase="Pending", Reason="", readiness=false. Elapsed: 31.76705ms
Jan  3 02:59:54.661: INFO: The phase of Pod pod-hostip-39757cf2-d58c-49ea-a578-f2cccc160de4 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 02:59:56.663: INFO: Pod "pod-hostip-39757cf2-d58c-49ea-a578-f2cccc160de4": Phase="Running", Reason="", readiness=true. Elapsed: 2.033901077s
Jan  3 02:59:56.663: INFO: The phase of Pod pod-hostip-39757cf2-d58c-49ea-a578-f2cccc160de4 is Running (Ready = true)
Jan  3 02:59:56.663: INFO: Pod "pod-hostip-39757cf2-d58c-49ea-a578-f2cccc160de4" satisfied condition "running and ready"
Jan  3 02:59:56.666: INFO: Pod pod-hostip-39757cf2-d58c-49ea-a578-f2cccc160de4 has hostIP: 172.21.7.5
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan  3 02:59:56.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8850" for this suite. 01/03/23 02:59:56.669
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","completed":263,"skipped":4896,"failed":0}
------------------------------
• [2.100 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:59:54.571
    Jan  3 02:59:54.571: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename pods 01/03/23 02:59:54.572
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:59:54.619
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:59:54.621
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:203
    STEP: creating pod 01/03/23 02:59:54.623
    Jan  3 02:59:54.629: INFO: Waiting up to 5m0s for pod "pod-hostip-39757cf2-d58c-49ea-a578-f2cccc160de4" in namespace "pods-8850" to be "running and ready"
    Jan  3 02:59:54.661: INFO: Pod "pod-hostip-39757cf2-d58c-49ea-a578-f2cccc160de4": Phase="Pending", Reason="", readiness=false. Elapsed: 31.76705ms
    Jan  3 02:59:54.661: INFO: The phase of Pod pod-hostip-39757cf2-d58c-49ea-a578-f2cccc160de4 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 02:59:56.663: INFO: Pod "pod-hostip-39757cf2-d58c-49ea-a578-f2cccc160de4": Phase="Running", Reason="", readiness=true. Elapsed: 2.033901077s
    Jan  3 02:59:56.663: INFO: The phase of Pod pod-hostip-39757cf2-d58c-49ea-a578-f2cccc160de4 is Running (Ready = true)
    Jan  3 02:59:56.663: INFO: Pod "pod-hostip-39757cf2-d58c-49ea-a578-f2cccc160de4" satisfied condition "running and ready"
    Jan  3 02:59:56.666: INFO: Pod pod-hostip-39757cf2-d58c-49ea-a578-f2cccc160de4 has hostIP: 172.21.7.5
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan  3 02:59:56.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-8850" for this suite. 01/03/23 02:59:56.669
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 02:59:56.673
Jan  3 02:59:56.673: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename init-container 01/03/23 02:59:56.674
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:59:56.682
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:59:56.684
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
STEP: creating the pod 01/03/23 02:59:56.686
Jan  3 02:59:56.686: INFO: PodSpec: initContainers in spec.initContainers
Jan  3 03:00:43.673: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-ed7e1c2f-d885-4fb2-8e3e-2c68b0ddd79c", GenerateName:"", Namespace:"init-container-5138", SelfLink:"", UID:"c921f600-3dbe-4283-8154-893451b883e4", ResourceVersion:"1009092", Generation:0, CreationTimestamp:time.Date(2023, time.January, 3, 2, 59, 56, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"686262394"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"14236ff0f4f096d18ea24e10cd24ab502f5edea70e36ab86dd2a980419f44dd6", "cni.projectcalico.org/podIP":"10.244.246.121/32", "cni.projectcalico.org/podIPs":"10.244.246.121/32", "createdTime":"2023-01-03T11:59:56.698884512+09:00", "creator":"system:serviceaccount:sonobuoy:sonobuoy-serviceaccount", "updatedTime":"2023-01-03T11:59:56.698884512+09:00", "updater":"system:serviceaccount:sonobuoy:sonobuoy-serviceaccount"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 3, 2, 59, 56, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003bb4d20), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 3, 2, 59, 57, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003bb4d50), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 3, 3, 0, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003bb4d98), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-lszgb", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0062e23a0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-lszgb", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-lszgb", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-lszgb", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0042de018), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"cncf-master", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0034e4930), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0042de0a0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0042de0c0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0042de0c8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0042de0cc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000d669a0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 3, 2, 59, 56, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 3, 2, 59, 56, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 3, 2, 59, 56, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 3, 2, 59, 56, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.21.7.5", PodIP:"10.244.246.121", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.246.121"}}, StartTime:time.Date(2023, time.January, 3, 2, 59, 56, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0034e4a10)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0034e4a80)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"cri-o://840d7105da4f04a9f1706ea8eea35711520dcb2a98cfeda9a9b15af9d6f18b04", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0062e2440), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0062e2400), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc0042de14f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jan  3 03:00:43.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5138" for this suite. 01/03/23 03:00:43.693
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","completed":264,"skipped":4950,"failed":0}
------------------------------
• [SLOW TEST] [47.085 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 02:59:56.673
    Jan  3 02:59:56.673: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename init-container 01/03/23 02:59:56.674
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 02:59:56.682
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 02:59:56.684
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:333
    STEP: creating the pod 01/03/23 02:59:56.686
    Jan  3 02:59:56.686: INFO: PodSpec: initContainers in spec.initContainers
    Jan  3 03:00:43.673: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-ed7e1c2f-d885-4fb2-8e3e-2c68b0ddd79c", GenerateName:"", Namespace:"init-container-5138", SelfLink:"", UID:"c921f600-3dbe-4283-8154-893451b883e4", ResourceVersion:"1009092", Generation:0, CreationTimestamp:time.Date(2023, time.January, 3, 2, 59, 56, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"686262394"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"14236ff0f4f096d18ea24e10cd24ab502f5edea70e36ab86dd2a980419f44dd6", "cni.projectcalico.org/podIP":"10.244.246.121/32", "cni.projectcalico.org/podIPs":"10.244.246.121/32", "createdTime":"2023-01-03T11:59:56.698884512+09:00", "creator":"system:serviceaccount:sonobuoy:sonobuoy-serviceaccount", "updatedTime":"2023-01-03T11:59:56.698884512+09:00", "updater":"system:serviceaccount:sonobuoy:sonobuoy-serviceaccount"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 3, 2, 59, 56, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003bb4d20), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 3, 2, 59, 57, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003bb4d50), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 3, 3, 0, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003bb4d98), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-lszgb", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0062e23a0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-lszgb", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-lszgb", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-lszgb", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0042de018), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"cncf-master", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0034e4930), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0042de0a0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0042de0c0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0042de0c8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0042de0cc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000d669a0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 3, 2, 59, 56, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 3, 2, 59, 56, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 3, 2, 59, 56, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 3, 2, 59, 56, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.21.7.5", PodIP:"10.244.246.121", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.246.121"}}, StartTime:time.Date(2023, time.January, 3, 2, 59, 56, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0034e4a10)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0034e4a80)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"cri-o://840d7105da4f04a9f1706ea8eea35711520dcb2a98cfeda9a9b15af9d6f18b04", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0062e2440), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0062e2400), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc0042de14f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan  3 03:00:43.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-5138" for this suite. 01/03/23 03:00:43.693
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:00:43.759
Jan  3 03:00:43.759: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename resourcequota 01/03/23 03:00:43.761
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:00:43.775
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:00:43.777
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
STEP: Discovering how many secrets are in namespace by default 01/03/23 03:00:43.779
STEP: Counting existing ResourceQuota 01/03/23 03:00:48.786
STEP: Creating a ResourceQuota 01/03/23 03:00:53.791
STEP: Ensuring resource quota status is calculated 01/03/23 03:00:53.794
STEP: Creating a Secret 01/03/23 03:00:55.814
STEP: Ensuring resource quota status captures secret creation 01/03/23 03:00:55.838
STEP: Deleting a secret 01/03/23 03:00:57.842
STEP: Ensuring resource quota status released usage 01/03/23 03:00:57.845
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan  3 03:00:59.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6149" for this suite. 01/03/23 03:00:59.852
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","completed":265,"skipped":4975,"failed":0}
------------------------------
• [SLOW TEST] [16.096 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:00:43.759
    Jan  3 03:00:43.759: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename resourcequota 01/03/23 03:00:43.761
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:00:43.775
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:00:43.777
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:150
    STEP: Discovering how many secrets are in namespace by default 01/03/23 03:00:43.779
    STEP: Counting existing ResourceQuota 01/03/23 03:00:48.786
    STEP: Creating a ResourceQuota 01/03/23 03:00:53.791
    STEP: Ensuring resource quota status is calculated 01/03/23 03:00:53.794
    STEP: Creating a Secret 01/03/23 03:00:55.814
    STEP: Ensuring resource quota status captures secret creation 01/03/23 03:00:55.838
    STEP: Deleting a secret 01/03/23 03:00:57.842
    STEP: Ensuring resource quota status released usage 01/03/23 03:00:57.845
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan  3 03:00:59.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-6149" for this suite. 01/03/23 03:00:59.852
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:00:59.86
Jan  3 03:00:59.860: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename aggregator 01/03/23 03:00:59.861
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:00:59.873
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:00:59.875
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Jan  3 03:00:59.877: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 01/03/23 03:00:59.878
Jan  3 03:01:00.382: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jan  3 03:01:02.434: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 03:01:04.437: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 03:01:06.436: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 03:01:08.436: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 03:01:10.436: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 03:01:12.436: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 03:01:14.437: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 03:01:16.437: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 03:01:18.438: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 03:01:20.438: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 03:01:22.437: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 03:01:24.438: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 03:01:26.436: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 03:01:28.436: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 03:01:30.437: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  3 03:01:32.554: INFO: Waited 113.450649ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 01/03/23 03:01:32.581
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 01/03/23 03:01:32.596
STEP: List APIServices 01/03/23 03:01:32.649
Jan  3 03:01:32.700: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:187
Jan  3 03:01:33.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-7834" for this suite. 01/03/23 03:01:33.571
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","completed":266,"skipped":5143,"failed":0}
------------------------------
• [SLOW TEST] [33.736 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:00:59.86
    Jan  3 03:00:59.860: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename aggregator 01/03/23 03:00:59.861
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:00:59.873
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:00:59.875
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Jan  3 03:00:59.877: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 01/03/23 03:00:59.878
    Jan  3 03:01:00.382: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Jan  3 03:01:02.434: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 03:01:04.437: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 03:01:06.436: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 03:01:08.436: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 03:01:10.436: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 03:01:12.436: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 03:01:14.437: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 03:01:16.437: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 03:01:18.438: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 03:01:20.438: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 03:01:22.437: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 03:01:24.438: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 03:01:26.436: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 03:01:28.436: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 03:01:30.437: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 1, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  3 03:01:32.554: INFO: Waited 113.450649ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 01/03/23 03:01:32.581
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 01/03/23 03:01:32.596
    STEP: List APIServices 01/03/23 03:01:32.649
    Jan  3 03:01:32.700: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:187
    Jan  3 03:01:33.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "aggregator-7834" for this suite. 01/03/23 03:01:33.571
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:01:33.597
Jan  3 03:01:33.598: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename statefulset 01/03/23 03:01:33.598
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:01:33.606
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:01:33.608
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-9532 01/03/23 03:01:33.612
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
Jan  3 03:01:33.661: INFO: Found 0 stateful pods, waiting for 1
Jan  3 03:01:43.663: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 01/03/23 03:01:43.667
W0103 03:01:43.670589      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan  3 03:01:43.687: INFO: Found 1 stateful pods, waiting for 2
Jan  3 03:01:53.796: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 03:01:53.796: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 01/03/23 03:01:53.8
STEP: Delete all of the StatefulSets 01/03/23 03:01:53.802
STEP: Verify that StatefulSets have been deleted 01/03/23 03:01:53.805
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan  3 03:01:53.808: INFO: Deleting all statefulset in ns statefulset-9532
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan  3 03:01:53.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9532" for this suite. 01/03/23 03:01:53.877
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","completed":267,"skipped":5157,"failed":0}
------------------------------
• [SLOW TEST] [20.282 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:906

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:01:33.597
    Jan  3 03:01:33.598: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename statefulset 01/03/23 03:01:33.598
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:01:33.606
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:01:33.608
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-9532 01/03/23 03:01:33.612
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:906
    Jan  3 03:01:33.661: INFO: Found 0 stateful pods, waiting for 1
    Jan  3 03:01:43.663: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 01/03/23 03:01:43.667
    W0103 03:01:43.670589      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan  3 03:01:43.687: INFO: Found 1 stateful pods, waiting for 2
    Jan  3 03:01:53.796: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan  3 03:01:53.796: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 01/03/23 03:01:53.8
    STEP: Delete all of the StatefulSets 01/03/23 03:01:53.802
    STEP: Verify that StatefulSets have been deleted 01/03/23 03:01:53.805
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan  3 03:01:53.808: INFO: Deleting all statefulset in ns statefulset-9532
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan  3 03:01:53.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-9532" for this suite. 01/03/23 03:01:53.877
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:01:53.88
Jan  3 03:01:53.881: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename container-runtime 01/03/23 03:01:53.881
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:01:53.889
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:01:53.891
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 01/03/23 03:01:53.949
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 01/03/23 03:02:12.023
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 01/03/23 03:02:12.024
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 01/03/23 03:02:12.027
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 01/03/23 03:02:12.027
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 01/03/23 03:02:12.054
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 01/03/23 03:02:14.087
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 01/03/23 03:02:16.094
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 01/03/23 03:02:16.097
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 01/03/23 03:02:16.097
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 01/03/23 03:02:16.131
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 01/03/23 03:02:17.137
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 01/03/23 03:02:20.145
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 01/03/23 03:02:20.149
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 01/03/23 03:02:20.149
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan  3 03:02:20.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-950" for this suite. 01/03/23 03:02:20.211
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","completed":268,"skipped":5161,"failed":0}
------------------------------
• [SLOW TEST] [26.333 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    when starting a container that exits
    test/e2e/common/node/runtime.go:44
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:01:53.88
    Jan  3 03:01:53.881: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename container-runtime 01/03/23 03:01:53.881
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:01:53.889
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:01:53.891
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 01/03/23 03:01:53.949
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 01/03/23 03:02:12.023
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 01/03/23 03:02:12.024
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 01/03/23 03:02:12.027
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 01/03/23 03:02:12.027
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 01/03/23 03:02:12.054
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 01/03/23 03:02:14.087
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 01/03/23 03:02:16.094
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 01/03/23 03:02:16.097
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 01/03/23 03:02:16.097
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 01/03/23 03:02:16.131
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 01/03/23 03:02:17.137
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 01/03/23 03:02:20.145
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 01/03/23 03:02:20.149
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 01/03/23 03:02:20.149
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan  3 03:02:20.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-950" for this suite. 01/03/23 03:02:20.211
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:02:20.214
Jan  3 03:02:20.214: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename kubectl 01/03/23 03:02:20.215
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:02:20.232
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:02:20.234
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1570
STEP: creating an pod 01/03/23 03:02:20.236
Jan  3 03:02:20.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-5204 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Jan  3 03:02:20.298: INFO: stderr: ""
Jan  3 03:02:20.298: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
STEP: Waiting for log generator to start. 01/03/23 03:02:20.298
Jan  3 03:02:20.298: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jan  3 03:02:20.298: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-5204" to be "running and ready, or succeeded"
Jan  3 03:02:20.321: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 22.86275ms
Jan  3 03:02:20.321: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'cncf-master' to be 'Running' but was 'Pending'
Jan  3 03:02:22.324: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.025633399s
Jan  3 03:02:22.324: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jan  3 03:02:22.324: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 01/03/23 03:02:22.324
Jan  3 03:02:22.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-5204 logs logs-generator logs-generator'
Jan  3 03:02:22.388: INFO: stderr: ""
Jan  3 03:02:22.388: INFO: stdout: "I0103 03:02:21.095658       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/h9l 488\nI0103 03:02:21.295838       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/kc9 527\nI0103 03:02:21.496444       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/6xbx 371\nI0103 03:02:21.695788       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/xbp 206\nI0103 03:02:21.896094       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/5rp 469\nI0103 03:02:22.096392       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/475f 269\nI0103 03:02:22.295714       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/qwrm 562\n"
STEP: limiting log lines 01/03/23 03:02:22.388
Jan  3 03:02:22.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-5204 logs logs-generator logs-generator --tail=1'
Jan  3 03:02:22.446: INFO: stderr: ""
Jan  3 03:02:22.446: INFO: stdout: "I0103 03:02:22.295714       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/qwrm 562\n"
Jan  3 03:02:22.446: INFO: got output "I0103 03:02:22.295714       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/qwrm 562\n"
STEP: limiting log bytes 01/03/23 03:02:22.446
Jan  3 03:02:22.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-5204 logs logs-generator logs-generator --limit-bytes=1'
Jan  3 03:02:22.503: INFO: stderr: ""
Jan  3 03:02:22.503: INFO: stdout: "I"
Jan  3 03:02:22.503: INFO: got output "I"
STEP: exposing timestamps 01/03/23 03:02:22.503
Jan  3 03:02:22.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-5204 logs logs-generator logs-generator --tail=1 --timestamps'
Jan  3 03:02:22.560: INFO: stderr: ""
Jan  3 03:02:22.560: INFO: stdout: "2023-01-03T12:02:22.496143149+09:00 I0103 03:02:22.496111       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/vptw 216\n"
Jan  3 03:02:22.560: INFO: got output "2023-01-03T12:02:22.496143149+09:00 I0103 03:02:22.496111       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/vptw 216\n"
STEP: restricting to a time range 01/03/23 03:02:22.56
Jan  3 03:02:25.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-5204 logs logs-generator logs-generator --since=1s'
Jan  3 03:02:25.122: INFO: stderr: ""
Jan  3 03:02:25.122: INFO: stdout: "I0103 03:02:24.296151       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/gl5 457\nI0103 03:02:24.496544       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/24b8 564\nI0103 03:02:24.695792       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/pc6s 394\nI0103 03:02:24.896148       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/4wl 446\nI0103 03:02:25.096571       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/lc5 321\n"
Jan  3 03:02:25.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-5204 logs logs-generator logs-generator --since=24h'
Jan  3 03:02:25.181: INFO: stderr: ""
Jan  3 03:02:25.181: INFO: stdout: "I0103 03:02:21.095658       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/h9l 488\nI0103 03:02:21.295838       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/kc9 527\nI0103 03:02:21.496444       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/6xbx 371\nI0103 03:02:21.695788       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/xbp 206\nI0103 03:02:21.896094       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/5rp 469\nI0103 03:02:22.096392       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/475f 269\nI0103 03:02:22.295714       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/qwrm 562\nI0103 03:02:22.496111       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/vptw 216\nI0103 03:02:22.696436       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/r4j 532\nI0103 03:02:22.895714       1 logs_generator.go:76] 9 POST /api/v1/namespaces/ns/pods/wdhr 411\nI0103 03:02:23.096058       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/bg5t 439\nI0103 03:02:23.296474       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/p4bd 452\nI0103 03:02:23.495810       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/default/pods/qfhb 494\nI0103 03:02:23.696170       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/l8k 210\nI0103 03:02:23.896526       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/l9n 332\nI0103 03:02:24.095811       1 logs_generator.go:76] 15 POST /api/v1/namespaces/default/pods/vzq5 584\nI0103 03:02:24.296151       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/gl5 457\nI0103 03:02:24.496544       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/24b8 564\nI0103 03:02:24.695792       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/pc6s 394\nI0103 03:02:24.896148       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/4wl 446\nI0103 03:02:25.096571       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/lc5 321\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1575
Jan  3 03:02:25.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-5204 delete pod logs-generator'
Jan  3 03:02:25.952: INFO: stderr: ""
Jan  3 03:02:25.952: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan  3 03:02:25.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5204" for this suite. 01/03/23 03:02:25.96
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","completed":269,"skipped":5169,"failed":0}
------------------------------
• [SLOW TEST] [5.749 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1567
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1590

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:02:20.214
    Jan  3 03:02:20.214: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename kubectl 01/03/23 03:02:20.215
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:02:20.232
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:02:20.234
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1570
    STEP: creating an pod 01/03/23 03:02:20.236
    Jan  3 03:02:20.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-5204 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Jan  3 03:02:20.298: INFO: stderr: ""
    Jan  3 03:02:20.298: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1590
    STEP: Waiting for log generator to start. 01/03/23 03:02:20.298
    Jan  3 03:02:20.298: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Jan  3 03:02:20.298: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-5204" to be "running and ready, or succeeded"
    Jan  3 03:02:20.321: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 22.86275ms
    Jan  3 03:02:20.321: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'cncf-master' to be 'Running' but was 'Pending'
    Jan  3 03:02:22.324: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.025633399s
    Jan  3 03:02:22.324: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Jan  3 03:02:22.324: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 01/03/23 03:02:22.324
    Jan  3 03:02:22.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-5204 logs logs-generator logs-generator'
    Jan  3 03:02:22.388: INFO: stderr: ""
    Jan  3 03:02:22.388: INFO: stdout: "I0103 03:02:21.095658       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/h9l 488\nI0103 03:02:21.295838       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/kc9 527\nI0103 03:02:21.496444       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/6xbx 371\nI0103 03:02:21.695788       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/xbp 206\nI0103 03:02:21.896094       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/5rp 469\nI0103 03:02:22.096392       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/475f 269\nI0103 03:02:22.295714       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/qwrm 562\n"
    STEP: limiting log lines 01/03/23 03:02:22.388
    Jan  3 03:02:22.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-5204 logs logs-generator logs-generator --tail=1'
    Jan  3 03:02:22.446: INFO: stderr: ""
    Jan  3 03:02:22.446: INFO: stdout: "I0103 03:02:22.295714       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/qwrm 562\n"
    Jan  3 03:02:22.446: INFO: got output "I0103 03:02:22.295714       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/qwrm 562\n"
    STEP: limiting log bytes 01/03/23 03:02:22.446
    Jan  3 03:02:22.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-5204 logs logs-generator logs-generator --limit-bytes=1'
    Jan  3 03:02:22.503: INFO: stderr: ""
    Jan  3 03:02:22.503: INFO: stdout: "I"
    Jan  3 03:02:22.503: INFO: got output "I"
    STEP: exposing timestamps 01/03/23 03:02:22.503
    Jan  3 03:02:22.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-5204 logs logs-generator logs-generator --tail=1 --timestamps'
    Jan  3 03:02:22.560: INFO: stderr: ""
    Jan  3 03:02:22.560: INFO: stdout: "2023-01-03T12:02:22.496143149+09:00 I0103 03:02:22.496111       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/vptw 216\n"
    Jan  3 03:02:22.560: INFO: got output "2023-01-03T12:02:22.496143149+09:00 I0103 03:02:22.496111       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/vptw 216\n"
    STEP: restricting to a time range 01/03/23 03:02:22.56
    Jan  3 03:02:25.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-5204 logs logs-generator logs-generator --since=1s'
    Jan  3 03:02:25.122: INFO: stderr: ""
    Jan  3 03:02:25.122: INFO: stdout: "I0103 03:02:24.296151       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/gl5 457\nI0103 03:02:24.496544       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/24b8 564\nI0103 03:02:24.695792       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/pc6s 394\nI0103 03:02:24.896148       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/4wl 446\nI0103 03:02:25.096571       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/lc5 321\n"
    Jan  3 03:02:25.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-5204 logs logs-generator logs-generator --since=24h'
    Jan  3 03:02:25.181: INFO: stderr: ""
    Jan  3 03:02:25.181: INFO: stdout: "I0103 03:02:21.095658       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/h9l 488\nI0103 03:02:21.295838       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/kc9 527\nI0103 03:02:21.496444       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/6xbx 371\nI0103 03:02:21.695788       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/xbp 206\nI0103 03:02:21.896094       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/5rp 469\nI0103 03:02:22.096392       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/475f 269\nI0103 03:02:22.295714       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/qwrm 562\nI0103 03:02:22.496111       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/vptw 216\nI0103 03:02:22.696436       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/r4j 532\nI0103 03:02:22.895714       1 logs_generator.go:76] 9 POST /api/v1/namespaces/ns/pods/wdhr 411\nI0103 03:02:23.096058       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/bg5t 439\nI0103 03:02:23.296474       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/p4bd 452\nI0103 03:02:23.495810       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/default/pods/qfhb 494\nI0103 03:02:23.696170       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/l8k 210\nI0103 03:02:23.896526       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/l9n 332\nI0103 03:02:24.095811       1 logs_generator.go:76] 15 POST /api/v1/namespaces/default/pods/vzq5 584\nI0103 03:02:24.296151       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/gl5 457\nI0103 03:02:24.496544       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/24b8 564\nI0103 03:02:24.695792       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/pc6s 394\nI0103 03:02:24.896148       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/4wl 446\nI0103 03:02:25.096571       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/lc5 321\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1575
    Jan  3 03:02:25.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-5204 delete pod logs-generator'
    Jan  3 03:02:25.952: INFO: stderr: ""
    Jan  3 03:02:25.952: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan  3 03:02:25.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5204" for this suite. 01/03/23 03:02:25.96
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:02:25.965
Jan  3 03:02:25.965: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename custom-resource-definition 01/03/23 03:02:25.966
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:02:25.999
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:02:26.001
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Jan  3 03:02:26.003: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  3 03:02:32.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4008" for this suite. 01/03/23 03:02:32.623
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","completed":270,"skipped":5212,"failed":0}
------------------------------
• [SLOW TEST] [6.661 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:02:25.965
    Jan  3 03:02:25.965: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename custom-resource-definition 01/03/23 03:02:25.966
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:02:25.999
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:02:26.001
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Jan  3 03:02:26.003: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  3 03:02:32.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-4008" for this suite. 01/03/23 03:02:32.623
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:02:32.626
Jan  3 03:02:32.627: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename runtimeclass 01/03/23 03:02:32.628
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:02:32.635
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:02:32.637
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-9831-delete-me 01/03/23 03:02:32.651
STEP: Waiting for the RuntimeClass to disappear 01/03/23 03:02:32.657
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan  3 03:02:32.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-9831" for this suite. 01/03/23 03:02:32.668
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]","completed":271,"skipped":5222,"failed":0}
------------------------------
• [0.044 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:02:32.626
    Jan  3 03:02:32.627: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename runtimeclass 01/03/23 03:02:32.628
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:02:32.635
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:02:32.637
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-9831-delete-me 01/03/23 03:02:32.651
    STEP: Waiting for the RuntimeClass to disappear 01/03/23 03:02:32.657
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan  3 03:02:32.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-9831" for this suite. 01/03/23 03:02:32.668
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:02:32.67
Jan  3 03:02:32.671: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename services 01/03/23 03:02:32.671
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:02:32.715
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:02:32.717
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3125 01/03/23 03:02:32.72
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/03/23 03:02:32.74
STEP: creating service externalsvc in namespace services-3125 01/03/23 03:02:32.74
STEP: creating replication controller externalsvc in namespace services-3125 01/03/23 03:02:32.768
I0103 03:02:32.785671      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3125, replica count: 2
I0103 03:02:35.837536      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 01/03/23 03:02:35.839
Jan  3 03:02:35.862: INFO: Creating new exec pod
Jan  3 03:02:35.873: INFO: Waiting up to 5m0s for pod "execpodk882x" in namespace "services-3125" to be "running"
Jan  3 03:02:35.880: INFO: Pod "execpodk882x": Phase="Pending", Reason="", readiness=false. Elapsed: 7.342813ms
Jan  3 03:02:37.883: INFO: Pod "execpodk882x": Phase="Running", Reason="", readiness=true. Elapsed: 2.009749169s
Jan  3 03:02:37.883: INFO: Pod "execpodk882x" satisfied condition "running"
Jan  3 03:02:37.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-3125 exec execpodk882x -- /bin/sh -x -c nslookup clusterip-service.services-3125.svc.cluster.local'
Jan  3 03:02:38.090: INFO: stderr: "+ nslookup clusterip-service.services-3125.svc.cluster.local\n"
Jan  3 03:02:38.090: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-3125.svc.cluster.local\tcanonical name = externalsvc.services-3125.svc.cluster.local.\nName:\texternalsvc.services-3125.svc.cluster.local\nAddress: 10.96.99.196\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3125, will wait for the garbage collector to delete the pods 01/03/23 03:02:38.09
Jan  3 03:02:38.145: INFO: Deleting ReplicationController externalsvc took: 2.59301ms
Jan  3 03:02:38.246: INFO: Terminating ReplicationController externalsvc pods took: 100.371582ms
Jan  3 03:02:40.259: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  3 03:02:40.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3125" for this suite. 01/03/23 03:02:40.311
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","completed":272,"skipped":5222,"failed":0}
------------------------------
• [SLOW TEST] [7.663 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:02:32.67
    Jan  3 03:02:32.671: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename services 01/03/23 03:02:32.671
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:02:32.715
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:02:32.717
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1481
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3125 01/03/23 03:02:32.72
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/03/23 03:02:32.74
    STEP: creating service externalsvc in namespace services-3125 01/03/23 03:02:32.74
    STEP: creating replication controller externalsvc in namespace services-3125 01/03/23 03:02:32.768
    I0103 03:02:32.785671      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3125, replica count: 2
    I0103 03:02:35.837536      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 01/03/23 03:02:35.839
    Jan  3 03:02:35.862: INFO: Creating new exec pod
    Jan  3 03:02:35.873: INFO: Waiting up to 5m0s for pod "execpodk882x" in namespace "services-3125" to be "running"
    Jan  3 03:02:35.880: INFO: Pod "execpodk882x": Phase="Pending", Reason="", readiness=false. Elapsed: 7.342813ms
    Jan  3 03:02:37.883: INFO: Pod "execpodk882x": Phase="Running", Reason="", readiness=true. Elapsed: 2.009749169s
    Jan  3 03:02:37.883: INFO: Pod "execpodk882x" satisfied condition "running"
    Jan  3 03:02:37.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-3125 exec execpodk882x -- /bin/sh -x -c nslookup clusterip-service.services-3125.svc.cluster.local'
    Jan  3 03:02:38.090: INFO: stderr: "+ nslookup clusterip-service.services-3125.svc.cluster.local\n"
    Jan  3 03:02:38.090: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-3125.svc.cluster.local\tcanonical name = externalsvc.services-3125.svc.cluster.local.\nName:\texternalsvc.services-3125.svc.cluster.local\nAddress: 10.96.99.196\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-3125, will wait for the garbage collector to delete the pods 01/03/23 03:02:38.09
    Jan  3 03:02:38.145: INFO: Deleting ReplicationController externalsvc took: 2.59301ms
    Jan  3 03:02:38.246: INFO: Terminating ReplicationController externalsvc pods took: 100.371582ms
    Jan  3 03:02:40.259: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  3 03:02:40.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3125" for this suite. 01/03/23 03:02:40.311
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:02:40.334
Jan  3 03:02:40.334: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename projected 01/03/23 03:02:40.335
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:02:40.344
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:02:40.346
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
STEP: Creating configMap with name projected-configmap-test-volume-map-a16770f9-da00-4675-904f-70b0e798a2c1 01/03/23 03:02:40.348
STEP: Creating a pod to test consume configMaps 01/03/23 03:02:40.361
Jan  3 03:02:40.370: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6054d277-2947-41ae-beb1-b4aa69144244" in namespace "projected-5318" to be "Succeeded or Failed"
Jan  3 03:02:40.384: INFO: Pod "pod-projected-configmaps-6054d277-2947-41ae-beb1-b4aa69144244": Phase="Pending", Reason="", readiness=false. Elapsed: 14.10245ms
Jan  3 03:02:42.387: INFO: Pod "pod-projected-configmaps-6054d277-2947-41ae-beb1-b4aa69144244": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017307032s
Jan  3 03:02:44.387: INFO: Pod "pod-projected-configmaps-6054d277-2947-41ae-beb1-b4aa69144244": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016820043s
STEP: Saw pod success 01/03/23 03:02:44.387
Jan  3 03:02:44.387: INFO: Pod "pod-projected-configmaps-6054d277-2947-41ae-beb1-b4aa69144244" satisfied condition "Succeeded or Failed"
Jan  3 03:02:44.389: INFO: Trying to get logs from node cncf-master pod pod-projected-configmaps-6054d277-2947-41ae-beb1-b4aa69144244 container agnhost-container: <nil>
STEP: delete the pod 01/03/23 03:02:44.392
Jan  3 03:02:44.429: INFO: Waiting for pod pod-projected-configmaps-6054d277-2947-41ae-beb1-b4aa69144244 to disappear
Jan  3 03:02:44.431: INFO: Pod pod-projected-configmaps-6054d277-2947-41ae-beb1-b4aa69144244 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan  3 03:02:44.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5318" for this suite. 01/03/23 03:02:44.433
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":273,"skipped":5223,"failed":0}
------------------------------
• [4.102 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:02:40.334
    Jan  3 03:02:40.334: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename projected 01/03/23 03:02:40.335
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:02:40.344
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:02:40.346
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:88
    STEP: Creating configMap with name projected-configmap-test-volume-map-a16770f9-da00-4675-904f-70b0e798a2c1 01/03/23 03:02:40.348
    STEP: Creating a pod to test consume configMaps 01/03/23 03:02:40.361
    Jan  3 03:02:40.370: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6054d277-2947-41ae-beb1-b4aa69144244" in namespace "projected-5318" to be "Succeeded or Failed"
    Jan  3 03:02:40.384: INFO: Pod "pod-projected-configmaps-6054d277-2947-41ae-beb1-b4aa69144244": Phase="Pending", Reason="", readiness=false. Elapsed: 14.10245ms
    Jan  3 03:02:42.387: INFO: Pod "pod-projected-configmaps-6054d277-2947-41ae-beb1-b4aa69144244": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017307032s
    Jan  3 03:02:44.387: INFO: Pod "pod-projected-configmaps-6054d277-2947-41ae-beb1-b4aa69144244": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016820043s
    STEP: Saw pod success 01/03/23 03:02:44.387
    Jan  3 03:02:44.387: INFO: Pod "pod-projected-configmaps-6054d277-2947-41ae-beb1-b4aa69144244" satisfied condition "Succeeded or Failed"
    Jan  3 03:02:44.389: INFO: Trying to get logs from node cncf-master pod pod-projected-configmaps-6054d277-2947-41ae-beb1-b4aa69144244 container agnhost-container: <nil>
    STEP: delete the pod 01/03/23 03:02:44.392
    Jan  3 03:02:44.429: INFO: Waiting for pod pod-projected-configmaps-6054d277-2947-41ae-beb1-b4aa69144244 to disappear
    Jan  3 03:02:44.431: INFO: Pod pod-projected-configmaps-6054d277-2947-41ae-beb1-b4aa69144244 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan  3 03:02:44.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5318" for this suite. 01/03/23 03:02:44.433
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:02:44.436
Jan  3 03:02:44.436: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename emptydir 01/03/23 03:02:44.437
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:02:44.448
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:02:44.45
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
STEP: Creating a pod to test emptydir 0777 on tmpfs 01/03/23 03:02:44.452
Jan  3 03:02:44.465: INFO: Waiting up to 5m0s for pod "pod-522d6dce-21d1-43f8-8b8d-49dbab328b85" in namespace "emptydir-6979" to be "Succeeded or Failed"
Jan  3 03:02:44.471: INFO: Pod "pod-522d6dce-21d1-43f8-8b8d-49dbab328b85": Phase="Pending", Reason="", readiness=false. Elapsed: 5.283285ms
Jan  3 03:02:46.473: INFO: Pod "pod-522d6dce-21d1-43f8-8b8d-49dbab328b85": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007459695s
Jan  3 03:02:48.474: INFO: Pod "pod-522d6dce-21d1-43f8-8b8d-49dbab328b85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008487268s
STEP: Saw pod success 01/03/23 03:02:48.474
Jan  3 03:02:48.474: INFO: Pod "pod-522d6dce-21d1-43f8-8b8d-49dbab328b85" satisfied condition "Succeeded or Failed"
Jan  3 03:02:48.476: INFO: Trying to get logs from node cncf-master pod pod-522d6dce-21d1-43f8-8b8d-49dbab328b85 container test-container: <nil>
STEP: delete the pod 01/03/23 03:02:48.481
Jan  3 03:02:48.528: INFO: Waiting for pod pod-522d6dce-21d1-43f8-8b8d-49dbab328b85 to disappear
Jan  3 03:02:48.534: INFO: Pod pod-522d6dce-21d1-43f8-8b8d-49dbab328b85 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan  3 03:02:48.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6979" for this suite. 01/03/23 03:02:48.537
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":274,"skipped":5224,"failed":0}
------------------------------
• [4.103 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:02:44.436
    Jan  3 03:02:44.436: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename emptydir 01/03/23 03:02:44.437
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:02:44.448
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:02:44.45
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:146
    STEP: Creating a pod to test emptydir 0777 on tmpfs 01/03/23 03:02:44.452
    Jan  3 03:02:44.465: INFO: Waiting up to 5m0s for pod "pod-522d6dce-21d1-43f8-8b8d-49dbab328b85" in namespace "emptydir-6979" to be "Succeeded or Failed"
    Jan  3 03:02:44.471: INFO: Pod "pod-522d6dce-21d1-43f8-8b8d-49dbab328b85": Phase="Pending", Reason="", readiness=false. Elapsed: 5.283285ms
    Jan  3 03:02:46.473: INFO: Pod "pod-522d6dce-21d1-43f8-8b8d-49dbab328b85": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007459695s
    Jan  3 03:02:48.474: INFO: Pod "pod-522d6dce-21d1-43f8-8b8d-49dbab328b85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008487268s
    STEP: Saw pod success 01/03/23 03:02:48.474
    Jan  3 03:02:48.474: INFO: Pod "pod-522d6dce-21d1-43f8-8b8d-49dbab328b85" satisfied condition "Succeeded or Failed"
    Jan  3 03:02:48.476: INFO: Trying to get logs from node cncf-master pod pod-522d6dce-21d1-43f8-8b8d-49dbab328b85 container test-container: <nil>
    STEP: delete the pod 01/03/23 03:02:48.481
    Jan  3 03:02:48.528: INFO: Waiting for pod pod-522d6dce-21d1-43f8-8b8d-49dbab328b85 to disappear
    Jan  3 03:02:48.534: INFO: Pod pod-522d6dce-21d1-43f8-8b8d-49dbab328b85 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan  3 03:02:48.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-6979" for this suite. 01/03/23 03:02:48.537
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:02:48.539
Jan  3 03:02:48.539: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename sched-preemption 01/03/23 03:02:48.54
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:02:48.551
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:02:48.553
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan  3 03:02:48.580: INFO: Waiting up to 1m0s for all nodes to be ready
Jan  3 03:03:48.658: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:03:48.659
Jan  3 03:03:48.660: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename sched-preemption-path 01/03/23 03:03:48.66
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:03:48.677
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:03:48.679
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:496
STEP: Finding an available node 01/03/23 03:03:48.681
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/03/23 03:03:48.682
Jan  3 03:03:48.687: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-3366" to be "running"
Jan  3 03:03:48.694: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 7.185134ms
Jan  3 03:03:50.698: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.010367601s
Jan  3 03:03:50.698: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/03/23 03:03:50.699
Jan  3 03:03:50.743: INFO: found a healthy node: cncf-master
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
Jan  3 03:04:06.852: INFO: pods created so far: [1 1 1]
Jan  3 03:04:06.852: INFO: length of pods created so far: 3
Jan  3 03:04:10.872: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:187
Jan  3 03:04:17.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-3366" for this suite. 01/03/23 03:04:17.878
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:470
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jan  3 03:04:17.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-554" for this suite. 01/03/23 03:04:17.915
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","completed":275,"skipped":5224,"failed":0}
------------------------------
• [SLOW TEST] [89.443 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:458
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:543

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:02:48.539
    Jan  3 03:02:48.539: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename sched-preemption 01/03/23 03:02:48.54
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:02:48.551
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:02:48.553
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jan  3 03:02:48.580: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan  3 03:03:48.658: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:03:48.659
    Jan  3 03:03:48.660: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename sched-preemption-path 01/03/23 03:03:48.66
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:03:48.677
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:03:48.679
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:496
    STEP: Finding an available node 01/03/23 03:03:48.681
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/03/23 03:03:48.682
    Jan  3 03:03:48.687: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-3366" to be "running"
    Jan  3 03:03:48.694: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 7.185134ms
    Jan  3 03:03:50.698: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.010367601s
    Jan  3 03:03:50.698: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/03/23 03:03:50.699
    Jan  3 03:03:50.743: INFO: found a healthy node: cncf-master
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:543
    Jan  3 03:04:06.852: INFO: pods created so far: [1 1 1]
    Jan  3 03:04:06.852: INFO: length of pods created so far: 3
    Jan  3 03:04:10.872: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:187
    Jan  3 03:04:17.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-3366" for this suite. 01/03/23 03:04:17.878
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:470
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jan  3 03:04:17.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-554" for this suite. 01/03/23 03:04:17.915
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:04:17.985
Jan  3 03:04:17.985: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename projected 01/03/23 03:04:17.987
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:04:18.005
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:04:18.007
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
STEP: Creating projection with secret that has name projected-secret-test-5e40600d-a445-45db-a4eb-45a5db5dece8 01/03/23 03:04:18.009
STEP: Creating a pod to test consume secrets 01/03/23 03:04:18.013
Jan  3 03:04:18.031: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2a8154c6-4d5e-4c1e-81cd-08044470fb84" in namespace "projected-3177" to be "Succeeded or Failed"
Jan  3 03:04:18.035: INFO: Pod "pod-projected-secrets-2a8154c6-4d5e-4c1e-81cd-08044470fb84": Phase="Pending", Reason="", readiness=false. Elapsed: 3.619413ms
Jan  3 03:04:20.037: INFO: Pod "pod-projected-secrets-2a8154c6-4d5e-4c1e-81cd-08044470fb84": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005847684s
Jan  3 03:04:22.038: INFO: Pod "pod-projected-secrets-2a8154c6-4d5e-4c1e-81cd-08044470fb84": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00676306s
Jan  3 03:04:24.037: INFO: Pod "pod-projected-secrets-2a8154c6-4d5e-4c1e-81cd-08044470fb84": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006519737s
STEP: Saw pod success 01/03/23 03:04:24.037
Jan  3 03:04:24.038: INFO: Pod "pod-projected-secrets-2a8154c6-4d5e-4c1e-81cd-08044470fb84" satisfied condition "Succeeded or Failed"
Jan  3 03:04:24.039: INFO: Trying to get logs from node cncf-master pod pod-projected-secrets-2a8154c6-4d5e-4c1e-81cd-08044470fb84 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/03/23 03:04:24.05
Jan  3 03:04:24.072: INFO: Waiting for pod pod-projected-secrets-2a8154c6-4d5e-4c1e-81cd-08044470fb84 to disappear
Jan  3 03:04:24.075: INFO: Pod pod-projected-secrets-2a8154c6-4d5e-4c1e-81cd-08044470fb84 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan  3 03:04:24.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3177" for this suite. 01/03/23 03:04:24.078
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","completed":276,"skipped":5264,"failed":0}
------------------------------
• [SLOW TEST] [6.095 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:04:17.985
    Jan  3 03:04:17.985: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename projected 01/03/23 03:04:17.987
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:04:18.005
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:04:18.007
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:45
    STEP: Creating projection with secret that has name projected-secret-test-5e40600d-a445-45db-a4eb-45a5db5dece8 01/03/23 03:04:18.009
    STEP: Creating a pod to test consume secrets 01/03/23 03:04:18.013
    Jan  3 03:04:18.031: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2a8154c6-4d5e-4c1e-81cd-08044470fb84" in namespace "projected-3177" to be "Succeeded or Failed"
    Jan  3 03:04:18.035: INFO: Pod "pod-projected-secrets-2a8154c6-4d5e-4c1e-81cd-08044470fb84": Phase="Pending", Reason="", readiness=false. Elapsed: 3.619413ms
    Jan  3 03:04:20.037: INFO: Pod "pod-projected-secrets-2a8154c6-4d5e-4c1e-81cd-08044470fb84": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005847684s
    Jan  3 03:04:22.038: INFO: Pod "pod-projected-secrets-2a8154c6-4d5e-4c1e-81cd-08044470fb84": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00676306s
    Jan  3 03:04:24.037: INFO: Pod "pod-projected-secrets-2a8154c6-4d5e-4c1e-81cd-08044470fb84": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006519737s
    STEP: Saw pod success 01/03/23 03:04:24.037
    Jan  3 03:04:24.038: INFO: Pod "pod-projected-secrets-2a8154c6-4d5e-4c1e-81cd-08044470fb84" satisfied condition "Succeeded or Failed"
    Jan  3 03:04:24.039: INFO: Trying to get logs from node cncf-master pod pod-projected-secrets-2a8154c6-4d5e-4c1e-81cd-08044470fb84 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/03/23 03:04:24.05
    Jan  3 03:04:24.072: INFO: Waiting for pod pod-projected-secrets-2a8154c6-4d5e-4c1e-81cd-08044470fb84 to disappear
    Jan  3 03:04:24.075: INFO: Pod pod-projected-secrets-2a8154c6-4d5e-4c1e-81cd-08044470fb84 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan  3 03:04:24.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3177" for this suite. 01/03/23 03:04:24.078
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:04:24.081
Jan  3 03:04:24.081: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename endpointslice 01/03/23 03:04:24.082
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:04:24.093
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:04:24.094
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
STEP: referencing a single matching pod 01/03/23 03:04:29.306
STEP: referencing matching pods with named port 01/03/23 03:04:34.313
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 01/03/23 03:04:39.317
STEP: recreating EndpointSlices after they've been deleted 01/03/23 03:04:44.322
Jan  3 03:04:44.345: INFO: EndpointSlice for Service endpointslice-7155/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jan  3 03:04:54.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-7155" for this suite. 01/03/23 03:04:54.356
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","completed":277,"skipped":5277,"failed":0}
------------------------------
• [SLOW TEST] [30.278 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:04:24.081
    Jan  3 03:04:24.081: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename endpointslice 01/03/23 03:04:24.082
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:04:24.093
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:04:24.094
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:204
    STEP: referencing a single matching pod 01/03/23 03:04:29.306
    STEP: referencing matching pods with named port 01/03/23 03:04:34.313
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 01/03/23 03:04:39.317
    STEP: recreating EndpointSlices after they've been deleted 01/03/23 03:04:44.322
    Jan  3 03:04:44.345: INFO: EndpointSlice for Service endpointslice-7155/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jan  3 03:04:54.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-7155" for this suite. 01/03/23 03:04:54.356
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:04:54.36
Jan  3 03:04:54.360: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename containers 01/03/23 03:04:54.361
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:04:54.38
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:04:54.382
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
STEP: Creating a pod to test override command 01/03/23 03:04:54.384
Jan  3 03:04:54.389: INFO: Waiting up to 5m0s for pod "client-containers-01a0003e-d4a7-4be0-9215-3123c1498b61" in namespace "containers-2117" to be "Succeeded or Failed"
Jan  3 03:04:54.398: INFO: Pod "client-containers-01a0003e-d4a7-4be0-9215-3123c1498b61": Phase="Pending", Reason="", readiness=false. Elapsed: 8.358172ms
Jan  3 03:04:56.400: INFO: Pod "client-containers-01a0003e-d4a7-4be0-9215-3123c1498b61": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010355339s
Jan  3 03:04:58.400: INFO: Pod "client-containers-01a0003e-d4a7-4be0-9215-3123c1498b61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010956448s
STEP: Saw pod success 01/03/23 03:04:58.4
Jan  3 03:04:58.401: INFO: Pod "client-containers-01a0003e-d4a7-4be0-9215-3123c1498b61" satisfied condition "Succeeded or Failed"
Jan  3 03:04:58.402: INFO: Trying to get logs from node cncf-master pod client-containers-01a0003e-d4a7-4be0-9215-3123c1498b61 container agnhost-container: <nil>
STEP: delete the pod 01/03/23 03:04:58.405
Jan  3 03:04:58.417: INFO: Waiting for pod client-containers-01a0003e-d4a7-4be0-9215-3123c1498b61 to disappear
Jan  3 03:04:58.420: INFO: Pod client-containers-01a0003e-d4a7-4be0-9215-3123c1498b61 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jan  3 03:04:58.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2117" for this suite. 01/03/23 03:04:58.422
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]","completed":278,"skipped":5299,"failed":0}
------------------------------
• [4.090 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:04:54.36
    Jan  3 03:04:54.360: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename containers 01/03/23 03:04:54.361
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:04:54.38
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:04:54.382
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:72
    STEP: Creating a pod to test override command 01/03/23 03:04:54.384
    Jan  3 03:04:54.389: INFO: Waiting up to 5m0s for pod "client-containers-01a0003e-d4a7-4be0-9215-3123c1498b61" in namespace "containers-2117" to be "Succeeded or Failed"
    Jan  3 03:04:54.398: INFO: Pod "client-containers-01a0003e-d4a7-4be0-9215-3123c1498b61": Phase="Pending", Reason="", readiness=false. Elapsed: 8.358172ms
    Jan  3 03:04:56.400: INFO: Pod "client-containers-01a0003e-d4a7-4be0-9215-3123c1498b61": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010355339s
    Jan  3 03:04:58.400: INFO: Pod "client-containers-01a0003e-d4a7-4be0-9215-3123c1498b61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010956448s
    STEP: Saw pod success 01/03/23 03:04:58.4
    Jan  3 03:04:58.401: INFO: Pod "client-containers-01a0003e-d4a7-4be0-9215-3123c1498b61" satisfied condition "Succeeded or Failed"
    Jan  3 03:04:58.402: INFO: Trying to get logs from node cncf-master pod client-containers-01a0003e-d4a7-4be0-9215-3123c1498b61 container agnhost-container: <nil>
    STEP: delete the pod 01/03/23 03:04:58.405
    Jan  3 03:04:58.417: INFO: Waiting for pod client-containers-01a0003e-d4a7-4be0-9215-3123c1498b61 to disappear
    Jan  3 03:04:58.420: INFO: Pod client-containers-01a0003e-d4a7-4be0-9215-3123c1498b61 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jan  3 03:04:58.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-2117" for this suite. 01/03/23 03:04:58.422
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:04:58.45
Jan  3 03:04:58.451: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename resourcequota 01/03/23 03:04:58.451
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:04:58.466
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:04:58.468
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
STEP: Counting existing ResourceQuota 01/03/23 03:04:58.47
STEP: Creating a ResourceQuota 01/03/23 03:05:03.472
STEP: Ensuring resource quota status is calculated 01/03/23 03:05:03.476
STEP: Creating a Pod that fits quota 01/03/23 03:05:05.478
STEP: Ensuring ResourceQuota status captures the pod usage 01/03/23 03:05:05.5
STEP: Not allowing a pod to be created that exceeds remaining quota 01/03/23 03:05:07.502
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 01/03/23 03:05:07.506
STEP: Ensuring a pod cannot update its resource requirements 01/03/23 03:05:07.509
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 01/03/23 03:05:07.512
STEP: Deleting the pod 01/03/23 03:05:09.515
STEP: Ensuring resource quota status released the pod usage 01/03/23 03:05:09.54
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan  3 03:05:11.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6017" for this suite. 01/03/23 03:05:11.546
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","completed":279,"skipped":5310,"failed":0}
------------------------------
• [SLOW TEST] [13.098 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:04:58.45
    Jan  3 03:04:58.451: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename resourcequota 01/03/23 03:04:58.451
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:04:58.466
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:04:58.468
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:220
    STEP: Counting existing ResourceQuota 01/03/23 03:04:58.47
    STEP: Creating a ResourceQuota 01/03/23 03:05:03.472
    STEP: Ensuring resource quota status is calculated 01/03/23 03:05:03.476
    STEP: Creating a Pod that fits quota 01/03/23 03:05:05.478
    STEP: Ensuring ResourceQuota status captures the pod usage 01/03/23 03:05:05.5
    STEP: Not allowing a pod to be created that exceeds remaining quota 01/03/23 03:05:07.502
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 01/03/23 03:05:07.506
    STEP: Ensuring a pod cannot update its resource requirements 01/03/23 03:05:07.509
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 01/03/23 03:05:07.512
    STEP: Deleting the pod 01/03/23 03:05:09.515
    STEP: Ensuring resource quota status released the pod usage 01/03/23 03:05:09.54
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan  3 03:05:11.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-6017" for this suite. 01/03/23 03:05:11.546
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:05:11.552
Jan  3 03:05:11.553: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename kubectl 01/03/23 03:05:11.553
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:05:11.564
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:05:11.566
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
STEP: creating all guestbook components 01/03/23 03:05:11.568
Jan  3 03:05:11.568: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jan  3 03:05:11.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-7357 create -f -'
Jan  3 03:05:13.001: INFO: stderr: ""
Jan  3 03:05:13.001: INFO: stdout: "service/agnhost-replica created\n"
Jan  3 03:05:13.001: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jan  3 03:05:13.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-7357 create -f -'
Jan  3 03:05:13.275: INFO: stderr: ""
Jan  3 03:05:13.275: INFO: stdout: "service/agnhost-primary created\n"
Jan  3 03:05:13.275: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jan  3 03:05:13.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-7357 create -f -'
Jan  3 03:05:13.538: INFO: stderr: ""
Jan  3 03:05:13.538: INFO: stdout: "service/frontend created\n"
Jan  3 03:05:13.538: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jan  3 03:05:13.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-7357 create -f -'
Jan  3 03:05:13.755: INFO: stderr: ""
Jan  3 03:05:13.755: INFO: stdout: "deployment.apps/frontend created\n"
Jan  3 03:05:13.755: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan  3 03:05:13.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-7357 create -f -'
Jan  3 03:05:13.992: INFO: stderr: ""
Jan  3 03:05:13.992: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jan  3 03:05:13.992: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan  3 03:05:13.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-7357 create -f -'
Jan  3 03:05:14.243: INFO: stderr: ""
Jan  3 03:05:14.243: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 01/03/23 03:05:14.243
Jan  3 03:05:14.244: INFO: Waiting for all frontend pods to be Running.
Jan  3 03:05:19.297: INFO: Waiting for frontend to serve content.
Jan  3 03:05:19.303: INFO: Trying to add a new entry to the guestbook.
Jan  3 03:05:19.310: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 01/03/23 03:05:19.316
Jan  3 03:05:19.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-7357 delete --grace-period=0 --force -f -'
Jan  3 03:05:19.450: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  3 03:05:19.450: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 01/03/23 03:05:19.451
Jan  3 03:05:19.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-7357 delete --grace-period=0 --force -f -'
Jan  3 03:05:19.792: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  3 03:05:19.792: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 01/03/23 03:05:19.792
Jan  3 03:05:19.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-7357 delete --grace-period=0 --force -f -'
Jan  3 03:05:19.899: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  3 03:05:19.899: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 01/03/23 03:05:19.899
Jan  3 03:05:19.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-7357 delete --grace-period=0 --force -f -'
Jan  3 03:05:19.957: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  3 03:05:19.957: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 01/03/23 03:05:19.957
Jan  3 03:05:19.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-7357 delete --grace-period=0 --force -f -'
Jan  3 03:05:20.031: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  3 03:05:20.031: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 01/03/23 03:05:20.031
Jan  3 03:05:20.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-7357 delete --grace-period=0 --force -f -'
Jan  3 03:05:20.095: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  3 03:05:20.095: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan  3 03:05:20.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7357" for this suite. 01/03/23 03:05:20.099
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","completed":280,"skipped":5343,"failed":0}
------------------------------
• [SLOW TEST] [8.599 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:367
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:05:11.552
    Jan  3 03:05:11.553: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename kubectl 01/03/23 03:05:11.553
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:05:11.564
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:05:11.566
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:392
    STEP: creating all guestbook components 01/03/23 03:05:11.568
    Jan  3 03:05:11.568: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Jan  3 03:05:11.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-7357 create -f -'
    Jan  3 03:05:13.001: INFO: stderr: ""
    Jan  3 03:05:13.001: INFO: stdout: "service/agnhost-replica created\n"
    Jan  3 03:05:13.001: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Jan  3 03:05:13.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-7357 create -f -'
    Jan  3 03:05:13.275: INFO: stderr: ""
    Jan  3 03:05:13.275: INFO: stdout: "service/agnhost-primary created\n"
    Jan  3 03:05:13.275: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Jan  3 03:05:13.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-7357 create -f -'
    Jan  3 03:05:13.538: INFO: stderr: ""
    Jan  3 03:05:13.538: INFO: stdout: "service/frontend created\n"
    Jan  3 03:05:13.538: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Jan  3 03:05:13.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-7357 create -f -'
    Jan  3 03:05:13.755: INFO: stderr: ""
    Jan  3 03:05:13.755: INFO: stdout: "deployment.apps/frontend created\n"
    Jan  3 03:05:13.755: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jan  3 03:05:13.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-7357 create -f -'
    Jan  3 03:05:13.992: INFO: stderr: ""
    Jan  3 03:05:13.992: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Jan  3 03:05:13.992: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jan  3 03:05:13.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-7357 create -f -'
    Jan  3 03:05:14.243: INFO: stderr: ""
    Jan  3 03:05:14.243: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 01/03/23 03:05:14.243
    Jan  3 03:05:14.244: INFO: Waiting for all frontend pods to be Running.
    Jan  3 03:05:19.297: INFO: Waiting for frontend to serve content.
    Jan  3 03:05:19.303: INFO: Trying to add a new entry to the guestbook.
    Jan  3 03:05:19.310: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 01/03/23 03:05:19.316
    Jan  3 03:05:19.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-7357 delete --grace-period=0 --force -f -'
    Jan  3 03:05:19.450: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  3 03:05:19.450: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 01/03/23 03:05:19.451
    Jan  3 03:05:19.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-7357 delete --grace-period=0 --force -f -'
    Jan  3 03:05:19.792: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  3 03:05:19.792: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 01/03/23 03:05:19.792
    Jan  3 03:05:19.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-7357 delete --grace-period=0 --force -f -'
    Jan  3 03:05:19.899: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  3 03:05:19.899: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 01/03/23 03:05:19.899
    Jan  3 03:05:19.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-7357 delete --grace-period=0 --force -f -'
    Jan  3 03:05:19.957: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  3 03:05:19.957: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 01/03/23 03:05:19.957
    Jan  3 03:05:19.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-7357 delete --grace-period=0 --force -f -'
    Jan  3 03:05:20.031: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  3 03:05:20.031: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 01/03/23 03:05:20.031
    Jan  3 03:05:20.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-7357 delete --grace-period=0 --force -f -'
    Jan  3 03:05:20.095: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  3 03:05:20.095: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan  3 03:05:20.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7357" for this suite. 01/03/23 03:05:20.099
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:05:20.154
Jan  3 03:05:20.154: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename crd-publish-openapi 01/03/23 03:05:20.155
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:05:20.18
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:05:20.182
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
STEP: set up a multi version CRD 01/03/23 03:05:20.185
Jan  3 03:05:20.185: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: rename a version 01/03/23 03:05:32.799
STEP: check the new version name is served 01/03/23 03:05:32.809
STEP: check the old version name is removed 01/03/23 03:05:37.667
STEP: check the other version is not changed 01/03/23 03:05:40.081
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  3 03:05:50.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1368" for this suite. 01/03/23 03:05:50.246
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","completed":281,"skipped":5393,"failed":0}
------------------------------
• [SLOW TEST] [30.095 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:05:20.154
    Jan  3 03:05:20.154: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename crd-publish-openapi 01/03/23 03:05:20.155
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:05:20.18
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:05:20.182
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:390
    STEP: set up a multi version CRD 01/03/23 03:05:20.185
    Jan  3 03:05:20.185: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: rename a version 01/03/23 03:05:32.799
    STEP: check the new version name is served 01/03/23 03:05:32.809
    STEP: check the old version name is removed 01/03/23 03:05:37.667
    STEP: check the other version is not changed 01/03/23 03:05:40.081
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  3 03:05:50.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-1368" for this suite. 01/03/23 03:05:50.246
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:05:50.249
Jan  3 03:05:50.249: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename disruption 01/03/23 03:05:50.25
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:05:50.26
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:05:50.262
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
STEP: Waiting for the pdb to be processed 01/03/23 03:05:50.305
STEP: Waiting for all pods to be running 01/03/23 03:05:52.332
Jan  3 03:05:52.337: INFO: running pods: 0 < 3
Jan  3 03:05:54.341: INFO: running pods: 2 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan  3 03:05:56.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9671" for this suite. 01/03/23 03:05:56.355
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","completed":282,"skipped":5393,"failed":0}
------------------------------
• [SLOW TEST] [6.109 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:05:50.249
    Jan  3 03:05:50.249: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename disruption 01/03/23 03:05:50.25
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:05:50.26
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:05:50.262
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:140
    STEP: Waiting for the pdb to be processed 01/03/23 03:05:50.305
    STEP: Waiting for all pods to be running 01/03/23 03:05:52.332
    Jan  3 03:05:52.337: INFO: running pods: 0 < 3
    Jan  3 03:05:54.341: INFO: running pods: 2 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan  3 03:05:56.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-9671" for this suite. 01/03/23 03:05:56.355
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:05:56.358
Jan  3 03:05:56.358: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename secrets 01/03/23 03:05:56.359
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:05:56.369
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:05:56.371
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
STEP: Creating secret with name secret-test-0a21ef4a-f350-4eea-9b62-731a916340a3 01/03/23 03:05:56.373
STEP: Creating a pod to test consume secrets 01/03/23 03:05:56.383
Jan  3 03:05:56.395: INFO: Waiting up to 5m0s for pod "pod-secrets-d510a17b-7e0e-4349-9ef6-bab89c407eef" in namespace "secrets-7476" to be "Succeeded or Failed"
Jan  3 03:05:56.406: INFO: Pod "pod-secrets-d510a17b-7e0e-4349-9ef6-bab89c407eef": Phase="Pending", Reason="", readiness=false. Elapsed: 11.362888ms
Jan  3 03:05:58.431: INFO: Pod "pod-secrets-d510a17b-7e0e-4349-9ef6-bab89c407eef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036332351s
Jan  3 03:06:00.409: INFO: Pod "pod-secrets-d510a17b-7e0e-4349-9ef6-bab89c407eef": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014370317s
Jan  3 03:06:02.409: INFO: Pod "pod-secrets-d510a17b-7e0e-4349-9ef6-bab89c407eef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01438494s
STEP: Saw pod success 01/03/23 03:06:02.409
Jan  3 03:06:02.410: INFO: Pod "pod-secrets-d510a17b-7e0e-4349-9ef6-bab89c407eef" satisfied condition "Succeeded or Failed"
Jan  3 03:06:02.411: INFO: Trying to get logs from node cncf-master pod pod-secrets-d510a17b-7e0e-4349-9ef6-bab89c407eef container secret-volume-test: <nil>
STEP: delete the pod 01/03/23 03:06:02.415
Jan  3 03:06:02.505: INFO: Waiting for pod pod-secrets-d510a17b-7e0e-4349-9ef6-bab89c407eef to disappear
Jan  3 03:06:02.525: INFO: Pod pod-secrets-d510a17b-7e0e-4349-9ef6-bab89c407eef no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan  3 03:06:02.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7476" for this suite. 01/03/23 03:06:02.531
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":283,"skipped":5395,"failed":0}
------------------------------
• [SLOW TEST] [6.179 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:05:56.358
    Jan  3 03:05:56.358: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename secrets 01/03/23 03:05:56.359
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:05:56.369
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:05:56.371
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:56
    STEP: Creating secret with name secret-test-0a21ef4a-f350-4eea-9b62-731a916340a3 01/03/23 03:05:56.373
    STEP: Creating a pod to test consume secrets 01/03/23 03:05:56.383
    Jan  3 03:05:56.395: INFO: Waiting up to 5m0s for pod "pod-secrets-d510a17b-7e0e-4349-9ef6-bab89c407eef" in namespace "secrets-7476" to be "Succeeded or Failed"
    Jan  3 03:05:56.406: INFO: Pod "pod-secrets-d510a17b-7e0e-4349-9ef6-bab89c407eef": Phase="Pending", Reason="", readiness=false. Elapsed: 11.362888ms
    Jan  3 03:05:58.431: INFO: Pod "pod-secrets-d510a17b-7e0e-4349-9ef6-bab89c407eef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036332351s
    Jan  3 03:06:00.409: INFO: Pod "pod-secrets-d510a17b-7e0e-4349-9ef6-bab89c407eef": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014370317s
    Jan  3 03:06:02.409: INFO: Pod "pod-secrets-d510a17b-7e0e-4349-9ef6-bab89c407eef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01438494s
    STEP: Saw pod success 01/03/23 03:06:02.409
    Jan  3 03:06:02.410: INFO: Pod "pod-secrets-d510a17b-7e0e-4349-9ef6-bab89c407eef" satisfied condition "Succeeded or Failed"
    Jan  3 03:06:02.411: INFO: Trying to get logs from node cncf-master pod pod-secrets-d510a17b-7e0e-4349-9ef6-bab89c407eef container secret-volume-test: <nil>
    STEP: delete the pod 01/03/23 03:06:02.415
    Jan  3 03:06:02.505: INFO: Waiting for pod pod-secrets-d510a17b-7e0e-4349-9ef6-bab89c407eef to disappear
    Jan  3 03:06:02.525: INFO: Pod pod-secrets-d510a17b-7e0e-4349-9ef6-bab89c407eef no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan  3 03:06:02.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-7476" for this suite. 01/03/23 03:06:02.531
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:06:02.537
Jan  3 03:06:02.537: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename emptydir 01/03/23 03:06:02.538
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:06:02.582
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:06:02.584
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
STEP: Creating a pod to test emptydir volume type on node default medium 01/03/23 03:06:02.586
Jan  3 03:06:02.662: INFO: Waiting up to 5m0s for pod "pod-abf141e4-fcff-468a-bd4a-c2f04a6337df" in namespace "emptydir-179" to be "Succeeded or Failed"
Jan  3 03:06:02.680: INFO: Pod "pod-abf141e4-fcff-468a-bd4a-c2f04a6337df": Phase="Pending", Reason="", readiness=false. Elapsed: 18.066796ms
Jan  3 03:06:04.682: INFO: Pod "pod-abf141e4-fcff-468a-bd4a-c2f04a6337df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02024792s
Jan  3 03:06:06.682: INFO: Pod "pod-abf141e4-fcff-468a-bd4a-c2f04a6337df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020444934s
STEP: Saw pod success 01/03/23 03:06:06.682
Jan  3 03:06:06.682: INFO: Pod "pod-abf141e4-fcff-468a-bd4a-c2f04a6337df" satisfied condition "Succeeded or Failed"
Jan  3 03:06:06.684: INFO: Trying to get logs from node cncf-master pod pod-abf141e4-fcff-468a-bd4a-c2f04a6337df container test-container: <nil>
STEP: delete the pod 01/03/23 03:06:06.687
Jan  3 03:06:06.702: INFO: Waiting for pod pod-abf141e4-fcff-468a-bd4a-c2f04a6337df to disappear
Jan  3 03:06:06.707: INFO: Pod pod-abf141e4-fcff-468a-bd4a-c2f04a6337df no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan  3 03:06:06.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-179" for this suite. 01/03/23 03:06:06.71
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":284,"skipped":5395,"failed":0}
------------------------------
• [4.186 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:06:02.537
    Jan  3 03:06:02.537: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename emptydir 01/03/23 03:06:02.538
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:06:02.582
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:06:02.584
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:156
    STEP: Creating a pod to test emptydir volume type on node default medium 01/03/23 03:06:02.586
    Jan  3 03:06:02.662: INFO: Waiting up to 5m0s for pod "pod-abf141e4-fcff-468a-bd4a-c2f04a6337df" in namespace "emptydir-179" to be "Succeeded or Failed"
    Jan  3 03:06:02.680: INFO: Pod "pod-abf141e4-fcff-468a-bd4a-c2f04a6337df": Phase="Pending", Reason="", readiness=false. Elapsed: 18.066796ms
    Jan  3 03:06:04.682: INFO: Pod "pod-abf141e4-fcff-468a-bd4a-c2f04a6337df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02024792s
    Jan  3 03:06:06.682: INFO: Pod "pod-abf141e4-fcff-468a-bd4a-c2f04a6337df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020444934s
    STEP: Saw pod success 01/03/23 03:06:06.682
    Jan  3 03:06:06.682: INFO: Pod "pod-abf141e4-fcff-468a-bd4a-c2f04a6337df" satisfied condition "Succeeded or Failed"
    Jan  3 03:06:06.684: INFO: Trying to get logs from node cncf-master pod pod-abf141e4-fcff-468a-bd4a-c2f04a6337df container test-container: <nil>
    STEP: delete the pod 01/03/23 03:06:06.687
    Jan  3 03:06:06.702: INFO: Waiting for pod pod-abf141e4-fcff-468a-bd4a-c2f04a6337df to disappear
    Jan  3 03:06:06.707: INFO: Pod pod-abf141e4-fcff-468a-bd4a-c2f04a6337df no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan  3 03:06:06.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-179" for this suite. 01/03/23 03:06:06.71
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:06:06.726
Jan  3 03:06:06.726: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename kubectl 01/03/23 03:06:06.727
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:06:06.736
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:06:06.738
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
STEP: creating a replication controller 01/03/23 03:06:06.74
Jan  3 03:06:06.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-1792 create -f -'
Jan  3 03:06:07.614: INFO: stderr: ""
Jan  3 03:06:07.614: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/03/23 03:06:07.614
Jan  3 03:06:07.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-1792 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan  3 03:06:07.693: INFO: stderr: ""
Jan  3 03:06:07.693: INFO: stdout: "update-demo-nautilus-4xqkd update-demo-nautilus-9rmc6 "
Jan  3 03:06:07.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-1792 get pods update-demo-nautilus-4xqkd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  3 03:06:07.751: INFO: stderr: ""
Jan  3 03:06:07.751: INFO: stdout: ""
Jan  3 03:06:07.751: INFO: update-demo-nautilus-4xqkd is created but not running
Jan  3 03:06:12.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-1792 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan  3 03:06:12.810: INFO: stderr: ""
Jan  3 03:06:12.810: INFO: stdout: "update-demo-nautilus-4xqkd update-demo-nautilus-9rmc6 "
Jan  3 03:06:12.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-1792 get pods update-demo-nautilus-4xqkd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  3 03:06:12.867: INFO: stderr: ""
Jan  3 03:06:12.867: INFO: stdout: "true"
Jan  3 03:06:12.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-1792 get pods update-demo-nautilus-4xqkd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan  3 03:06:12.925: INFO: stderr: ""
Jan  3 03:06:12.925: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan  3 03:06:12.925: INFO: validating pod update-demo-nautilus-4xqkd
Jan  3 03:06:12.928: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  3 03:06:12.928: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  3 03:06:12.928: INFO: update-demo-nautilus-4xqkd is verified up and running
Jan  3 03:06:12.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-1792 get pods update-demo-nautilus-9rmc6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  3 03:06:12.989: INFO: stderr: ""
Jan  3 03:06:12.989: INFO: stdout: "true"
Jan  3 03:06:12.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-1792 get pods update-demo-nautilus-9rmc6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan  3 03:06:13.044: INFO: stderr: ""
Jan  3 03:06:13.044: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan  3 03:06:13.044: INFO: validating pod update-demo-nautilus-9rmc6
Jan  3 03:06:13.047: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  3 03:06:13.047: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  3 03:06:13.047: INFO: update-demo-nautilus-9rmc6 is verified up and running
STEP: using delete to clean up resources 01/03/23 03:06:13.047
Jan  3 03:06:13.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-1792 delete --grace-period=0 --force -f -'
Jan  3 03:06:13.116: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  3 03:06:13.116: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan  3 03:06:13.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-1792 get rc,svc -l name=update-demo --no-headers'
Jan  3 03:06:13.217: INFO: stderr: "No resources found in kubectl-1792 namespace.\n"
Jan  3 03:06:13.217: INFO: stdout: ""
Jan  3 03:06:13.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-1792 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan  3 03:06:13.280: INFO: stderr: ""
Jan  3 03:06:13.280: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan  3 03:06:13.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1792" for this suite. 01/03/23 03:06:13.284
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","completed":285,"skipped":5454,"failed":0}
------------------------------
• [SLOW TEST] [6.560 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:337

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:06:06.726
    Jan  3 03:06:06.726: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename kubectl 01/03/23 03:06:06.727
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:06:06.736
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:06:06.738
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:337
    STEP: creating a replication controller 01/03/23 03:06:06.74
    Jan  3 03:06:06.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-1792 create -f -'
    Jan  3 03:06:07.614: INFO: stderr: ""
    Jan  3 03:06:07.614: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/03/23 03:06:07.614
    Jan  3 03:06:07.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-1792 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan  3 03:06:07.693: INFO: stderr: ""
    Jan  3 03:06:07.693: INFO: stdout: "update-demo-nautilus-4xqkd update-demo-nautilus-9rmc6 "
    Jan  3 03:06:07.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-1792 get pods update-demo-nautilus-4xqkd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  3 03:06:07.751: INFO: stderr: ""
    Jan  3 03:06:07.751: INFO: stdout: ""
    Jan  3 03:06:07.751: INFO: update-demo-nautilus-4xqkd is created but not running
    Jan  3 03:06:12.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-1792 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan  3 03:06:12.810: INFO: stderr: ""
    Jan  3 03:06:12.810: INFO: stdout: "update-demo-nautilus-4xqkd update-demo-nautilus-9rmc6 "
    Jan  3 03:06:12.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-1792 get pods update-demo-nautilus-4xqkd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  3 03:06:12.867: INFO: stderr: ""
    Jan  3 03:06:12.867: INFO: stdout: "true"
    Jan  3 03:06:12.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-1792 get pods update-demo-nautilus-4xqkd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan  3 03:06:12.925: INFO: stderr: ""
    Jan  3 03:06:12.925: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan  3 03:06:12.925: INFO: validating pod update-demo-nautilus-4xqkd
    Jan  3 03:06:12.928: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan  3 03:06:12.928: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan  3 03:06:12.928: INFO: update-demo-nautilus-4xqkd is verified up and running
    Jan  3 03:06:12.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-1792 get pods update-demo-nautilus-9rmc6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  3 03:06:12.989: INFO: stderr: ""
    Jan  3 03:06:12.989: INFO: stdout: "true"
    Jan  3 03:06:12.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-1792 get pods update-demo-nautilus-9rmc6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan  3 03:06:13.044: INFO: stderr: ""
    Jan  3 03:06:13.044: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan  3 03:06:13.044: INFO: validating pod update-demo-nautilus-9rmc6
    Jan  3 03:06:13.047: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan  3 03:06:13.047: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan  3 03:06:13.047: INFO: update-demo-nautilus-9rmc6 is verified up and running
    STEP: using delete to clean up resources 01/03/23 03:06:13.047
    Jan  3 03:06:13.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-1792 delete --grace-period=0 --force -f -'
    Jan  3 03:06:13.116: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  3 03:06:13.116: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jan  3 03:06:13.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-1792 get rc,svc -l name=update-demo --no-headers'
    Jan  3 03:06:13.217: INFO: stderr: "No resources found in kubectl-1792 namespace.\n"
    Jan  3 03:06:13.217: INFO: stdout: ""
    Jan  3 03:06:13.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-1792 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan  3 03:06:13.280: INFO: stderr: ""
    Jan  3 03:06:13.280: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan  3 03:06:13.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1792" for this suite. 01/03/23 03:06:13.284
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:06:13.287
Jan  3 03:06:13.287: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename container-probe 01/03/23 03:06:13.288
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:06:13.297
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:06:13.299
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
STEP: Creating pod liveness-2acae310-4a39-4a71-b4fa-9b55574491e8 in namespace container-probe-549 01/03/23 03:06:13.301
Jan  3 03:06:13.360: INFO: Waiting up to 5m0s for pod "liveness-2acae310-4a39-4a71-b4fa-9b55574491e8" in namespace "container-probe-549" to be "not pending"
Jan  3 03:06:13.366: INFO: Pod "liveness-2acae310-4a39-4a71-b4fa-9b55574491e8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.890135ms
Jan  3 03:06:15.368: INFO: Pod "liveness-2acae310-4a39-4a71-b4fa-9b55574491e8": Phase="Running", Reason="", readiness=true. Elapsed: 2.008097744s
Jan  3 03:06:15.368: INFO: Pod "liveness-2acae310-4a39-4a71-b4fa-9b55574491e8" satisfied condition "not pending"
Jan  3 03:06:15.368: INFO: Started pod liveness-2acae310-4a39-4a71-b4fa-9b55574491e8 in namespace container-probe-549
STEP: checking the pod's current state and verifying that restartCount is present 01/03/23 03:06:15.368
Jan  3 03:06:15.370: INFO: Initial restart count of pod liveness-2acae310-4a39-4a71-b4fa-9b55574491e8 is 0
Jan  3 03:06:35.403: INFO: Restart count of pod container-probe-549/liveness-2acae310-4a39-4a71-b4fa-9b55574491e8 is now 1 (20.033394373s elapsed)
STEP: deleting the pod 01/03/23 03:06:35.403
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan  3 03:06:35.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-549" for this suite. 01/03/23 03:06:35.431
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":286,"skipped":5462,"failed":0}
------------------------------
• [SLOW TEST] [22.147 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:06:13.287
    Jan  3 03:06:13.287: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename container-probe 01/03/23 03:06:13.288
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:06:13.297
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:06:13.299
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:165
    STEP: Creating pod liveness-2acae310-4a39-4a71-b4fa-9b55574491e8 in namespace container-probe-549 01/03/23 03:06:13.301
    Jan  3 03:06:13.360: INFO: Waiting up to 5m0s for pod "liveness-2acae310-4a39-4a71-b4fa-9b55574491e8" in namespace "container-probe-549" to be "not pending"
    Jan  3 03:06:13.366: INFO: Pod "liveness-2acae310-4a39-4a71-b4fa-9b55574491e8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.890135ms
    Jan  3 03:06:15.368: INFO: Pod "liveness-2acae310-4a39-4a71-b4fa-9b55574491e8": Phase="Running", Reason="", readiness=true. Elapsed: 2.008097744s
    Jan  3 03:06:15.368: INFO: Pod "liveness-2acae310-4a39-4a71-b4fa-9b55574491e8" satisfied condition "not pending"
    Jan  3 03:06:15.368: INFO: Started pod liveness-2acae310-4a39-4a71-b4fa-9b55574491e8 in namespace container-probe-549
    STEP: checking the pod's current state and verifying that restartCount is present 01/03/23 03:06:15.368
    Jan  3 03:06:15.370: INFO: Initial restart count of pod liveness-2acae310-4a39-4a71-b4fa-9b55574491e8 is 0
    Jan  3 03:06:35.403: INFO: Restart count of pod container-probe-549/liveness-2acae310-4a39-4a71-b4fa-9b55574491e8 is now 1 (20.033394373s elapsed)
    STEP: deleting the pod 01/03/23 03:06:35.403
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan  3 03:06:35.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-549" for this suite. 01/03/23 03:06:35.431
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:06:35.435
Jan  3 03:06:35.435: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 01/03/23 03:06:35.435
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:06:35.447
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:06:35.448
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 01/03/23 03:06:35.45
STEP: Creating hostNetwork=false pod 01/03/23 03:06:35.45
Jan  3 03:06:35.460: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-9940" to be "running and ready"
Jan  3 03:06:35.468: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.214437ms
Jan  3 03:06:35.468: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jan  3 03:06:37.471: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011133658s
Jan  3 03:06:37.471: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jan  3 03:06:39.471: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.011450908s
Jan  3 03:06:39.471: INFO: The phase of Pod test-pod is Running (Ready = true)
Jan  3 03:06:39.471: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 01/03/23 03:06:39.473
Jan  3 03:06:39.478: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-9940" to be "running and ready"
Jan  3 03:06:39.515: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 37.792813ms
Jan  3 03:06:39.515: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jan  3 03:06:41.518: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.040320058s
Jan  3 03:06:41.518: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Jan  3 03:06:41.518: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 01/03/23 03:06:41.52
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 01/03/23 03:06:41.52
Jan  3 03:06:41.520: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9940 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 03:06:41.520: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
Jan  3 03:06:41.520: INFO: ExecWithOptions: Clientset creation
Jan  3 03:06:41.520: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9940/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan  3 03:06:41.603: INFO: Exec stderr: ""
Jan  3 03:06:41.603: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9940 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 03:06:41.603: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
Jan  3 03:06:41.603: INFO: ExecWithOptions: Clientset creation
Jan  3 03:06:41.603: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9940/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan  3 03:06:41.685: INFO: Exec stderr: ""
Jan  3 03:06:41.685: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9940 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 03:06:41.685: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
Jan  3 03:06:41.685: INFO: ExecWithOptions: Clientset creation
Jan  3 03:06:41.685: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9940/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan  3 03:06:41.749: INFO: Exec stderr: ""
Jan  3 03:06:41.749: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9940 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 03:06:41.749: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
Jan  3 03:06:41.749: INFO: ExecWithOptions: Clientset creation
Jan  3 03:06:41.749: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9940/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan  3 03:06:41.833: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 01/03/23 03:06:41.833
Jan  3 03:06:41.833: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9940 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 03:06:41.833: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
Jan  3 03:06:41.834: INFO: ExecWithOptions: Clientset creation
Jan  3 03:06:41.834: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9940/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan  3 03:06:41.894: INFO: Exec stderr: ""
Jan  3 03:06:41.894: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9940 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 03:06:41.894: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
Jan  3 03:06:41.894: INFO: ExecWithOptions: Clientset creation
Jan  3 03:06:41.894: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9940/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan  3 03:06:41.961: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 01/03/23 03:06:41.961
Jan  3 03:06:41.961: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9940 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 03:06:41.961: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
Jan  3 03:06:41.961: INFO: ExecWithOptions: Clientset creation
Jan  3 03:06:41.962: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9940/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan  3 03:06:42.032: INFO: Exec stderr: ""
Jan  3 03:06:42.032: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9940 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 03:06:42.032: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
Jan  3 03:06:42.032: INFO: ExecWithOptions: Clientset creation
Jan  3 03:06:42.032: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9940/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan  3 03:06:42.089: INFO: Exec stderr: ""
Jan  3 03:06:42.089: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9940 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 03:06:42.089: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
Jan  3 03:06:42.089: INFO: ExecWithOptions: Clientset creation
Jan  3 03:06:42.089: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9940/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan  3 03:06:42.148: INFO: Exec stderr: ""
Jan  3 03:06:42.148: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9940 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 03:06:42.148: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
Jan  3 03:06:42.148: INFO: ExecWithOptions: Clientset creation
Jan  3 03:06:42.148: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9940/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan  3 03:06:42.216: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:187
Jan  3 03:06:42.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-9940" for this suite. 01/03/23 03:06:42.237
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","completed":287,"skipped":5496,"failed":0}
------------------------------
• [SLOW TEST] [6.805 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:06:35.435
    Jan  3 03:06:35.435: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 01/03/23 03:06:35.435
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:06:35.447
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:06:35.448
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 01/03/23 03:06:35.45
    STEP: Creating hostNetwork=false pod 01/03/23 03:06:35.45
    Jan  3 03:06:35.460: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-9940" to be "running and ready"
    Jan  3 03:06:35.468: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.214437ms
    Jan  3 03:06:35.468: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 03:06:37.471: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011133658s
    Jan  3 03:06:37.471: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 03:06:39.471: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.011450908s
    Jan  3 03:06:39.471: INFO: The phase of Pod test-pod is Running (Ready = true)
    Jan  3 03:06:39.471: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 01/03/23 03:06:39.473
    Jan  3 03:06:39.478: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-9940" to be "running and ready"
    Jan  3 03:06:39.515: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 37.792813ms
    Jan  3 03:06:39.515: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 03:06:41.518: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.040320058s
    Jan  3 03:06:41.518: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Jan  3 03:06:41.518: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 01/03/23 03:06:41.52
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 01/03/23 03:06:41.52
    Jan  3 03:06:41.520: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9940 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 03:06:41.520: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    Jan  3 03:06:41.520: INFO: ExecWithOptions: Clientset creation
    Jan  3 03:06:41.520: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9940/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan  3 03:06:41.603: INFO: Exec stderr: ""
    Jan  3 03:06:41.603: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9940 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 03:06:41.603: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    Jan  3 03:06:41.603: INFO: ExecWithOptions: Clientset creation
    Jan  3 03:06:41.603: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9940/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan  3 03:06:41.685: INFO: Exec stderr: ""
    Jan  3 03:06:41.685: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9940 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 03:06:41.685: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    Jan  3 03:06:41.685: INFO: ExecWithOptions: Clientset creation
    Jan  3 03:06:41.685: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9940/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan  3 03:06:41.749: INFO: Exec stderr: ""
    Jan  3 03:06:41.749: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9940 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 03:06:41.749: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    Jan  3 03:06:41.749: INFO: ExecWithOptions: Clientset creation
    Jan  3 03:06:41.749: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9940/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan  3 03:06:41.833: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 01/03/23 03:06:41.833
    Jan  3 03:06:41.833: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9940 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 03:06:41.833: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    Jan  3 03:06:41.834: INFO: ExecWithOptions: Clientset creation
    Jan  3 03:06:41.834: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9940/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jan  3 03:06:41.894: INFO: Exec stderr: ""
    Jan  3 03:06:41.894: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9940 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 03:06:41.894: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    Jan  3 03:06:41.894: INFO: ExecWithOptions: Clientset creation
    Jan  3 03:06:41.894: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9940/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jan  3 03:06:41.961: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 01/03/23 03:06:41.961
    Jan  3 03:06:41.961: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9940 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 03:06:41.961: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    Jan  3 03:06:41.961: INFO: ExecWithOptions: Clientset creation
    Jan  3 03:06:41.962: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9940/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan  3 03:06:42.032: INFO: Exec stderr: ""
    Jan  3 03:06:42.032: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9940 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 03:06:42.032: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    Jan  3 03:06:42.032: INFO: ExecWithOptions: Clientset creation
    Jan  3 03:06:42.032: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9940/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan  3 03:06:42.089: INFO: Exec stderr: ""
    Jan  3 03:06:42.089: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9940 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 03:06:42.089: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    Jan  3 03:06:42.089: INFO: ExecWithOptions: Clientset creation
    Jan  3 03:06:42.089: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9940/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan  3 03:06:42.148: INFO: Exec stderr: ""
    Jan  3 03:06:42.148: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9940 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 03:06:42.148: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    Jan  3 03:06:42.148: INFO: ExecWithOptions: Clientset creation
    Jan  3 03:06:42.148: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9940/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan  3 03:06:42.216: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:187
    Jan  3 03:06:42.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-9940" for this suite. 01/03/23 03:06:42.237
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:06:42.24
Jan  3 03:06:42.240: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename projected 01/03/23 03:06:42.241
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:06:42.253
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:06:42.254
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
STEP: Creating a pod to test downward API volume plugin 01/03/23 03:06:42.256
Jan  3 03:06:42.273: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3eed95b9-e78d-4f57-9ff0-251e86a521c0" in namespace "projected-2671" to be "Succeeded or Failed"
Jan  3 03:06:42.287: INFO: Pod "downwardapi-volume-3eed95b9-e78d-4f57-9ff0-251e86a521c0": Phase="Pending", Reason="", readiness=false. Elapsed: 13.899053ms
Jan  3 03:06:44.290: INFO: Pod "downwardapi-volume-3eed95b9-e78d-4f57-9ff0-251e86a521c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016762927s
Jan  3 03:06:46.290: INFO: Pod "downwardapi-volume-3eed95b9-e78d-4f57-9ff0-251e86a521c0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016605676s
Jan  3 03:06:48.290: INFO: Pod "downwardapi-volume-3eed95b9-e78d-4f57-9ff0-251e86a521c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016488526s
STEP: Saw pod success 01/03/23 03:06:48.29
Jan  3 03:06:48.290: INFO: Pod "downwardapi-volume-3eed95b9-e78d-4f57-9ff0-251e86a521c0" satisfied condition "Succeeded or Failed"
Jan  3 03:06:48.291: INFO: Trying to get logs from node cncf-master pod downwardapi-volume-3eed95b9-e78d-4f57-9ff0-251e86a521c0 container client-container: <nil>
STEP: delete the pod 01/03/23 03:06:48.295
Jan  3 03:06:48.310: INFO: Waiting for pod downwardapi-volume-3eed95b9-e78d-4f57-9ff0-251e86a521c0 to disappear
Jan  3 03:06:48.316: INFO: Pod downwardapi-volume-3eed95b9-e78d-4f57-9ff0-251e86a521c0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan  3 03:06:48.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2671" for this suite. 01/03/23 03:06:48.319
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","completed":288,"skipped":5521,"failed":0}
------------------------------
• [SLOW TEST] [6.081 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:06:42.24
    Jan  3 03:06:42.240: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename projected 01/03/23 03:06:42.241
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:06:42.253
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:06:42.254
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:206
    STEP: Creating a pod to test downward API volume plugin 01/03/23 03:06:42.256
    Jan  3 03:06:42.273: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3eed95b9-e78d-4f57-9ff0-251e86a521c0" in namespace "projected-2671" to be "Succeeded or Failed"
    Jan  3 03:06:42.287: INFO: Pod "downwardapi-volume-3eed95b9-e78d-4f57-9ff0-251e86a521c0": Phase="Pending", Reason="", readiness=false. Elapsed: 13.899053ms
    Jan  3 03:06:44.290: INFO: Pod "downwardapi-volume-3eed95b9-e78d-4f57-9ff0-251e86a521c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016762927s
    Jan  3 03:06:46.290: INFO: Pod "downwardapi-volume-3eed95b9-e78d-4f57-9ff0-251e86a521c0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016605676s
    Jan  3 03:06:48.290: INFO: Pod "downwardapi-volume-3eed95b9-e78d-4f57-9ff0-251e86a521c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016488526s
    STEP: Saw pod success 01/03/23 03:06:48.29
    Jan  3 03:06:48.290: INFO: Pod "downwardapi-volume-3eed95b9-e78d-4f57-9ff0-251e86a521c0" satisfied condition "Succeeded or Failed"
    Jan  3 03:06:48.291: INFO: Trying to get logs from node cncf-master pod downwardapi-volume-3eed95b9-e78d-4f57-9ff0-251e86a521c0 container client-container: <nil>
    STEP: delete the pod 01/03/23 03:06:48.295
    Jan  3 03:06:48.310: INFO: Waiting for pod downwardapi-volume-3eed95b9-e78d-4f57-9ff0-251e86a521c0 to disappear
    Jan  3 03:06:48.316: INFO: Pod downwardapi-volume-3eed95b9-e78d-4f57-9ff0-251e86a521c0 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan  3 03:06:48.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2671" for this suite. 01/03/23 03:06:48.319
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:06:48.322
Jan  3 03:06:48.323: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename webhook 01/03/23 03:06:48.323
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:06:48.339
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:06:48.341
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/03/23 03:06:48.363
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/23 03:06:49.007
STEP: Deploying the webhook pod 01/03/23 03:06:49.011
STEP: Wait for the deployment to be ready 01/03/23 03:06:49.081
Jan  3 03:06:49.084: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/03/23 03:06:51.09
STEP: Verifying the service has paired with the endpoint 01/03/23 03:06:51.103
Jan  3 03:06:52.103: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
STEP: Registering the webhook via the AdmissionRegistration API 01/03/23 03:06:52.105
STEP: create a pod 01/03/23 03:06:52.116
Jan  3 03:06:52.120: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-3869" to be "running"
Jan  3 03:06:52.128: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.990387ms
Jan  3 03:06:54.130: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010214342s
Jan  3 03:06:54.130: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 01/03/23 03:06:54.13
Jan  3 03:06:54.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=webhook-3869 attach --namespace=webhook-3869 to-be-attached-pod -i -c=container1'
Jan  3 03:06:54.198: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  3 03:06:54.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3869" for this suite. 01/03/23 03:06:54.205
STEP: Destroying namespace "webhook-3869-markers" for this suite. 01/03/23 03:06:54.224
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","completed":289,"skipped":5547,"failed":0}
------------------------------
• [SLOW TEST] [5.986 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:06:48.322
    Jan  3 03:06:48.323: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename webhook 01/03/23 03:06:48.323
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:06:48.339
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:06:48.341
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/03/23 03:06:48.363
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/23 03:06:49.007
    STEP: Deploying the webhook pod 01/03/23 03:06:49.011
    STEP: Wait for the deployment to be ready 01/03/23 03:06:49.081
    Jan  3 03:06:49.084: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/03/23 03:06:51.09
    STEP: Verifying the service has paired with the endpoint 01/03/23 03:06:51.103
    Jan  3 03:06:52.103: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:208
    STEP: Registering the webhook via the AdmissionRegistration API 01/03/23 03:06:52.105
    STEP: create a pod 01/03/23 03:06:52.116
    Jan  3 03:06:52.120: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-3869" to be "running"
    Jan  3 03:06:52.128: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.990387ms
    Jan  3 03:06:54.130: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010214342s
    Jan  3 03:06:54.130: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 01/03/23 03:06:54.13
    Jan  3 03:06:54.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=webhook-3869 attach --namespace=webhook-3869 to-be-attached-pod -i -c=container1'
    Jan  3 03:06:54.198: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  3 03:06:54.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-3869" for this suite. 01/03/23 03:06:54.205
    STEP: Destroying namespace "webhook-3869-markers" for this suite. 01/03/23 03:06:54.224
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:06:54.309
Jan  3 03:06:54.309: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename projected 01/03/23 03:06:54.31
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:06:54.375
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:06:54.377
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
STEP: Creating a pod to test downward API volume plugin 01/03/23 03:06:54.379
Jan  3 03:06:54.398: INFO: Waiting up to 5m0s for pod "downwardapi-volume-48647eb6-2f15-4e0d-9d8a-f99591e89c9c" in namespace "projected-3248" to be "Succeeded or Failed"
Jan  3 03:06:54.404: INFO: Pod "downwardapi-volume-48647eb6-2f15-4e0d-9d8a-f99591e89c9c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.060431ms
Jan  3 03:06:56.406: INFO: Pod "downwardapi-volume-48647eb6-2f15-4e0d-9d8a-f99591e89c9c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008826823s
Jan  3 03:06:58.406: INFO: Pod "downwardapi-volume-48647eb6-2f15-4e0d-9d8a-f99591e89c9c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008709907s
Jan  3 03:07:00.407: INFO: Pod "downwardapi-volume-48647eb6-2f15-4e0d-9d8a-f99591e89c9c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009728766s
STEP: Saw pod success 01/03/23 03:07:00.407
Jan  3 03:07:00.407: INFO: Pod "downwardapi-volume-48647eb6-2f15-4e0d-9d8a-f99591e89c9c" satisfied condition "Succeeded or Failed"
Jan  3 03:07:00.409: INFO: Trying to get logs from node cncf-master pod downwardapi-volume-48647eb6-2f15-4e0d-9d8a-f99591e89c9c container client-container: <nil>
STEP: delete the pod 01/03/23 03:07:00.412
Jan  3 03:07:00.433: INFO: Waiting for pod downwardapi-volume-48647eb6-2f15-4e0d-9d8a-f99591e89c9c to disappear
Jan  3 03:07:00.438: INFO: Pod downwardapi-volume-48647eb6-2f15-4e0d-9d8a-f99591e89c9c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan  3 03:07:00.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3248" for this suite. 01/03/23 03:07:00.459
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","completed":290,"skipped":5565,"failed":0}
------------------------------
• [SLOW TEST] [6.152 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:06:54.309
    Jan  3 03:06:54.309: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename projected 01/03/23 03:06:54.31
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:06:54.375
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:06:54.377
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:192
    STEP: Creating a pod to test downward API volume plugin 01/03/23 03:06:54.379
    Jan  3 03:06:54.398: INFO: Waiting up to 5m0s for pod "downwardapi-volume-48647eb6-2f15-4e0d-9d8a-f99591e89c9c" in namespace "projected-3248" to be "Succeeded or Failed"
    Jan  3 03:06:54.404: INFO: Pod "downwardapi-volume-48647eb6-2f15-4e0d-9d8a-f99591e89c9c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.060431ms
    Jan  3 03:06:56.406: INFO: Pod "downwardapi-volume-48647eb6-2f15-4e0d-9d8a-f99591e89c9c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008826823s
    Jan  3 03:06:58.406: INFO: Pod "downwardapi-volume-48647eb6-2f15-4e0d-9d8a-f99591e89c9c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008709907s
    Jan  3 03:07:00.407: INFO: Pod "downwardapi-volume-48647eb6-2f15-4e0d-9d8a-f99591e89c9c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009728766s
    STEP: Saw pod success 01/03/23 03:07:00.407
    Jan  3 03:07:00.407: INFO: Pod "downwardapi-volume-48647eb6-2f15-4e0d-9d8a-f99591e89c9c" satisfied condition "Succeeded or Failed"
    Jan  3 03:07:00.409: INFO: Trying to get logs from node cncf-master pod downwardapi-volume-48647eb6-2f15-4e0d-9d8a-f99591e89c9c container client-container: <nil>
    STEP: delete the pod 01/03/23 03:07:00.412
    Jan  3 03:07:00.433: INFO: Waiting for pod downwardapi-volume-48647eb6-2f15-4e0d-9d8a-f99591e89c9c to disappear
    Jan  3 03:07:00.438: INFO: Pod downwardapi-volume-48647eb6-2f15-4e0d-9d8a-f99591e89c9c no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan  3 03:07:00.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3248" for this suite. 01/03/23 03:07:00.459
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:07:00.462
Jan  3 03:07:00.462: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename crd-publish-openapi 01/03/23 03:07:00.463
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:07:00.485
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:07:00.487
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 01/03/23 03:07:00.489
Jan  3 03:07:00.489: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
Jan  3 03:07:05.853: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  3 03:07:26.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6156" for this suite. 01/03/23 03:07:26.685
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","completed":291,"skipped":5569,"failed":0}
------------------------------
• [SLOW TEST] [26.226 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:07:00.462
    Jan  3 03:07:00.462: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename crd-publish-openapi 01/03/23 03:07:00.463
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:07:00.485
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:07:00.487
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:275
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 01/03/23 03:07:00.489
    Jan  3 03:07:00.489: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    Jan  3 03:07:05.853: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  3 03:07:26.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-6156" for this suite. 01/03/23 03:07:26.685
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:07:26.688
Jan  3 03:07:26.688: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename subpath 01/03/23 03:07:26.689
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:07:26.702
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:07:26.704
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/03/23 03:07:26.707
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-lzpk 01/03/23 03:07:26.741
STEP: Creating a pod to test atomic-volume-subpath 01/03/23 03:07:26.741
Jan  3 03:07:26.751: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-lzpk" in namespace "subpath-8791" to be "Succeeded or Failed"
Jan  3 03:07:26.762: INFO: Pod "pod-subpath-test-configmap-lzpk": Phase="Pending", Reason="", readiness=false. Elapsed: 11.140723ms
Jan  3 03:07:28.765: INFO: Pod "pod-subpath-test-configmap-lzpk": Phase="Running", Reason="", readiness=true. Elapsed: 2.014269151s
Jan  3 03:07:30.767: INFO: Pod "pod-subpath-test-configmap-lzpk": Phase="Running", Reason="", readiness=true. Elapsed: 4.015591905s
Jan  3 03:07:32.766: INFO: Pod "pod-subpath-test-configmap-lzpk": Phase="Running", Reason="", readiness=true. Elapsed: 6.014577909s
Jan  3 03:07:34.765: INFO: Pod "pod-subpath-test-configmap-lzpk": Phase="Running", Reason="", readiness=true. Elapsed: 8.014103612s
Jan  3 03:07:36.765: INFO: Pod "pod-subpath-test-configmap-lzpk": Phase="Running", Reason="", readiness=true. Elapsed: 10.013641944s
Jan  3 03:07:38.765: INFO: Pod "pod-subpath-test-configmap-lzpk": Phase="Running", Reason="", readiness=true. Elapsed: 12.014514568s
Jan  3 03:07:40.765: INFO: Pod "pod-subpath-test-configmap-lzpk": Phase="Running", Reason="", readiness=true. Elapsed: 14.014325423s
Jan  3 03:07:42.766: INFO: Pod "pod-subpath-test-configmap-lzpk": Phase="Running", Reason="", readiness=true. Elapsed: 16.014973221s
Jan  3 03:07:44.765: INFO: Pod "pod-subpath-test-configmap-lzpk": Phase="Running", Reason="", readiness=true. Elapsed: 18.013674347s
Jan  3 03:07:46.764: INFO: Pod "pod-subpath-test-configmap-lzpk": Phase="Running", Reason="", readiness=true. Elapsed: 20.013327969s
Jan  3 03:07:48.766: INFO: Pod "pod-subpath-test-configmap-lzpk": Phase="Running", Reason="", readiness=false. Elapsed: 22.014804766s
Jan  3 03:07:50.765: INFO: Pod "pod-subpath-test-configmap-lzpk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.013886431s
STEP: Saw pod success 01/03/23 03:07:50.765
Jan  3 03:07:50.765: INFO: Pod "pod-subpath-test-configmap-lzpk" satisfied condition "Succeeded or Failed"
Jan  3 03:07:50.767: INFO: Trying to get logs from node cncf-master pod pod-subpath-test-configmap-lzpk container test-container-subpath-configmap-lzpk: <nil>
STEP: delete the pod 01/03/23 03:07:50.77
Jan  3 03:07:50.791: INFO: Waiting for pod pod-subpath-test-configmap-lzpk to disappear
Jan  3 03:07:50.794: INFO: Pod pod-subpath-test-configmap-lzpk no longer exists
STEP: Deleting pod pod-subpath-test-configmap-lzpk 01/03/23 03:07:50.794
Jan  3 03:07:50.794: INFO: Deleting pod "pod-subpath-test-configmap-lzpk" in namespace "subpath-8791"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan  3 03:07:50.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8791" for this suite. 01/03/23 03:07:50.798
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]","completed":292,"skipped":5576,"failed":0}
------------------------------
• [SLOW TEST] [24.112 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:07:26.688
    Jan  3 03:07:26.688: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename subpath 01/03/23 03:07:26.689
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:07:26.702
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:07:26.704
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/03/23 03:07:26.707
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-lzpk 01/03/23 03:07:26.741
    STEP: Creating a pod to test atomic-volume-subpath 01/03/23 03:07:26.741
    Jan  3 03:07:26.751: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-lzpk" in namespace "subpath-8791" to be "Succeeded or Failed"
    Jan  3 03:07:26.762: INFO: Pod "pod-subpath-test-configmap-lzpk": Phase="Pending", Reason="", readiness=false. Elapsed: 11.140723ms
    Jan  3 03:07:28.765: INFO: Pod "pod-subpath-test-configmap-lzpk": Phase="Running", Reason="", readiness=true. Elapsed: 2.014269151s
    Jan  3 03:07:30.767: INFO: Pod "pod-subpath-test-configmap-lzpk": Phase="Running", Reason="", readiness=true. Elapsed: 4.015591905s
    Jan  3 03:07:32.766: INFO: Pod "pod-subpath-test-configmap-lzpk": Phase="Running", Reason="", readiness=true. Elapsed: 6.014577909s
    Jan  3 03:07:34.765: INFO: Pod "pod-subpath-test-configmap-lzpk": Phase="Running", Reason="", readiness=true. Elapsed: 8.014103612s
    Jan  3 03:07:36.765: INFO: Pod "pod-subpath-test-configmap-lzpk": Phase="Running", Reason="", readiness=true. Elapsed: 10.013641944s
    Jan  3 03:07:38.765: INFO: Pod "pod-subpath-test-configmap-lzpk": Phase="Running", Reason="", readiness=true. Elapsed: 12.014514568s
    Jan  3 03:07:40.765: INFO: Pod "pod-subpath-test-configmap-lzpk": Phase="Running", Reason="", readiness=true. Elapsed: 14.014325423s
    Jan  3 03:07:42.766: INFO: Pod "pod-subpath-test-configmap-lzpk": Phase="Running", Reason="", readiness=true. Elapsed: 16.014973221s
    Jan  3 03:07:44.765: INFO: Pod "pod-subpath-test-configmap-lzpk": Phase="Running", Reason="", readiness=true. Elapsed: 18.013674347s
    Jan  3 03:07:46.764: INFO: Pod "pod-subpath-test-configmap-lzpk": Phase="Running", Reason="", readiness=true. Elapsed: 20.013327969s
    Jan  3 03:07:48.766: INFO: Pod "pod-subpath-test-configmap-lzpk": Phase="Running", Reason="", readiness=false. Elapsed: 22.014804766s
    Jan  3 03:07:50.765: INFO: Pod "pod-subpath-test-configmap-lzpk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.013886431s
    STEP: Saw pod success 01/03/23 03:07:50.765
    Jan  3 03:07:50.765: INFO: Pod "pod-subpath-test-configmap-lzpk" satisfied condition "Succeeded or Failed"
    Jan  3 03:07:50.767: INFO: Trying to get logs from node cncf-master pod pod-subpath-test-configmap-lzpk container test-container-subpath-configmap-lzpk: <nil>
    STEP: delete the pod 01/03/23 03:07:50.77
    Jan  3 03:07:50.791: INFO: Waiting for pod pod-subpath-test-configmap-lzpk to disappear
    Jan  3 03:07:50.794: INFO: Pod pod-subpath-test-configmap-lzpk no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-lzpk 01/03/23 03:07:50.794
    Jan  3 03:07:50.794: INFO: Deleting pod "pod-subpath-test-configmap-lzpk" in namespace "subpath-8791"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan  3 03:07:50.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-8791" for this suite. 01/03/23 03:07:50.798
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:07:50.801
Jan  3 03:07:50.801: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename webhook 01/03/23 03:07:50.802
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:07:50.822
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:07:50.824
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/03/23 03:07:50.879
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/23 03:07:51.38
STEP: Deploying the webhook pod 01/03/23 03:07:51.386
STEP: Wait for the deployment to be ready 01/03/23 03:07:51.412
Jan  3 03:07:51.470: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/03/23 03:07:53.476
STEP: Verifying the service has paired with the endpoint 01/03/23 03:07:53.491
Jan  3 03:07:54.491: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/03/23 03:07:54.493
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/03/23 03:07:54.503
STEP: Creating a dummy validating-webhook-configuration object 01/03/23 03:07:54.514
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 01/03/23 03:07:54.522
STEP: Creating a dummy mutating-webhook-configuration object 01/03/23 03:07:54.549
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 01/03/23 03:07:54.558
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  3 03:07:54.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-884" for this suite. 01/03/23 03:07:54.591
STEP: Destroying namespace "webhook-884-markers" for this suite. 01/03/23 03:07:54.593
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","completed":293,"skipped":5582,"failed":0}
------------------------------
• [3.879 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:07:50.801
    Jan  3 03:07:50.801: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename webhook 01/03/23 03:07:50.802
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:07:50.822
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:07:50.824
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/03/23 03:07:50.879
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/23 03:07:51.38
    STEP: Deploying the webhook pod 01/03/23 03:07:51.386
    STEP: Wait for the deployment to be ready 01/03/23 03:07:51.412
    Jan  3 03:07:51.470: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/03/23 03:07:53.476
    STEP: Verifying the service has paired with the endpoint 01/03/23 03:07:53.491
    Jan  3 03:07:54.491: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:276
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/03/23 03:07:54.493
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/03/23 03:07:54.503
    STEP: Creating a dummy validating-webhook-configuration object 01/03/23 03:07:54.514
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 01/03/23 03:07:54.522
    STEP: Creating a dummy mutating-webhook-configuration object 01/03/23 03:07:54.549
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 01/03/23 03:07:54.558
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  3 03:07:54.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-884" for this suite. 01/03/23 03:07:54.591
    STEP: Destroying namespace "webhook-884-markers" for this suite. 01/03/23 03:07:54.593
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:07:54.681
Jan  3 03:07:54.681: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename projected 01/03/23 03:07:54.682
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:07:54.698
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:07:54.7
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
STEP: Creating a pod to test downward API volume plugin 01/03/23 03:07:54.702
Jan  3 03:07:54.725: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cf64d3aa-9578-47e7-ab47-ce19a345715a" in namespace "projected-7757" to be "Succeeded or Failed"
Jan  3 03:07:54.733: INFO: Pod "downwardapi-volume-cf64d3aa-9578-47e7-ab47-ce19a345715a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.296154ms
Jan  3 03:07:56.736: INFO: Pod "downwardapi-volume-cf64d3aa-9578-47e7-ab47-ce19a345715a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010235832s
Jan  3 03:07:58.736: INFO: Pod "downwardapi-volume-cf64d3aa-9578-47e7-ab47-ce19a345715a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011100064s
STEP: Saw pod success 01/03/23 03:07:58.736
Jan  3 03:07:58.737: INFO: Pod "downwardapi-volume-cf64d3aa-9578-47e7-ab47-ce19a345715a" satisfied condition "Succeeded or Failed"
Jan  3 03:07:58.738: INFO: Trying to get logs from node cncf-master pod downwardapi-volume-cf64d3aa-9578-47e7-ab47-ce19a345715a container client-container: <nil>
STEP: delete the pod 01/03/23 03:07:58.741
Jan  3 03:07:58.752: INFO: Waiting for pod downwardapi-volume-cf64d3aa-9578-47e7-ab47-ce19a345715a to disappear
Jan  3 03:07:58.755: INFO: Pod downwardapi-volume-cf64d3aa-9578-47e7-ab47-ce19a345715a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan  3 03:07:58.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7757" for this suite. 01/03/23 03:07:58.758
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","completed":294,"skipped":5606,"failed":0}
------------------------------
• [4.079 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:07:54.681
    Jan  3 03:07:54.681: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename projected 01/03/23 03:07:54.682
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:07:54.698
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:07:54.7
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:234
    STEP: Creating a pod to test downward API volume plugin 01/03/23 03:07:54.702
    Jan  3 03:07:54.725: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cf64d3aa-9578-47e7-ab47-ce19a345715a" in namespace "projected-7757" to be "Succeeded or Failed"
    Jan  3 03:07:54.733: INFO: Pod "downwardapi-volume-cf64d3aa-9578-47e7-ab47-ce19a345715a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.296154ms
    Jan  3 03:07:56.736: INFO: Pod "downwardapi-volume-cf64d3aa-9578-47e7-ab47-ce19a345715a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010235832s
    Jan  3 03:07:58.736: INFO: Pod "downwardapi-volume-cf64d3aa-9578-47e7-ab47-ce19a345715a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011100064s
    STEP: Saw pod success 01/03/23 03:07:58.736
    Jan  3 03:07:58.737: INFO: Pod "downwardapi-volume-cf64d3aa-9578-47e7-ab47-ce19a345715a" satisfied condition "Succeeded or Failed"
    Jan  3 03:07:58.738: INFO: Trying to get logs from node cncf-master pod downwardapi-volume-cf64d3aa-9578-47e7-ab47-ce19a345715a container client-container: <nil>
    STEP: delete the pod 01/03/23 03:07:58.741
    Jan  3 03:07:58.752: INFO: Waiting for pod downwardapi-volume-cf64d3aa-9578-47e7-ab47-ce19a345715a to disappear
    Jan  3 03:07:58.755: INFO: Pod downwardapi-volume-cf64d3aa-9578-47e7-ab47-ce19a345715a no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan  3 03:07:58.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7757" for this suite. 01/03/23 03:07:58.758
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:07:58.761
Jan  3 03:07:58.761: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename runtimeclass 01/03/23 03:07:58.762
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:07:58.772
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:07:58.774
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Jan  3 03:07:58.800: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-8209 to be scheduled
Jan  3 03:07:58.807: INFO: 1 pods are not scheduled: [runtimeclass-8209/test-runtimeclass-runtimeclass-8209-preconfigured-handler-kn4ml(9453dab8-2eeb-44fe-b60d-c15bf6b2ba03)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan  3 03:08:00.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-8209" for this suite. 01/03/23 03:08:00.822
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]","completed":295,"skipped":5613,"failed":0}
------------------------------
• [2.064 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:07:58.761
    Jan  3 03:07:58.761: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename runtimeclass 01/03/23 03:07:58.762
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:07:58.772
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:07:58.774
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Jan  3 03:07:58.800: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-8209 to be scheduled
    Jan  3 03:07:58.807: INFO: 1 pods are not scheduled: [runtimeclass-8209/test-runtimeclass-runtimeclass-8209-preconfigured-handler-kn4ml(9453dab8-2eeb-44fe-b60d-c15bf6b2ba03)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan  3 03:08:00.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-8209" for this suite. 01/03/23 03:08:00.822
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:08:00.826
Jan  3 03:08:00.826: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename svc-latency 01/03/23 03:08:00.827
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:08:00.841
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:08:00.843
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Jan  3 03:08:00.847: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: creating replication controller svc-latency-rc in namespace svc-latency-5565 01/03/23 03:08:00.848
I0103 03:08:00.853393      23 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5565, replica count: 1
I0103 03:08:01.903963      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  3 03:08:02.021: INFO: Created: latency-svc-xgcxb
Jan  3 03:08:02.032: INFO: Got endpoints: latency-svc-xgcxb [27.985722ms]
Jan  3 03:08:02.072: INFO: Created: latency-svc-rkwd6
Jan  3 03:08:02.077: INFO: Got endpoints: latency-svc-rkwd6 [44.650156ms]
Jan  3 03:08:02.095: INFO: Created: latency-svc-6sb7k
Jan  3 03:08:02.105: INFO: Got endpoints: latency-svc-6sb7k [73.109177ms]
Jan  3 03:08:02.129: INFO: Created: latency-svc-gxwwx
Jan  3 03:08:02.134: INFO: Got endpoints: latency-svc-gxwwx [101.553345ms]
Jan  3 03:08:02.152: INFO: Created: latency-svc-c67m4
Jan  3 03:08:02.157: INFO: Got endpoints: latency-svc-c67m4 [124.614857ms]
Jan  3 03:08:02.209: INFO: Created: latency-svc-t8tpx
Jan  3 03:08:02.221: INFO: Got endpoints: latency-svc-t8tpx [188.554744ms]
Jan  3 03:08:02.232: INFO: Created: latency-svc-2w6qm
Jan  3 03:08:02.243: INFO: Got endpoints: latency-svc-2w6qm [211.296679ms]
Jan  3 03:08:02.255: INFO: Created: latency-svc-4kmn2
Jan  3 03:08:02.266: INFO: Got endpoints: latency-svc-4kmn2 [234.496485ms]
Jan  3 03:08:02.278: INFO: Created: latency-svc-n7krl
Jan  3 03:08:02.283: INFO: Got endpoints: latency-svc-n7krl [250.701841ms]
Jan  3 03:08:02.301: INFO: Created: latency-svc-2hhg5
Jan  3 03:08:02.389: INFO: Got endpoints: latency-svc-2hhg5 [356.550063ms]
Jan  3 03:08:02.389: INFO: Created: latency-svc-qmv5z
Jan  3 03:08:02.409: INFO: Got endpoints: latency-svc-qmv5z [377.33083ms]
Jan  3 03:08:02.421: INFO: Created: latency-svc-mh7gj
Jan  3 03:08:02.432: INFO: Got endpoints: latency-svc-mh7gj [400.321565ms]
Jan  3 03:08:02.449: INFO: Created: latency-svc-8nmb5
Jan  3 03:08:02.454: INFO: Got endpoints: latency-svc-8nmb5 [422.348248ms]
Jan  3 03:08:02.484: INFO: Created: latency-svc-xcv6q
Jan  3 03:08:02.543: INFO: Got endpoints: latency-svc-xcv6q [510.584872ms]
Jan  3 03:08:02.544: INFO: Created: latency-svc-msw8k
Jan  3 03:08:02.552: INFO: Got endpoints: latency-svc-msw8k [519.909617ms]
Jan  3 03:08:02.570: INFO: Created: latency-svc-gsvg6
Jan  3 03:08:02.581: INFO: Got endpoints: latency-svc-gsvg6 [548.912117ms]
Jan  3 03:08:02.592: INFO: Created: latency-svc-69lnx
Jan  3 03:08:02.598: INFO: Got endpoints: latency-svc-69lnx [520.946507ms]
Jan  3 03:08:02.615: INFO: Created: latency-svc-gkkg5
Jan  3 03:08:02.627: INFO: Got endpoints: latency-svc-gkkg5 [521.579118ms]
Jan  3 03:08:02.638: INFO: Created: latency-svc-wphwf
Jan  3 03:08:02.674: INFO: Got endpoints: latency-svc-wphwf [540.599705ms]
Jan  3 03:08:02.684: INFO: Created: latency-svc-rmzj4
Jan  3 03:08:02.689: INFO: Got endpoints: latency-svc-rmzj4 [532.791325ms]
Jan  3 03:08:02.707: INFO: Created: latency-svc-9dkhs
Jan  3 03:08:02.712: INFO: Got endpoints: latency-svc-9dkhs [491.759664ms]
Jan  3 03:08:02.730: INFO: Created: latency-svc-svfq7
Jan  3 03:08:02.735: INFO: Got endpoints: latency-svc-svfq7 [491.757116ms]
Jan  3 03:08:02.753: INFO: Created: latency-svc-wdnb5
Jan  3 03:08:02.770: INFO: Got endpoints: latency-svc-wdnb5 [503.066368ms]
Jan  3 03:08:02.804: INFO: Created: latency-svc-pdq2n
Jan  3 03:08:02.810: INFO: Got endpoints: latency-svc-pdq2n [526.799989ms]
Jan  3 03:08:02.827: INFO: Created: latency-svc-gvr2f
Jan  3 03:08:02.838: INFO: Got endpoints: latency-svc-gvr2f [449.644721ms]
Jan  3 03:08:02.873: INFO: Created: latency-svc-78rmx
Jan  3 03:08:02.884: INFO: Got endpoints: latency-svc-78rmx [474.72454ms]
Jan  3 03:08:02.937: INFO: Created: latency-svc-4wvnw
Jan  3 03:08:02.947: INFO: Created: latency-svc-9684x
Jan  3 03:08:02.947: INFO: Got endpoints: latency-svc-4wvnw [514.707026ms]
Jan  3 03:08:02.959: INFO: Got endpoints: latency-svc-9684x [504.187694ms]
Jan  3 03:08:02.975: INFO: Created: latency-svc-k97hw
Jan  3 03:08:02.987: INFO: Got endpoints: latency-svc-k97hw [444.50799ms]
Jan  3 03:08:03.004: INFO: Created: latency-svc-qdgbj
Jan  3 03:08:03.016: INFO: Got endpoints: latency-svc-qdgbj [463.792818ms]
Jan  3 03:08:03.033: INFO: Created: latency-svc-ktx7b
Jan  3 03:08:03.080: INFO: Got endpoints: latency-svc-ktx7b [498.689646ms]
Jan  3 03:08:03.081: INFO: Created: latency-svc-zswc7
Jan  3 03:08:03.085: INFO: Got endpoints: latency-svc-zswc7 [486.944346ms]
Jan  3 03:08:03.113: INFO: Created: latency-svc-zqkhj
Jan  3 03:08:03.142: INFO: Got endpoints: latency-svc-zqkhj [515.258591ms]
Jan  3 03:08:03.159: INFO: Created: latency-svc-g4tbq
Jan  3 03:08:03.170: INFO: Got endpoints: latency-svc-g4tbq [496.336605ms]
Jan  3 03:08:03.217: INFO: Created: latency-svc-qwwhv
Jan  3 03:08:03.228: INFO: Created: latency-svc-4x6mx
Jan  3 03:08:03.228: INFO: Got endpoints: latency-svc-qwwhv [538.266168ms]
Jan  3 03:08:03.245: INFO: Got endpoints: latency-svc-4x6mx [532.169806ms]
Jan  3 03:08:03.256: INFO: Created: latency-svc-rfhkj
Jan  3 03:08:03.267: INFO: Got endpoints: latency-svc-rfhkj [532.218443ms]
Jan  3 03:08:03.279: INFO: Created: latency-svc-gbm5m
Jan  3 03:08:03.285: INFO: Got endpoints: latency-svc-gbm5m [515.2331ms]
Jan  3 03:08:03.302: INFO: Created: latency-svc-rtdpk
Jan  3 03:08:03.308: INFO: Got endpoints: latency-svc-rtdpk [498.311136ms]
Jan  3 03:08:03.360: INFO: Created: latency-svc-75m9t
Jan  3 03:08:03.370: INFO: Got endpoints: latency-svc-75m9t [532.185111ms]
Jan  3 03:08:03.371: INFO: Created: latency-svc-qwxft
Jan  3 03:08:03.382: INFO: Got endpoints: latency-svc-qwxft [497.599415ms]
Jan  3 03:08:03.393: INFO: Created: latency-svc-lg9nj
Jan  3 03:08:03.405: INFO: Got endpoints: latency-svc-lg9nj [458.142312ms]
Jan  3 03:08:03.422: INFO: Created: latency-svc-mblt5
Jan  3 03:08:03.428: INFO: Got endpoints: latency-svc-mblt5 [469.620551ms]
Jan  3 03:08:03.445: INFO: Created: latency-svc-9ckmh
Jan  3 03:08:03.451: INFO: Got endpoints: latency-svc-9ckmh [464.085905ms]
Jan  3 03:08:03.549: INFO: Created: latency-svc-dvttw
Jan  3 03:08:03.565: INFO: Created: latency-svc-v288k
Jan  3 03:08:03.565: INFO: Got endpoints: latency-svc-dvttw [549.123325ms]
Jan  3 03:08:03.576: INFO: Got endpoints: latency-svc-v288k [496.55863ms]
Jan  3 03:08:03.588: INFO: Created: latency-svc-d4cnm
Jan  3 03:08:03.599: INFO: Got endpoints: latency-svc-d4cnm [514.473777ms]
Jan  3 03:08:03.610: INFO: Created: latency-svc-b65gk
Jan  3 03:08:03.617: INFO: Got endpoints: latency-svc-b65gk [475.382876ms]
Jan  3 03:08:03.633: INFO: Created: latency-svc-w5gg5
Jan  3 03:08:03.640: INFO: Got endpoints: latency-svc-w5gg5 [469.719623ms]
Jan  3 03:08:03.692: INFO: Created: latency-svc-fhtlm
Jan  3 03:08:03.697: INFO: Got endpoints: latency-svc-fhtlm [469.785786ms]
Jan  3 03:08:03.708: INFO: Created: latency-svc-gfkb7
Jan  3 03:08:03.715: INFO: Got endpoints: latency-svc-gfkb7 [470.114284ms]
Jan  3 03:08:03.730: INFO: Created: latency-svc-gxtv9
Jan  3 03:08:03.742: INFO: Got endpoints: latency-svc-gxtv9 [474.684795ms]
Jan  3 03:08:03.753: INFO: Created: latency-svc-szh2v
Jan  3 03:08:03.765: INFO: Got endpoints: latency-svc-szh2v [479.893621ms]
Jan  3 03:08:03.788: INFO: Created: latency-svc-6bcz4
Jan  3 03:08:03.845: INFO: Got endpoints: latency-svc-6bcz4 [537.523194ms]
Jan  3 03:08:03.847: INFO: Created: latency-svc-nhtv4
Jan  3 03:08:03.852: INFO: Got endpoints: latency-svc-nhtv4 [481.853894ms]
Jan  3 03:08:03.885: INFO: Created: latency-svc-x9vkm
Jan  3 03:08:03.896: INFO: Got endpoints: latency-svc-x9vkm [514.646196ms]
Jan  3 03:08:03.919: INFO: Created: latency-svc-hwbz6
Jan  3 03:08:03.926: INFO: Got endpoints: latency-svc-hwbz6 [521.121018ms]
Jan  3 03:08:03.942: INFO: Created: latency-svc-tsf2f
Jan  3 03:08:03.983: INFO: Got endpoints: latency-svc-tsf2f [554.376918ms]
Jan  3 03:08:03.984: INFO: Created: latency-svc-d5j4c
Jan  3 03:08:03.990: INFO: Got endpoints: latency-svc-d5j4c [538.310007ms]
Jan  3 03:08:04.005: INFO: Created: latency-svc-w4x97
Jan  3 03:08:04.012: INFO: Got endpoints: latency-svc-w4x97 [447.447198ms]
Jan  3 03:08:04.028: INFO: Created: latency-svc-jfhb4
Jan  3 03:08:04.039: INFO: Got endpoints: latency-svc-jfhb4 [462.963528ms]
Jan  3 03:08:04.051: INFO: Created: latency-svc-qxmzj
Jan  3 03:08:04.062: INFO: Got endpoints: latency-svc-qxmzj [463.025201ms]
Jan  3 03:08:04.074: INFO: Created: latency-svc-56262
Jan  3 03:08:04.081: INFO: Got endpoints: latency-svc-56262 [463.983448ms]
Jan  3 03:08:04.137: INFO: Created: latency-svc-xtx5v
Jan  3 03:08:04.144: INFO: Got endpoints: latency-svc-xtx5v [504.149411ms]
Jan  3 03:08:04.160: INFO: Created: latency-svc-bwffh
Jan  3 03:08:04.167: INFO: Got endpoints: latency-svc-bwffh [469.68285ms]
Jan  3 03:08:04.182: INFO: Created: latency-svc-2qhfm
Jan  3 03:08:04.200: INFO: Created: latency-svc-8v76r
Jan  3 03:08:04.200: INFO: Got endpoints: latency-svc-2qhfm [484.848815ms]
Jan  3 03:08:04.205: INFO: Got endpoints: latency-svc-8v76r [462.974224ms]
Jan  3 03:08:04.211: INFO: Created: latency-svc-5zw7w
Jan  3 03:08:04.234: INFO: Got endpoints: latency-svc-5zw7w [468.792469ms]
Jan  3 03:08:04.234: INFO: Created: latency-svc-g44xk
Jan  3 03:08:04.268: INFO: Got endpoints: latency-svc-g44xk [422.920995ms]
Jan  3 03:08:04.280: INFO: Created: latency-svc-kbpfz
Jan  3 03:08:04.285: INFO: Got endpoints: latency-svc-kbpfz [432.732708ms]
Jan  3 03:08:04.302: INFO: Created: latency-svc-kh9vm
Jan  3 03:08:04.308: INFO: Got endpoints: latency-svc-kh9vm [411.57167ms]
Jan  3 03:08:04.458: INFO: Created: latency-svc-kdzqx
Jan  3 03:08:04.458: INFO: Created: latency-svc-pz4sv
Jan  3 03:08:04.458: INFO: Created: latency-svc-hvjft
Jan  3 03:08:04.458: INFO: Created: latency-svc-sfn6n
Jan  3 03:08:04.458: INFO: Created: latency-svc-m2shk
Jan  3 03:08:04.458: INFO: Created: latency-svc-p9shr
Jan  3 03:08:04.458: INFO: Created: latency-svc-tjdng
Jan  3 03:08:04.458: INFO: Created: latency-svc-fc96v
Jan  3 03:08:04.458: INFO: Created: latency-svc-j5d4v
Jan  3 03:08:04.459: INFO: Created: latency-svc-tpsvb
Jan  3 03:08:04.459: INFO: Created: latency-svc-gtss6
Jan  3 03:08:04.459: INFO: Created: latency-svc-69bms
Jan  3 03:08:04.459: INFO: Created: latency-svc-mzb2t
Jan  3 03:08:04.459: INFO: Created: latency-svc-4wdz7
Jan  3 03:08:04.459: INFO: Created: latency-svc-kd5cl
Jan  3 03:08:04.462: INFO: Got endpoints: latency-svc-hvjft [400.365236ms]
Jan  3 03:08:04.463: INFO: Got endpoints: latency-svc-p9shr [194.15483ms]
Jan  3 03:08:04.468: INFO: Got endpoints: latency-svc-fc96v [456.047266ms]
Jan  3 03:08:04.469: INFO: Got endpoints: latency-svc-kd5cl [542.110596ms]
Jan  3 03:08:04.469: INFO: Got endpoints: latency-svc-69bms [269.177213ms]
Jan  3 03:08:04.469: INFO: Got endpoints: latency-svc-mzb2t [235.21312ms]
Jan  3 03:08:04.469: INFO: Got endpoints: latency-svc-kdzqx [160.927772ms]
Jan  3 03:08:04.485: INFO: Got endpoints: latency-svc-sfn6n [318.13483ms]
Jan  3 03:08:04.486: INFO: Created: latency-svc-pjm2g
Jan  3 03:08:04.508: INFO: Created: latency-svc-26vxj
Jan  3 03:08:04.589: INFO: Got endpoints: latency-svc-m2shk [444.451927ms]
Jan  3 03:08:04.589: INFO: Got endpoints: latency-svc-tpsvb [549.68281ms]
Jan  3 03:08:04.589: INFO: Created: latency-svc-4xtd7
Jan  3 03:08:04.623: INFO: Created: latency-svc-g5g72
Jan  3 03:08:04.626: INFO: Got endpoints: latency-svc-4wdz7 [636.890807ms]
Jan  3 03:08:04.645: INFO: Created: latency-svc-f5tvt
Jan  3 03:08:04.685: INFO: Got endpoints: latency-svc-pz4sv [400.313332ms]
Jan  3 03:08:04.686: INFO: Created: latency-svc-c6kfq
Jan  3 03:08:04.729: INFO: Got endpoints: latency-svc-gtss6 [524.36809ms]
Jan  3 03:08:04.743: INFO: Created: latency-svc-76t7c
Jan  3 03:08:04.777: INFO: Got endpoints: latency-svc-j5d4v [794.489687ms]
Jan  3 03:08:04.777: INFO: Created: latency-svc-hbrgs
Jan  3 03:08:04.817: INFO: Created: latency-svc-d96ds
Jan  3 03:08:04.880: INFO: Got endpoints: latency-svc-tjdng [798.535829ms]
Jan  3 03:08:04.880: INFO: Created: latency-svc-swws4
Jan  3 03:08:04.884: INFO: Got endpoints: latency-svc-pjm2g [421.533053ms]
Jan  3 03:08:04.897: INFO: Created: latency-svc-8vvb8
Jan  3 03:08:04.920: INFO: Created: latency-svc-sr796
Jan  3 03:08:04.943: INFO: Created: latency-svc-5cfmz
Jan  3 03:08:04.943: INFO: Got endpoints: latency-svc-26vxj [480.325501ms]
Jan  3 03:08:04.966: INFO: Created: latency-svc-hjh7z
Jan  3 03:08:05.023: INFO: Created: latency-svc-96pmg
Jan  3 03:08:05.023: INFO: Got endpoints: latency-svc-4xtd7 [554.356962ms]
Jan  3 03:08:05.034: INFO: Got endpoints: latency-svc-g5g72 [565.779176ms]
Jan  3 03:08:05.034: INFO: Created: latency-svc-wd7d8
Jan  3 03:08:05.057: INFO: Created: latency-svc-jzf55
Jan  3 03:08:05.080: INFO: Got endpoints: latency-svc-f5tvt [611.135447ms]
Jan  3 03:08:05.080: INFO: Created: latency-svc-khsdm
Jan  3 03:08:05.103: INFO: Created: latency-svc-9jkdm
Jan  3 03:08:05.166: INFO: Got endpoints: latency-svc-c6kfq [696.78516ms]
Jan  3 03:08:05.166: INFO: Created: latency-svc-c4v8m
Jan  3 03:08:05.183: INFO: Got endpoints: latency-svc-76t7c [713.903393ms]
Jan  3 03:08:05.183: INFO: Created: latency-svc-gqc9c
Jan  3 03:08:05.206: INFO: Created: latency-svc-48wlz
Jan  3 03:08:05.228: INFO: Got endpoints: latency-svc-hbrgs [742.438166ms]
Jan  3 03:08:05.257: INFO: Created: latency-svc-f2tzq
Jan  3 03:08:05.303: INFO: Got endpoints: latency-svc-d96ds [714.025066ms]
Jan  3 03:08:05.326: INFO: Got endpoints: latency-svc-swws4 [736.770494ms]
Jan  3 03:08:05.326: INFO: Created: latency-svc-znwp7
Jan  3 03:08:05.349: INFO: Created: latency-svc-5thmd
Jan  3 03:08:05.376: INFO: Got endpoints: latency-svc-8vvb8 [750.013265ms]
Jan  3 03:08:05.451: INFO: Got endpoints: latency-svc-sr796 [765.986687ms]
Jan  3 03:08:05.452: INFO: Created: latency-svc-flc2w
Jan  3 03:08:05.469: INFO: Created: latency-svc-7547g
Jan  3 03:08:05.479: INFO: Got endpoints: latency-svc-5cfmz [749.978073ms]
Jan  3 03:08:05.509: INFO: Created: latency-svc-t6wbb
Jan  3 03:08:05.525: INFO: Got endpoints: latency-svc-hjh7z [748.200497ms]
Jan  3 03:08:05.594: INFO: Created: latency-svc-l5jp2
Jan  3 03:08:05.594: INFO: Got endpoints: latency-svc-96pmg [714.423367ms]
Jan  3 03:08:05.617: INFO: Created: latency-svc-7nqqc
Jan  3 03:08:05.628: INFO: Got endpoints: latency-svc-wd7d8 [744.225246ms]
Jan  3 03:08:05.669: INFO: Created: latency-svc-zwzmv
Jan  3 03:08:05.680: INFO: Got endpoints: latency-svc-jzf55 [737.240426ms]
Jan  3 03:08:05.732: INFO: Got endpoints: latency-svc-khsdm [708.721842ms]
Jan  3 03:08:05.772: INFO: Created: latency-svc-lkqhk
Jan  3 03:08:05.777: INFO: Got endpoints: latency-svc-9jkdm [742.965343ms]
Jan  3 03:08:05.795: INFO: Created: latency-svc-ph5kx
Jan  3 03:08:05.880: INFO: Created: latency-svc-9fcbv
Jan  3 03:08:05.880: INFO: Got endpoints: latency-svc-c4v8m [799.921321ms]
Jan  3 03:08:05.886: INFO: Got endpoints: latency-svc-gqc9c [720.287098ms]
Jan  3 03:08:05.909: INFO: Created: latency-svc-lxfzw
Jan  3 03:08:05.932: INFO: Got endpoints: latency-svc-48wlz [749.086619ms]
Jan  3 03:08:05.932: INFO: Created: latency-svc-dvh2h
Jan  3 03:08:05.954: INFO: Created: latency-svc-7k4xp
Jan  3 03:08:05.978: INFO: Got endpoints: latency-svc-f2tzq [749.790176ms]
Jan  3 03:08:06.029: INFO: Created: latency-svc-gvtdf
Jan  3 03:08:06.029: INFO: Got endpoints: latency-svc-znwp7 [726.08735ms]
Jan  3 03:08:06.052: INFO: Created: latency-svc-5q5qd
Jan  3 03:08:06.098: INFO: Got endpoints: latency-svc-5thmd [772.173946ms]
Jan  3 03:08:06.143: INFO: Created: latency-svc-prg7s
Jan  3 03:08:06.143: INFO: Got endpoints: latency-svc-flc2w [766.923488ms]
Jan  3 03:08:06.166: INFO: Created: latency-svc-87ss5
Jan  3 03:08:06.177: INFO: Got endpoints: latency-svc-7547g [725.911137ms]
Jan  3 03:08:06.200: INFO: Created: latency-svc-shpkl
Jan  3 03:08:06.230: INFO: Got endpoints: latency-svc-t6wbb [750.096451ms]
Jan  3 03:08:06.292: INFO: Got endpoints: latency-svc-l5jp2 [766.197838ms]
Jan  3 03:08:06.292: INFO: Created: latency-svc-v4km8
Jan  3 03:08:06.309: INFO: Created: latency-svc-b5fjx
Jan  3 03:08:06.326: INFO: Got endpoints: latency-svc-7nqqc [732.076682ms]
Jan  3 03:08:06.349: INFO: Created: latency-svc-nz6zk
Jan  3 03:08:06.378: INFO: Got endpoints: latency-svc-zwzmv [749.849584ms]
Jan  3 03:08:06.423: INFO: Created: latency-svc-4f9vf
Jan  3 03:08:06.435: INFO: Got endpoints: latency-svc-lkqhk [754.713033ms]
Jan  3 03:08:06.457: INFO: Created: latency-svc-cprzc
Jan  3 03:08:06.475: INFO: Got endpoints: latency-svc-ph5kx [743.696931ms]
Jan  3 03:08:06.503: INFO: Created: latency-svc-fd4vl
Jan  3 03:08:06.560: INFO: Got endpoints: latency-svc-9fcbv [782.567406ms]
Jan  3 03:08:06.572: INFO: Created: latency-svc-fr5vb
Jan  3 03:08:06.579: INFO: Got endpoints: latency-svc-lxfzw [698.711341ms]
Jan  3 03:08:06.606: INFO: Created: latency-svc-lz885
Jan  3 03:08:06.630: INFO: Got endpoints: latency-svc-dvh2h [744.168248ms]
Jan  3 03:08:06.658: INFO: Created: latency-svc-hw54b
Jan  3 03:08:06.714: INFO: Got endpoints: latency-svc-7k4xp [782.154723ms]
Jan  3 03:08:06.732: INFO: Got endpoints: latency-svc-gvtdf [754.3608ms]
Jan  3 03:08:06.732: INFO: Created: latency-svc-8rz6v
Jan  3 03:08:06.761: INFO: Created: latency-svc-86zfg
Jan  3 03:08:06.779: INFO: Got endpoints: latency-svc-5q5qd [750.101503ms]
Jan  3 03:08:06.874: INFO: Got endpoints: latency-svc-prg7s [776.009642ms]
Jan  3 03:08:06.882: INFO: Got endpoints: latency-svc-87ss5 [738.789321ms]
Jan  3 03:08:06.882: INFO: Created: latency-svc-f5pbh
Jan  3 03:08:06.909: INFO: Created: latency-svc-4q5td
Jan  3 03:08:06.932: INFO: Created: latency-svc-cgpxf
Jan  3 03:08:06.932: INFO: Got endpoints: latency-svc-shpkl [754.87656ms]
Jan  3 03:08:06.955: INFO: Created: latency-svc-ghxdm
Jan  3 03:08:07.028: INFO: Got endpoints: latency-svc-v4km8 [798.736326ms]
Jan  3 03:08:07.035: INFO: Got endpoints: latency-svc-b5fjx [743.251923ms]
Jan  3 03:08:07.058: INFO: Created: latency-svc-qclqj
Jan  3 03:08:07.081: INFO: Created: latency-svc-gp6tb
Jan  3 03:08:07.081: INFO: Got endpoints: latency-svc-nz6zk [754.718372ms]
Jan  3 03:08:07.103: INFO: Created: latency-svc-spzlg
Jan  3 03:08:07.160: INFO: Got endpoints: latency-svc-4f9vf [781.478925ms]
Jan  3 03:08:07.172: INFO: Created: latency-svc-nwrw8
Jan  3 03:08:07.180: INFO: Got endpoints: latency-svc-cprzc [744.885234ms]
Jan  3 03:08:07.206: INFO: Created: latency-svc-9d6lf
Jan  3 03:08:07.229: INFO: Got endpoints: latency-svc-fd4vl [753.981758ms]
Jan  3 03:08:07.246: INFO: Created: latency-svc-nsb9t
Jan  3 03:08:07.291: INFO: Got endpoints: latency-svc-fr5vb [731.126211ms]
Jan  3 03:08:07.304: INFO: Created: latency-svc-f5925
Jan  3 03:08:07.332: INFO: Got endpoints: latency-svc-lz885 [753.425884ms]
Jan  3 03:08:07.350: INFO: Created: latency-svc-rv5hj
Jan  3 03:08:07.389: INFO: Got endpoints: latency-svc-hw54b [759.230604ms]
Jan  3 03:08:07.432: INFO: Got endpoints: latency-svc-8rz6v [717.464031ms]
Jan  3 03:08:07.441: INFO: Created: latency-svc-6crfn
Jan  3 03:08:07.458: INFO: Created: latency-svc-pnhdl
Jan  3 03:08:07.486: INFO: Got endpoints: latency-svc-86zfg [754.476718ms]
Jan  3 03:08:07.504: INFO: Created: latency-svc-trtk7
Jan  3 03:08:07.577: INFO: Got endpoints: latency-svc-f5pbh [797.363657ms]
Jan  3 03:08:07.581: INFO: Got endpoints: latency-svc-4q5td [706.649053ms]
Jan  3 03:08:07.601: INFO: Created: latency-svc-w774d
Jan  3 03:08:07.612: INFO: Created: latency-svc-xr4rf
Jan  3 03:08:07.626: INFO: Got endpoints: latency-svc-cgpxf [743.766733ms]
Jan  3 03:08:07.652: INFO: Created: latency-svc-s74bn
Jan  3 03:08:07.708: INFO: Got endpoints: latency-svc-ghxdm [775.923592ms]
Jan  3 03:08:07.721: INFO: Created: latency-svc-2jd9k
Jan  3 03:08:07.729: INFO: Got endpoints: latency-svc-qclqj [700.847249ms]
Jan  3 03:08:07.750: INFO: Created: latency-svc-gxw5m
Jan  3 03:08:07.781: INFO: Got endpoints: latency-svc-gp6tb [746.103601ms]
Jan  3 03:08:07.801: INFO: Created: latency-svc-b5xns
Jan  3 03:08:07.838: INFO: Got endpoints: latency-svc-spzlg [757.143768ms]
Jan  3 03:08:07.858: INFO: Created: latency-svc-9z8cj
Jan  3 03:08:07.878: INFO: Got endpoints: latency-svc-nwrw8 [718.370654ms]
Jan  3 03:08:07.898: INFO: Created: latency-svc-dv6fn
Jan  3 03:08:07.972: INFO: Got endpoints: latency-svc-9d6lf [792.66117ms]
Jan  3 03:08:07.981: INFO: Got endpoints: latency-svc-nsb9t [751.874421ms]
Jan  3 03:08:08.007: INFO: Created: latency-svc-h4thw
Jan  3 03:08:08.024: INFO: Created: latency-svc-rfpfc
Jan  3 03:08:08.030: INFO: Got endpoints: latency-svc-f5925 [738.479527ms]
Jan  3 03:08:08.047: INFO: Created: latency-svc-cqvdp
Jan  3 03:08:08.102: INFO: Got endpoints: latency-svc-rv5hj [770.172852ms]
Jan  3 03:08:08.116: INFO: Created: latency-svc-wv5df
Jan  3 03:08:08.138: INFO: Got endpoints: latency-svc-6crfn [748.9465ms]
Jan  3 03:08:08.156: INFO: Created: latency-svc-zqm2l
Jan  3 03:08:08.178: INFO: Got endpoints: latency-svc-pnhdl [746.346018ms]
Jan  3 03:08:08.195: INFO: Created: latency-svc-xnznb
Jan  3 03:08:08.240: INFO: Got endpoints: latency-svc-trtk7 [753.07291ms]
Jan  3 03:08:08.253: INFO: Created: latency-svc-v2qzq
Jan  3 03:08:08.279: INFO: Got endpoints: latency-svc-w774d [702.037345ms]
Jan  3 03:08:08.298: INFO: Created: latency-svc-xrnfm
Jan  3 03:08:08.339: INFO: Got endpoints: latency-svc-xr4rf [757.875129ms]
Jan  3 03:08:08.379: INFO: Created: latency-svc-r9zs8
Jan  3 03:08:08.379: INFO: Got endpoints: latency-svc-s74bn [752.616757ms]
Jan  3 03:08:08.390: INFO: Created: latency-svc-hdj5m
Jan  3 03:08:08.427: INFO: Got endpoints: latency-svc-2jd9k [719.233684ms]
Jan  3 03:08:08.447: INFO: Created: latency-svc-w7sbn
Jan  3 03:08:08.503: INFO: Got endpoints: latency-svc-gxw5m [773.33901ms]
Jan  3 03:08:08.516: INFO: Created: latency-svc-pvhnq
Jan  3 03:08:08.531: INFO: Got endpoints: latency-svc-b5xns [749.555409ms]
Jan  3 03:08:08.550: INFO: Created: latency-svc-kj8sh
Jan  3 03:08:08.576: INFO: Got endpoints: latency-svc-9z8cj [737.860588ms]
Jan  3 03:08:08.601: INFO: Created: latency-svc-dw8xz
Jan  3 03:08:08.647: INFO: Got endpoints: latency-svc-dv6fn [769.125994ms]
Jan  3 03:08:08.670: INFO: Created: latency-svc-sxkxr
Jan  3 03:08:08.681: INFO: Got endpoints: latency-svc-h4thw [708.914858ms]
Jan  3 03:08:08.704: INFO: Created: latency-svc-ppqlg
Jan  3 03:08:08.733: INFO: Got endpoints: latency-svc-rfpfc [751.514705ms]
Jan  3 03:08:08.794: INFO: Got endpoints: latency-svc-cqvdp [764.699834ms]
Jan  3 03:08:08.794: INFO: Created: latency-svc-m82lf
Jan  3 03:08:08.824: INFO: Created: latency-svc-hkb97
Jan  3 03:08:08.836: INFO: Got endpoints: latency-svc-wv5df [733.106525ms]
Jan  3 03:08:08.858: INFO: Created: latency-svc-csl4b
Jan  3 03:08:08.893: INFO: Got endpoints: latency-svc-zqm2l [754.432369ms]
Jan  3 03:08:08.937: INFO: Got endpoints: latency-svc-xnznb [759.168823ms]
Jan  3 03:08:08.950: INFO: Created: latency-svc-w47n4
Jan  3 03:08:08.973: INFO: Created: latency-svc-4mwgf
Jan  3 03:08:08.977: INFO: Got endpoints: latency-svc-v2qzq [737.569492ms]
Jan  3 03:08:09.019: INFO: Created: latency-svc-566jq
Jan  3 03:08:09.029: INFO: Got endpoints: latency-svc-xrnfm [749.879771ms]
Jan  3 03:08:09.088: INFO: Got endpoints: latency-svc-r9zs8 [749.039693ms]
Jan  3 03:08:09.088: INFO: Created: latency-svc-dfs6h
Jan  3 03:08:09.127: INFO: Got endpoints: latency-svc-hdj5m [748.685156ms]
Jan  3 03:08:09.128: INFO: Created: latency-svc-b5srb
Jan  3 03:08:09.156: INFO: Created: latency-svc-8pb46
Jan  3 03:08:09.205: INFO: Got endpoints: latency-svc-w7sbn [777.872438ms]
Jan  3 03:08:09.236: INFO: Got endpoints: latency-svc-pvhnq [733.34534ms]
Jan  3 03:08:09.236: INFO: Created: latency-svc-cml79
Jan  3 03:08:09.259: INFO: Created: latency-svc-9k44d
Jan  3 03:08:09.276: INFO: Got endpoints: latency-svc-kj8sh [745.179139ms]
Jan  3 03:08:09.299: INFO: Created: latency-svc-hjp72
Jan  3 03:08:09.343: INFO: Got endpoints: latency-svc-dw8xz [767.207723ms]
Jan  3 03:08:09.367: INFO: Created: latency-svc-8q4p4
Jan  3 03:08:09.390: INFO: Got endpoints: latency-svc-sxkxr [742.915881ms]
Jan  3 03:08:09.413: INFO: Created: latency-svc-r9ffk
Jan  3 03:08:09.468: INFO: Got endpoints: latency-svc-ppqlg [786.765641ms]
Jan  3 03:08:09.481: INFO: Got endpoints: latency-svc-m82lf [747.82316ms]
Jan  3 03:08:09.499: INFO: Created: latency-svc-tgb4k
Jan  3 03:08:09.522: INFO: Created: latency-svc-rnh5x
Jan  3 03:08:09.527: INFO: Got endpoints: latency-svc-hkb97 [732.32319ms]
Jan  3 03:08:09.556: INFO: Created: latency-svc-c5l5q
Jan  3 03:08:09.613: INFO: Got endpoints: latency-svc-csl4b [777.700813ms]
Jan  3 03:08:09.636: INFO: Got endpoints: latency-svc-w47n4 [743.254308ms]
Jan  3 03:08:09.636: INFO: Created: latency-svc-9wrm8
Jan  3 03:08:09.659: INFO: Created: latency-svc-7bxq5
Jan  3 03:08:09.675: INFO: Got endpoints: latency-svc-4mwgf [737.918299ms]
Jan  3 03:08:09.705: INFO: Created: latency-svc-hrh9b
Jan  3 03:08:09.771: INFO: Got endpoints: latency-svc-566jq [793.955914ms]
Jan  3 03:08:09.784: INFO: Got endpoints: latency-svc-dfs6h [755.734188ms]
Jan  3 03:08:09.808: INFO: Created: latency-svc-mnmps
Jan  3 03:08:09.831: INFO: Created: latency-svc-pqh6h
Jan  3 03:08:09.831: INFO: Got endpoints: latency-svc-b5srb [742.986178ms]
Jan  3 03:08:09.865: INFO: Created: latency-svc-f75tp
Jan  3 03:08:09.897: INFO: Got endpoints: latency-svc-8pb46 [769.250056ms]
Jan  3 03:08:09.933: INFO: Got endpoints: latency-svc-cml79 [727.760446ms]
Jan  3 03:08:09.978: INFO: Got endpoints: latency-svc-9k44d [742.398389ms]
Jan  3 03:08:10.036: INFO: Got endpoints: latency-svc-hjp72 [760.263734ms]
Jan  3 03:08:10.082: INFO: Got endpoints: latency-svc-8q4p4 [738.270159ms]
Jan  3 03:08:10.171: INFO: Got endpoints: latency-svc-r9ffk [781.038288ms]
Jan  3 03:08:10.185: INFO: Got endpoints: latency-svc-tgb4k [716.553145ms]
Jan  3 03:08:10.236: INFO: Got endpoints: latency-svc-rnh5x [755.552228ms]
Jan  3 03:08:10.305: INFO: Got endpoints: latency-svc-c5l5q [778.076734ms]
Jan  3 03:08:10.333: INFO: Got endpoints: latency-svc-9wrm8 [720.040477ms]
Jan  3 03:08:10.379: INFO: Got endpoints: latency-svc-7bxq5 [742.562207ms]
Jan  3 03:08:10.430: INFO: Got endpoints: latency-svc-hrh9b [754.910356ms]
Jan  3 03:08:10.482: INFO: Got endpoints: latency-svc-mnmps [710.820442ms]
Jan  3 03:08:10.550: INFO: Got endpoints: latency-svc-pqh6h [765.883624ms]
Jan  3 03:08:10.591: INFO: Got endpoints: latency-svc-f75tp [759.885975ms]
Jan  3 03:08:10.591: INFO: Latencies: [44.650156ms 73.109177ms 101.553345ms 124.614857ms 160.927772ms 188.554744ms 194.15483ms 211.296679ms 234.496485ms 235.21312ms 250.701841ms 269.177213ms 318.13483ms 356.550063ms 377.33083ms 400.313332ms 400.321565ms 400.365236ms 411.57167ms 421.533053ms 422.348248ms 422.920995ms 432.732708ms 444.451927ms 444.50799ms 447.447198ms 449.644721ms 456.047266ms 458.142312ms 462.963528ms 462.974224ms 463.025201ms 463.792818ms 463.983448ms 464.085905ms 468.792469ms 469.620551ms 469.68285ms 469.719623ms 469.785786ms 470.114284ms 474.684795ms 474.72454ms 475.382876ms 479.893621ms 480.325501ms 481.853894ms 484.848815ms 486.944346ms 491.757116ms 491.759664ms 496.336605ms 496.55863ms 497.599415ms 498.311136ms 498.689646ms 503.066368ms 504.149411ms 504.187694ms 510.584872ms 514.473777ms 514.646196ms 514.707026ms 515.2331ms 515.258591ms 519.909617ms 520.946507ms 521.121018ms 521.579118ms 524.36809ms 526.799989ms 532.169806ms 532.185111ms 532.218443ms 532.791325ms 537.523194ms 538.266168ms 538.310007ms 540.599705ms 542.110596ms 548.912117ms 549.123325ms 549.68281ms 554.356962ms 554.376918ms 565.779176ms 611.135447ms 636.890807ms 696.78516ms 698.711341ms 700.847249ms 702.037345ms 706.649053ms 708.721842ms 708.914858ms 710.820442ms 713.903393ms 714.025066ms 714.423367ms 716.553145ms 717.464031ms 718.370654ms 719.233684ms 720.040477ms 720.287098ms 725.911137ms 726.08735ms 727.760446ms 731.126211ms 732.076682ms 732.32319ms 733.106525ms 733.34534ms 736.770494ms 737.240426ms 737.569492ms 737.860588ms 737.918299ms 738.270159ms 738.479527ms 738.789321ms 742.398389ms 742.438166ms 742.562207ms 742.915881ms 742.965343ms 742.986178ms 743.251923ms 743.254308ms 743.696931ms 743.766733ms 744.168248ms 744.225246ms 744.885234ms 745.179139ms 746.103601ms 746.346018ms 747.82316ms 748.200497ms 748.685156ms 748.9465ms 749.039693ms 749.086619ms 749.555409ms 749.790176ms 749.849584ms 749.879771ms 749.978073ms 750.013265ms 750.096451ms 750.101503ms 751.514705ms 751.874421ms 752.616757ms 753.07291ms 753.425884ms 753.981758ms 754.3608ms 754.432369ms 754.476718ms 754.713033ms 754.718372ms 754.87656ms 754.910356ms 755.552228ms 755.734188ms 757.143768ms 757.875129ms 759.168823ms 759.230604ms 759.885975ms 760.263734ms 764.699834ms 765.883624ms 765.986687ms 766.197838ms 766.923488ms 767.207723ms 769.125994ms 769.250056ms 770.172852ms 772.173946ms 773.33901ms 775.923592ms 776.009642ms 777.700813ms 777.872438ms 778.076734ms 781.038288ms 781.478925ms 782.154723ms 782.567406ms 786.765641ms 792.66117ms 793.955914ms 794.489687ms 797.363657ms 798.535829ms 798.736326ms 799.921321ms]
Jan  3 03:08:10.591: INFO: 50 %ile: 717.464031ms
Jan  3 03:08:10.591: INFO: 90 %ile: 770.172852ms
Jan  3 03:08:10.591: INFO: 99 %ile: 798.736326ms
Jan  3 03:08:10.591: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:187
Jan  3 03:08:10.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-5565" for this suite. 01/03/23 03:08:10.599
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","completed":296,"skipped":5627,"failed":0}
------------------------------
• [SLOW TEST] [9.776 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:08:00.826
    Jan  3 03:08:00.826: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename svc-latency 01/03/23 03:08:00.827
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:08:00.841
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:08:00.843
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Jan  3 03:08:00.847: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-5565 01/03/23 03:08:00.848
    I0103 03:08:00.853393      23 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5565, replica count: 1
    I0103 03:08:01.903963      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  3 03:08:02.021: INFO: Created: latency-svc-xgcxb
    Jan  3 03:08:02.032: INFO: Got endpoints: latency-svc-xgcxb [27.985722ms]
    Jan  3 03:08:02.072: INFO: Created: latency-svc-rkwd6
    Jan  3 03:08:02.077: INFO: Got endpoints: latency-svc-rkwd6 [44.650156ms]
    Jan  3 03:08:02.095: INFO: Created: latency-svc-6sb7k
    Jan  3 03:08:02.105: INFO: Got endpoints: latency-svc-6sb7k [73.109177ms]
    Jan  3 03:08:02.129: INFO: Created: latency-svc-gxwwx
    Jan  3 03:08:02.134: INFO: Got endpoints: latency-svc-gxwwx [101.553345ms]
    Jan  3 03:08:02.152: INFO: Created: latency-svc-c67m4
    Jan  3 03:08:02.157: INFO: Got endpoints: latency-svc-c67m4 [124.614857ms]
    Jan  3 03:08:02.209: INFO: Created: latency-svc-t8tpx
    Jan  3 03:08:02.221: INFO: Got endpoints: latency-svc-t8tpx [188.554744ms]
    Jan  3 03:08:02.232: INFO: Created: latency-svc-2w6qm
    Jan  3 03:08:02.243: INFO: Got endpoints: latency-svc-2w6qm [211.296679ms]
    Jan  3 03:08:02.255: INFO: Created: latency-svc-4kmn2
    Jan  3 03:08:02.266: INFO: Got endpoints: latency-svc-4kmn2 [234.496485ms]
    Jan  3 03:08:02.278: INFO: Created: latency-svc-n7krl
    Jan  3 03:08:02.283: INFO: Got endpoints: latency-svc-n7krl [250.701841ms]
    Jan  3 03:08:02.301: INFO: Created: latency-svc-2hhg5
    Jan  3 03:08:02.389: INFO: Got endpoints: latency-svc-2hhg5 [356.550063ms]
    Jan  3 03:08:02.389: INFO: Created: latency-svc-qmv5z
    Jan  3 03:08:02.409: INFO: Got endpoints: latency-svc-qmv5z [377.33083ms]
    Jan  3 03:08:02.421: INFO: Created: latency-svc-mh7gj
    Jan  3 03:08:02.432: INFO: Got endpoints: latency-svc-mh7gj [400.321565ms]
    Jan  3 03:08:02.449: INFO: Created: latency-svc-8nmb5
    Jan  3 03:08:02.454: INFO: Got endpoints: latency-svc-8nmb5 [422.348248ms]
    Jan  3 03:08:02.484: INFO: Created: latency-svc-xcv6q
    Jan  3 03:08:02.543: INFO: Got endpoints: latency-svc-xcv6q [510.584872ms]
    Jan  3 03:08:02.544: INFO: Created: latency-svc-msw8k
    Jan  3 03:08:02.552: INFO: Got endpoints: latency-svc-msw8k [519.909617ms]
    Jan  3 03:08:02.570: INFO: Created: latency-svc-gsvg6
    Jan  3 03:08:02.581: INFO: Got endpoints: latency-svc-gsvg6 [548.912117ms]
    Jan  3 03:08:02.592: INFO: Created: latency-svc-69lnx
    Jan  3 03:08:02.598: INFO: Got endpoints: latency-svc-69lnx [520.946507ms]
    Jan  3 03:08:02.615: INFO: Created: latency-svc-gkkg5
    Jan  3 03:08:02.627: INFO: Got endpoints: latency-svc-gkkg5 [521.579118ms]
    Jan  3 03:08:02.638: INFO: Created: latency-svc-wphwf
    Jan  3 03:08:02.674: INFO: Got endpoints: latency-svc-wphwf [540.599705ms]
    Jan  3 03:08:02.684: INFO: Created: latency-svc-rmzj4
    Jan  3 03:08:02.689: INFO: Got endpoints: latency-svc-rmzj4 [532.791325ms]
    Jan  3 03:08:02.707: INFO: Created: latency-svc-9dkhs
    Jan  3 03:08:02.712: INFO: Got endpoints: latency-svc-9dkhs [491.759664ms]
    Jan  3 03:08:02.730: INFO: Created: latency-svc-svfq7
    Jan  3 03:08:02.735: INFO: Got endpoints: latency-svc-svfq7 [491.757116ms]
    Jan  3 03:08:02.753: INFO: Created: latency-svc-wdnb5
    Jan  3 03:08:02.770: INFO: Got endpoints: latency-svc-wdnb5 [503.066368ms]
    Jan  3 03:08:02.804: INFO: Created: latency-svc-pdq2n
    Jan  3 03:08:02.810: INFO: Got endpoints: latency-svc-pdq2n [526.799989ms]
    Jan  3 03:08:02.827: INFO: Created: latency-svc-gvr2f
    Jan  3 03:08:02.838: INFO: Got endpoints: latency-svc-gvr2f [449.644721ms]
    Jan  3 03:08:02.873: INFO: Created: latency-svc-78rmx
    Jan  3 03:08:02.884: INFO: Got endpoints: latency-svc-78rmx [474.72454ms]
    Jan  3 03:08:02.937: INFO: Created: latency-svc-4wvnw
    Jan  3 03:08:02.947: INFO: Created: latency-svc-9684x
    Jan  3 03:08:02.947: INFO: Got endpoints: latency-svc-4wvnw [514.707026ms]
    Jan  3 03:08:02.959: INFO: Got endpoints: latency-svc-9684x [504.187694ms]
    Jan  3 03:08:02.975: INFO: Created: latency-svc-k97hw
    Jan  3 03:08:02.987: INFO: Got endpoints: latency-svc-k97hw [444.50799ms]
    Jan  3 03:08:03.004: INFO: Created: latency-svc-qdgbj
    Jan  3 03:08:03.016: INFO: Got endpoints: latency-svc-qdgbj [463.792818ms]
    Jan  3 03:08:03.033: INFO: Created: latency-svc-ktx7b
    Jan  3 03:08:03.080: INFO: Got endpoints: latency-svc-ktx7b [498.689646ms]
    Jan  3 03:08:03.081: INFO: Created: latency-svc-zswc7
    Jan  3 03:08:03.085: INFO: Got endpoints: latency-svc-zswc7 [486.944346ms]
    Jan  3 03:08:03.113: INFO: Created: latency-svc-zqkhj
    Jan  3 03:08:03.142: INFO: Got endpoints: latency-svc-zqkhj [515.258591ms]
    Jan  3 03:08:03.159: INFO: Created: latency-svc-g4tbq
    Jan  3 03:08:03.170: INFO: Got endpoints: latency-svc-g4tbq [496.336605ms]
    Jan  3 03:08:03.217: INFO: Created: latency-svc-qwwhv
    Jan  3 03:08:03.228: INFO: Created: latency-svc-4x6mx
    Jan  3 03:08:03.228: INFO: Got endpoints: latency-svc-qwwhv [538.266168ms]
    Jan  3 03:08:03.245: INFO: Got endpoints: latency-svc-4x6mx [532.169806ms]
    Jan  3 03:08:03.256: INFO: Created: latency-svc-rfhkj
    Jan  3 03:08:03.267: INFO: Got endpoints: latency-svc-rfhkj [532.218443ms]
    Jan  3 03:08:03.279: INFO: Created: latency-svc-gbm5m
    Jan  3 03:08:03.285: INFO: Got endpoints: latency-svc-gbm5m [515.2331ms]
    Jan  3 03:08:03.302: INFO: Created: latency-svc-rtdpk
    Jan  3 03:08:03.308: INFO: Got endpoints: latency-svc-rtdpk [498.311136ms]
    Jan  3 03:08:03.360: INFO: Created: latency-svc-75m9t
    Jan  3 03:08:03.370: INFO: Got endpoints: latency-svc-75m9t [532.185111ms]
    Jan  3 03:08:03.371: INFO: Created: latency-svc-qwxft
    Jan  3 03:08:03.382: INFO: Got endpoints: latency-svc-qwxft [497.599415ms]
    Jan  3 03:08:03.393: INFO: Created: latency-svc-lg9nj
    Jan  3 03:08:03.405: INFO: Got endpoints: latency-svc-lg9nj [458.142312ms]
    Jan  3 03:08:03.422: INFO: Created: latency-svc-mblt5
    Jan  3 03:08:03.428: INFO: Got endpoints: latency-svc-mblt5 [469.620551ms]
    Jan  3 03:08:03.445: INFO: Created: latency-svc-9ckmh
    Jan  3 03:08:03.451: INFO: Got endpoints: latency-svc-9ckmh [464.085905ms]
    Jan  3 03:08:03.549: INFO: Created: latency-svc-dvttw
    Jan  3 03:08:03.565: INFO: Created: latency-svc-v288k
    Jan  3 03:08:03.565: INFO: Got endpoints: latency-svc-dvttw [549.123325ms]
    Jan  3 03:08:03.576: INFO: Got endpoints: latency-svc-v288k [496.55863ms]
    Jan  3 03:08:03.588: INFO: Created: latency-svc-d4cnm
    Jan  3 03:08:03.599: INFO: Got endpoints: latency-svc-d4cnm [514.473777ms]
    Jan  3 03:08:03.610: INFO: Created: latency-svc-b65gk
    Jan  3 03:08:03.617: INFO: Got endpoints: latency-svc-b65gk [475.382876ms]
    Jan  3 03:08:03.633: INFO: Created: latency-svc-w5gg5
    Jan  3 03:08:03.640: INFO: Got endpoints: latency-svc-w5gg5 [469.719623ms]
    Jan  3 03:08:03.692: INFO: Created: latency-svc-fhtlm
    Jan  3 03:08:03.697: INFO: Got endpoints: latency-svc-fhtlm [469.785786ms]
    Jan  3 03:08:03.708: INFO: Created: latency-svc-gfkb7
    Jan  3 03:08:03.715: INFO: Got endpoints: latency-svc-gfkb7 [470.114284ms]
    Jan  3 03:08:03.730: INFO: Created: latency-svc-gxtv9
    Jan  3 03:08:03.742: INFO: Got endpoints: latency-svc-gxtv9 [474.684795ms]
    Jan  3 03:08:03.753: INFO: Created: latency-svc-szh2v
    Jan  3 03:08:03.765: INFO: Got endpoints: latency-svc-szh2v [479.893621ms]
    Jan  3 03:08:03.788: INFO: Created: latency-svc-6bcz4
    Jan  3 03:08:03.845: INFO: Got endpoints: latency-svc-6bcz4 [537.523194ms]
    Jan  3 03:08:03.847: INFO: Created: latency-svc-nhtv4
    Jan  3 03:08:03.852: INFO: Got endpoints: latency-svc-nhtv4 [481.853894ms]
    Jan  3 03:08:03.885: INFO: Created: latency-svc-x9vkm
    Jan  3 03:08:03.896: INFO: Got endpoints: latency-svc-x9vkm [514.646196ms]
    Jan  3 03:08:03.919: INFO: Created: latency-svc-hwbz6
    Jan  3 03:08:03.926: INFO: Got endpoints: latency-svc-hwbz6 [521.121018ms]
    Jan  3 03:08:03.942: INFO: Created: latency-svc-tsf2f
    Jan  3 03:08:03.983: INFO: Got endpoints: latency-svc-tsf2f [554.376918ms]
    Jan  3 03:08:03.984: INFO: Created: latency-svc-d5j4c
    Jan  3 03:08:03.990: INFO: Got endpoints: latency-svc-d5j4c [538.310007ms]
    Jan  3 03:08:04.005: INFO: Created: latency-svc-w4x97
    Jan  3 03:08:04.012: INFO: Got endpoints: latency-svc-w4x97 [447.447198ms]
    Jan  3 03:08:04.028: INFO: Created: latency-svc-jfhb4
    Jan  3 03:08:04.039: INFO: Got endpoints: latency-svc-jfhb4 [462.963528ms]
    Jan  3 03:08:04.051: INFO: Created: latency-svc-qxmzj
    Jan  3 03:08:04.062: INFO: Got endpoints: latency-svc-qxmzj [463.025201ms]
    Jan  3 03:08:04.074: INFO: Created: latency-svc-56262
    Jan  3 03:08:04.081: INFO: Got endpoints: latency-svc-56262 [463.983448ms]
    Jan  3 03:08:04.137: INFO: Created: latency-svc-xtx5v
    Jan  3 03:08:04.144: INFO: Got endpoints: latency-svc-xtx5v [504.149411ms]
    Jan  3 03:08:04.160: INFO: Created: latency-svc-bwffh
    Jan  3 03:08:04.167: INFO: Got endpoints: latency-svc-bwffh [469.68285ms]
    Jan  3 03:08:04.182: INFO: Created: latency-svc-2qhfm
    Jan  3 03:08:04.200: INFO: Created: latency-svc-8v76r
    Jan  3 03:08:04.200: INFO: Got endpoints: latency-svc-2qhfm [484.848815ms]
    Jan  3 03:08:04.205: INFO: Got endpoints: latency-svc-8v76r [462.974224ms]
    Jan  3 03:08:04.211: INFO: Created: latency-svc-5zw7w
    Jan  3 03:08:04.234: INFO: Got endpoints: latency-svc-5zw7w [468.792469ms]
    Jan  3 03:08:04.234: INFO: Created: latency-svc-g44xk
    Jan  3 03:08:04.268: INFO: Got endpoints: latency-svc-g44xk [422.920995ms]
    Jan  3 03:08:04.280: INFO: Created: latency-svc-kbpfz
    Jan  3 03:08:04.285: INFO: Got endpoints: latency-svc-kbpfz [432.732708ms]
    Jan  3 03:08:04.302: INFO: Created: latency-svc-kh9vm
    Jan  3 03:08:04.308: INFO: Got endpoints: latency-svc-kh9vm [411.57167ms]
    Jan  3 03:08:04.458: INFO: Created: latency-svc-kdzqx
    Jan  3 03:08:04.458: INFO: Created: latency-svc-pz4sv
    Jan  3 03:08:04.458: INFO: Created: latency-svc-hvjft
    Jan  3 03:08:04.458: INFO: Created: latency-svc-sfn6n
    Jan  3 03:08:04.458: INFO: Created: latency-svc-m2shk
    Jan  3 03:08:04.458: INFO: Created: latency-svc-p9shr
    Jan  3 03:08:04.458: INFO: Created: latency-svc-tjdng
    Jan  3 03:08:04.458: INFO: Created: latency-svc-fc96v
    Jan  3 03:08:04.458: INFO: Created: latency-svc-j5d4v
    Jan  3 03:08:04.459: INFO: Created: latency-svc-tpsvb
    Jan  3 03:08:04.459: INFO: Created: latency-svc-gtss6
    Jan  3 03:08:04.459: INFO: Created: latency-svc-69bms
    Jan  3 03:08:04.459: INFO: Created: latency-svc-mzb2t
    Jan  3 03:08:04.459: INFO: Created: latency-svc-4wdz7
    Jan  3 03:08:04.459: INFO: Created: latency-svc-kd5cl
    Jan  3 03:08:04.462: INFO: Got endpoints: latency-svc-hvjft [400.365236ms]
    Jan  3 03:08:04.463: INFO: Got endpoints: latency-svc-p9shr [194.15483ms]
    Jan  3 03:08:04.468: INFO: Got endpoints: latency-svc-fc96v [456.047266ms]
    Jan  3 03:08:04.469: INFO: Got endpoints: latency-svc-kd5cl [542.110596ms]
    Jan  3 03:08:04.469: INFO: Got endpoints: latency-svc-69bms [269.177213ms]
    Jan  3 03:08:04.469: INFO: Got endpoints: latency-svc-mzb2t [235.21312ms]
    Jan  3 03:08:04.469: INFO: Got endpoints: latency-svc-kdzqx [160.927772ms]
    Jan  3 03:08:04.485: INFO: Got endpoints: latency-svc-sfn6n [318.13483ms]
    Jan  3 03:08:04.486: INFO: Created: latency-svc-pjm2g
    Jan  3 03:08:04.508: INFO: Created: latency-svc-26vxj
    Jan  3 03:08:04.589: INFO: Got endpoints: latency-svc-m2shk [444.451927ms]
    Jan  3 03:08:04.589: INFO: Got endpoints: latency-svc-tpsvb [549.68281ms]
    Jan  3 03:08:04.589: INFO: Created: latency-svc-4xtd7
    Jan  3 03:08:04.623: INFO: Created: latency-svc-g5g72
    Jan  3 03:08:04.626: INFO: Got endpoints: latency-svc-4wdz7 [636.890807ms]
    Jan  3 03:08:04.645: INFO: Created: latency-svc-f5tvt
    Jan  3 03:08:04.685: INFO: Got endpoints: latency-svc-pz4sv [400.313332ms]
    Jan  3 03:08:04.686: INFO: Created: latency-svc-c6kfq
    Jan  3 03:08:04.729: INFO: Got endpoints: latency-svc-gtss6 [524.36809ms]
    Jan  3 03:08:04.743: INFO: Created: latency-svc-76t7c
    Jan  3 03:08:04.777: INFO: Got endpoints: latency-svc-j5d4v [794.489687ms]
    Jan  3 03:08:04.777: INFO: Created: latency-svc-hbrgs
    Jan  3 03:08:04.817: INFO: Created: latency-svc-d96ds
    Jan  3 03:08:04.880: INFO: Got endpoints: latency-svc-tjdng [798.535829ms]
    Jan  3 03:08:04.880: INFO: Created: latency-svc-swws4
    Jan  3 03:08:04.884: INFO: Got endpoints: latency-svc-pjm2g [421.533053ms]
    Jan  3 03:08:04.897: INFO: Created: latency-svc-8vvb8
    Jan  3 03:08:04.920: INFO: Created: latency-svc-sr796
    Jan  3 03:08:04.943: INFO: Created: latency-svc-5cfmz
    Jan  3 03:08:04.943: INFO: Got endpoints: latency-svc-26vxj [480.325501ms]
    Jan  3 03:08:04.966: INFO: Created: latency-svc-hjh7z
    Jan  3 03:08:05.023: INFO: Created: latency-svc-96pmg
    Jan  3 03:08:05.023: INFO: Got endpoints: latency-svc-4xtd7 [554.356962ms]
    Jan  3 03:08:05.034: INFO: Got endpoints: latency-svc-g5g72 [565.779176ms]
    Jan  3 03:08:05.034: INFO: Created: latency-svc-wd7d8
    Jan  3 03:08:05.057: INFO: Created: latency-svc-jzf55
    Jan  3 03:08:05.080: INFO: Got endpoints: latency-svc-f5tvt [611.135447ms]
    Jan  3 03:08:05.080: INFO: Created: latency-svc-khsdm
    Jan  3 03:08:05.103: INFO: Created: latency-svc-9jkdm
    Jan  3 03:08:05.166: INFO: Got endpoints: latency-svc-c6kfq [696.78516ms]
    Jan  3 03:08:05.166: INFO: Created: latency-svc-c4v8m
    Jan  3 03:08:05.183: INFO: Got endpoints: latency-svc-76t7c [713.903393ms]
    Jan  3 03:08:05.183: INFO: Created: latency-svc-gqc9c
    Jan  3 03:08:05.206: INFO: Created: latency-svc-48wlz
    Jan  3 03:08:05.228: INFO: Got endpoints: latency-svc-hbrgs [742.438166ms]
    Jan  3 03:08:05.257: INFO: Created: latency-svc-f2tzq
    Jan  3 03:08:05.303: INFO: Got endpoints: latency-svc-d96ds [714.025066ms]
    Jan  3 03:08:05.326: INFO: Got endpoints: latency-svc-swws4 [736.770494ms]
    Jan  3 03:08:05.326: INFO: Created: latency-svc-znwp7
    Jan  3 03:08:05.349: INFO: Created: latency-svc-5thmd
    Jan  3 03:08:05.376: INFO: Got endpoints: latency-svc-8vvb8 [750.013265ms]
    Jan  3 03:08:05.451: INFO: Got endpoints: latency-svc-sr796 [765.986687ms]
    Jan  3 03:08:05.452: INFO: Created: latency-svc-flc2w
    Jan  3 03:08:05.469: INFO: Created: latency-svc-7547g
    Jan  3 03:08:05.479: INFO: Got endpoints: latency-svc-5cfmz [749.978073ms]
    Jan  3 03:08:05.509: INFO: Created: latency-svc-t6wbb
    Jan  3 03:08:05.525: INFO: Got endpoints: latency-svc-hjh7z [748.200497ms]
    Jan  3 03:08:05.594: INFO: Created: latency-svc-l5jp2
    Jan  3 03:08:05.594: INFO: Got endpoints: latency-svc-96pmg [714.423367ms]
    Jan  3 03:08:05.617: INFO: Created: latency-svc-7nqqc
    Jan  3 03:08:05.628: INFO: Got endpoints: latency-svc-wd7d8 [744.225246ms]
    Jan  3 03:08:05.669: INFO: Created: latency-svc-zwzmv
    Jan  3 03:08:05.680: INFO: Got endpoints: latency-svc-jzf55 [737.240426ms]
    Jan  3 03:08:05.732: INFO: Got endpoints: latency-svc-khsdm [708.721842ms]
    Jan  3 03:08:05.772: INFO: Created: latency-svc-lkqhk
    Jan  3 03:08:05.777: INFO: Got endpoints: latency-svc-9jkdm [742.965343ms]
    Jan  3 03:08:05.795: INFO: Created: latency-svc-ph5kx
    Jan  3 03:08:05.880: INFO: Created: latency-svc-9fcbv
    Jan  3 03:08:05.880: INFO: Got endpoints: latency-svc-c4v8m [799.921321ms]
    Jan  3 03:08:05.886: INFO: Got endpoints: latency-svc-gqc9c [720.287098ms]
    Jan  3 03:08:05.909: INFO: Created: latency-svc-lxfzw
    Jan  3 03:08:05.932: INFO: Got endpoints: latency-svc-48wlz [749.086619ms]
    Jan  3 03:08:05.932: INFO: Created: latency-svc-dvh2h
    Jan  3 03:08:05.954: INFO: Created: latency-svc-7k4xp
    Jan  3 03:08:05.978: INFO: Got endpoints: latency-svc-f2tzq [749.790176ms]
    Jan  3 03:08:06.029: INFO: Created: latency-svc-gvtdf
    Jan  3 03:08:06.029: INFO: Got endpoints: latency-svc-znwp7 [726.08735ms]
    Jan  3 03:08:06.052: INFO: Created: latency-svc-5q5qd
    Jan  3 03:08:06.098: INFO: Got endpoints: latency-svc-5thmd [772.173946ms]
    Jan  3 03:08:06.143: INFO: Created: latency-svc-prg7s
    Jan  3 03:08:06.143: INFO: Got endpoints: latency-svc-flc2w [766.923488ms]
    Jan  3 03:08:06.166: INFO: Created: latency-svc-87ss5
    Jan  3 03:08:06.177: INFO: Got endpoints: latency-svc-7547g [725.911137ms]
    Jan  3 03:08:06.200: INFO: Created: latency-svc-shpkl
    Jan  3 03:08:06.230: INFO: Got endpoints: latency-svc-t6wbb [750.096451ms]
    Jan  3 03:08:06.292: INFO: Got endpoints: latency-svc-l5jp2 [766.197838ms]
    Jan  3 03:08:06.292: INFO: Created: latency-svc-v4km8
    Jan  3 03:08:06.309: INFO: Created: latency-svc-b5fjx
    Jan  3 03:08:06.326: INFO: Got endpoints: latency-svc-7nqqc [732.076682ms]
    Jan  3 03:08:06.349: INFO: Created: latency-svc-nz6zk
    Jan  3 03:08:06.378: INFO: Got endpoints: latency-svc-zwzmv [749.849584ms]
    Jan  3 03:08:06.423: INFO: Created: latency-svc-4f9vf
    Jan  3 03:08:06.435: INFO: Got endpoints: latency-svc-lkqhk [754.713033ms]
    Jan  3 03:08:06.457: INFO: Created: latency-svc-cprzc
    Jan  3 03:08:06.475: INFO: Got endpoints: latency-svc-ph5kx [743.696931ms]
    Jan  3 03:08:06.503: INFO: Created: latency-svc-fd4vl
    Jan  3 03:08:06.560: INFO: Got endpoints: latency-svc-9fcbv [782.567406ms]
    Jan  3 03:08:06.572: INFO: Created: latency-svc-fr5vb
    Jan  3 03:08:06.579: INFO: Got endpoints: latency-svc-lxfzw [698.711341ms]
    Jan  3 03:08:06.606: INFO: Created: latency-svc-lz885
    Jan  3 03:08:06.630: INFO: Got endpoints: latency-svc-dvh2h [744.168248ms]
    Jan  3 03:08:06.658: INFO: Created: latency-svc-hw54b
    Jan  3 03:08:06.714: INFO: Got endpoints: latency-svc-7k4xp [782.154723ms]
    Jan  3 03:08:06.732: INFO: Got endpoints: latency-svc-gvtdf [754.3608ms]
    Jan  3 03:08:06.732: INFO: Created: latency-svc-8rz6v
    Jan  3 03:08:06.761: INFO: Created: latency-svc-86zfg
    Jan  3 03:08:06.779: INFO: Got endpoints: latency-svc-5q5qd [750.101503ms]
    Jan  3 03:08:06.874: INFO: Got endpoints: latency-svc-prg7s [776.009642ms]
    Jan  3 03:08:06.882: INFO: Got endpoints: latency-svc-87ss5 [738.789321ms]
    Jan  3 03:08:06.882: INFO: Created: latency-svc-f5pbh
    Jan  3 03:08:06.909: INFO: Created: latency-svc-4q5td
    Jan  3 03:08:06.932: INFO: Created: latency-svc-cgpxf
    Jan  3 03:08:06.932: INFO: Got endpoints: latency-svc-shpkl [754.87656ms]
    Jan  3 03:08:06.955: INFO: Created: latency-svc-ghxdm
    Jan  3 03:08:07.028: INFO: Got endpoints: latency-svc-v4km8 [798.736326ms]
    Jan  3 03:08:07.035: INFO: Got endpoints: latency-svc-b5fjx [743.251923ms]
    Jan  3 03:08:07.058: INFO: Created: latency-svc-qclqj
    Jan  3 03:08:07.081: INFO: Created: latency-svc-gp6tb
    Jan  3 03:08:07.081: INFO: Got endpoints: latency-svc-nz6zk [754.718372ms]
    Jan  3 03:08:07.103: INFO: Created: latency-svc-spzlg
    Jan  3 03:08:07.160: INFO: Got endpoints: latency-svc-4f9vf [781.478925ms]
    Jan  3 03:08:07.172: INFO: Created: latency-svc-nwrw8
    Jan  3 03:08:07.180: INFO: Got endpoints: latency-svc-cprzc [744.885234ms]
    Jan  3 03:08:07.206: INFO: Created: latency-svc-9d6lf
    Jan  3 03:08:07.229: INFO: Got endpoints: latency-svc-fd4vl [753.981758ms]
    Jan  3 03:08:07.246: INFO: Created: latency-svc-nsb9t
    Jan  3 03:08:07.291: INFO: Got endpoints: latency-svc-fr5vb [731.126211ms]
    Jan  3 03:08:07.304: INFO: Created: latency-svc-f5925
    Jan  3 03:08:07.332: INFO: Got endpoints: latency-svc-lz885 [753.425884ms]
    Jan  3 03:08:07.350: INFO: Created: latency-svc-rv5hj
    Jan  3 03:08:07.389: INFO: Got endpoints: latency-svc-hw54b [759.230604ms]
    Jan  3 03:08:07.432: INFO: Got endpoints: latency-svc-8rz6v [717.464031ms]
    Jan  3 03:08:07.441: INFO: Created: latency-svc-6crfn
    Jan  3 03:08:07.458: INFO: Created: latency-svc-pnhdl
    Jan  3 03:08:07.486: INFO: Got endpoints: latency-svc-86zfg [754.476718ms]
    Jan  3 03:08:07.504: INFO: Created: latency-svc-trtk7
    Jan  3 03:08:07.577: INFO: Got endpoints: latency-svc-f5pbh [797.363657ms]
    Jan  3 03:08:07.581: INFO: Got endpoints: latency-svc-4q5td [706.649053ms]
    Jan  3 03:08:07.601: INFO: Created: latency-svc-w774d
    Jan  3 03:08:07.612: INFO: Created: latency-svc-xr4rf
    Jan  3 03:08:07.626: INFO: Got endpoints: latency-svc-cgpxf [743.766733ms]
    Jan  3 03:08:07.652: INFO: Created: latency-svc-s74bn
    Jan  3 03:08:07.708: INFO: Got endpoints: latency-svc-ghxdm [775.923592ms]
    Jan  3 03:08:07.721: INFO: Created: latency-svc-2jd9k
    Jan  3 03:08:07.729: INFO: Got endpoints: latency-svc-qclqj [700.847249ms]
    Jan  3 03:08:07.750: INFO: Created: latency-svc-gxw5m
    Jan  3 03:08:07.781: INFO: Got endpoints: latency-svc-gp6tb [746.103601ms]
    Jan  3 03:08:07.801: INFO: Created: latency-svc-b5xns
    Jan  3 03:08:07.838: INFO: Got endpoints: latency-svc-spzlg [757.143768ms]
    Jan  3 03:08:07.858: INFO: Created: latency-svc-9z8cj
    Jan  3 03:08:07.878: INFO: Got endpoints: latency-svc-nwrw8 [718.370654ms]
    Jan  3 03:08:07.898: INFO: Created: latency-svc-dv6fn
    Jan  3 03:08:07.972: INFO: Got endpoints: latency-svc-9d6lf [792.66117ms]
    Jan  3 03:08:07.981: INFO: Got endpoints: latency-svc-nsb9t [751.874421ms]
    Jan  3 03:08:08.007: INFO: Created: latency-svc-h4thw
    Jan  3 03:08:08.024: INFO: Created: latency-svc-rfpfc
    Jan  3 03:08:08.030: INFO: Got endpoints: latency-svc-f5925 [738.479527ms]
    Jan  3 03:08:08.047: INFO: Created: latency-svc-cqvdp
    Jan  3 03:08:08.102: INFO: Got endpoints: latency-svc-rv5hj [770.172852ms]
    Jan  3 03:08:08.116: INFO: Created: latency-svc-wv5df
    Jan  3 03:08:08.138: INFO: Got endpoints: latency-svc-6crfn [748.9465ms]
    Jan  3 03:08:08.156: INFO: Created: latency-svc-zqm2l
    Jan  3 03:08:08.178: INFO: Got endpoints: latency-svc-pnhdl [746.346018ms]
    Jan  3 03:08:08.195: INFO: Created: latency-svc-xnznb
    Jan  3 03:08:08.240: INFO: Got endpoints: latency-svc-trtk7 [753.07291ms]
    Jan  3 03:08:08.253: INFO: Created: latency-svc-v2qzq
    Jan  3 03:08:08.279: INFO: Got endpoints: latency-svc-w774d [702.037345ms]
    Jan  3 03:08:08.298: INFO: Created: latency-svc-xrnfm
    Jan  3 03:08:08.339: INFO: Got endpoints: latency-svc-xr4rf [757.875129ms]
    Jan  3 03:08:08.379: INFO: Created: latency-svc-r9zs8
    Jan  3 03:08:08.379: INFO: Got endpoints: latency-svc-s74bn [752.616757ms]
    Jan  3 03:08:08.390: INFO: Created: latency-svc-hdj5m
    Jan  3 03:08:08.427: INFO: Got endpoints: latency-svc-2jd9k [719.233684ms]
    Jan  3 03:08:08.447: INFO: Created: latency-svc-w7sbn
    Jan  3 03:08:08.503: INFO: Got endpoints: latency-svc-gxw5m [773.33901ms]
    Jan  3 03:08:08.516: INFO: Created: latency-svc-pvhnq
    Jan  3 03:08:08.531: INFO: Got endpoints: latency-svc-b5xns [749.555409ms]
    Jan  3 03:08:08.550: INFO: Created: latency-svc-kj8sh
    Jan  3 03:08:08.576: INFO: Got endpoints: latency-svc-9z8cj [737.860588ms]
    Jan  3 03:08:08.601: INFO: Created: latency-svc-dw8xz
    Jan  3 03:08:08.647: INFO: Got endpoints: latency-svc-dv6fn [769.125994ms]
    Jan  3 03:08:08.670: INFO: Created: latency-svc-sxkxr
    Jan  3 03:08:08.681: INFO: Got endpoints: latency-svc-h4thw [708.914858ms]
    Jan  3 03:08:08.704: INFO: Created: latency-svc-ppqlg
    Jan  3 03:08:08.733: INFO: Got endpoints: latency-svc-rfpfc [751.514705ms]
    Jan  3 03:08:08.794: INFO: Got endpoints: latency-svc-cqvdp [764.699834ms]
    Jan  3 03:08:08.794: INFO: Created: latency-svc-m82lf
    Jan  3 03:08:08.824: INFO: Created: latency-svc-hkb97
    Jan  3 03:08:08.836: INFO: Got endpoints: latency-svc-wv5df [733.106525ms]
    Jan  3 03:08:08.858: INFO: Created: latency-svc-csl4b
    Jan  3 03:08:08.893: INFO: Got endpoints: latency-svc-zqm2l [754.432369ms]
    Jan  3 03:08:08.937: INFO: Got endpoints: latency-svc-xnznb [759.168823ms]
    Jan  3 03:08:08.950: INFO: Created: latency-svc-w47n4
    Jan  3 03:08:08.973: INFO: Created: latency-svc-4mwgf
    Jan  3 03:08:08.977: INFO: Got endpoints: latency-svc-v2qzq [737.569492ms]
    Jan  3 03:08:09.019: INFO: Created: latency-svc-566jq
    Jan  3 03:08:09.029: INFO: Got endpoints: latency-svc-xrnfm [749.879771ms]
    Jan  3 03:08:09.088: INFO: Got endpoints: latency-svc-r9zs8 [749.039693ms]
    Jan  3 03:08:09.088: INFO: Created: latency-svc-dfs6h
    Jan  3 03:08:09.127: INFO: Got endpoints: latency-svc-hdj5m [748.685156ms]
    Jan  3 03:08:09.128: INFO: Created: latency-svc-b5srb
    Jan  3 03:08:09.156: INFO: Created: latency-svc-8pb46
    Jan  3 03:08:09.205: INFO: Got endpoints: latency-svc-w7sbn [777.872438ms]
    Jan  3 03:08:09.236: INFO: Got endpoints: latency-svc-pvhnq [733.34534ms]
    Jan  3 03:08:09.236: INFO: Created: latency-svc-cml79
    Jan  3 03:08:09.259: INFO: Created: latency-svc-9k44d
    Jan  3 03:08:09.276: INFO: Got endpoints: latency-svc-kj8sh [745.179139ms]
    Jan  3 03:08:09.299: INFO: Created: latency-svc-hjp72
    Jan  3 03:08:09.343: INFO: Got endpoints: latency-svc-dw8xz [767.207723ms]
    Jan  3 03:08:09.367: INFO: Created: latency-svc-8q4p4
    Jan  3 03:08:09.390: INFO: Got endpoints: latency-svc-sxkxr [742.915881ms]
    Jan  3 03:08:09.413: INFO: Created: latency-svc-r9ffk
    Jan  3 03:08:09.468: INFO: Got endpoints: latency-svc-ppqlg [786.765641ms]
    Jan  3 03:08:09.481: INFO: Got endpoints: latency-svc-m82lf [747.82316ms]
    Jan  3 03:08:09.499: INFO: Created: latency-svc-tgb4k
    Jan  3 03:08:09.522: INFO: Created: latency-svc-rnh5x
    Jan  3 03:08:09.527: INFO: Got endpoints: latency-svc-hkb97 [732.32319ms]
    Jan  3 03:08:09.556: INFO: Created: latency-svc-c5l5q
    Jan  3 03:08:09.613: INFO: Got endpoints: latency-svc-csl4b [777.700813ms]
    Jan  3 03:08:09.636: INFO: Got endpoints: latency-svc-w47n4 [743.254308ms]
    Jan  3 03:08:09.636: INFO: Created: latency-svc-9wrm8
    Jan  3 03:08:09.659: INFO: Created: latency-svc-7bxq5
    Jan  3 03:08:09.675: INFO: Got endpoints: latency-svc-4mwgf [737.918299ms]
    Jan  3 03:08:09.705: INFO: Created: latency-svc-hrh9b
    Jan  3 03:08:09.771: INFO: Got endpoints: latency-svc-566jq [793.955914ms]
    Jan  3 03:08:09.784: INFO: Got endpoints: latency-svc-dfs6h [755.734188ms]
    Jan  3 03:08:09.808: INFO: Created: latency-svc-mnmps
    Jan  3 03:08:09.831: INFO: Created: latency-svc-pqh6h
    Jan  3 03:08:09.831: INFO: Got endpoints: latency-svc-b5srb [742.986178ms]
    Jan  3 03:08:09.865: INFO: Created: latency-svc-f75tp
    Jan  3 03:08:09.897: INFO: Got endpoints: latency-svc-8pb46 [769.250056ms]
    Jan  3 03:08:09.933: INFO: Got endpoints: latency-svc-cml79 [727.760446ms]
    Jan  3 03:08:09.978: INFO: Got endpoints: latency-svc-9k44d [742.398389ms]
    Jan  3 03:08:10.036: INFO: Got endpoints: latency-svc-hjp72 [760.263734ms]
    Jan  3 03:08:10.082: INFO: Got endpoints: latency-svc-8q4p4 [738.270159ms]
    Jan  3 03:08:10.171: INFO: Got endpoints: latency-svc-r9ffk [781.038288ms]
    Jan  3 03:08:10.185: INFO: Got endpoints: latency-svc-tgb4k [716.553145ms]
    Jan  3 03:08:10.236: INFO: Got endpoints: latency-svc-rnh5x [755.552228ms]
    Jan  3 03:08:10.305: INFO: Got endpoints: latency-svc-c5l5q [778.076734ms]
    Jan  3 03:08:10.333: INFO: Got endpoints: latency-svc-9wrm8 [720.040477ms]
    Jan  3 03:08:10.379: INFO: Got endpoints: latency-svc-7bxq5 [742.562207ms]
    Jan  3 03:08:10.430: INFO: Got endpoints: latency-svc-hrh9b [754.910356ms]
    Jan  3 03:08:10.482: INFO: Got endpoints: latency-svc-mnmps [710.820442ms]
    Jan  3 03:08:10.550: INFO: Got endpoints: latency-svc-pqh6h [765.883624ms]
    Jan  3 03:08:10.591: INFO: Got endpoints: latency-svc-f75tp [759.885975ms]
    Jan  3 03:08:10.591: INFO: Latencies: [44.650156ms 73.109177ms 101.553345ms 124.614857ms 160.927772ms 188.554744ms 194.15483ms 211.296679ms 234.496485ms 235.21312ms 250.701841ms 269.177213ms 318.13483ms 356.550063ms 377.33083ms 400.313332ms 400.321565ms 400.365236ms 411.57167ms 421.533053ms 422.348248ms 422.920995ms 432.732708ms 444.451927ms 444.50799ms 447.447198ms 449.644721ms 456.047266ms 458.142312ms 462.963528ms 462.974224ms 463.025201ms 463.792818ms 463.983448ms 464.085905ms 468.792469ms 469.620551ms 469.68285ms 469.719623ms 469.785786ms 470.114284ms 474.684795ms 474.72454ms 475.382876ms 479.893621ms 480.325501ms 481.853894ms 484.848815ms 486.944346ms 491.757116ms 491.759664ms 496.336605ms 496.55863ms 497.599415ms 498.311136ms 498.689646ms 503.066368ms 504.149411ms 504.187694ms 510.584872ms 514.473777ms 514.646196ms 514.707026ms 515.2331ms 515.258591ms 519.909617ms 520.946507ms 521.121018ms 521.579118ms 524.36809ms 526.799989ms 532.169806ms 532.185111ms 532.218443ms 532.791325ms 537.523194ms 538.266168ms 538.310007ms 540.599705ms 542.110596ms 548.912117ms 549.123325ms 549.68281ms 554.356962ms 554.376918ms 565.779176ms 611.135447ms 636.890807ms 696.78516ms 698.711341ms 700.847249ms 702.037345ms 706.649053ms 708.721842ms 708.914858ms 710.820442ms 713.903393ms 714.025066ms 714.423367ms 716.553145ms 717.464031ms 718.370654ms 719.233684ms 720.040477ms 720.287098ms 725.911137ms 726.08735ms 727.760446ms 731.126211ms 732.076682ms 732.32319ms 733.106525ms 733.34534ms 736.770494ms 737.240426ms 737.569492ms 737.860588ms 737.918299ms 738.270159ms 738.479527ms 738.789321ms 742.398389ms 742.438166ms 742.562207ms 742.915881ms 742.965343ms 742.986178ms 743.251923ms 743.254308ms 743.696931ms 743.766733ms 744.168248ms 744.225246ms 744.885234ms 745.179139ms 746.103601ms 746.346018ms 747.82316ms 748.200497ms 748.685156ms 748.9465ms 749.039693ms 749.086619ms 749.555409ms 749.790176ms 749.849584ms 749.879771ms 749.978073ms 750.013265ms 750.096451ms 750.101503ms 751.514705ms 751.874421ms 752.616757ms 753.07291ms 753.425884ms 753.981758ms 754.3608ms 754.432369ms 754.476718ms 754.713033ms 754.718372ms 754.87656ms 754.910356ms 755.552228ms 755.734188ms 757.143768ms 757.875129ms 759.168823ms 759.230604ms 759.885975ms 760.263734ms 764.699834ms 765.883624ms 765.986687ms 766.197838ms 766.923488ms 767.207723ms 769.125994ms 769.250056ms 770.172852ms 772.173946ms 773.33901ms 775.923592ms 776.009642ms 777.700813ms 777.872438ms 778.076734ms 781.038288ms 781.478925ms 782.154723ms 782.567406ms 786.765641ms 792.66117ms 793.955914ms 794.489687ms 797.363657ms 798.535829ms 798.736326ms 799.921321ms]
    Jan  3 03:08:10.591: INFO: 50 %ile: 717.464031ms
    Jan  3 03:08:10.591: INFO: 90 %ile: 770.172852ms
    Jan  3 03:08:10.591: INFO: 99 %ile: 798.736326ms
    Jan  3 03:08:10.591: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:187
    Jan  3 03:08:10.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svc-latency-5565" for this suite. 01/03/23 03:08:10.599
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:08:10.604
Jan  3 03:08:10.604: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename services 01/03/23 03:08:10.605
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:08:10.613
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:08:10.615
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
STEP: creating service in namespace services-2128 01/03/23 03:08:10.617
STEP: creating service affinity-clusterip-transition in namespace services-2128 01/03/23 03:08:10.617
STEP: creating replication controller affinity-clusterip-transition in namespace services-2128 01/03/23 03:08:10.642
I0103 03:08:10.674343      23 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-2128, replica count: 3
I0103 03:08:13.724753      23 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  3 03:08:13.728: INFO: Creating new exec pod
Jan  3 03:08:13.732: INFO: Waiting up to 5m0s for pod "execpod-affinitykpz8v" in namespace "services-2128" to be "running"
Jan  3 03:08:13.765: INFO: Pod "execpod-affinitykpz8v": Phase="Pending", Reason="", readiness=false. Elapsed: 32.451087ms
Jan  3 03:08:15.776: INFO: Pod "execpod-affinitykpz8v": Phase="Running", Reason="", readiness=true. Elapsed: 2.044383513s
Jan  3 03:08:15.776: INFO: Pod "execpod-affinitykpz8v" satisfied condition "running"
Jan  3 03:08:16.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-2128 exec execpod-affinitykpz8v -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Jan  3 03:08:16.922: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jan  3 03:08:16.922: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  3 03:08:16.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-2128 exec execpod-affinitykpz8v -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.68.219 80'
Jan  3 03:08:17.117: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.68.219 80\nConnection to 10.96.68.219 80 port [tcp/http] succeeded!\n"
Jan  3 03:08:17.117: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  3 03:08:17.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-2128 exec execpod-affinitykpz8v -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.68.219:80/ ; done'
Jan  3 03:08:17.363: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n"
Jan  3 03:08:17.363: INFO: stdout: "\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r"
Jan  3 03:08:17.363: INFO: Received response from host: affinity-clusterip-transition-z858r
Jan  3 03:08:17.363: INFO: Received response from host: affinity-clusterip-transition-z858r
Jan  3 03:08:17.363: INFO: Received response from host: affinity-clusterip-transition-z858r
Jan  3 03:08:17.363: INFO: Received response from host: affinity-clusterip-transition-z858r
Jan  3 03:08:17.363: INFO: Received response from host: affinity-clusterip-transition-z858r
Jan  3 03:08:17.363: INFO: Received response from host: affinity-clusterip-transition-z858r
Jan  3 03:08:17.363: INFO: Received response from host: affinity-clusterip-transition-z858r
Jan  3 03:08:17.363: INFO: Received response from host: affinity-clusterip-transition-z858r
Jan  3 03:08:17.363: INFO: Received response from host: affinity-clusterip-transition-z858r
Jan  3 03:08:17.363: INFO: Received response from host: affinity-clusterip-transition-z858r
Jan  3 03:08:17.363: INFO: Received response from host: affinity-clusterip-transition-z858r
Jan  3 03:08:17.363: INFO: Received response from host: affinity-clusterip-transition-z858r
Jan  3 03:08:17.363: INFO: Received response from host: affinity-clusterip-transition-z858r
Jan  3 03:08:17.363: INFO: Received response from host: affinity-clusterip-transition-z858r
Jan  3 03:08:17.363: INFO: Received response from host: affinity-clusterip-transition-z858r
Jan  3 03:08:17.363: INFO: Received response from host: affinity-clusterip-transition-z858r
Jan  3 03:08:47.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-2128 exec execpod-affinitykpz8v -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.68.219:80/ ; done'
Jan  3 03:08:47.590: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n"
Jan  3 03:08:47.590: INFO: stdout: "\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-hp7k8\naffinity-clusterip-transition-zjsk7\naffinity-clusterip-transition-zjsk7\naffinity-clusterip-transition-hp7k8\naffinity-clusterip-transition-zjsk7\naffinity-clusterip-transition-hp7k8\naffinity-clusterip-transition-hp7k8\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-zjsk7\naffinity-clusterip-transition-zjsk7\naffinity-clusterip-transition-zjsk7\naffinity-clusterip-transition-hp7k8\naffinity-clusterip-transition-zjsk7"
Jan  3 03:08:47.590: INFO: Received response from host: affinity-clusterip-transition-z858r
Jan  3 03:08:47.590: INFO: Received response from host: affinity-clusterip-transition-z858r
Jan  3 03:08:47.590: INFO: Received response from host: affinity-clusterip-transition-hp7k8
Jan  3 03:08:47.590: INFO: Received response from host: affinity-clusterip-transition-zjsk7
Jan  3 03:08:47.590: INFO: Received response from host: affinity-clusterip-transition-zjsk7
Jan  3 03:08:47.590: INFO: Received response from host: affinity-clusterip-transition-hp7k8
Jan  3 03:08:47.590: INFO: Received response from host: affinity-clusterip-transition-zjsk7
Jan  3 03:08:47.590: INFO: Received response from host: affinity-clusterip-transition-hp7k8
Jan  3 03:08:47.590: INFO: Received response from host: affinity-clusterip-transition-hp7k8
Jan  3 03:08:47.590: INFO: Received response from host: affinity-clusterip-transition-z858r
Jan  3 03:08:47.590: INFO: Received response from host: affinity-clusterip-transition-z858r
Jan  3 03:08:47.590: INFO: Received response from host: affinity-clusterip-transition-zjsk7
Jan  3 03:08:47.590: INFO: Received response from host: affinity-clusterip-transition-zjsk7
Jan  3 03:08:47.590: INFO: Received response from host: affinity-clusterip-transition-zjsk7
Jan  3 03:08:47.590: INFO: Received response from host: affinity-clusterip-transition-hp7k8
Jan  3 03:08:47.590: INFO: Received response from host: affinity-clusterip-transition-zjsk7
Jan  3 03:08:47.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-2128 exec execpod-affinitykpz8v -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.68.219:80/ ; done'
Jan  3 03:08:47.775: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n"
Jan  3 03:08:47.775: INFO: stdout: "\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r"
Jan  3 03:08:47.775: INFO: Received response from host: affinity-clusterip-transition-z858r
Jan  3 03:08:47.775: INFO: Received response from host: affinity-clusterip-transition-z858r
Jan  3 03:08:47.775: INFO: Received response from host: affinity-clusterip-transition-z858r
Jan  3 03:08:47.775: INFO: Received response from host: affinity-clusterip-transition-z858r
Jan  3 03:08:47.775: INFO: Received response from host: affinity-clusterip-transition-z858r
Jan  3 03:08:47.775: INFO: Received response from host: affinity-clusterip-transition-z858r
Jan  3 03:08:47.775: INFO: Received response from host: affinity-clusterip-transition-z858r
Jan  3 03:08:47.775: INFO: Received response from host: affinity-clusterip-transition-z858r
Jan  3 03:08:47.775: INFO: Received response from host: affinity-clusterip-transition-z858r
Jan  3 03:08:47.775: INFO: Received response from host: affinity-clusterip-transition-z858r
Jan  3 03:08:47.775: INFO: Received response from host: affinity-clusterip-transition-z858r
Jan  3 03:08:47.775: INFO: Received response from host: affinity-clusterip-transition-z858r
Jan  3 03:08:47.775: INFO: Received response from host: affinity-clusterip-transition-z858r
Jan  3 03:08:47.775: INFO: Received response from host: affinity-clusterip-transition-z858r
Jan  3 03:08:47.775: INFO: Received response from host: affinity-clusterip-transition-z858r
Jan  3 03:08:47.775: INFO: Received response from host: affinity-clusterip-transition-z858r
Jan  3 03:08:47.775: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-2128, will wait for the garbage collector to delete the pods 01/03/23 03:08:47.811
Jan  3 03:08:47.870: INFO: Deleting ReplicationController affinity-clusterip-transition took: 2.735185ms
Jan  3 03:08:47.970: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.302325ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  3 03:08:50.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2128" for this suite. 01/03/23 03:08:50.413
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","completed":297,"skipped":5668,"failed":0}
------------------------------
• [SLOW TEST] [39.812 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:08:10.604
    Jan  3 03:08:10.604: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename services 01/03/23 03:08:10.605
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:08:10.613
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:08:10.615
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2189
    STEP: creating service in namespace services-2128 01/03/23 03:08:10.617
    STEP: creating service affinity-clusterip-transition in namespace services-2128 01/03/23 03:08:10.617
    STEP: creating replication controller affinity-clusterip-transition in namespace services-2128 01/03/23 03:08:10.642
    I0103 03:08:10.674343      23 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-2128, replica count: 3
    I0103 03:08:13.724753      23 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  3 03:08:13.728: INFO: Creating new exec pod
    Jan  3 03:08:13.732: INFO: Waiting up to 5m0s for pod "execpod-affinitykpz8v" in namespace "services-2128" to be "running"
    Jan  3 03:08:13.765: INFO: Pod "execpod-affinitykpz8v": Phase="Pending", Reason="", readiness=false. Elapsed: 32.451087ms
    Jan  3 03:08:15.776: INFO: Pod "execpod-affinitykpz8v": Phase="Running", Reason="", readiness=true. Elapsed: 2.044383513s
    Jan  3 03:08:15.776: INFO: Pod "execpod-affinitykpz8v" satisfied condition "running"
    Jan  3 03:08:16.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-2128 exec execpod-affinitykpz8v -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
    Jan  3 03:08:16.922: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Jan  3 03:08:16.922: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  3 03:08:16.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-2128 exec execpod-affinitykpz8v -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.68.219 80'
    Jan  3 03:08:17.117: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.68.219 80\nConnection to 10.96.68.219 80 port [tcp/http] succeeded!\n"
    Jan  3 03:08:17.117: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  3 03:08:17.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-2128 exec execpod-affinitykpz8v -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.68.219:80/ ; done'
    Jan  3 03:08:17.363: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n"
    Jan  3 03:08:17.363: INFO: stdout: "\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r"
    Jan  3 03:08:17.363: INFO: Received response from host: affinity-clusterip-transition-z858r
    Jan  3 03:08:17.363: INFO: Received response from host: affinity-clusterip-transition-z858r
    Jan  3 03:08:17.363: INFO: Received response from host: affinity-clusterip-transition-z858r
    Jan  3 03:08:17.363: INFO: Received response from host: affinity-clusterip-transition-z858r
    Jan  3 03:08:17.363: INFO: Received response from host: affinity-clusterip-transition-z858r
    Jan  3 03:08:17.363: INFO: Received response from host: affinity-clusterip-transition-z858r
    Jan  3 03:08:17.363: INFO: Received response from host: affinity-clusterip-transition-z858r
    Jan  3 03:08:17.363: INFO: Received response from host: affinity-clusterip-transition-z858r
    Jan  3 03:08:17.363: INFO: Received response from host: affinity-clusterip-transition-z858r
    Jan  3 03:08:17.363: INFO: Received response from host: affinity-clusterip-transition-z858r
    Jan  3 03:08:17.363: INFO: Received response from host: affinity-clusterip-transition-z858r
    Jan  3 03:08:17.363: INFO: Received response from host: affinity-clusterip-transition-z858r
    Jan  3 03:08:17.363: INFO: Received response from host: affinity-clusterip-transition-z858r
    Jan  3 03:08:17.363: INFO: Received response from host: affinity-clusterip-transition-z858r
    Jan  3 03:08:17.363: INFO: Received response from host: affinity-clusterip-transition-z858r
    Jan  3 03:08:17.363: INFO: Received response from host: affinity-clusterip-transition-z858r
    Jan  3 03:08:47.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-2128 exec execpod-affinitykpz8v -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.68.219:80/ ; done'
    Jan  3 03:08:47.590: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n"
    Jan  3 03:08:47.590: INFO: stdout: "\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-hp7k8\naffinity-clusterip-transition-zjsk7\naffinity-clusterip-transition-zjsk7\naffinity-clusterip-transition-hp7k8\naffinity-clusterip-transition-zjsk7\naffinity-clusterip-transition-hp7k8\naffinity-clusterip-transition-hp7k8\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-zjsk7\naffinity-clusterip-transition-zjsk7\naffinity-clusterip-transition-zjsk7\naffinity-clusterip-transition-hp7k8\naffinity-clusterip-transition-zjsk7"
    Jan  3 03:08:47.590: INFO: Received response from host: affinity-clusterip-transition-z858r
    Jan  3 03:08:47.590: INFO: Received response from host: affinity-clusterip-transition-z858r
    Jan  3 03:08:47.590: INFO: Received response from host: affinity-clusterip-transition-hp7k8
    Jan  3 03:08:47.590: INFO: Received response from host: affinity-clusterip-transition-zjsk7
    Jan  3 03:08:47.590: INFO: Received response from host: affinity-clusterip-transition-zjsk7
    Jan  3 03:08:47.590: INFO: Received response from host: affinity-clusterip-transition-hp7k8
    Jan  3 03:08:47.590: INFO: Received response from host: affinity-clusterip-transition-zjsk7
    Jan  3 03:08:47.590: INFO: Received response from host: affinity-clusterip-transition-hp7k8
    Jan  3 03:08:47.590: INFO: Received response from host: affinity-clusterip-transition-hp7k8
    Jan  3 03:08:47.590: INFO: Received response from host: affinity-clusterip-transition-z858r
    Jan  3 03:08:47.590: INFO: Received response from host: affinity-clusterip-transition-z858r
    Jan  3 03:08:47.590: INFO: Received response from host: affinity-clusterip-transition-zjsk7
    Jan  3 03:08:47.590: INFO: Received response from host: affinity-clusterip-transition-zjsk7
    Jan  3 03:08:47.590: INFO: Received response from host: affinity-clusterip-transition-zjsk7
    Jan  3 03:08:47.590: INFO: Received response from host: affinity-clusterip-transition-hp7k8
    Jan  3 03:08:47.590: INFO: Received response from host: affinity-clusterip-transition-zjsk7
    Jan  3 03:08:47.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-2128 exec execpod-affinitykpz8v -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.68.219:80/ ; done'
    Jan  3 03:08:47.775: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.68.219:80/\n"
    Jan  3 03:08:47.775: INFO: stdout: "\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r\naffinity-clusterip-transition-z858r"
    Jan  3 03:08:47.775: INFO: Received response from host: affinity-clusterip-transition-z858r
    Jan  3 03:08:47.775: INFO: Received response from host: affinity-clusterip-transition-z858r
    Jan  3 03:08:47.775: INFO: Received response from host: affinity-clusterip-transition-z858r
    Jan  3 03:08:47.775: INFO: Received response from host: affinity-clusterip-transition-z858r
    Jan  3 03:08:47.775: INFO: Received response from host: affinity-clusterip-transition-z858r
    Jan  3 03:08:47.775: INFO: Received response from host: affinity-clusterip-transition-z858r
    Jan  3 03:08:47.775: INFO: Received response from host: affinity-clusterip-transition-z858r
    Jan  3 03:08:47.775: INFO: Received response from host: affinity-clusterip-transition-z858r
    Jan  3 03:08:47.775: INFO: Received response from host: affinity-clusterip-transition-z858r
    Jan  3 03:08:47.775: INFO: Received response from host: affinity-clusterip-transition-z858r
    Jan  3 03:08:47.775: INFO: Received response from host: affinity-clusterip-transition-z858r
    Jan  3 03:08:47.775: INFO: Received response from host: affinity-clusterip-transition-z858r
    Jan  3 03:08:47.775: INFO: Received response from host: affinity-clusterip-transition-z858r
    Jan  3 03:08:47.775: INFO: Received response from host: affinity-clusterip-transition-z858r
    Jan  3 03:08:47.775: INFO: Received response from host: affinity-clusterip-transition-z858r
    Jan  3 03:08:47.775: INFO: Received response from host: affinity-clusterip-transition-z858r
    Jan  3 03:08:47.775: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-2128, will wait for the garbage collector to delete the pods 01/03/23 03:08:47.811
    Jan  3 03:08:47.870: INFO: Deleting ReplicationController affinity-clusterip-transition took: 2.735185ms
    Jan  3 03:08:47.970: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.302325ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  3 03:08:50.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2128" for this suite. 01/03/23 03:08:50.413
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:08:50.416
Jan  3 03:08:50.416: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename cronjob 01/03/23 03:08:50.417
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:08:50.431
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:08:50.433
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 01/03/23 03:08:50.435
STEP: Ensuring a job is scheduled 01/03/23 03:08:50.439
STEP: Ensuring exactly one is scheduled 01/03/23 03:09:00.442
STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/03/23 03:09:00.444
STEP: Ensuring the job is replaced with a new one 01/03/23 03:09:00.446
STEP: Removing cronjob 01/03/23 03:10:00.449
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan  3 03:10:00.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-7788" for this suite. 01/03/23 03:10:00.455
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","completed":298,"skipped":5677,"failed":0}
------------------------------
• [SLOW TEST] [70.089 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:08:50.416
    Jan  3 03:08:50.416: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename cronjob 01/03/23 03:08:50.417
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:08:50.431
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:08:50.433
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 01/03/23 03:08:50.435
    STEP: Ensuring a job is scheduled 01/03/23 03:08:50.439
    STEP: Ensuring exactly one is scheduled 01/03/23 03:09:00.442
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/03/23 03:09:00.444
    STEP: Ensuring the job is replaced with a new one 01/03/23 03:09:00.446
    STEP: Removing cronjob 01/03/23 03:10:00.449
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan  3 03:10:00.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-7788" for this suite. 01/03/23 03:10:00.455
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:10:00.507
Jan  3 03:10:00.507: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename kubectl 01/03/23 03:10:00.507
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:10:00.54
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:10:00.542
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
STEP: creating Agnhost RC 01/03/23 03:10:00.544
Jan  3 03:10:00.544: INFO: namespace kubectl-5196
Jan  3 03:10:00.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-5196 create -f -'
Jan  3 03:10:01.425: INFO: stderr: ""
Jan  3 03:10:01.425: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/03/23 03:10:01.425
Jan  3 03:10:02.427: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  3 03:10:02.427: INFO: Found 0 / 1
Jan  3 03:10:03.427: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  3 03:10:03.427: INFO: Found 1 / 1
Jan  3 03:10:03.427: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan  3 03:10:03.429: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  3 03:10:03.429: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan  3 03:10:03.429: INFO: wait on agnhost-primary startup in kubectl-5196 
Jan  3 03:10:03.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-5196 logs agnhost-primary-th6jp agnhost-primary'
Jan  3 03:10:03.495: INFO: stderr: ""
Jan  3 03:10:03.495: INFO: stdout: "Paused\n"
STEP: exposing RC 01/03/23 03:10:03.495
Jan  3 03:10:03.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-5196 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jan  3 03:10:03.588: INFO: stderr: ""
Jan  3 03:10:03.588: INFO: stdout: "service/rm2 exposed\n"
Jan  3 03:10:03.591: INFO: Service rm2 in namespace kubectl-5196 found.
STEP: exposing service 01/03/23 03:10:05.603
Jan  3 03:10:05.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-5196 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jan  3 03:10:05.676: INFO: stderr: ""
Jan  3 03:10:05.676: INFO: stdout: "service/rm3 exposed\n"
Jan  3 03:10:05.694: INFO: Service rm3 in namespace kubectl-5196 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan  3 03:10:07.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5196" for this suite. 01/03/23 03:10:07.724
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","completed":299,"skipped":5698,"failed":0}
------------------------------
• [SLOW TEST] [7.221 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1407
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:10:00.507
    Jan  3 03:10:00.507: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename kubectl 01/03/23 03:10:00.507
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:10:00.54
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:10:00.542
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1413
    STEP: creating Agnhost RC 01/03/23 03:10:00.544
    Jan  3 03:10:00.544: INFO: namespace kubectl-5196
    Jan  3 03:10:00.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-5196 create -f -'
    Jan  3 03:10:01.425: INFO: stderr: ""
    Jan  3 03:10:01.425: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/03/23 03:10:01.425
    Jan  3 03:10:02.427: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  3 03:10:02.427: INFO: Found 0 / 1
    Jan  3 03:10:03.427: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  3 03:10:03.427: INFO: Found 1 / 1
    Jan  3 03:10:03.427: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jan  3 03:10:03.429: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  3 03:10:03.429: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan  3 03:10:03.429: INFO: wait on agnhost-primary startup in kubectl-5196 
    Jan  3 03:10:03.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-5196 logs agnhost-primary-th6jp agnhost-primary'
    Jan  3 03:10:03.495: INFO: stderr: ""
    Jan  3 03:10:03.495: INFO: stdout: "Paused\n"
    STEP: exposing RC 01/03/23 03:10:03.495
    Jan  3 03:10:03.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-5196 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Jan  3 03:10:03.588: INFO: stderr: ""
    Jan  3 03:10:03.588: INFO: stdout: "service/rm2 exposed\n"
    Jan  3 03:10:03.591: INFO: Service rm2 in namespace kubectl-5196 found.
    STEP: exposing service 01/03/23 03:10:05.603
    Jan  3 03:10:05.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-5196 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Jan  3 03:10:05.676: INFO: stderr: ""
    Jan  3 03:10:05.676: INFO: stdout: "service/rm3 exposed\n"
    Jan  3 03:10:05.694: INFO: Service rm3 in namespace kubectl-5196 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan  3 03:10:07.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5196" for this suite. 01/03/23 03:10:07.724
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:10:07.727
Jan  3 03:10:07.728: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename projected 01/03/23 03:10:07.728
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:10:07.737
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:10:07.739
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
STEP: Creating configMap with name projected-configmap-test-volume-20461b5e-b0a1-433b-ad78-898e545908ca 01/03/23 03:10:07.741
STEP: Creating a pod to test consume configMaps 01/03/23 03:10:07.745
Jan  3 03:10:07.752: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2ea49630-ec46-47fb-9632-9e5b83ff2fbc" in namespace "projected-2659" to be "Succeeded or Failed"
Jan  3 03:10:07.757: INFO: Pod "pod-projected-configmaps-2ea49630-ec46-47fb-9632-9e5b83ff2fbc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.686605ms
Jan  3 03:10:09.761: INFO: Pod "pod-projected-configmaps-2ea49630-ec46-47fb-9632-9e5b83ff2fbc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00918693s
Jan  3 03:10:11.761: INFO: Pod "pod-projected-configmaps-2ea49630-ec46-47fb-9632-9e5b83ff2fbc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008839744s
STEP: Saw pod success 01/03/23 03:10:11.761
Jan  3 03:10:11.761: INFO: Pod "pod-projected-configmaps-2ea49630-ec46-47fb-9632-9e5b83ff2fbc" satisfied condition "Succeeded or Failed"
Jan  3 03:10:11.762: INFO: Trying to get logs from node cncf-master pod pod-projected-configmaps-2ea49630-ec46-47fb-9632-9e5b83ff2fbc container projected-configmap-volume-test: <nil>
STEP: delete the pod 01/03/23 03:10:11.765
Jan  3 03:10:11.780: INFO: Waiting for pod pod-projected-configmaps-2ea49630-ec46-47fb-9632-9e5b83ff2fbc to disappear
Jan  3 03:10:11.797: INFO: Pod pod-projected-configmaps-2ea49630-ec46-47fb-9632-9e5b83ff2fbc no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan  3 03:10:11.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2659" for this suite. 01/03/23 03:10:11.799
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":300,"skipped":5700,"failed":0}
------------------------------
• [4.074 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:10:07.727
    Jan  3 03:10:07.728: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename projected 01/03/23 03:10:07.728
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:10:07.737
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:10:07.739
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:374
    STEP: Creating configMap with name projected-configmap-test-volume-20461b5e-b0a1-433b-ad78-898e545908ca 01/03/23 03:10:07.741
    STEP: Creating a pod to test consume configMaps 01/03/23 03:10:07.745
    Jan  3 03:10:07.752: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2ea49630-ec46-47fb-9632-9e5b83ff2fbc" in namespace "projected-2659" to be "Succeeded or Failed"
    Jan  3 03:10:07.757: INFO: Pod "pod-projected-configmaps-2ea49630-ec46-47fb-9632-9e5b83ff2fbc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.686605ms
    Jan  3 03:10:09.761: INFO: Pod "pod-projected-configmaps-2ea49630-ec46-47fb-9632-9e5b83ff2fbc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00918693s
    Jan  3 03:10:11.761: INFO: Pod "pod-projected-configmaps-2ea49630-ec46-47fb-9632-9e5b83ff2fbc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008839744s
    STEP: Saw pod success 01/03/23 03:10:11.761
    Jan  3 03:10:11.761: INFO: Pod "pod-projected-configmaps-2ea49630-ec46-47fb-9632-9e5b83ff2fbc" satisfied condition "Succeeded or Failed"
    Jan  3 03:10:11.762: INFO: Trying to get logs from node cncf-master pod pod-projected-configmaps-2ea49630-ec46-47fb-9632-9e5b83ff2fbc container projected-configmap-volume-test: <nil>
    STEP: delete the pod 01/03/23 03:10:11.765
    Jan  3 03:10:11.780: INFO: Waiting for pod pod-projected-configmaps-2ea49630-ec46-47fb-9632-9e5b83ff2fbc to disappear
    Jan  3 03:10:11.797: INFO: Pod pod-projected-configmaps-2ea49630-ec46-47fb-9632-9e5b83ff2fbc no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan  3 03:10:11.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2659" for this suite. 01/03/23 03:10:11.799
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:10:11.802
Jan  3 03:10:11.802: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename services 01/03/23 03:10:11.803
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:10:11.815
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:10:11.817
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
STEP: creating service nodeport-test with type=NodePort in namespace services-6641 01/03/23 03:10:11.819
STEP: creating replication controller nodeport-test in namespace services-6641 01/03/23 03:10:11.884
I0103 03:10:11.889620      23 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-6641, replica count: 2
I0103 03:10:14.941352      23 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  3 03:10:14.941: INFO: Creating new exec pod
Jan  3 03:10:14.945: INFO: Waiting up to 5m0s for pod "execpod4sk7g" in namespace "services-6641" to be "running"
Jan  3 03:10:14.953: INFO: Pod "execpod4sk7g": Phase="Pending", Reason="", readiness=false. Elapsed: 7.471017ms
Jan  3 03:10:16.959: INFO: Pod "execpod4sk7g": Phase="Running", Reason="", readiness=true. Elapsed: 2.013533281s
Jan  3 03:10:16.959: INFO: Pod "execpod4sk7g" satisfied condition "running"
Jan  3 03:10:17.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-6641 exec execpod4sk7g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan  3 03:10:18.108: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan  3 03:10:18.108: INFO: stdout: "nodeport-test-d2298"
Jan  3 03:10:18.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-6641 exec execpod4sk7g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.30.21 80'
Jan  3 03:10:18.240: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.30.21 80\nConnection to 10.96.30.21 80 port [tcp/http] succeeded!\n"
Jan  3 03:10:18.240: INFO: stdout: ""
Jan  3 03:10:19.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-6641 exec execpod4sk7g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.30.21 80'
Jan  3 03:10:19.376: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.30.21 80\nConnection to 10.96.30.21 80 port [tcp/http] succeeded!\n"
Jan  3 03:10:19.376: INFO: stdout: "nodeport-test-hfq84"
Jan  3 03:10:19.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-6641 exec execpod4sk7g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.7.13 31304'
Jan  3 03:10:19.517: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.7.13 31304\nConnection to 172.21.7.13 31304 port [tcp/*] succeeded!\n"
Jan  3 03:10:19.517: INFO: stdout: "nodeport-test-hfq84"
Jan  3 03:10:19.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-6641 exec execpod4sk7g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.7.5 31304'
Jan  3 03:10:19.650: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.7.5 31304\nConnection to 172.21.7.5 31304 port [tcp/*] succeeded!\n"
Jan  3 03:10:19.650: INFO: stdout: "nodeport-test-d2298"
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  3 03:10:19.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6641" for this suite. 01/03/23 03:10:19.653
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","completed":301,"skipped":5702,"failed":0}
------------------------------
• [SLOW TEST] [7.854 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:10:11.802
    Jan  3 03:10:11.802: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename services 01/03/23 03:10:11.803
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:10:11.815
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:10:11.817
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1268
    STEP: creating service nodeport-test with type=NodePort in namespace services-6641 01/03/23 03:10:11.819
    STEP: creating replication controller nodeport-test in namespace services-6641 01/03/23 03:10:11.884
    I0103 03:10:11.889620      23 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-6641, replica count: 2
    I0103 03:10:14.941352      23 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  3 03:10:14.941: INFO: Creating new exec pod
    Jan  3 03:10:14.945: INFO: Waiting up to 5m0s for pod "execpod4sk7g" in namespace "services-6641" to be "running"
    Jan  3 03:10:14.953: INFO: Pod "execpod4sk7g": Phase="Pending", Reason="", readiness=false. Elapsed: 7.471017ms
    Jan  3 03:10:16.959: INFO: Pod "execpod4sk7g": Phase="Running", Reason="", readiness=true. Elapsed: 2.013533281s
    Jan  3 03:10:16.959: INFO: Pod "execpod4sk7g" satisfied condition "running"
    Jan  3 03:10:17.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-6641 exec execpod4sk7g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan  3 03:10:18.108: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan  3 03:10:18.108: INFO: stdout: "nodeport-test-d2298"
    Jan  3 03:10:18.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-6641 exec execpod4sk7g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.30.21 80'
    Jan  3 03:10:18.240: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.30.21 80\nConnection to 10.96.30.21 80 port [tcp/http] succeeded!\n"
    Jan  3 03:10:18.240: INFO: stdout: ""
    Jan  3 03:10:19.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-6641 exec execpod4sk7g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.30.21 80'
    Jan  3 03:10:19.376: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.30.21 80\nConnection to 10.96.30.21 80 port [tcp/http] succeeded!\n"
    Jan  3 03:10:19.376: INFO: stdout: "nodeport-test-hfq84"
    Jan  3 03:10:19.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-6641 exec execpod4sk7g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.7.13 31304'
    Jan  3 03:10:19.517: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.7.13 31304\nConnection to 172.21.7.13 31304 port [tcp/*] succeeded!\n"
    Jan  3 03:10:19.517: INFO: stdout: "nodeport-test-hfq84"
    Jan  3 03:10:19.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-6641 exec execpod4sk7g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.7.5 31304'
    Jan  3 03:10:19.650: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.7.5 31304\nConnection to 172.21.7.5 31304 port [tcp/*] succeeded!\n"
    Jan  3 03:10:19.650: INFO: stdout: "nodeport-test-d2298"
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  3 03:10:19.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6641" for this suite. 01/03/23 03:10:19.653
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:10:19.657
Jan  3 03:10:19.657: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename kubelet-test 01/03/23 03:10:19.657
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:10:19.678
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:10:19.679
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Jan  3 03:10:19.705: INFO: Waiting up to 5m0s for pod "busybox-scheduling-54589459-42d5-4cbc-807c-8f0b7461e58b" in namespace "kubelet-test-1242" to be "running and ready"
Jan  3 03:10:19.718: INFO: Pod "busybox-scheduling-54589459-42d5-4cbc-807c-8f0b7461e58b": Phase="Pending", Reason="", readiness=false. Elapsed: 13.280578ms
Jan  3 03:10:19.718: INFO: The phase of Pod busybox-scheduling-54589459-42d5-4cbc-807c-8f0b7461e58b is Pending, waiting for it to be Running (with Ready = true)
Jan  3 03:10:21.720: INFO: Pod "busybox-scheduling-54589459-42d5-4cbc-807c-8f0b7461e58b": Phase="Running", Reason="", readiness=true. Elapsed: 2.015881891s
Jan  3 03:10:21.720: INFO: The phase of Pod busybox-scheduling-54589459-42d5-4cbc-807c-8f0b7461e58b is Running (Ready = true)
Jan  3 03:10:21.720: INFO: Pod "busybox-scheduling-54589459-42d5-4cbc-807c-8f0b7461e58b" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan  3 03:10:21.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1242" for this suite. 01/03/23 03:10:21.728
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","completed":302,"skipped":5733,"failed":0}
------------------------------
• [2.074 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:10:19.657
    Jan  3 03:10:19.657: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename kubelet-test 01/03/23 03:10:19.657
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:10:19.678
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:10:19.679
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Jan  3 03:10:19.705: INFO: Waiting up to 5m0s for pod "busybox-scheduling-54589459-42d5-4cbc-807c-8f0b7461e58b" in namespace "kubelet-test-1242" to be "running and ready"
    Jan  3 03:10:19.718: INFO: Pod "busybox-scheduling-54589459-42d5-4cbc-807c-8f0b7461e58b": Phase="Pending", Reason="", readiness=false. Elapsed: 13.280578ms
    Jan  3 03:10:19.718: INFO: The phase of Pod busybox-scheduling-54589459-42d5-4cbc-807c-8f0b7461e58b is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 03:10:21.720: INFO: Pod "busybox-scheduling-54589459-42d5-4cbc-807c-8f0b7461e58b": Phase="Running", Reason="", readiness=true. Elapsed: 2.015881891s
    Jan  3 03:10:21.720: INFO: The phase of Pod busybox-scheduling-54589459-42d5-4cbc-807c-8f0b7461e58b is Running (Ready = true)
    Jan  3 03:10:21.720: INFO: Pod "busybox-scheduling-54589459-42d5-4cbc-807c-8f0b7461e58b" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan  3 03:10:21.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-1242" for this suite. 01/03/23 03:10:21.728
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:10:21.733
Jan  3 03:10:21.733: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename projected 01/03/23 03:10:21.734
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:10:21.742
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:10:21.744
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
STEP: Creating projection with secret that has name projected-secret-test-map-7c09700b-a707-425d-bb54-d352664eac28 01/03/23 03:10:21.746
STEP: Creating a pod to test consume secrets 01/03/23 03:10:21.777
Jan  3 03:10:21.785: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-81d35f1f-a7af-4730-93ec-5f5de0da9ee7" in namespace "projected-3654" to be "Succeeded or Failed"
Jan  3 03:10:21.800: INFO: Pod "pod-projected-secrets-81d35f1f-a7af-4730-93ec-5f5de0da9ee7": Phase="Pending", Reason="", readiness=false. Elapsed: 15.02953ms
Jan  3 03:10:23.803: INFO: Pod "pod-projected-secrets-81d35f1f-a7af-4730-93ec-5f5de0da9ee7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01838049s
Jan  3 03:10:25.803: INFO: Pod "pod-projected-secrets-81d35f1f-a7af-4730-93ec-5f5de0da9ee7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017855735s
Jan  3 03:10:27.804: INFO: Pod "pod-projected-secrets-81d35f1f-a7af-4730-93ec-5f5de0da9ee7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018803852s
STEP: Saw pod success 01/03/23 03:10:27.804
Jan  3 03:10:27.804: INFO: Pod "pod-projected-secrets-81d35f1f-a7af-4730-93ec-5f5de0da9ee7" satisfied condition "Succeeded or Failed"
Jan  3 03:10:27.806: INFO: Trying to get logs from node cncf-master pod pod-projected-secrets-81d35f1f-a7af-4730-93ec-5f5de0da9ee7 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/03/23 03:10:27.809
Jan  3 03:10:27.879: INFO: Waiting for pod pod-projected-secrets-81d35f1f-a7af-4730-93ec-5f5de0da9ee7 to disappear
Jan  3 03:10:27.886: INFO: Pod pod-projected-secrets-81d35f1f-a7af-4730-93ec-5f5de0da9ee7 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan  3 03:10:27.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3654" for this suite. 01/03/23 03:10:27.893
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":303,"skipped":5790,"failed":0}
------------------------------
• [SLOW TEST] [6.165 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:10:21.733
    Jan  3 03:10:21.733: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename projected 01/03/23 03:10:21.734
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:10:21.742
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:10:21.744
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:87
    STEP: Creating projection with secret that has name projected-secret-test-map-7c09700b-a707-425d-bb54-d352664eac28 01/03/23 03:10:21.746
    STEP: Creating a pod to test consume secrets 01/03/23 03:10:21.777
    Jan  3 03:10:21.785: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-81d35f1f-a7af-4730-93ec-5f5de0da9ee7" in namespace "projected-3654" to be "Succeeded or Failed"
    Jan  3 03:10:21.800: INFO: Pod "pod-projected-secrets-81d35f1f-a7af-4730-93ec-5f5de0da9ee7": Phase="Pending", Reason="", readiness=false. Elapsed: 15.02953ms
    Jan  3 03:10:23.803: INFO: Pod "pod-projected-secrets-81d35f1f-a7af-4730-93ec-5f5de0da9ee7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01838049s
    Jan  3 03:10:25.803: INFO: Pod "pod-projected-secrets-81d35f1f-a7af-4730-93ec-5f5de0da9ee7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017855735s
    Jan  3 03:10:27.804: INFO: Pod "pod-projected-secrets-81d35f1f-a7af-4730-93ec-5f5de0da9ee7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018803852s
    STEP: Saw pod success 01/03/23 03:10:27.804
    Jan  3 03:10:27.804: INFO: Pod "pod-projected-secrets-81d35f1f-a7af-4730-93ec-5f5de0da9ee7" satisfied condition "Succeeded or Failed"
    Jan  3 03:10:27.806: INFO: Trying to get logs from node cncf-master pod pod-projected-secrets-81d35f1f-a7af-4730-93ec-5f5de0da9ee7 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/03/23 03:10:27.809
    Jan  3 03:10:27.879: INFO: Waiting for pod pod-projected-secrets-81d35f1f-a7af-4730-93ec-5f5de0da9ee7 to disappear
    Jan  3 03:10:27.886: INFO: Pod pod-projected-secrets-81d35f1f-a7af-4730-93ec-5f5de0da9ee7 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan  3 03:10:27.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3654" for this suite. 01/03/23 03:10:27.893
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:10:27.898
Jan  3 03:10:27.898: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename dns 01/03/23 03:10:27.899
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:10:27.926
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:10:27.928
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 01/03/23 03:10:27.93
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-259.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-259.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 01/03/23 03:10:27.943
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-259.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-259.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 01/03/23 03:10:27.943
STEP: creating a pod to probe DNS 01/03/23 03:10:27.943
STEP: submitting the pod to kubernetes 01/03/23 03:10:27.943
Jan  3 03:10:27.955: INFO: Waiting up to 15m0s for pod "dns-test-7d20ebd2-8b14-4098-a372-d663f4a4792c" in namespace "dns-259" to be "running"
Jan  3 03:10:27.971: INFO: Pod "dns-test-7d20ebd2-8b14-4098-a372-d663f4a4792c": Phase="Pending", Reason="", readiness=false. Elapsed: 16.457286ms
Jan  3 03:10:29.974: INFO: Pod "dns-test-7d20ebd2-8b14-4098-a372-d663f4a4792c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019583066s
Jan  3 03:10:31.974: INFO: Pod "dns-test-7d20ebd2-8b14-4098-a372-d663f4a4792c": Phase="Running", Reason="", readiness=true. Elapsed: 4.019659372s
Jan  3 03:10:31.975: INFO: Pod "dns-test-7d20ebd2-8b14-4098-a372-d663f4a4792c" satisfied condition "running"
STEP: retrieving the pod 01/03/23 03:10:31.975
STEP: looking for the results for each expected name from probers 01/03/23 03:10:31.976
Jan  3 03:10:31.984: INFO: DNS probes using dns-259/dns-test-7d20ebd2-8b14-4098-a372-d663f4a4792c succeeded

STEP: deleting the pod 01/03/23 03:10:31.984
STEP: deleting the test headless service 01/03/23 03:10:32.001
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan  3 03:10:32.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-259" for this suite. 01/03/23 03:10:32.055
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [Conformance]","completed":304,"skipped":5797,"failed":0}
------------------------------
• [4.182 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:10:27.898
    Jan  3 03:10:27.898: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename dns 01/03/23 03:10:27.899
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:10:27.926
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:10:27.928
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 01/03/23 03:10:27.93
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-259.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-259.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     01/03/23 03:10:27.943
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-259.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-259.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     01/03/23 03:10:27.943
    STEP: creating a pod to probe DNS 01/03/23 03:10:27.943
    STEP: submitting the pod to kubernetes 01/03/23 03:10:27.943
    Jan  3 03:10:27.955: INFO: Waiting up to 15m0s for pod "dns-test-7d20ebd2-8b14-4098-a372-d663f4a4792c" in namespace "dns-259" to be "running"
    Jan  3 03:10:27.971: INFO: Pod "dns-test-7d20ebd2-8b14-4098-a372-d663f4a4792c": Phase="Pending", Reason="", readiness=false. Elapsed: 16.457286ms
    Jan  3 03:10:29.974: INFO: Pod "dns-test-7d20ebd2-8b14-4098-a372-d663f4a4792c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019583066s
    Jan  3 03:10:31.974: INFO: Pod "dns-test-7d20ebd2-8b14-4098-a372-d663f4a4792c": Phase="Running", Reason="", readiness=true. Elapsed: 4.019659372s
    Jan  3 03:10:31.975: INFO: Pod "dns-test-7d20ebd2-8b14-4098-a372-d663f4a4792c" satisfied condition "running"
    STEP: retrieving the pod 01/03/23 03:10:31.975
    STEP: looking for the results for each expected name from probers 01/03/23 03:10:31.976
    Jan  3 03:10:31.984: INFO: DNS probes using dns-259/dns-test-7d20ebd2-8b14-4098-a372-d663f4a4792c succeeded

    STEP: deleting the pod 01/03/23 03:10:31.984
    STEP: deleting the test headless service 01/03/23 03:10:32.001
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan  3 03:10:32.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-259" for this suite. 01/03/23 03:10:32.055
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:10:32.081
Jan  3 03:10:32.081: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename security-context-test 01/03/23 03:10:32.082
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:10:32.098
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:10:32.1
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
Jan  3 03:10:32.116: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-185d316e-4f61-4eba-8f26-eba8948bb379" in namespace "security-context-test-7900" to be "Succeeded or Failed"
Jan  3 03:10:32.136: INFO: Pod "busybox-readonly-false-185d316e-4f61-4eba-8f26-eba8948bb379": Phase="Pending", Reason="", readiness=false. Elapsed: 19.835806ms
Jan  3 03:10:34.139: INFO: Pod "busybox-readonly-false-185d316e-4f61-4eba-8f26-eba8948bb379": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022533422s
Jan  3 03:10:36.139: INFO: Pod "busybox-readonly-false-185d316e-4f61-4eba-8f26-eba8948bb379": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022664032s
Jan  3 03:10:36.139: INFO: Pod "busybox-readonly-false-185d316e-4f61-4eba-8f26-eba8948bb379" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan  3 03:10:36.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7900" for this suite. 01/03/23 03:10:36.142
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","completed":305,"skipped":5831,"failed":0}
------------------------------
• [4.064 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:429
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:485

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:10:32.081
    Jan  3 03:10:32.081: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename security-context-test 01/03/23 03:10:32.082
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:10:32.098
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:10:32.1
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:485
    Jan  3 03:10:32.116: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-185d316e-4f61-4eba-8f26-eba8948bb379" in namespace "security-context-test-7900" to be "Succeeded or Failed"
    Jan  3 03:10:32.136: INFO: Pod "busybox-readonly-false-185d316e-4f61-4eba-8f26-eba8948bb379": Phase="Pending", Reason="", readiness=false. Elapsed: 19.835806ms
    Jan  3 03:10:34.139: INFO: Pod "busybox-readonly-false-185d316e-4f61-4eba-8f26-eba8948bb379": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022533422s
    Jan  3 03:10:36.139: INFO: Pod "busybox-readonly-false-185d316e-4f61-4eba-8f26-eba8948bb379": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022664032s
    Jan  3 03:10:36.139: INFO: Pod "busybox-readonly-false-185d316e-4f61-4eba-8f26-eba8948bb379" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan  3 03:10:36.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-7900" for this suite. 01/03/23 03:10:36.142
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:10:36.145
Jan  3 03:10:36.145: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename ephemeral-containers-test 01/03/23 03:10:36.147
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:10:36.166
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:10:36.168
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 01/03/23 03:10:36.17
Jan  3 03:10:36.199: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-1829" to be "running and ready"
Jan  3 03:10:36.212: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.739637ms
Jan  3 03:10:36.212: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Jan  3 03:10:38.215: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015547578s
Jan  3 03:10:38.215: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Jan  3 03:10:38.215: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 01/03/23 03:10:38.216
Jan  3 03:10:38.225: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-1829" to be "container debugger running"
Jan  3 03:10:38.248: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 23.088797ms
Jan  3 03:10:40.251: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.026063573s
Jan  3 03:10:40.251: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 01/03/23 03:10:40.251
Jan  3 03:10:40.251: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-1829 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 03:10:40.251: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
Jan  3 03:10:40.252: INFO: ExecWithOptions: Clientset creation
Jan  3 03:10:40.252: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-1829/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Jan  3 03:10:40.320: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:187
Jan  3 03:10:40.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ephemeral-containers-test-1829" for this suite. 01/03/23 03:10:40.326
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]","completed":306,"skipped":5839,"failed":0}
------------------------------
• [4.183 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:10:36.145
    Jan  3 03:10:36.145: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename ephemeral-containers-test 01/03/23 03:10:36.147
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:10:36.166
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:10:36.168
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 01/03/23 03:10:36.17
    Jan  3 03:10:36.199: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-1829" to be "running and ready"
    Jan  3 03:10:36.212: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.739637ms
    Jan  3 03:10:36.212: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 03:10:38.215: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015547578s
    Jan  3 03:10:38.215: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Jan  3 03:10:38.215: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 01/03/23 03:10:38.216
    Jan  3 03:10:38.225: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-1829" to be "container debugger running"
    Jan  3 03:10:38.248: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 23.088797ms
    Jan  3 03:10:40.251: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.026063573s
    Jan  3 03:10:40.251: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 01/03/23 03:10:40.251
    Jan  3 03:10:40.251: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-1829 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 03:10:40.251: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    Jan  3 03:10:40.252: INFO: ExecWithOptions: Clientset creation
    Jan  3 03:10:40.252: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-1829/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Jan  3 03:10:40.320: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan  3 03:10:40.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ephemeral-containers-test-1829" for this suite. 01/03/23 03:10:40.326
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:10:40.329
Jan  3 03:10:40.329: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename resourcequota 01/03/23 03:10:40.33
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:10:40.339
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:10:40.341
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
STEP: Counting existing ResourceQuota 01/03/23 03:10:40.343
STEP: Creating a ResourceQuota 01/03/23 03:10:45.345
STEP: Ensuring resource quota status is calculated 01/03/23 03:10:45.349
STEP: Creating a ReplicationController 01/03/23 03:10:47.352
STEP: Ensuring resource quota status captures replication controller creation 01/03/23 03:10:47.382
STEP: Deleting a ReplicationController 01/03/23 03:10:49.385
STEP: Ensuring resource quota status released usage 01/03/23 03:10:49.388
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan  3 03:10:51.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9068" for this suite. 01/03/23 03:10:51.393
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","completed":307,"skipped":5852,"failed":0}
------------------------------
• [SLOW TEST] [11.067 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:10:40.329
    Jan  3 03:10:40.329: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename resourcequota 01/03/23 03:10:40.33
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:10:40.339
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:10:40.341
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:382
    STEP: Counting existing ResourceQuota 01/03/23 03:10:40.343
    STEP: Creating a ResourceQuota 01/03/23 03:10:45.345
    STEP: Ensuring resource quota status is calculated 01/03/23 03:10:45.349
    STEP: Creating a ReplicationController 01/03/23 03:10:47.352
    STEP: Ensuring resource quota status captures replication controller creation 01/03/23 03:10:47.382
    STEP: Deleting a ReplicationController 01/03/23 03:10:49.385
    STEP: Ensuring resource quota status released usage 01/03/23 03:10:49.388
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan  3 03:10:51.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-9068" for this suite. 01/03/23 03:10:51.393
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:10:51.396
Jan  3 03:10:51.397: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename downward-api 01/03/23 03:10:51.397
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:10:51.408
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:10:51.41
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
STEP: Creating a pod to test downward api env vars 01/03/23 03:10:51.413
Jan  3 03:10:51.420: INFO: Waiting up to 5m0s for pod "downward-api-2473f200-d4cf-41c9-917c-a37f4c7256cc" in namespace "downward-api-2334" to be "Succeeded or Failed"
Jan  3 03:10:51.429: INFO: Pod "downward-api-2473f200-d4cf-41c9-917c-a37f4c7256cc": Phase="Pending", Reason="", readiness=false. Elapsed: 8.134938ms
Jan  3 03:10:53.432: INFO: Pod "downward-api-2473f200-d4cf-41c9-917c-a37f4c7256cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011281978s
Jan  3 03:10:55.431: INFO: Pod "downward-api-2473f200-d4cf-41c9-917c-a37f4c7256cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010961828s
STEP: Saw pod success 01/03/23 03:10:55.431
Jan  3 03:10:55.432: INFO: Pod "downward-api-2473f200-d4cf-41c9-917c-a37f4c7256cc" satisfied condition "Succeeded or Failed"
Jan  3 03:10:55.433: INFO: Trying to get logs from node cncf-master pod downward-api-2473f200-d4cf-41c9-917c-a37f4c7256cc container dapi-container: <nil>
STEP: delete the pod 01/03/23 03:10:55.437
Jan  3 03:10:55.460: INFO: Waiting for pod downward-api-2473f200-d4cf-41c9-917c-a37f4c7256cc to disappear
Jan  3 03:10:55.466: INFO: Pod downward-api-2473f200-d4cf-41c9-917c-a37f4c7256cc no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan  3 03:10:55.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2334" for this suite. 01/03/23 03:10:55.468
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","completed":308,"skipped":5868,"failed":0}
------------------------------
• [4.086 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:10:51.396
    Jan  3 03:10:51.397: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename downward-api 01/03/23 03:10:51.397
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:10:51.408
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:10:51.41
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:89
    STEP: Creating a pod to test downward api env vars 01/03/23 03:10:51.413
    Jan  3 03:10:51.420: INFO: Waiting up to 5m0s for pod "downward-api-2473f200-d4cf-41c9-917c-a37f4c7256cc" in namespace "downward-api-2334" to be "Succeeded or Failed"
    Jan  3 03:10:51.429: INFO: Pod "downward-api-2473f200-d4cf-41c9-917c-a37f4c7256cc": Phase="Pending", Reason="", readiness=false. Elapsed: 8.134938ms
    Jan  3 03:10:53.432: INFO: Pod "downward-api-2473f200-d4cf-41c9-917c-a37f4c7256cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011281978s
    Jan  3 03:10:55.431: INFO: Pod "downward-api-2473f200-d4cf-41c9-917c-a37f4c7256cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010961828s
    STEP: Saw pod success 01/03/23 03:10:55.431
    Jan  3 03:10:55.432: INFO: Pod "downward-api-2473f200-d4cf-41c9-917c-a37f4c7256cc" satisfied condition "Succeeded or Failed"
    Jan  3 03:10:55.433: INFO: Trying to get logs from node cncf-master pod downward-api-2473f200-d4cf-41c9-917c-a37f4c7256cc container dapi-container: <nil>
    STEP: delete the pod 01/03/23 03:10:55.437
    Jan  3 03:10:55.460: INFO: Waiting for pod downward-api-2473f200-d4cf-41c9-917c-a37f4c7256cc to disappear
    Jan  3 03:10:55.466: INFO: Pod downward-api-2473f200-d4cf-41c9-917c-a37f4c7256cc no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan  3 03:10:55.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2334" for this suite. 01/03/23 03:10:55.468
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:10:55.483
Jan  3 03:10:55.483: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename containers 01/03/23 03:10:55.484
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:10:55.506
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:10:55.508
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
STEP: Creating a pod to test override arguments 01/03/23 03:10:55.51
Jan  3 03:10:55.520: INFO: Waiting up to 5m0s for pod "client-containers-3e8fb0c8-1f20-43b9-842d-13bc2492bec1" in namespace "containers-5872" to be "Succeeded or Failed"
Jan  3 03:10:55.535: INFO: Pod "client-containers-3e8fb0c8-1f20-43b9-842d-13bc2492bec1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.388504ms
Jan  3 03:10:57.537: INFO: Pod "client-containers-3e8fb0c8-1f20-43b9-842d-13bc2492bec1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016768826s
Jan  3 03:10:59.537: INFO: Pod "client-containers-3e8fb0c8-1f20-43b9-842d-13bc2492bec1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017076759s
STEP: Saw pod success 01/03/23 03:10:59.537
Jan  3 03:10:59.537: INFO: Pod "client-containers-3e8fb0c8-1f20-43b9-842d-13bc2492bec1" satisfied condition "Succeeded or Failed"
Jan  3 03:10:59.539: INFO: Trying to get logs from node cncf-master pod client-containers-3e8fb0c8-1f20-43b9-842d-13bc2492bec1 container agnhost-container: <nil>
STEP: delete the pod 01/03/23 03:10:59.542
Jan  3 03:10:59.558: INFO: Waiting for pod client-containers-3e8fb0c8-1f20-43b9-842d-13bc2492bec1 to disappear
Jan  3 03:10:59.563: INFO: Pod client-containers-3e8fb0c8-1f20-43b9-842d-13bc2492bec1 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jan  3 03:10:59.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5872" for this suite. 01/03/23 03:10:59.566
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]","completed":309,"skipped":5883,"failed":0}
------------------------------
• [4.085 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:10:55.483
    Jan  3 03:10:55.483: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename containers 01/03/23 03:10:55.484
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:10:55.506
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:10:55.508
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:58
    STEP: Creating a pod to test override arguments 01/03/23 03:10:55.51
    Jan  3 03:10:55.520: INFO: Waiting up to 5m0s for pod "client-containers-3e8fb0c8-1f20-43b9-842d-13bc2492bec1" in namespace "containers-5872" to be "Succeeded or Failed"
    Jan  3 03:10:55.535: INFO: Pod "client-containers-3e8fb0c8-1f20-43b9-842d-13bc2492bec1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.388504ms
    Jan  3 03:10:57.537: INFO: Pod "client-containers-3e8fb0c8-1f20-43b9-842d-13bc2492bec1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016768826s
    Jan  3 03:10:59.537: INFO: Pod "client-containers-3e8fb0c8-1f20-43b9-842d-13bc2492bec1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017076759s
    STEP: Saw pod success 01/03/23 03:10:59.537
    Jan  3 03:10:59.537: INFO: Pod "client-containers-3e8fb0c8-1f20-43b9-842d-13bc2492bec1" satisfied condition "Succeeded or Failed"
    Jan  3 03:10:59.539: INFO: Trying to get logs from node cncf-master pod client-containers-3e8fb0c8-1f20-43b9-842d-13bc2492bec1 container agnhost-container: <nil>
    STEP: delete the pod 01/03/23 03:10:59.542
    Jan  3 03:10:59.558: INFO: Waiting for pod client-containers-3e8fb0c8-1f20-43b9-842d-13bc2492bec1 to disappear
    Jan  3 03:10:59.563: INFO: Pod client-containers-3e8fb0c8-1f20-43b9-842d-13bc2492bec1 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jan  3 03:10:59.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-5872" for this suite. 01/03/23 03:10:59.566
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:10:59.569
Jan  3 03:10:59.569: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename deployment 01/03/23 03:10:59.57
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:10:59.581
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:10:59.583
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Jan  3 03:10:59.620: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jan  3 03:11:04.624: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/03/23 03:11:04.624
Jan  3 03:11:04.625: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 01/03/23 03:11:04.633
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan  3 03:11:06.668: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-1064  7cd88d25-0fc1-4588-b0f5-eebc8abb0478 1015811 1 2023-01-03 03:11:04 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[createdTime:2023-01-03T12:11:04.628975138+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T12:11:04.628975138+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update apps/v1 2023-01-03 03:11:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-03 03:11:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007354a48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-03 03:11:04 +0000 UTC,LastTransitionTime:2023-01-03 03:11:04 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-69cb9c5497" has successfully progressed.,LastUpdateTime:2023-01-03 03:11:06 +0000 UTC,LastTransitionTime:2023-01-03 03:11:04 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan  3 03:11:06.669: INFO: New ReplicaSet "test-cleanup-deployment-69cb9c5497" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-69cb9c5497  deployment-1064  4ba6cbaf-9058-4a44-85fe-c5e685179d0f 1015801 1 2023-01-03 03:11:04 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[createdTime:2023-01-03T12:11:04.628975138+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T12:11:04.628975138+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [{apps/v1 Deployment test-cleanup-deployment 7cd88d25-0fc1-4588-b0f5-eebc8abb0478 0xc007354ea7 0xc007354ea8}] [] [{kube-controller-manager Update apps/v1 2023-01-03 03:11:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:createdTime":{},"f:creator":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{},"f:updatedTime":{},"f:updater":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7cd88d25-0fc1-4588-b0f5-eebc8abb0478\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-03 03:11:06 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 69cb9c5497,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007354f58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan  3 03:11:06.671: INFO: Pod "test-cleanup-deployment-69cb9c5497-m8g6z" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-69cb9c5497-m8g6z test-cleanup-deployment-69cb9c5497- deployment-1064  c438ae81-5180-4417-b79e-f7ba0291ca48 1015800 0 2023-01-03 03:11:04 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[cni.projectcalico.org/containerID:53f9c0ad69ab26105bbf32a829ae6a0d999cb41f6070f2f4392123f4f2e28a72 cni.projectcalico.org/podIP:10.244.246.113/32 cni.projectcalico.org/podIPs:10.244.246.113/32 createdTime:2023-01-03T12:11:04.663011121+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T12:11:04.663011121+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet test-cleanup-deployment-69cb9c5497 4ba6cbaf-9058-4a44-85fe-c5e685179d0f 0xc007355317 0xc007355318}] [] [{kube-controller-manager Update v1 2023-01-03 03:11:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ba6cbaf-9058-4a44-85fe-c5e685179d0f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-03 03:11:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-03 03:11:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.246.113\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8hn8v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8hn8v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-master,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 03:11:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 03:11:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 03:11:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 03:11:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.5,PodIP:10.244.246.113,StartTime:2023-01-03 03:11:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-03 03:11:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787,ContainerID:cri-o://15ea8245d846a7682dd436410456e054971187eb8b2e956ca2499e51517f2154,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.246.113,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan  3 03:11:06.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1064" for this suite. 01/03/23 03:11:06.674
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","completed":310,"skipped":5894,"failed":0}
------------------------------
• [SLOW TEST] [7.107 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:10:59.569
    Jan  3 03:10:59.569: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename deployment 01/03/23 03:10:59.57
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:10:59.581
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:10:59.583
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Jan  3 03:10:59.620: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Jan  3 03:11:04.624: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/03/23 03:11:04.624
    Jan  3 03:11:04.625: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 01/03/23 03:11:04.633
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan  3 03:11:06.668: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-1064  7cd88d25-0fc1-4588-b0f5-eebc8abb0478 1015811 1 2023-01-03 03:11:04 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[createdTime:2023-01-03T12:11:04.628975138+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T12:11:04.628975138+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update apps/v1 2023-01-03 03:11:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-03 03:11:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007354a48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-03 03:11:04 +0000 UTC,LastTransitionTime:2023-01-03 03:11:04 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-69cb9c5497" has successfully progressed.,LastUpdateTime:2023-01-03 03:11:06 +0000 UTC,LastTransitionTime:2023-01-03 03:11:04 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan  3 03:11:06.669: INFO: New ReplicaSet "test-cleanup-deployment-69cb9c5497" of Deployment "test-cleanup-deployment":
    &ReplicaSet{ObjectMeta:{test-cleanup-deployment-69cb9c5497  deployment-1064  4ba6cbaf-9058-4a44-85fe-c5e685179d0f 1015801 1 2023-01-03 03:11:04 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[createdTime:2023-01-03T12:11:04.628975138+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1 updatedTime:2023-01-03T12:11:04.628975138+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [{apps/v1 Deployment test-cleanup-deployment 7cd88d25-0fc1-4588-b0f5-eebc8abb0478 0xc007354ea7 0xc007354ea8}] [] [{kube-controller-manager Update apps/v1 2023-01-03 03:11:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:createdTime":{},"f:creator":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{},"f:updatedTime":{},"f:updater":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7cd88d25-0fc1-4588-b0f5-eebc8abb0478\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-03 03:11:06 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 69cb9c5497,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007354f58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan  3 03:11:06.671: INFO: Pod "test-cleanup-deployment-69cb9c5497-m8g6z" is available:
    &Pod{ObjectMeta:{test-cleanup-deployment-69cb9c5497-m8g6z test-cleanup-deployment-69cb9c5497- deployment-1064  c438ae81-5180-4417-b79e-f7ba0291ca48 1015800 0 2023-01-03 03:11:04 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[cni.projectcalico.org/containerID:53f9c0ad69ab26105bbf32a829ae6a0d999cb41f6070f2f4392123f4f2e28a72 cni.projectcalico.org/podIP:10.244.246.113/32 cni.projectcalico.org/podIPs:10.244.246.113/32 createdTime:2023-01-03T12:11:04.663011121+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2023-01-03T12:11:04.663011121+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet test-cleanup-deployment-69cb9c5497 4ba6cbaf-9058-4a44-85fe-c5e685179d0f 0xc007355317 0xc007355318}] [] [{kube-controller-manager Update v1 2023-01-03 03:11:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ba6cbaf-9058-4a44-85fe-c5e685179d0f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-03 03:11:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-03 03:11:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.246.113\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8hn8v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8hn8v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-master,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 03:11:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 03:11:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 03:11:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-03 03:11:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.5,PodIP:10.244.246.113,StartTime:2023-01-03 03:11:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-03 03:11:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787,ContainerID:cri-o://15ea8245d846a7682dd436410456e054971187eb8b2e956ca2499e51517f2154,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.246.113,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan  3 03:11:06.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-1064" for this suite. 01/03/23 03:11:06.674
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:11:06.677
Jan  3 03:11:06.677: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename namespaces 01/03/23 03:11:06.679
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:11:06.696
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:11:06.698
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
STEP: Creating a test namespace 01/03/23 03:11:06.7
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:11:06.719
STEP: Creating a pod in the namespace 01/03/23 03:11:06.721
STEP: Waiting for the pod to have running status 01/03/23 03:11:06.727
Jan  3 03:11:06.727: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-3139" to be "running"
Jan  3 03:11:06.743: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 15.200971ms
Jan  3 03:11:08.745: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.01777016s
Jan  3 03:11:08.745: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 01/03/23 03:11:08.745
STEP: Waiting for the namespace to be removed. 01/03/23 03:11:08.748
STEP: Recreating the namespace 01/03/23 03:11:19.75
STEP: Verifying there are no pods in the namespace 01/03/23 03:11:19.76
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jan  3 03:11:19.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-311" for this suite. 01/03/23 03:11:19.804
STEP: Destroying namespace "nsdeletetest-3139" for this suite. 01/03/23 03:11:19.806
Jan  3 03:11:19.811: INFO: Namespace nsdeletetest-3139 was already deleted
STEP: Destroying namespace "nsdeletetest-4202" for this suite. 01/03/23 03:11:19.811
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","completed":311,"skipped":5900,"failed":0}
------------------------------
• [SLOW TEST] [13.136 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:11:06.677
    Jan  3 03:11:06.677: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename namespaces 01/03/23 03:11:06.679
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:11:06.696
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:11:06.698
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:242
    STEP: Creating a test namespace 01/03/23 03:11:06.7
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:11:06.719
    STEP: Creating a pod in the namespace 01/03/23 03:11:06.721
    STEP: Waiting for the pod to have running status 01/03/23 03:11:06.727
    Jan  3 03:11:06.727: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-3139" to be "running"
    Jan  3 03:11:06.743: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 15.200971ms
    Jan  3 03:11:08.745: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.01777016s
    Jan  3 03:11:08.745: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 01/03/23 03:11:08.745
    STEP: Waiting for the namespace to be removed. 01/03/23 03:11:08.748
    STEP: Recreating the namespace 01/03/23 03:11:19.75
    STEP: Verifying there are no pods in the namespace 01/03/23 03:11:19.76
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jan  3 03:11:19.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-311" for this suite. 01/03/23 03:11:19.804
    STEP: Destroying namespace "nsdeletetest-3139" for this suite. 01/03/23 03:11:19.806
    Jan  3 03:11:19.811: INFO: Namespace nsdeletetest-3139 was already deleted
    STEP: Destroying namespace "nsdeletetest-4202" for this suite. 01/03/23 03:11:19.811
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:11:19.814
Jan  3 03:11:19.814: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename secrets 01/03/23 03:11:19.815
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:11:19.834
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:11:19.836
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
STEP: Creating secret with name secret-test-map-8ea0d7b1-a07a-47fa-919e-60f87c118481 01/03/23 03:11:19.838
STEP: Creating a pod to test consume secrets 01/03/23 03:11:19.852
Jan  3 03:11:19.860: INFO: Waiting up to 5m0s for pod "pod-secrets-330e42a3-c115-4eaa-afd5-2a635abcf626" in namespace "secrets-4929" to be "Succeeded or Failed"
Jan  3 03:11:19.863: INFO: Pod "pod-secrets-330e42a3-c115-4eaa-afd5-2a635abcf626": Phase="Pending", Reason="", readiness=false. Elapsed: 2.814476ms
Jan  3 03:11:21.866: INFO: Pod "pod-secrets-330e42a3-c115-4eaa-afd5-2a635abcf626": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00579145s
Jan  3 03:11:23.867: INFO: Pod "pod-secrets-330e42a3-c115-4eaa-afd5-2a635abcf626": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006741402s
STEP: Saw pod success 01/03/23 03:11:23.867
Jan  3 03:11:23.867: INFO: Pod "pod-secrets-330e42a3-c115-4eaa-afd5-2a635abcf626" satisfied condition "Succeeded or Failed"
Jan  3 03:11:23.868: INFO: Trying to get logs from node cncf-master pod pod-secrets-330e42a3-c115-4eaa-afd5-2a635abcf626 container secret-volume-test: <nil>
STEP: delete the pod 01/03/23 03:11:23.872
Jan  3 03:11:23.892: INFO: Waiting for pod pod-secrets-330e42a3-c115-4eaa-afd5-2a635abcf626 to disappear
Jan  3 03:11:23.897: INFO: Pod pod-secrets-330e42a3-c115-4eaa-afd5-2a635abcf626 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan  3 03:11:23.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4929" for this suite. 01/03/23 03:11:23.9
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":312,"skipped":5900,"failed":0}
------------------------------
• [4.089 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:11:19.814
    Jan  3 03:11:19.814: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename secrets 01/03/23 03:11:19.815
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:11:19.834
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:11:19.836
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:78
    STEP: Creating secret with name secret-test-map-8ea0d7b1-a07a-47fa-919e-60f87c118481 01/03/23 03:11:19.838
    STEP: Creating a pod to test consume secrets 01/03/23 03:11:19.852
    Jan  3 03:11:19.860: INFO: Waiting up to 5m0s for pod "pod-secrets-330e42a3-c115-4eaa-afd5-2a635abcf626" in namespace "secrets-4929" to be "Succeeded or Failed"
    Jan  3 03:11:19.863: INFO: Pod "pod-secrets-330e42a3-c115-4eaa-afd5-2a635abcf626": Phase="Pending", Reason="", readiness=false. Elapsed: 2.814476ms
    Jan  3 03:11:21.866: INFO: Pod "pod-secrets-330e42a3-c115-4eaa-afd5-2a635abcf626": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00579145s
    Jan  3 03:11:23.867: INFO: Pod "pod-secrets-330e42a3-c115-4eaa-afd5-2a635abcf626": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006741402s
    STEP: Saw pod success 01/03/23 03:11:23.867
    Jan  3 03:11:23.867: INFO: Pod "pod-secrets-330e42a3-c115-4eaa-afd5-2a635abcf626" satisfied condition "Succeeded or Failed"
    Jan  3 03:11:23.868: INFO: Trying to get logs from node cncf-master pod pod-secrets-330e42a3-c115-4eaa-afd5-2a635abcf626 container secret-volume-test: <nil>
    STEP: delete the pod 01/03/23 03:11:23.872
    Jan  3 03:11:23.892: INFO: Waiting for pod pod-secrets-330e42a3-c115-4eaa-afd5-2a635abcf626 to disappear
    Jan  3 03:11:23.897: INFO: Pod pod-secrets-330e42a3-c115-4eaa-afd5-2a635abcf626 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan  3 03:11:23.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-4929" for this suite. 01/03/23 03:11:23.9
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:11:23.903
Jan  3 03:11:23.904: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename kubectl 01/03/23 03:11:23.905
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:11:23.92
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:11:23.922
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1732
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/03/23 03:11:23.924
Jan  3 03:11:23.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-4510 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan  3 03:11:23.997: INFO: stderr: ""
Jan  3 03:11:23.997: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 01/03/23 03:11:23.997
STEP: verifying the pod e2e-test-httpd-pod was created 01/03/23 03:11:29.048
Jan  3 03:11:29.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-4510 get pod e2e-test-httpd-pod -o json'
Jan  3 03:11:29.105: INFO: stderr: ""
Jan  3 03:11:29.105: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"4750a1b67ed4a3f9b21980183f066cc7ca45c81661879161faf45c10183cc91e\",\n            \"cni.projectcalico.org/podIP\": \"10.244.246.121/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.244.246.121/32\",\n            \"createdTime\": \"2023-01-03T12:11:23.990637932+09:00\",\n            \"creator\": \"system:serviceaccount:sonobuoy:sonobuoy-serviceaccount\",\n            \"updatedTime\": \"2023-01-03T12:11:23.990637932+09:00\",\n            \"updater\": \"system:serviceaccount:sonobuoy:sonobuoy-serviceaccount\"\n        },\n        \"creationTimestamp\": \"2023-01-03T03:11:23Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4510\",\n        \"resourceVersion\": \"1016009\",\n        \"uid\": \"b7f22df5-3a85-42d3-ab86-2ff98e121447\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-lpvmz\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"cncf-master\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-lpvmz\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-03T03:11:24Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-03T03:11:25Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-03T03:11:25Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-03T03:11:23Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://9b0a076bb2b09ab1df2dc885424b8828cb41ab828f1747660153bcb01c49991a\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-01-03T03:11:24Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.21.7.5\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.246.121\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.246.121\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-01-03T03:11:24Z\"\n    }\n}\n"
STEP: replace the image in the pod 01/03/23 03:11:29.105
Jan  3 03:11:29.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-4510 replace -f -'
Jan  3 03:11:29.321: INFO: stderr: ""
Jan  3 03:11:29.321: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 01/03/23 03:11:29.321
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1736
Jan  3 03:11:29.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-4510 delete pods e2e-test-httpd-pod'
Jan  3 03:11:31.244: INFO: stderr: ""
Jan  3 03:11:31.244: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan  3 03:11:31.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4510" for this suite. 01/03/23 03:11:31.271
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","completed":313,"skipped":5903,"failed":0}
------------------------------
• [SLOW TEST] [7.370 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1729
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1745

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:11:23.903
    Jan  3 03:11:23.904: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename kubectl 01/03/23 03:11:23.905
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:11:23.92
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:11:23.922
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1732
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1745
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/03/23 03:11:23.924
    Jan  3 03:11:23.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-4510 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jan  3 03:11:23.997: INFO: stderr: ""
    Jan  3 03:11:23.997: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 01/03/23 03:11:23.997
    STEP: verifying the pod e2e-test-httpd-pod was created 01/03/23 03:11:29.048
    Jan  3 03:11:29.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-4510 get pod e2e-test-httpd-pod -o json'
    Jan  3 03:11:29.105: INFO: stderr: ""
    Jan  3 03:11:29.105: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"4750a1b67ed4a3f9b21980183f066cc7ca45c81661879161faf45c10183cc91e\",\n            \"cni.projectcalico.org/podIP\": \"10.244.246.121/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.244.246.121/32\",\n            \"createdTime\": \"2023-01-03T12:11:23.990637932+09:00\",\n            \"creator\": \"system:serviceaccount:sonobuoy:sonobuoy-serviceaccount\",\n            \"updatedTime\": \"2023-01-03T12:11:23.990637932+09:00\",\n            \"updater\": \"system:serviceaccount:sonobuoy:sonobuoy-serviceaccount\"\n        },\n        \"creationTimestamp\": \"2023-01-03T03:11:23Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4510\",\n        \"resourceVersion\": \"1016009\",\n        \"uid\": \"b7f22df5-3a85-42d3-ab86-2ff98e121447\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-lpvmz\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"cncf-master\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-lpvmz\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-03T03:11:24Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-03T03:11:25Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-03T03:11:25Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-03T03:11:23Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://9b0a076bb2b09ab1df2dc885424b8828cb41ab828f1747660153bcb01c49991a\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-01-03T03:11:24Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.21.7.5\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.246.121\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.246.121\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-01-03T03:11:24Z\"\n    }\n}\n"
    STEP: replace the image in the pod 01/03/23 03:11:29.105
    Jan  3 03:11:29.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-4510 replace -f -'
    Jan  3 03:11:29.321: INFO: stderr: ""
    Jan  3 03:11:29.321: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 01/03/23 03:11:29.321
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1736
    Jan  3 03:11:29.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-4510 delete pods e2e-test-httpd-pod'
    Jan  3 03:11:31.244: INFO: stderr: ""
    Jan  3 03:11:31.244: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan  3 03:11:31.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4510" for this suite. 01/03/23 03:11:31.271
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:11:31.275
Jan  3 03:11:31.275: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename watch 01/03/23 03:11:31.276
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:11:31.292
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:11:31.294
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 01/03/23 03:11:31.296
STEP: starting a background goroutine to produce watch events 01/03/23 03:11:31.31
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 01/03/23 03:11:31.31
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan  3 03:11:34.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-458" for this suite. 01/03/23 03:11:34.162
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","completed":314,"skipped":5940,"failed":0}
------------------------------
• [2.910 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:11:31.275
    Jan  3 03:11:31.275: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename watch 01/03/23 03:11:31.276
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:11:31.292
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:11:31.294
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 01/03/23 03:11:31.296
    STEP: starting a background goroutine to produce watch events 01/03/23 03:11:31.31
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 01/03/23 03:11:31.31
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan  3 03:11:34.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-458" for this suite. 01/03/23 03:11:34.162
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:11:34.185
Jan  3 03:11:34.185: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename statefulset 01/03/23 03:11:34.186
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:11:34.208
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:11:34.21
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-5535 01/03/23 03:11:34.212
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
STEP: Initializing watcher for selector baz=blah,foo=bar 01/03/23 03:11:34.215
STEP: Creating stateful set ss in namespace statefulset-5535 01/03/23 03:11:34.218
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5535 01/03/23 03:11:34.237
Jan  3 03:11:34.241: INFO: Found 0 stateful pods, waiting for 1
Jan  3 03:11:44.259: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 01/03/23 03:11:44.259
Jan  3 03:11:44.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=statefulset-5535 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  3 03:11:44.411: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  3 03:11:44.411: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  3 03:11:44.411: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  3 03:11:44.413: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan  3 03:11:54.418: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan  3 03:11:54.418: INFO: Waiting for statefulset status.replicas updated to 0
Jan  3 03:11:54.435: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999759s
Jan  3 03:11:55.437: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.988991319s
Jan  3 03:11:56.440: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.985728059s
Jan  3 03:11:57.442: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.983431227s
Jan  3 03:11:58.445: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.980804208s
Jan  3 03:11:59.448: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.978329245s
Jan  3 03:12:00.450: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.975861227s
Jan  3 03:12:01.454: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.972812773s
Jan  3 03:12:02.456: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.969979431s
Jan  3 03:12:03.458: INFO: Verifying statefulset ss doesn't scale past 1 for another 967.276442ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5535 01/03/23 03:12:04.459
Jan  3 03:12:04.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=statefulset-5535 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  3 03:12:04.580: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan  3 03:12:04.580: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  3 03:12:04.580: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  3 03:12:04.582: INFO: Found 1 stateful pods, waiting for 3
Jan  3 03:12:14.589: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 03:12:14.589: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan  3 03:12:14.589: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 01/03/23 03:12:14.589
STEP: Scale down will halt with unhealthy stateful pod 01/03/23 03:12:14.589
Jan  3 03:12:14.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=statefulset-5535 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  3 03:12:14.744: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  3 03:12:14.744: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  3 03:12:14.744: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  3 03:12:14.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=statefulset-5535 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  3 03:12:14.887: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  3 03:12:14.887: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  3 03:12:14.887: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  3 03:12:14.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=statefulset-5535 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  3 03:12:15.018: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  3 03:12:15.018: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  3 03:12:15.018: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  3 03:12:15.018: INFO: Waiting for statefulset status.replicas updated to 0
Jan  3 03:12:15.031: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jan  3 03:12:25.037: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan  3 03:12:25.037: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan  3 03:12:25.037: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan  3 03:12:25.050: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999793s
Jan  3 03:12:26.053: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99086811s
Jan  3 03:12:27.056: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.988105312s
Jan  3 03:12:28.059: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.985267658s
Jan  3 03:12:29.075: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.982285301s
Jan  3 03:12:30.078: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.966677614s
Jan  3 03:12:31.081: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.963431047s
Jan  3 03:12:32.084: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.960586566s
Jan  3 03:12:33.086: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.957502391s
Jan  3 03:12:34.089: INFO: Verifying statefulset ss doesn't scale past 3 for another 954.589923ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5535 01/03/23 03:12:35.089
Jan  3 03:12:35.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=statefulset-5535 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  3 03:12:35.239: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan  3 03:12:35.239: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  3 03:12:35.239: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  3 03:12:35.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=statefulset-5535 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  3 03:12:35.387: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan  3 03:12:35.387: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  3 03:12:35.387: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  3 03:12:35.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=statefulset-5535 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  3 03:12:35.501: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan  3 03:12:35.501: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  3 03:12:35.501: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  3 03:12:35.501: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 01/03/23 03:12:45.513
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan  3 03:12:45.513: INFO: Deleting all statefulset in ns statefulset-5535
Jan  3 03:12:45.514: INFO: Scaling statefulset ss to 0
Jan  3 03:12:45.522: INFO: Waiting for statefulset status.replicas updated to 0
Jan  3 03:12:45.523: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan  3 03:12:45.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5535" for this suite. 01/03/23 03:12:45.536
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","completed":315,"skipped":5942,"failed":0}
------------------------------
• [SLOW TEST] [71.358 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:585

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:11:34.185
    Jan  3 03:11:34.185: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename statefulset 01/03/23 03:11:34.186
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:11:34.208
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:11:34.21
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-5535 01/03/23 03:11:34.212
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:585
    STEP: Initializing watcher for selector baz=blah,foo=bar 01/03/23 03:11:34.215
    STEP: Creating stateful set ss in namespace statefulset-5535 01/03/23 03:11:34.218
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5535 01/03/23 03:11:34.237
    Jan  3 03:11:34.241: INFO: Found 0 stateful pods, waiting for 1
    Jan  3 03:11:44.259: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 01/03/23 03:11:44.259
    Jan  3 03:11:44.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=statefulset-5535 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  3 03:11:44.411: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  3 03:11:44.411: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  3 03:11:44.411: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  3 03:11:44.413: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jan  3 03:11:54.418: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan  3 03:11:54.418: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  3 03:11:54.435: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999759s
    Jan  3 03:11:55.437: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.988991319s
    Jan  3 03:11:56.440: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.985728059s
    Jan  3 03:11:57.442: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.983431227s
    Jan  3 03:11:58.445: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.980804208s
    Jan  3 03:11:59.448: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.978329245s
    Jan  3 03:12:00.450: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.975861227s
    Jan  3 03:12:01.454: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.972812773s
    Jan  3 03:12:02.456: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.969979431s
    Jan  3 03:12:03.458: INFO: Verifying statefulset ss doesn't scale past 1 for another 967.276442ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5535 01/03/23 03:12:04.459
    Jan  3 03:12:04.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=statefulset-5535 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  3 03:12:04.580: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan  3 03:12:04.580: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan  3 03:12:04.580: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan  3 03:12:04.582: INFO: Found 1 stateful pods, waiting for 3
    Jan  3 03:12:14.589: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan  3 03:12:14.589: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan  3 03:12:14.589: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 01/03/23 03:12:14.589
    STEP: Scale down will halt with unhealthy stateful pod 01/03/23 03:12:14.589
    Jan  3 03:12:14.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=statefulset-5535 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  3 03:12:14.744: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  3 03:12:14.744: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  3 03:12:14.744: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  3 03:12:14.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=statefulset-5535 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  3 03:12:14.887: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  3 03:12:14.887: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  3 03:12:14.887: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  3 03:12:14.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=statefulset-5535 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  3 03:12:15.018: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  3 03:12:15.018: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  3 03:12:15.018: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  3 03:12:15.018: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  3 03:12:15.031: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Jan  3 03:12:25.037: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan  3 03:12:25.037: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jan  3 03:12:25.037: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jan  3 03:12:25.050: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999793s
    Jan  3 03:12:26.053: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99086811s
    Jan  3 03:12:27.056: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.988105312s
    Jan  3 03:12:28.059: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.985267658s
    Jan  3 03:12:29.075: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.982285301s
    Jan  3 03:12:30.078: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.966677614s
    Jan  3 03:12:31.081: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.963431047s
    Jan  3 03:12:32.084: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.960586566s
    Jan  3 03:12:33.086: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.957502391s
    Jan  3 03:12:34.089: INFO: Verifying statefulset ss doesn't scale past 3 for another 954.589923ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5535 01/03/23 03:12:35.089
    Jan  3 03:12:35.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=statefulset-5535 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  3 03:12:35.239: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan  3 03:12:35.239: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan  3 03:12:35.239: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan  3 03:12:35.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=statefulset-5535 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  3 03:12:35.387: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan  3 03:12:35.387: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan  3 03:12:35.387: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan  3 03:12:35.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=statefulset-5535 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  3 03:12:35.501: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan  3 03:12:35.501: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan  3 03:12:35.501: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan  3 03:12:35.501: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 01/03/23 03:12:45.513
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan  3 03:12:45.513: INFO: Deleting all statefulset in ns statefulset-5535
    Jan  3 03:12:45.514: INFO: Scaling statefulset ss to 0
    Jan  3 03:12:45.522: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  3 03:12:45.523: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan  3 03:12:45.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-5535" for this suite. 01/03/23 03:12:45.536
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:12:45.544
Jan  3 03:12:45.544: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename crd-webhook 01/03/23 03:12:45.545
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:12:45.588
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:12:45.591
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 01/03/23 03:12:45.593
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/03/23 03:12:46.011
STEP: Deploying the custom resource conversion webhook pod 01/03/23 03:12:46.017
STEP: Wait for the deployment to be ready 01/03/23 03:12:46.074
Jan  3 03:12:46.078: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/03/23 03:12:48.089
STEP: Verifying the service has paired with the endpoint 01/03/23 03:12:48.106
Jan  3 03:12:49.106: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Jan  3 03:12:49.108: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Creating a v1 custom resource 01/03/23 03:12:51.677
STEP: Create a v2 custom resource 01/03/23 03:12:51.691
STEP: List CRs in v1 01/03/23 03:12:51.73
STEP: List CRs in v2 01/03/23 03:12:51.733
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  3 03:12:52.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-3538" for this suite. 01/03/23 03:12:52.245
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","completed":316,"skipped":5952,"failed":0}
------------------------------
• [SLOW TEST] [6.777 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:12:45.544
    Jan  3 03:12:45.544: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename crd-webhook 01/03/23 03:12:45.545
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:12:45.588
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:12:45.591
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 01/03/23 03:12:45.593
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/03/23 03:12:46.011
    STEP: Deploying the custom resource conversion webhook pod 01/03/23 03:12:46.017
    STEP: Wait for the deployment to be ready 01/03/23 03:12:46.074
    Jan  3 03:12:46.078: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/03/23 03:12:48.089
    STEP: Verifying the service has paired with the endpoint 01/03/23 03:12:48.106
    Jan  3 03:12:49.106: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Jan  3 03:12:49.108: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Creating a v1 custom resource 01/03/23 03:12:51.677
    STEP: Create a v2 custom resource 01/03/23 03:12:51.691
    STEP: List CRs in v1 01/03/23 03:12:51.73
    STEP: List CRs in v2 01/03/23 03:12:51.733
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  3 03:12:52.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-3538" for this suite. 01/03/23 03:12:52.245
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:12:52.322
Jan  3 03:12:52.322: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename pod-network-test 01/03/23 03:12:52.323
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:12:52.341
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:12:52.342
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-6775 01/03/23 03:12:52.344
STEP: creating a selector 01/03/23 03:12:52.344
STEP: Creating the service pods in kubernetes 01/03/23 03:12:52.344
Jan  3 03:12:52.344: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan  3 03:12:52.392: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6775" to be "running and ready"
Jan  3 03:12:52.434: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 41.42616ms
Jan  3 03:12:52.434: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 03:12:54.437: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.044761018s
Jan  3 03:12:54.437: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 03:12:56.437: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.044193096s
Jan  3 03:12:56.437: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 03:12:58.437: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.044894821s
Jan  3 03:12:58.437: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 03:13:00.438: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.045159069s
Jan  3 03:13:00.438: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 03:13:02.437: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.044420795s
Jan  3 03:13:02.437: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 03:13:04.436: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.043979403s
Jan  3 03:13:04.436: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 03:13:06.437: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.044757205s
Jan  3 03:13:06.437: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 03:13:08.437: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.044647929s
Jan  3 03:13:08.437: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 03:13:10.437: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.044216444s
Jan  3 03:13:10.437: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 03:13:12.451: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.058333251s
Jan  3 03:13:12.451: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 03:13:14.437: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.044233658s
Jan  3 03:13:14.437: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan  3 03:13:14.437: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan  3 03:13:14.439: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6775" to be "running and ready"
Jan  3 03:13:14.440: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.746144ms
Jan  3 03:13:14.440: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan  3 03:13:14.440: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jan  3 03:13:14.442: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-6775" to be "running and ready"
Jan  3 03:13:14.444: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 1.644609ms
Jan  3 03:13:14.444: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jan  3 03:13:14.444: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 01/03/23 03:13:14.445
Jan  3 03:13:14.450: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6775" to be "running"
Jan  3 03:13:14.461: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.547967ms
Jan  3 03:13:16.464: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013769257s
Jan  3 03:13:16.464: INFO: Pod "test-container-pod" satisfied condition "running"
Jan  3 03:13:16.466: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jan  3 03:13:16.466: INFO: Breadth first check of 10.244.246.118 on host 172.21.7.5...
Jan  3 03:13:16.467: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.246.120:9080/dial?request=hostname&protocol=udp&host=10.244.246.118&port=8081&tries=1'] Namespace:pod-network-test-6775 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 03:13:16.467: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
Jan  3 03:13:16.468: INFO: ExecWithOptions: Clientset creation
Jan  3 03:13:16.468: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6775/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.246.120%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.246.118%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan  3 03:13:16.548: INFO: Waiting for responses: map[]
Jan  3 03:13:16.548: INFO: reached 10.244.246.118 after 0/1 tries
Jan  3 03:13:16.548: INFO: Breadth first check of 10.244.232.41 on host 172.21.7.7...
Jan  3 03:13:16.549: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.246.120:9080/dial?request=hostname&protocol=udp&host=10.244.232.41&port=8081&tries=1'] Namespace:pod-network-test-6775 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 03:13:16.549: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
Jan  3 03:13:16.550: INFO: ExecWithOptions: Clientset creation
Jan  3 03:13:16.550: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6775/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.246.120%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.232.41%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan  3 03:13:16.627: INFO: Waiting for responses: map[]
Jan  3 03:13:16.627: INFO: reached 10.244.232.41 after 0/1 tries
Jan  3 03:13:16.627: INFO: Breadth first check of 10.244.89.66 on host 172.21.7.13...
Jan  3 03:13:16.629: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.246.120:9080/dial?request=hostname&protocol=udp&host=10.244.89.66&port=8081&tries=1'] Namespace:pod-network-test-6775 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 03:13:16.629: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
Jan  3 03:13:16.629: INFO: ExecWithOptions: Clientset creation
Jan  3 03:13:16.629: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6775/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.246.120%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.89.66%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan  3 03:13:16.695: INFO: Waiting for responses: map[]
Jan  3 03:13:16.695: INFO: reached 10.244.89.66 after 0/1 tries
Jan  3 03:13:16.695: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jan  3 03:13:16.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6775" for this suite. 01/03/23 03:13:16.698
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","completed":317,"skipped":5971,"failed":0}
------------------------------
• [SLOW TEST] [24.378 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:12:52.322
    Jan  3 03:12:52.322: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename pod-network-test 01/03/23 03:12:52.323
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:12:52.341
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:12:52.342
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-6775 01/03/23 03:12:52.344
    STEP: creating a selector 01/03/23 03:12:52.344
    STEP: Creating the service pods in kubernetes 01/03/23 03:12:52.344
    Jan  3 03:12:52.344: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan  3 03:12:52.392: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6775" to be "running and ready"
    Jan  3 03:12:52.434: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 41.42616ms
    Jan  3 03:12:52.434: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 03:12:54.437: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.044761018s
    Jan  3 03:12:54.437: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 03:12:56.437: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.044193096s
    Jan  3 03:12:56.437: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 03:12:58.437: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.044894821s
    Jan  3 03:12:58.437: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 03:13:00.438: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.045159069s
    Jan  3 03:13:00.438: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 03:13:02.437: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.044420795s
    Jan  3 03:13:02.437: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 03:13:04.436: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.043979403s
    Jan  3 03:13:04.436: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 03:13:06.437: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.044757205s
    Jan  3 03:13:06.437: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 03:13:08.437: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.044647929s
    Jan  3 03:13:08.437: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 03:13:10.437: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.044216444s
    Jan  3 03:13:10.437: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 03:13:12.451: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.058333251s
    Jan  3 03:13:12.451: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 03:13:14.437: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.044233658s
    Jan  3 03:13:14.437: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan  3 03:13:14.437: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan  3 03:13:14.439: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6775" to be "running and ready"
    Jan  3 03:13:14.440: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.746144ms
    Jan  3 03:13:14.440: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan  3 03:13:14.440: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jan  3 03:13:14.442: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-6775" to be "running and ready"
    Jan  3 03:13:14.444: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 1.644609ms
    Jan  3 03:13:14.444: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jan  3 03:13:14.444: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 01/03/23 03:13:14.445
    Jan  3 03:13:14.450: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6775" to be "running"
    Jan  3 03:13:14.461: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.547967ms
    Jan  3 03:13:16.464: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013769257s
    Jan  3 03:13:16.464: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan  3 03:13:16.466: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Jan  3 03:13:16.466: INFO: Breadth first check of 10.244.246.118 on host 172.21.7.5...
    Jan  3 03:13:16.467: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.246.120:9080/dial?request=hostname&protocol=udp&host=10.244.246.118&port=8081&tries=1'] Namespace:pod-network-test-6775 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 03:13:16.467: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    Jan  3 03:13:16.468: INFO: ExecWithOptions: Clientset creation
    Jan  3 03:13:16.468: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6775/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.246.120%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.246.118%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan  3 03:13:16.548: INFO: Waiting for responses: map[]
    Jan  3 03:13:16.548: INFO: reached 10.244.246.118 after 0/1 tries
    Jan  3 03:13:16.548: INFO: Breadth first check of 10.244.232.41 on host 172.21.7.7...
    Jan  3 03:13:16.549: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.246.120:9080/dial?request=hostname&protocol=udp&host=10.244.232.41&port=8081&tries=1'] Namespace:pod-network-test-6775 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 03:13:16.549: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    Jan  3 03:13:16.550: INFO: ExecWithOptions: Clientset creation
    Jan  3 03:13:16.550: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6775/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.246.120%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.232.41%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan  3 03:13:16.627: INFO: Waiting for responses: map[]
    Jan  3 03:13:16.627: INFO: reached 10.244.232.41 after 0/1 tries
    Jan  3 03:13:16.627: INFO: Breadth first check of 10.244.89.66 on host 172.21.7.13...
    Jan  3 03:13:16.629: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.246.120:9080/dial?request=hostname&protocol=udp&host=10.244.89.66&port=8081&tries=1'] Namespace:pod-network-test-6775 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 03:13:16.629: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    Jan  3 03:13:16.629: INFO: ExecWithOptions: Clientset creation
    Jan  3 03:13:16.629: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6775/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.246.120%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.89.66%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan  3 03:13:16.695: INFO: Waiting for responses: map[]
    Jan  3 03:13:16.695: INFO: reached 10.244.89.66 after 0/1 tries
    Jan  3 03:13:16.695: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jan  3 03:13:16.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-6775" for this suite. 01/03/23 03:13:16.698
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:13:16.702
Jan  3 03:13:16.702: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename emptydir 01/03/23 03:13:16.703
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:13:16.728
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:13:16.729
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
STEP: Creating a pod to test emptydir 0644 on tmpfs 01/03/23 03:13:16.731
Jan  3 03:13:16.744: INFO: Waiting up to 5m0s for pod "pod-cba75911-af6b-42d7-bd21-b5694d1accb4" in namespace "emptydir-1580" to be "Succeeded or Failed"
Jan  3 03:13:16.751: INFO: Pod "pod-cba75911-af6b-42d7-bd21-b5694d1accb4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.179466ms
Jan  3 03:13:18.754: INFO: Pod "pod-cba75911-af6b-42d7-bd21-b5694d1accb4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00909034s
Jan  3 03:13:20.764: INFO: Pod "pod-cba75911-af6b-42d7-bd21-b5694d1accb4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020010125s
STEP: Saw pod success 01/03/23 03:13:20.765
Jan  3 03:13:20.765: INFO: Pod "pod-cba75911-af6b-42d7-bd21-b5694d1accb4" satisfied condition "Succeeded or Failed"
Jan  3 03:13:20.766: INFO: Trying to get logs from node cncf-master pod pod-cba75911-af6b-42d7-bd21-b5694d1accb4 container test-container: <nil>
STEP: delete the pod 01/03/23 03:13:20.775
Jan  3 03:13:20.791: INFO: Waiting for pod pod-cba75911-af6b-42d7-bd21-b5694d1accb4 to disappear
Jan  3 03:13:20.797: INFO: Pod pod-cba75911-af6b-42d7-bd21-b5694d1accb4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan  3 03:13:20.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1580" for this suite. 01/03/23 03:13:20.799
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":318,"skipped":5990,"failed":0}
------------------------------
• [4.100 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:13:16.702
    Jan  3 03:13:16.702: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename emptydir 01/03/23 03:13:16.703
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:13:16.728
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:13:16.729
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:126
    STEP: Creating a pod to test emptydir 0644 on tmpfs 01/03/23 03:13:16.731
    Jan  3 03:13:16.744: INFO: Waiting up to 5m0s for pod "pod-cba75911-af6b-42d7-bd21-b5694d1accb4" in namespace "emptydir-1580" to be "Succeeded or Failed"
    Jan  3 03:13:16.751: INFO: Pod "pod-cba75911-af6b-42d7-bd21-b5694d1accb4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.179466ms
    Jan  3 03:13:18.754: INFO: Pod "pod-cba75911-af6b-42d7-bd21-b5694d1accb4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00909034s
    Jan  3 03:13:20.764: INFO: Pod "pod-cba75911-af6b-42d7-bd21-b5694d1accb4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020010125s
    STEP: Saw pod success 01/03/23 03:13:20.765
    Jan  3 03:13:20.765: INFO: Pod "pod-cba75911-af6b-42d7-bd21-b5694d1accb4" satisfied condition "Succeeded or Failed"
    Jan  3 03:13:20.766: INFO: Trying to get logs from node cncf-master pod pod-cba75911-af6b-42d7-bd21-b5694d1accb4 container test-container: <nil>
    STEP: delete the pod 01/03/23 03:13:20.775
    Jan  3 03:13:20.791: INFO: Waiting for pod pod-cba75911-af6b-42d7-bd21-b5694d1accb4 to disappear
    Jan  3 03:13:20.797: INFO: Pod pod-cba75911-af6b-42d7-bd21-b5694d1accb4 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan  3 03:13:20.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1580" for this suite. 01/03/23 03:13:20.799
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:13:20.802
Jan  3 03:13:20.802: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename endpointslice 01/03/23 03:13:20.803
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:13:20.818
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:13:20.82
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jan  3 03:13:22.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-1993" for this suite. 01/03/23 03:13:22.915
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","completed":319,"skipped":5992,"failed":0}
------------------------------
• [2.116 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:13:20.802
    Jan  3 03:13:20.802: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename endpointslice 01/03/23 03:13:20.803
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:13:20.818
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:13:20.82
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:101
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jan  3 03:13:22.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-1993" for this suite. 01/03/23 03:13:22.915
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:13:22.918
Jan  3 03:13:22.918: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename services 01/03/23 03:13:22.919
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:13:22.94
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:13:22.942
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
STEP: creating service in namespace services-4714 01/03/23 03:13:22.944
STEP: creating service affinity-nodeport in namespace services-4714 01/03/23 03:13:22.944
STEP: creating replication controller affinity-nodeport in namespace services-4714 01/03/23 03:13:22.969
I0103 03:13:22.975777      23 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-4714, replica count: 3
I0103 03:13:26.027267      23 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  3 03:13:26.051: INFO: Creating new exec pod
Jan  3 03:13:26.055: INFO: Waiting up to 5m0s for pod "execpod-affinity87xrn" in namespace "services-4714" to be "running"
Jan  3 03:13:26.079: INFO: Pod "execpod-affinity87xrn": Phase="Pending", Reason="", readiness=false. Elapsed: 24.122185ms
Jan  3 03:13:28.130: INFO: Pod "execpod-affinity87xrn": Phase="Running", Reason="", readiness=true. Elapsed: 2.074869246s
Jan  3 03:13:28.130: INFO: Pod "execpod-affinity87xrn" satisfied condition "running"
Jan  3 03:13:29.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-4714 exec execpod-affinity87xrn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Jan  3 03:13:29.276: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jan  3 03:13:29.276: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  3 03:13:29.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-4714 exec execpod-affinity87xrn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.93.16 80'
Jan  3 03:13:29.436: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.93.16 80\nConnection to 10.96.93.16 80 port [tcp/http] succeeded!\n"
Jan  3 03:13:29.436: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  3 03:13:29.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-4714 exec execpod-affinity87xrn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.7.7 31888'
Jan  3 03:13:29.566: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.7.7 31888\nConnection to 172.21.7.7 31888 port [tcp/*] succeeded!\n"
Jan  3 03:13:29.566: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  3 03:13:29.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-4714 exec execpod-affinity87xrn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.7.13 31888'
Jan  3 03:13:29.704: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.7.13 31888\nConnection to 172.21.7.13 31888 port [tcp/*] succeeded!\n"
Jan  3 03:13:29.704: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  3 03:13:29.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-4714 exec execpod-affinity87xrn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.7.5:31888/ ; done'
Jan  3 03:13:29.894: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31888/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31888/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31888/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31888/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31888/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31888/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31888/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31888/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31888/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31888/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31888/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31888/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31888/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31888/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31888/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31888/\n"
Jan  3 03:13:29.894: INFO: stdout: "\naffinity-nodeport-fjq6q\naffinity-nodeport-fjq6q\naffinity-nodeport-fjq6q\naffinity-nodeport-fjq6q\naffinity-nodeport-fjq6q\naffinity-nodeport-fjq6q\naffinity-nodeport-fjq6q\naffinity-nodeport-fjq6q\naffinity-nodeport-fjq6q\naffinity-nodeport-fjq6q\naffinity-nodeport-fjq6q\naffinity-nodeport-fjq6q\naffinity-nodeport-fjq6q\naffinity-nodeport-fjq6q\naffinity-nodeport-fjq6q\naffinity-nodeport-fjq6q"
Jan  3 03:13:29.894: INFO: Received response from host: affinity-nodeport-fjq6q
Jan  3 03:13:29.894: INFO: Received response from host: affinity-nodeport-fjq6q
Jan  3 03:13:29.894: INFO: Received response from host: affinity-nodeport-fjq6q
Jan  3 03:13:29.894: INFO: Received response from host: affinity-nodeport-fjq6q
Jan  3 03:13:29.894: INFO: Received response from host: affinity-nodeport-fjq6q
Jan  3 03:13:29.894: INFO: Received response from host: affinity-nodeport-fjq6q
Jan  3 03:13:29.894: INFO: Received response from host: affinity-nodeport-fjq6q
Jan  3 03:13:29.894: INFO: Received response from host: affinity-nodeport-fjq6q
Jan  3 03:13:29.894: INFO: Received response from host: affinity-nodeport-fjq6q
Jan  3 03:13:29.894: INFO: Received response from host: affinity-nodeport-fjq6q
Jan  3 03:13:29.894: INFO: Received response from host: affinity-nodeport-fjq6q
Jan  3 03:13:29.894: INFO: Received response from host: affinity-nodeport-fjq6q
Jan  3 03:13:29.894: INFO: Received response from host: affinity-nodeport-fjq6q
Jan  3 03:13:29.894: INFO: Received response from host: affinity-nodeport-fjq6q
Jan  3 03:13:29.894: INFO: Received response from host: affinity-nodeport-fjq6q
Jan  3 03:13:29.894: INFO: Received response from host: affinity-nodeport-fjq6q
Jan  3 03:13:29.894: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-4714, will wait for the garbage collector to delete the pods 01/03/23 03:13:29.933
Jan  3 03:13:29.999: INFO: Deleting ReplicationController affinity-nodeport took: 12.812764ms
Jan  3 03:13:30.100: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.920433ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  3 03:13:31.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4714" for this suite. 01/03/23 03:13:31.944
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","completed":320,"skipped":5995,"failed":0}
------------------------------
• [SLOW TEST] [9.053 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:13:22.918
    Jan  3 03:13:22.918: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename services 01/03/23 03:13:22.919
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:13:22.94
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:13:22.942
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2204
    STEP: creating service in namespace services-4714 01/03/23 03:13:22.944
    STEP: creating service affinity-nodeport in namespace services-4714 01/03/23 03:13:22.944
    STEP: creating replication controller affinity-nodeport in namespace services-4714 01/03/23 03:13:22.969
    I0103 03:13:22.975777      23 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-4714, replica count: 3
    I0103 03:13:26.027267      23 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  3 03:13:26.051: INFO: Creating new exec pod
    Jan  3 03:13:26.055: INFO: Waiting up to 5m0s for pod "execpod-affinity87xrn" in namespace "services-4714" to be "running"
    Jan  3 03:13:26.079: INFO: Pod "execpod-affinity87xrn": Phase="Pending", Reason="", readiness=false. Elapsed: 24.122185ms
    Jan  3 03:13:28.130: INFO: Pod "execpod-affinity87xrn": Phase="Running", Reason="", readiness=true. Elapsed: 2.074869246s
    Jan  3 03:13:28.130: INFO: Pod "execpod-affinity87xrn" satisfied condition "running"
    Jan  3 03:13:29.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-4714 exec execpod-affinity87xrn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
    Jan  3 03:13:29.276: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Jan  3 03:13:29.276: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  3 03:13:29.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-4714 exec execpod-affinity87xrn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.93.16 80'
    Jan  3 03:13:29.436: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.93.16 80\nConnection to 10.96.93.16 80 port [tcp/http] succeeded!\n"
    Jan  3 03:13:29.436: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  3 03:13:29.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-4714 exec execpod-affinity87xrn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.7.7 31888'
    Jan  3 03:13:29.566: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.7.7 31888\nConnection to 172.21.7.7 31888 port [tcp/*] succeeded!\n"
    Jan  3 03:13:29.566: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  3 03:13:29.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-4714 exec execpod-affinity87xrn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.7.13 31888'
    Jan  3 03:13:29.704: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.7.13 31888\nConnection to 172.21.7.13 31888 port [tcp/*] succeeded!\n"
    Jan  3 03:13:29.704: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  3 03:13:29.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-4714 exec execpod-affinity87xrn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.7.5:31888/ ; done'
    Jan  3 03:13:29.894: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31888/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31888/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31888/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31888/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31888/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31888/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31888/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31888/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31888/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31888/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31888/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31888/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31888/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31888/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31888/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31888/\n"
    Jan  3 03:13:29.894: INFO: stdout: "\naffinity-nodeport-fjq6q\naffinity-nodeport-fjq6q\naffinity-nodeport-fjq6q\naffinity-nodeport-fjq6q\naffinity-nodeport-fjq6q\naffinity-nodeport-fjq6q\naffinity-nodeport-fjq6q\naffinity-nodeport-fjq6q\naffinity-nodeport-fjq6q\naffinity-nodeport-fjq6q\naffinity-nodeport-fjq6q\naffinity-nodeport-fjq6q\naffinity-nodeport-fjq6q\naffinity-nodeport-fjq6q\naffinity-nodeport-fjq6q\naffinity-nodeport-fjq6q"
    Jan  3 03:13:29.894: INFO: Received response from host: affinity-nodeport-fjq6q
    Jan  3 03:13:29.894: INFO: Received response from host: affinity-nodeport-fjq6q
    Jan  3 03:13:29.894: INFO: Received response from host: affinity-nodeport-fjq6q
    Jan  3 03:13:29.894: INFO: Received response from host: affinity-nodeport-fjq6q
    Jan  3 03:13:29.894: INFO: Received response from host: affinity-nodeport-fjq6q
    Jan  3 03:13:29.894: INFO: Received response from host: affinity-nodeport-fjq6q
    Jan  3 03:13:29.894: INFO: Received response from host: affinity-nodeport-fjq6q
    Jan  3 03:13:29.894: INFO: Received response from host: affinity-nodeport-fjq6q
    Jan  3 03:13:29.894: INFO: Received response from host: affinity-nodeport-fjq6q
    Jan  3 03:13:29.894: INFO: Received response from host: affinity-nodeport-fjq6q
    Jan  3 03:13:29.894: INFO: Received response from host: affinity-nodeport-fjq6q
    Jan  3 03:13:29.894: INFO: Received response from host: affinity-nodeport-fjq6q
    Jan  3 03:13:29.894: INFO: Received response from host: affinity-nodeport-fjq6q
    Jan  3 03:13:29.894: INFO: Received response from host: affinity-nodeport-fjq6q
    Jan  3 03:13:29.894: INFO: Received response from host: affinity-nodeport-fjq6q
    Jan  3 03:13:29.894: INFO: Received response from host: affinity-nodeport-fjq6q
    Jan  3 03:13:29.894: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-4714, will wait for the garbage collector to delete the pods 01/03/23 03:13:29.933
    Jan  3 03:13:29.999: INFO: Deleting ReplicationController affinity-nodeport took: 12.812764ms
    Jan  3 03:13:30.100: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.920433ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  3 03:13:31.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4714" for this suite. 01/03/23 03:13:31.944
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:13:31.973
Jan  3 03:13:31.973: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename pods 01/03/23 03:13:31.974
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:13:31.983
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:13:31.984
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
Jan  3 03:13:32.000: INFO: Waiting up to 5m0s for pod "server-envvars-9c1fb7dc-d834-4e9c-8182-77655490ee3b" in namespace "pods-9830" to be "running and ready"
Jan  3 03:13:32.005: INFO: Pod "server-envvars-9c1fb7dc-d834-4e9c-8182-77655490ee3b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.144323ms
Jan  3 03:13:32.006: INFO: The phase of Pod server-envvars-9c1fb7dc-d834-4e9c-8182-77655490ee3b is Pending, waiting for it to be Running (with Ready = true)
Jan  3 03:13:34.008: INFO: Pod "server-envvars-9c1fb7dc-d834-4e9c-8182-77655490ee3b": Phase="Running", Reason="", readiness=true. Elapsed: 2.007617308s
Jan  3 03:13:34.008: INFO: The phase of Pod server-envvars-9c1fb7dc-d834-4e9c-8182-77655490ee3b is Running (Ready = true)
Jan  3 03:13:34.008: INFO: Pod "server-envvars-9c1fb7dc-d834-4e9c-8182-77655490ee3b" satisfied condition "running and ready"
Jan  3 03:13:34.029: INFO: Waiting up to 5m0s for pod "client-envvars-e6b6330d-0e38-4621-be9f-d5f95aa4867e" in namespace "pods-9830" to be "Succeeded or Failed"
Jan  3 03:13:34.045: INFO: Pod "client-envvars-e6b6330d-0e38-4621-be9f-d5f95aa4867e": Phase="Pending", Reason="", readiness=false. Elapsed: 16.407207ms
Jan  3 03:13:36.048: INFO: Pod "client-envvars-e6b6330d-0e38-4621-be9f-d5f95aa4867e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019152264s
Jan  3 03:13:38.049: INFO: Pod "client-envvars-e6b6330d-0e38-4621-be9f-d5f95aa4867e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020222908s
STEP: Saw pod success 01/03/23 03:13:38.049
Jan  3 03:13:38.049: INFO: Pod "client-envvars-e6b6330d-0e38-4621-be9f-d5f95aa4867e" satisfied condition "Succeeded or Failed"
Jan  3 03:13:38.051: INFO: Trying to get logs from node cncf-master pod client-envvars-e6b6330d-0e38-4621-be9f-d5f95aa4867e container env3cont: <nil>
STEP: delete the pod 01/03/23 03:13:38.054
Jan  3 03:13:38.075: INFO: Waiting for pod client-envvars-e6b6330d-0e38-4621-be9f-d5f95aa4867e to disappear
Jan  3 03:13:38.081: INFO: Pod client-envvars-e6b6330d-0e38-4621-be9f-d5f95aa4867e no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan  3 03:13:38.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9830" for this suite. 01/03/23 03:13:38.084
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","completed":321,"skipped":6026,"failed":0}
------------------------------
• [SLOW TEST] [6.113 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:13:31.973
    Jan  3 03:13:31.973: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename pods 01/03/23 03:13:31.974
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:13:31.983
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:13:31.984
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:443
    Jan  3 03:13:32.000: INFO: Waiting up to 5m0s for pod "server-envvars-9c1fb7dc-d834-4e9c-8182-77655490ee3b" in namespace "pods-9830" to be "running and ready"
    Jan  3 03:13:32.005: INFO: Pod "server-envvars-9c1fb7dc-d834-4e9c-8182-77655490ee3b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.144323ms
    Jan  3 03:13:32.006: INFO: The phase of Pod server-envvars-9c1fb7dc-d834-4e9c-8182-77655490ee3b is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 03:13:34.008: INFO: Pod "server-envvars-9c1fb7dc-d834-4e9c-8182-77655490ee3b": Phase="Running", Reason="", readiness=true. Elapsed: 2.007617308s
    Jan  3 03:13:34.008: INFO: The phase of Pod server-envvars-9c1fb7dc-d834-4e9c-8182-77655490ee3b is Running (Ready = true)
    Jan  3 03:13:34.008: INFO: Pod "server-envvars-9c1fb7dc-d834-4e9c-8182-77655490ee3b" satisfied condition "running and ready"
    Jan  3 03:13:34.029: INFO: Waiting up to 5m0s for pod "client-envvars-e6b6330d-0e38-4621-be9f-d5f95aa4867e" in namespace "pods-9830" to be "Succeeded or Failed"
    Jan  3 03:13:34.045: INFO: Pod "client-envvars-e6b6330d-0e38-4621-be9f-d5f95aa4867e": Phase="Pending", Reason="", readiness=false. Elapsed: 16.407207ms
    Jan  3 03:13:36.048: INFO: Pod "client-envvars-e6b6330d-0e38-4621-be9f-d5f95aa4867e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019152264s
    Jan  3 03:13:38.049: INFO: Pod "client-envvars-e6b6330d-0e38-4621-be9f-d5f95aa4867e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020222908s
    STEP: Saw pod success 01/03/23 03:13:38.049
    Jan  3 03:13:38.049: INFO: Pod "client-envvars-e6b6330d-0e38-4621-be9f-d5f95aa4867e" satisfied condition "Succeeded or Failed"
    Jan  3 03:13:38.051: INFO: Trying to get logs from node cncf-master pod client-envvars-e6b6330d-0e38-4621-be9f-d5f95aa4867e container env3cont: <nil>
    STEP: delete the pod 01/03/23 03:13:38.054
    Jan  3 03:13:38.075: INFO: Waiting for pod client-envvars-e6b6330d-0e38-4621-be9f-d5f95aa4867e to disappear
    Jan  3 03:13:38.081: INFO: Pod client-envvars-e6b6330d-0e38-4621-be9f-d5f95aa4867e no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan  3 03:13:38.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-9830" for this suite. 01/03/23 03:13:38.084
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:13:38.087
Jan  3 03:13:38.087: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename var-expansion 01/03/23 03:13:38.087
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:13:38.126
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:13:38.128
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
STEP: Creating a pod to test substitution in container's command 01/03/23 03:13:38.13
Jan  3 03:13:38.147: INFO: Waiting up to 5m0s for pod "var-expansion-9b50024c-3959-4d08-b363-ac9ded0e6f76" in namespace "var-expansion-8902" to be "Succeeded or Failed"
Jan  3 03:13:38.149: INFO: Pod "var-expansion-9b50024c-3959-4d08-b363-ac9ded0e6f76": Phase="Pending", Reason="", readiness=false. Elapsed: 1.764444ms
Jan  3 03:13:40.152: INFO: Pod "var-expansion-9b50024c-3959-4d08-b363-ac9ded0e6f76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00521749s
Jan  3 03:13:42.152: INFO: Pod "var-expansion-9b50024c-3959-4d08-b363-ac9ded0e6f76": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004627578s
STEP: Saw pod success 01/03/23 03:13:42.152
Jan  3 03:13:42.152: INFO: Pod "var-expansion-9b50024c-3959-4d08-b363-ac9ded0e6f76" satisfied condition "Succeeded or Failed"
Jan  3 03:13:42.154: INFO: Trying to get logs from node cncf-master pod var-expansion-9b50024c-3959-4d08-b363-ac9ded0e6f76 container dapi-container: <nil>
STEP: delete the pod 01/03/23 03:13:42.157
Jan  3 03:13:42.198: INFO: Waiting for pod var-expansion-9b50024c-3959-4d08-b363-ac9ded0e6f76 to disappear
Jan  3 03:13:42.201: INFO: Pod var-expansion-9b50024c-3959-4d08-b363-ac9ded0e6f76 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan  3 03:13:42.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8902" for this suite. 01/03/23 03:13:42.203
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","completed":322,"skipped":6028,"failed":0}
------------------------------
• [4.124 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:13:38.087
    Jan  3 03:13:38.087: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename var-expansion 01/03/23 03:13:38.087
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:13:38.126
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:13:38.128
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:72
    STEP: Creating a pod to test substitution in container's command 01/03/23 03:13:38.13
    Jan  3 03:13:38.147: INFO: Waiting up to 5m0s for pod "var-expansion-9b50024c-3959-4d08-b363-ac9ded0e6f76" in namespace "var-expansion-8902" to be "Succeeded or Failed"
    Jan  3 03:13:38.149: INFO: Pod "var-expansion-9b50024c-3959-4d08-b363-ac9ded0e6f76": Phase="Pending", Reason="", readiness=false. Elapsed: 1.764444ms
    Jan  3 03:13:40.152: INFO: Pod "var-expansion-9b50024c-3959-4d08-b363-ac9ded0e6f76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00521749s
    Jan  3 03:13:42.152: INFO: Pod "var-expansion-9b50024c-3959-4d08-b363-ac9ded0e6f76": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004627578s
    STEP: Saw pod success 01/03/23 03:13:42.152
    Jan  3 03:13:42.152: INFO: Pod "var-expansion-9b50024c-3959-4d08-b363-ac9ded0e6f76" satisfied condition "Succeeded or Failed"
    Jan  3 03:13:42.154: INFO: Trying to get logs from node cncf-master pod var-expansion-9b50024c-3959-4d08-b363-ac9ded0e6f76 container dapi-container: <nil>
    STEP: delete the pod 01/03/23 03:13:42.157
    Jan  3 03:13:42.198: INFO: Waiting for pod var-expansion-9b50024c-3959-4d08-b363-ac9ded0e6f76 to disappear
    Jan  3 03:13:42.201: INFO: Pod var-expansion-9b50024c-3959-4d08-b363-ac9ded0e6f76 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan  3 03:13:42.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-8902" for this suite. 01/03/23 03:13:42.203
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:13:42.213
Jan  3 03:13:42.213: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename daemonsets 01/03/23 03:13:42.213
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:13:42.224
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:13:42.226
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
STEP: Creating simple DaemonSet "daemon-set" 01/03/23 03:13:42.247
STEP: Check that daemon pods launch on every node of the cluster. 01/03/23 03:13:42.251
Jan  3 03:13:42.258: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  3 03:13:42.258: INFO: Node cncf-master is running 0 daemon pod, expected 1
Jan  3 03:13:43.282: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  3 03:13:43.282: INFO: Node cncf-master is running 0 daemon pod, expected 1
Jan  3 03:13:44.267: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan  3 03:13:44.267: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 01/03/23 03:13:44.268
Jan  3 03:13:44.284: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan  3 03:13:44.284: INFO: Node cncf-master is running 0 daemon pod, expected 1
Jan  3 03:13:45.289: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan  3 03:13:45.289: INFO: Node cncf-master is running 0 daemon pod, expected 1
Jan  3 03:13:46.290: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan  3 03:13:46.290: INFO: Node cncf-master is running 0 daemon pod, expected 1
Jan  3 03:13:47.291: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan  3 03:13:47.292: INFO: Node cncf-master is running 0 daemon pod, expected 1
Jan  3 03:13:48.299: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan  3 03:13:48.299: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/03/23 03:13:48.301
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2793, will wait for the garbage collector to delete the pods 01/03/23 03:13:48.301
Jan  3 03:13:48.355: INFO: Deleting DaemonSet.extensions daemon-set took: 2.813964ms
Jan  3 03:13:48.456: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.963631ms
Jan  3 03:13:50.659: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  3 03:13:50.659: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan  3 03:13:50.660: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1017528"},"items":null}

Jan  3 03:13:50.662: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1017528"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan  3 03:13:50.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2793" for this suite. 01/03/23 03:13:50.671
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","completed":323,"skipped":6066,"failed":0}
------------------------------
• [SLOW TEST] [8.461 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:13:42.213
    Jan  3 03:13:42.213: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename daemonsets 01/03/23 03:13:42.213
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:13:42.224
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:13:42.226
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:165
    STEP: Creating simple DaemonSet "daemon-set" 01/03/23 03:13:42.247
    STEP: Check that daemon pods launch on every node of the cluster. 01/03/23 03:13:42.251
    Jan  3 03:13:42.258: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  3 03:13:42.258: INFO: Node cncf-master is running 0 daemon pod, expected 1
    Jan  3 03:13:43.282: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  3 03:13:43.282: INFO: Node cncf-master is running 0 daemon pod, expected 1
    Jan  3 03:13:44.267: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan  3 03:13:44.267: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 01/03/23 03:13:44.268
    Jan  3 03:13:44.284: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan  3 03:13:44.284: INFO: Node cncf-master is running 0 daemon pod, expected 1
    Jan  3 03:13:45.289: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan  3 03:13:45.289: INFO: Node cncf-master is running 0 daemon pod, expected 1
    Jan  3 03:13:46.290: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan  3 03:13:46.290: INFO: Node cncf-master is running 0 daemon pod, expected 1
    Jan  3 03:13:47.291: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan  3 03:13:47.292: INFO: Node cncf-master is running 0 daemon pod, expected 1
    Jan  3 03:13:48.299: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan  3 03:13:48.299: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/03/23 03:13:48.301
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2793, will wait for the garbage collector to delete the pods 01/03/23 03:13:48.301
    Jan  3 03:13:48.355: INFO: Deleting DaemonSet.extensions daemon-set took: 2.813964ms
    Jan  3 03:13:48.456: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.963631ms
    Jan  3 03:13:50.659: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  3 03:13:50.659: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan  3 03:13:50.660: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1017528"},"items":null}

    Jan  3 03:13:50.662: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1017528"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan  3 03:13:50.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-2793" for this suite. 01/03/23 03:13:50.671
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:13:50.677
Jan  3 03:13:50.677: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename sched-preemption 01/03/23 03:13:50.678
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:13:50.685
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:13:50.686
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan  3 03:13:50.718: INFO: Waiting up to 1m0s for all nodes to be ready
Jan  3 03:14:50.778: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
STEP: Create pods that use 4/5 of node resources. 01/03/23 03:14:50.78
Jan  3 03:14:50.811: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan  3 03:14:50.834: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan  3 03:14:50.863: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan  3 03:14:50.868: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Jan  3 03:14:50.937: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Jan  3 03:14:50.948: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 01/03/23 03:14:50.948
Jan  3 03:14:50.948: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-8556" to be "running"
Jan  3 03:14:50.960: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 11.569998ms
Jan  3 03:14:52.974: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025945095s
Jan  3 03:14:54.986: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.037659092s
Jan  3 03:14:54.986: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jan  3 03:14:54.986: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-8556" to be "running"
Jan  3 03:14:54.987: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.559219ms
Jan  3 03:14:54.987: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jan  3 03:14:54.987: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-8556" to be "running"
Jan  3 03:14:54.989: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 1.559139ms
Jan  3 03:14:56.991: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003689521s
Jan  3 03:14:58.994: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006973279s
Jan  3 03:15:01.026: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.039133461s
Jan  3 03:15:01.026: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jan  3 03:15:01.027: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-8556" to be "running"
Jan  3 03:15:01.028: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.566657ms
Jan  3 03:15:01.028: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Jan  3 03:15:01.028: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-8556" to be "running"
Jan  3 03:15:01.117: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 89.001292ms
Jan  3 03:15:01.117: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Jan  3 03:15:01.117: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-8556" to be "running"
Jan  3 03:15:01.119: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.588384ms
Jan  3 03:15:01.119: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 01/03/23 03:15:01.119
Jan  3 03:15:01.126: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Jan  3 03:15:01.135: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.974692ms
Jan  3 03:15:03.138: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011781437s
Jan  3 03:15:05.137: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.011035447s
Jan  3 03:15:05.137: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jan  3 03:15:05.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-8556" for this suite. 01/03/23 03:15:05.193
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","completed":324,"skipped":6170,"failed":0}
------------------------------
• [SLOW TEST] [74.580 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:13:50.677
    Jan  3 03:13:50.677: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename sched-preemption 01/03/23 03:13:50.678
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:13:50.685
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:13:50.686
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jan  3 03:13:50.718: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan  3 03:14:50.778: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:218
    STEP: Create pods that use 4/5 of node resources. 01/03/23 03:14:50.78
    Jan  3 03:14:50.811: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jan  3 03:14:50.834: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jan  3 03:14:50.863: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jan  3 03:14:50.868: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Jan  3 03:14:50.937: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Jan  3 03:14:50.948: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 01/03/23 03:14:50.948
    Jan  3 03:14:50.948: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-8556" to be "running"
    Jan  3 03:14:50.960: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 11.569998ms
    Jan  3 03:14:52.974: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025945095s
    Jan  3 03:14:54.986: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.037659092s
    Jan  3 03:14:54.986: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jan  3 03:14:54.986: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-8556" to be "running"
    Jan  3 03:14:54.987: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.559219ms
    Jan  3 03:14:54.987: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan  3 03:14:54.987: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-8556" to be "running"
    Jan  3 03:14:54.989: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 1.559139ms
    Jan  3 03:14:56.991: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003689521s
    Jan  3 03:14:58.994: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006973279s
    Jan  3 03:15:01.026: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.039133461s
    Jan  3 03:15:01.026: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan  3 03:15:01.027: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-8556" to be "running"
    Jan  3 03:15:01.028: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.566657ms
    Jan  3 03:15:01.028: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan  3 03:15:01.028: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-8556" to be "running"
    Jan  3 03:15:01.117: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 89.001292ms
    Jan  3 03:15:01.117: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan  3 03:15:01.117: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-8556" to be "running"
    Jan  3 03:15:01.119: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.588384ms
    Jan  3 03:15:01.119: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 01/03/23 03:15:01.119
    Jan  3 03:15:01.126: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Jan  3 03:15:01.135: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.974692ms
    Jan  3 03:15:03.138: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011781437s
    Jan  3 03:15:05.137: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.011035447s
    Jan  3 03:15:05.137: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jan  3 03:15:05.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-8556" for this suite. 01/03/23 03:15:05.193
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:15:05.259
Jan  3 03:15:05.259: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename statefulset 01/03/23 03:15:05.261
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:15:05.27
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:15:05.272
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-440 01/03/23 03:15:05.274
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
STEP: Creating statefulset ss in namespace statefulset-440 01/03/23 03:15:05.311
Jan  3 03:15:05.338: INFO: Found 0 stateful pods, waiting for 1
Jan  3 03:15:15.341: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 01/03/23 03:15:15.344
STEP: updating a scale subresource 01/03/23 03:15:15.346
STEP: verifying the statefulset Spec.Replicas was modified 01/03/23 03:15:15.349
STEP: Patch a scale subresource 01/03/23 03:15:15.351
STEP: verifying the statefulset Spec.Replicas was modified 01/03/23 03:15:15.354
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan  3 03:15:15.366: INFO: Deleting all statefulset in ns statefulset-440
Jan  3 03:15:15.368: INFO: Scaling statefulset ss to 0
Jan  3 03:15:25.403: INFO: Waiting for statefulset status.replicas updated to 0
Jan  3 03:15:25.404: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan  3 03:15:25.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-440" for this suite. 01/03/23 03:15:25.414
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","completed":325,"skipped":6208,"failed":0}
------------------------------
• [SLOW TEST] [20.157 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:846

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:15:05.259
    Jan  3 03:15:05.259: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename statefulset 01/03/23 03:15:05.261
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:15:05.27
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:15:05.272
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-440 01/03/23 03:15:05.274
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:846
    STEP: Creating statefulset ss in namespace statefulset-440 01/03/23 03:15:05.311
    Jan  3 03:15:05.338: INFO: Found 0 stateful pods, waiting for 1
    Jan  3 03:15:15.341: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 01/03/23 03:15:15.344
    STEP: updating a scale subresource 01/03/23 03:15:15.346
    STEP: verifying the statefulset Spec.Replicas was modified 01/03/23 03:15:15.349
    STEP: Patch a scale subresource 01/03/23 03:15:15.351
    STEP: verifying the statefulset Spec.Replicas was modified 01/03/23 03:15:15.354
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan  3 03:15:15.366: INFO: Deleting all statefulset in ns statefulset-440
    Jan  3 03:15:15.368: INFO: Scaling statefulset ss to 0
    Jan  3 03:15:25.403: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  3 03:15:25.404: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan  3 03:15:25.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-440" for this suite. 01/03/23 03:15:25.414
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:15:25.417
Jan  3 03:15:25.417: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename daemonsets 01/03/23 03:15:25.419
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:15:25.465
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:15:25.467
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
STEP: Creating a simple DaemonSet "daemon-set" 01/03/23 03:15:25.489
STEP: Check that daemon pods launch on every node of the cluster. 01/03/23 03:15:25.493
Jan  3 03:15:25.511: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  3 03:15:25.511: INFO: Node cncf-master is running 0 daemon pod, expected 1
Jan  3 03:15:26.516: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  3 03:15:26.516: INFO: Node cncf-master is running 0 daemon pod, expected 1
Jan  3 03:15:27.517: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan  3 03:15:27.517: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 01/03/23 03:15:27.518
Jan  3 03:15:27.548: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan  3 03:15:27.548: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 01/03/23 03:15:27.548
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/03/23 03:15:27.588
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8202, will wait for the garbage collector to delete the pods 01/03/23 03:15:27.588
Jan  3 03:15:27.647: INFO: Deleting DaemonSet.extensions daemon-set took: 2.522212ms
Jan  3 03:15:27.748: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.075365ms
Jan  3 03:15:30.150: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  3 03:15:30.150: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan  3 03:15:30.152: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1018311"},"items":null}

Jan  3 03:15:30.153: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1018311"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan  3 03:15:30.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8202" for this suite. 01/03/23 03:15:30.163
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","completed":326,"skipped":6220,"failed":0}
------------------------------
• [4.748 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:15:25.417
    Jan  3 03:15:25.417: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename daemonsets 01/03/23 03:15:25.419
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:15:25.465
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:15:25.467
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:293
    STEP: Creating a simple DaemonSet "daemon-set" 01/03/23 03:15:25.489
    STEP: Check that daemon pods launch on every node of the cluster. 01/03/23 03:15:25.493
    Jan  3 03:15:25.511: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  3 03:15:25.511: INFO: Node cncf-master is running 0 daemon pod, expected 1
    Jan  3 03:15:26.516: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  3 03:15:26.516: INFO: Node cncf-master is running 0 daemon pod, expected 1
    Jan  3 03:15:27.517: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan  3 03:15:27.517: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 01/03/23 03:15:27.518
    Jan  3 03:15:27.548: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan  3 03:15:27.548: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 01/03/23 03:15:27.548
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/03/23 03:15:27.588
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8202, will wait for the garbage collector to delete the pods 01/03/23 03:15:27.588
    Jan  3 03:15:27.647: INFO: Deleting DaemonSet.extensions daemon-set took: 2.522212ms
    Jan  3 03:15:27.748: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.075365ms
    Jan  3 03:15:30.150: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  3 03:15:30.150: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan  3 03:15:30.152: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1018311"},"items":null}

    Jan  3 03:15:30.153: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1018311"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan  3 03:15:30.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-8202" for this suite. 01/03/23 03:15:30.163
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:15:30.168
Jan  3 03:15:30.168: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename security-context 01/03/23 03:15:30.168
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:15:30.177
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:15:30.179
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/03/23 03:15:30.183
Jan  3 03:15:30.200: INFO: Waiting up to 5m0s for pod "security-context-d89e52eb-8533-49fc-9efe-58281a28300f" in namespace "security-context-2254" to be "Succeeded or Failed"
Jan  3 03:15:30.236: INFO: Pod "security-context-d89e52eb-8533-49fc-9efe-58281a28300f": Phase="Pending", Reason="", readiness=false. Elapsed: 36.322513ms
Jan  3 03:15:32.259: INFO: Pod "security-context-d89e52eb-8533-49fc-9efe-58281a28300f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059069184s
Jan  3 03:15:34.239: INFO: Pod "security-context-d89e52eb-8533-49fc-9efe-58281a28300f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039176303s
STEP: Saw pod success 01/03/23 03:15:34.239
Jan  3 03:15:34.239: INFO: Pod "security-context-d89e52eb-8533-49fc-9efe-58281a28300f" satisfied condition "Succeeded or Failed"
Jan  3 03:15:34.241: INFO: Trying to get logs from node cncf-master pod security-context-d89e52eb-8533-49fc-9efe-58281a28300f container test-container: <nil>
STEP: delete the pod 01/03/23 03:15:34.252
Jan  3 03:15:34.311: INFO: Waiting for pod security-context-d89e52eb-8533-49fc-9efe-58281a28300f to disappear
Jan  3 03:15:34.315: INFO: Pod security-context-d89e52eb-8533-49fc-9efe-58281a28300f no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan  3 03:15:34.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-2254" for this suite. 01/03/23 03:15:34.318
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":327,"skipped":6283,"failed":0}
------------------------------
• [4.153 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:15:30.168
    Jan  3 03:15:30.168: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename security-context 01/03/23 03:15:30.168
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:15:30.177
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:15:30.179
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:97
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/03/23 03:15:30.183
    Jan  3 03:15:30.200: INFO: Waiting up to 5m0s for pod "security-context-d89e52eb-8533-49fc-9efe-58281a28300f" in namespace "security-context-2254" to be "Succeeded or Failed"
    Jan  3 03:15:30.236: INFO: Pod "security-context-d89e52eb-8533-49fc-9efe-58281a28300f": Phase="Pending", Reason="", readiness=false. Elapsed: 36.322513ms
    Jan  3 03:15:32.259: INFO: Pod "security-context-d89e52eb-8533-49fc-9efe-58281a28300f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059069184s
    Jan  3 03:15:34.239: INFO: Pod "security-context-d89e52eb-8533-49fc-9efe-58281a28300f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039176303s
    STEP: Saw pod success 01/03/23 03:15:34.239
    Jan  3 03:15:34.239: INFO: Pod "security-context-d89e52eb-8533-49fc-9efe-58281a28300f" satisfied condition "Succeeded or Failed"
    Jan  3 03:15:34.241: INFO: Trying to get logs from node cncf-master pod security-context-d89e52eb-8533-49fc-9efe-58281a28300f container test-container: <nil>
    STEP: delete the pod 01/03/23 03:15:34.252
    Jan  3 03:15:34.311: INFO: Waiting for pod security-context-d89e52eb-8533-49fc-9efe-58281a28300f to disappear
    Jan  3 03:15:34.315: INFO: Pod security-context-d89e52eb-8533-49fc-9efe-58281a28300f no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan  3 03:15:34.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-2254" for this suite. 01/03/23 03:15:34.318
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:15:34.32
Jan  3 03:15:34.320: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename security-context 01/03/23 03:15:34.321
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:15:34.343
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:15:34.345
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/03/23 03:15:34.347
Jan  3 03:15:34.352: INFO: Waiting up to 5m0s for pod "security-context-662cc351-ac14-47f3-bf3a-21f4f1cbc5f3" in namespace "security-context-9816" to be "Succeeded or Failed"
Jan  3 03:15:34.366: INFO: Pod "security-context-662cc351-ac14-47f3-bf3a-21f4f1cbc5f3": Phase="Pending", Reason="", readiness=false. Elapsed: 14.179159ms
Jan  3 03:15:36.369: INFO: Pod "security-context-662cc351-ac14-47f3-bf3a-21f4f1cbc5f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016847693s
Jan  3 03:15:38.369: INFO: Pod "security-context-662cc351-ac14-47f3-bf3a-21f4f1cbc5f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016988233s
STEP: Saw pod success 01/03/23 03:15:38.369
Jan  3 03:15:38.369: INFO: Pod "security-context-662cc351-ac14-47f3-bf3a-21f4f1cbc5f3" satisfied condition "Succeeded or Failed"
Jan  3 03:15:38.371: INFO: Trying to get logs from node cncf-master pod security-context-662cc351-ac14-47f3-bf3a-21f4f1cbc5f3 container test-container: <nil>
STEP: delete the pod 01/03/23 03:15:38.374
Jan  3 03:15:38.395: INFO: Waiting for pod security-context-662cc351-ac14-47f3-bf3a-21f4f1cbc5f3 to disappear
Jan  3 03:15:38.401: INFO: Pod security-context-662cc351-ac14-47f3-bf3a-21f4f1cbc5f3 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan  3 03:15:38.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-9816" for this suite. 01/03/23 03:15:38.404
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":328,"skipped":6284,"failed":0}
------------------------------
• [4.085 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:15:34.32
    Jan  3 03:15:34.320: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename security-context 01/03/23 03:15:34.321
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:15:34.343
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:15:34.345
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:132
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/03/23 03:15:34.347
    Jan  3 03:15:34.352: INFO: Waiting up to 5m0s for pod "security-context-662cc351-ac14-47f3-bf3a-21f4f1cbc5f3" in namespace "security-context-9816" to be "Succeeded or Failed"
    Jan  3 03:15:34.366: INFO: Pod "security-context-662cc351-ac14-47f3-bf3a-21f4f1cbc5f3": Phase="Pending", Reason="", readiness=false. Elapsed: 14.179159ms
    Jan  3 03:15:36.369: INFO: Pod "security-context-662cc351-ac14-47f3-bf3a-21f4f1cbc5f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016847693s
    Jan  3 03:15:38.369: INFO: Pod "security-context-662cc351-ac14-47f3-bf3a-21f4f1cbc5f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016988233s
    STEP: Saw pod success 01/03/23 03:15:38.369
    Jan  3 03:15:38.369: INFO: Pod "security-context-662cc351-ac14-47f3-bf3a-21f4f1cbc5f3" satisfied condition "Succeeded or Failed"
    Jan  3 03:15:38.371: INFO: Trying to get logs from node cncf-master pod security-context-662cc351-ac14-47f3-bf3a-21f4f1cbc5f3 container test-container: <nil>
    STEP: delete the pod 01/03/23 03:15:38.374
    Jan  3 03:15:38.395: INFO: Waiting for pod security-context-662cc351-ac14-47f3-bf3a-21f4f1cbc5f3 to disappear
    Jan  3 03:15:38.401: INFO: Pod security-context-662cc351-ac14-47f3-bf3a-21f4f1cbc5f3 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan  3 03:15:38.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-9816" for this suite. 01/03/23 03:15:38.404
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:15:38.406
Jan  3 03:15:38.406: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename projected 01/03/23 03:15:38.407
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:15:38.414
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:15:38.416
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
STEP: Creating a pod to test downward API volume plugin 01/03/23 03:15:38.431
Jan  3 03:15:38.462: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c9cf6713-d9f4-4ea4-8a22-a0144656918b" in namespace "projected-9958" to be "Succeeded or Failed"
Jan  3 03:15:38.482: INFO: Pod "downwardapi-volume-c9cf6713-d9f4-4ea4-8a22-a0144656918b": Phase="Pending", Reason="", readiness=false. Elapsed: 20.539308ms
Jan  3 03:15:40.486: INFO: Pod "downwardapi-volume-c9cf6713-d9f4-4ea4-8a22-a0144656918b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023617279s
Jan  3 03:15:42.486: INFO: Pod "downwardapi-volume-c9cf6713-d9f4-4ea4-8a22-a0144656918b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023807986s
STEP: Saw pod success 01/03/23 03:15:42.486
Jan  3 03:15:42.486: INFO: Pod "downwardapi-volume-c9cf6713-d9f4-4ea4-8a22-a0144656918b" satisfied condition "Succeeded or Failed"
Jan  3 03:15:42.487: INFO: Trying to get logs from node cncf-master pod downwardapi-volume-c9cf6713-d9f4-4ea4-8a22-a0144656918b container client-container: <nil>
STEP: delete the pod 01/03/23 03:15:42.49
Jan  3 03:15:42.502: INFO: Waiting for pod downwardapi-volume-c9cf6713-d9f4-4ea4-8a22-a0144656918b to disappear
Jan  3 03:15:42.504: INFO: Pod downwardapi-volume-c9cf6713-d9f4-4ea4-8a22-a0144656918b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan  3 03:15:42.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9958" for this suite. 01/03/23 03:15:42.507
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","completed":329,"skipped":6290,"failed":0}
------------------------------
• [4.103 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:15:38.406
    Jan  3 03:15:38.406: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename projected 01/03/23 03:15:38.407
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:15:38.414
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:15:38.416
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:52
    STEP: Creating a pod to test downward API volume plugin 01/03/23 03:15:38.431
    Jan  3 03:15:38.462: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c9cf6713-d9f4-4ea4-8a22-a0144656918b" in namespace "projected-9958" to be "Succeeded or Failed"
    Jan  3 03:15:38.482: INFO: Pod "downwardapi-volume-c9cf6713-d9f4-4ea4-8a22-a0144656918b": Phase="Pending", Reason="", readiness=false. Elapsed: 20.539308ms
    Jan  3 03:15:40.486: INFO: Pod "downwardapi-volume-c9cf6713-d9f4-4ea4-8a22-a0144656918b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023617279s
    Jan  3 03:15:42.486: INFO: Pod "downwardapi-volume-c9cf6713-d9f4-4ea4-8a22-a0144656918b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023807986s
    STEP: Saw pod success 01/03/23 03:15:42.486
    Jan  3 03:15:42.486: INFO: Pod "downwardapi-volume-c9cf6713-d9f4-4ea4-8a22-a0144656918b" satisfied condition "Succeeded or Failed"
    Jan  3 03:15:42.487: INFO: Trying to get logs from node cncf-master pod downwardapi-volume-c9cf6713-d9f4-4ea4-8a22-a0144656918b container client-container: <nil>
    STEP: delete the pod 01/03/23 03:15:42.49
    Jan  3 03:15:42.502: INFO: Waiting for pod downwardapi-volume-c9cf6713-d9f4-4ea4-8a22-a0144656918b to disappear
    Jan  3 03:15:42.504: INFO: Pod downwardapi-volume-c9cf6713-d9f4-4ea4-8a22-a0144656918b no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan  3 03:15:42.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9958" for this suite. 01/03/23 03:15:42.507
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:15:42.51
Jan  3 03:15:42.510: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename watch 01/03/23 03:15:42.511
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:15:42.522
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:15:42.524
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 01/03/23 03:15:42.526
STEP: creating a watch on configmaps with label B 01/03/23 03:15:42.527
STEP: creating a watch on configmaps with label A or B 01/03/23 03:15:42.528
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 01/03/23 03:15:42.529
Jan  3 03:15:42.536: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4238  4d401f52-32f3-4e89-8d0c-db1a183c15df 1018492 0 2023-01-03 03:15:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[createdTime:2023-01-03T12:15:42.530463708+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T12:15:42.530463708+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update v1 2023-01-03 03:15:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  3 03:15:42.536: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4238  4d401f52-32f3-4e89-8d0c-db1a183c15df 1018492 0 2023-01-03 03:15:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[createdTime:2023-01-03T12:15:42.530463708+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T12:15:42.530463708+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update v1 2023-01-03 03:15:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 01/03/23 03:15:42.536
Jan  3 03:15:42.541: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4238  4d401f52-32f3-4e89-8d0c-db1a183c15df 1018493 0 2023-01-03 03:15:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[createdTime:2023-01-03T12:15:42.530463708+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T12:15:42.530463708+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update v1 2023-01-03 03:15:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  3 03:15:42.541: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4238  4d401f52-32f3-4e89-8d0c-db1a183c15df 1018493 0 2023-01-03 03:15:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[createdTime:2023-01-03T12:15:42.530463708+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T12:15:42.530463708+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update v1 2023-01-03 03:15:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 01/03/23 03:15:42.541
Jan  3 03:15:42.547: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4238  4d401f52-32f3-4e89-8d0c-db1a183c15df 1018494 0 2023-01-03 03:15:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[createdTime:2023-01-03T12:15:42.530463708+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T12:15:42.530463708+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update v1 2023-01-03 03:15:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  3 03:15:42.547: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4238  4d401f52-32f3-4e89-8d0c-db1a183c15df 1018494 0 2023-01-03 03:15:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[createdTime:2023-01-03T12:15:42.530463708+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T12:15:42.530463708+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update v1 2023-01-03 03:15:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 01/03/23 03:15:42.547
Jan  3 03:15:42.551: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4238  4d401f52-32f3-4e89-8d0c-db1a183c15df 1018495 0 2023-01-03 03:15:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[createdTime:2023-01-03T12:15:42.530463708+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T12:15:42.530463708+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update v1 2023-01-03 03:15:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  3 03:15:42.551: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4238  4d401f52-32f3-4e89-8d0c-db1a183c15df 1018495 0 2023-01-03 03:15:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[createdTime:2023-01-03T12:15:42.530463708+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T12:15:42.530463708+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update v1 2023-01-03 03:15:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 01/03/23 03:15:42.551
Jan  3 03:15:42.565: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4238  2cbbfa50-beda-451e-a83a-6ed9a0031f93 1018496 0 2023-01-03 03:15:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[createdTime:2023-01-03T12:15:42.552512367+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T12:15:42.552512367+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update v1 2023-01-03 03:15:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  3 03:15:42.565: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4238  2cbbfa50-beda-451e-a83a-6ed9a0031f93 1018496 0 2023-01-03 03:15:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[createdTime:2023-01-03T12:15:42.552512367+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T12:15:42.552512367+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update v1 2023-01-03 03:15:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 01/03/23 03:15:52.565
Jan  3 03:15:52.569: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4238  2cbbfa50-beda-451e-a83a-6ed9a0031f93 1018553 0 2023-01-03 03:15:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[createdTime:2023-01-03T12:15:42.552512367+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T12:15:42.552512367+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update v1 2023-01-03 03:15:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  3 03:15:52.569: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4238  2cbbfa50-beda-451e-a83a-6ed9a0031f93 1018553 0 2023-01-03 03:15:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[createdTime:2023-01-03T12:15:42.552512367+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T12:15:42.552512367+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update v1 2023-01-03 03:15:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan  3 03:16:02.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4238" for this suite. 01/03/23 03:16:02.574
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","completed":330,"skipped":6304,"failed":0}
------------------------------
• [SLOW TEST] [20.067 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:15:42.51
    Jan  3 03:15:42.510: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename watch 01/03/23 03:15:42.511
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:15:42.522
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:15:42.524
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 01/03/23 03:15:42.526
    STEP: creating a watch on configmaps with label B 01/03/23 03:15:42.527
    STEP: creating a watch on configmaps with label A or B 01/03/23 03:15:42.528
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 01/03/23 03:15:42.529
    Jan  3 03:15:42.536: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4238  4d401f52-32f3-4e89-8d0c-db1a183c15df 1018492 0 2023-01-03 03:15:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[createdTime:2023-01-03T12:15:42.530463708+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T12:15:42.530463708+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update v1 2023-01-03 03:15:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  3 03:15:42.536: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4238  4d401f52-32f3-4e89-8d0c-db1a183c15df 1018492 0 2023-01-03 03:15:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[createdTime:2023-01-03T12:15:42.530463708+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T12:15:42.530463708+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update v1 2023-01-03 03:15:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 01/03/23 03:15:42.536
    Jan  3 03:15:42.541: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4238  4d401f52-32f3-4e89-8d0c-db1a183c15df 1018493 0 2023-01-03 03:15:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[createdTime:2023-01-03T12:15:42.530463708+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T12:15:42.530463708+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update v1 2023-01-03 03:15:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  3 03:15:42.541: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4238  4d401f52-32f3-4e89-8d0c-db1a183c15df 1018493 0 2023-01-03 03:15:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[createdTime:2023-01-03T12:15:42.530463708+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T12:15:42.530463708+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update v1 2023-01-03 03:15:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 01/03/23 03:15:42.541
    Jan  3 03:15:42.547: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4238  4d401f52-32f3-4e89-8d0c-db1a183c15df 1018494 0 2023-01-03 03:15:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[createdTime:2023-01-03T12:15:42.530463708+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T12:15:42.530463708+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update v1 2023-01-03 03:15:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  3 03:15:42.547: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4238  4d401f52-32f3-4e89-8d0c-db1a183c15df 1018494 0 2023-01-03 03:15:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[createdTime:2023-01-03T12:15:42.530463708+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T12:15:42.530463708+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update v1 2023-01-03 03:15:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 01/03/23 03:15:42.547
    Jan  3 03:15:42.551: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4238  4d401f52-32f3-4e89-8d0c-db1a183c15df 1018495 0 2023-01-03 03:15:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[createdTime:2023-01-03T12:15:42.530463708+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T12:15:42.530463708+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update v1 2023-01-03 03:15:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  3 03:15:42.551: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4238  4d401f52-32f3-4e89-8d0c-db1a183c15df 1018495 0 2023-01-03 03:15:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[createdTime:2023-01-03T12:15:42.530463708+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T12:15:42.530463708+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update v1 2023-01-03 03:15:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 01/03/23 03:15:42.551
    Jan  3 03:15:42.565: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4238  2cbbfa50-beda-451e-a83a-6ed9a0031f93 1018496 0 2023-01-03 03:15:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[createdTime:2023-01-03T12:15:42.552512367+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T12:15:42.552512367+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update v1 2023-01-03 03:15:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  3 03:15:42.565: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4238  2cbbfa50-beda-451e-a83a-6ed9a0031f93 1018496 0 2023-01-03 03:15:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[createdTime:2023-01-03T12:15:42.552512367+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T12:15:42.552512367+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update v1 2023-01-03 03:15:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 01/03/23 03:15:52.565
    Jan  3 03:15:52.569: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4238  2cbbfa50-beda-451e-a83a-6ed9a0031f93 1018553 0 2023-01-03 03:15:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[createdTime:2023-01-03T12:15:42.552512367+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T12:15:42.552512367+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update v1 2023-01-03 03:15:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  3 03:15:52.569: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4238  2cbbfa50-beda-451e-a83a-6ed9a0031f93 1018553 0 2023-01-03 03:15:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[createdTime:2023-01-03T12:15:42.552512367+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2023-01-03T12:15:42.552512367+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] [] [{e2e.test Update v1 2023-01-03 03:15:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan  3 03:16:02.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-4238" for this suite. 01/03/23 03:16:02.574
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:16:02.578
Jan  3 03:16:02.578: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename kubectl 01/03/23 03:16:02.579
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:16:02.589
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:16:02.591
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
STEP: Starting the proxy 01/03/23 03:16:02.593
Jan  3 03:16:02.593: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-5411 proxy --unix-socket=/tmp/kubectl-proxy-unix1431827853/test'
STEP: retrieving proxy /api/ output 01/03/23 03:16:02.634
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan  3 03:16:02.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5411" for this suite. 01/03/23 03:16:02.637
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","completed":331,"skipped":6316,"failed":0}
------------------------------
• [0.062 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:16:02.578
    Jan  3 03:16:02.578: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename kubectl 01/03/23 03:16:02.579
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:16:02.589
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:16:02.591
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1810
    STEP: Starting the proxy 01/03/23 03:16:02.593
    Jan  3 03:16:02.593: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-5411 proxy --unix-socket=/tmp/kubectl-proxy-unix1431827853/test'
    STEP: retrieving proxy /api/ output 01/03/23 03:16:02.634
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan  3 03:16:02.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5411" for this suite. 01/03/23 03:16:02.637
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:16:02.641
Jan  3 03:16:02.641: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename endpointslicemirroring 01/03/23 03:16:02.642
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:16:02.654
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:16:02.655
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 01/03/23 03:16:02.699
Jan  3 03:16:02.715: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 01/03/23 03:16:04.717
Jan  3 03:16:04.727: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 01/03/23 03:16:06.729
Jan  3 03:16:06.744: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:187
Jan  3 03:16:08.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-5256" for this suite. 01/03/23 03:16:08.749
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","completed":332,"skipped":6352,"failed":0}
------------------------------
• [SLOW TEST] [6.111 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:16:02.641
    Jan  3 03:16:02.641: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename endpointslicemirroring 01/03/23 03:16:02.642
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:16:02.654
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:16:02.655
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 01/03/23 03:16:02.699
    Jan  3 03:16:02.715: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 01/03/23 03:16:04.717
    Jan  3 03:16:04.727: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 01/03/23 03:16:06.729
    Jan  3 03:16:06.744: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:187
    Jan  3 03:16:08.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslicemirroring-5256" for this suite. 01/03/23 03:16:08.749
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:16:08.753
Jan  3 03:16:08.753: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename job 01/03/23 03:16:08.754
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:16:08.779
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:16:08.781
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
STEP: Creating Indexed job 01/03/23 03:16:08.783
STEP: Ensuring job reaches completions 01/03/23 03:16:08.818
STEP: Ensuring pods with index for job exist 01/03/23 03:16:18.821
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan  3 03:16:18.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5502" for this suite. 01/03/23 03:16:18.827
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]","completed":333,"skipped":6383,"failed":0}
------------------------------
• [SLOW TEST] [10.077 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:16:08.753
    Jan  3 03:16:08.753: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename job 01/03/23 03:16:08.754
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:16:08.779
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:16:08.781
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:194
    STEP: Creating Indexed job 01/03/23 03:16:08.783
    STEP: Ensuring job reaches completions 01/03/23 03:16:08.818
    STEP: Ensuring pods with index for job exist 01/03/23 03:16:18.821
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan  3 03:16:18.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-5502" for this suite. 01/03/23 03:16:18.827
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:16:18.83
Jan  3 03:16:18.831: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename services 01/03/23 03:16:18.831
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:16:18.855
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:16:18.858
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2212 01/03/23 03:16:18.86
STEP: changing the ExternalName service to type=NodePort 01/03/23 03:16:18.873
STEP: creating replication controller externalname-service in namespace services-2212 01/03/23 03:16:18.924
I0103 03:16:18.930444      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-2212, replica count: 2
I0103 03:16:21.981795      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  3 03:16:21.981: INFO: Creating new exec pod
Jan  3 03:16:21.986: INFO: Waiting up to 5m0s for pod "execpodwjpd7" in namespace "services-2212" to be "running"
Jan  3 03:16:21.993: INFO: Pod "execpodwjpd7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.038562ms
Jan  3 03:16:23.999: INFO: Pod "execpodwjpd7": Phase="Running", Reason="", readiness=true. Elapsed: 2.013038662s
Jan  3 03:16:23.999: INFO: Pod "execpodwjpd7" satisfied condition "running"
Jan  3 03:16:25.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-2212 exec execpodwjpd7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan  3 03:16:25.132: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan  3 03:16:25.132: INFO: stdout: "externalname-service-g6js7"
Jan  3 03:16:25.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-2212 exec execpodwjpd7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.67.155 80'
Jan  3 03:16:25.267: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.67.155 80\nConnection to 10.96.67.155 80 port [tcp/http] succeeded!\n"
Jan  3 03:16:25.267: INFO: stdout: "externalname-service-g6js7"
Jan  3 03:16:25.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-2212 exec execpodwjpd7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.7.7 30028'
Jan  3 03:16:25.405: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.7.7 30028\nConnection to 172.21.7.7 30028 port [tcp/*] succeeded!\n"
Jan  3 03:16:25.405: INFO: stdout: ""
Jan  3 03:16:26.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-2212 exec execpodwjpd7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.7.7 30028'
Jan  3 03:16:26.539: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.7.7 30028\nConnection to 172.21.7.7 30028 port [tcp/*] succeeded!\n"
Jan  3 03:16:26.539: INFO: stdout: "externalname-service-g6js7"
Jan  3 03:16:26.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-2212 exec execpodwjpd7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.7.13 30028'
Jan  3 03:16:26.679: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.7.13 30028\nConnection to 172.21.7.13 30028 port [tcp/*] succeeded!\n"
Jan  3 03:16:26.679: INFO: stdout: ""
Jan  3 03:16:27.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-2212 exec execpodwjpd7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.7.13 30028'
Jan  3 03:16:27.813: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.7.13 30028\nConnection to 172.21.7.13 30028 port [tcp/*] succeeded!\n"
Jan  3 03:16:27.813: INFO: stdout: "externalname-service-g6js7"
Jan  3 03:16:27.813: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  3 03:16:27.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2212" for this suite. 01/03/23 03:16:27.875
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","completed":334,"skipped":6407,"failed":0}
------------------------------
• [SLOW TEST] [9.047 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:16:18.83
    Jan  3 03:16:18.831: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename services 01/03/23 03:16:18.831
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:16:18.855
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:16:18.858
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1443
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-2212 01/03/23 03:16:18.86
    STEP: changing the ExternalName service to type=NodePort 01/03/23 03:16:18.873
    STEP: creating replication controller externalname-service in namespace services-2212 01/03/23 03:16:18.924
    I0103 03:16:18.930444      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-2212, replica count: 2
    I0103 03:16:21.981795      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  3 03:16:21.981: INFO: Creating new exec pod
    Jan  3 03:16:21.986: INFO: Waiting up to 5m0s for pod "execpodwjpd7" in namespace "services-2212" to be "running"
    Jan  3 03:16:21.993: INFO: Pod "execpodwjpd7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.038562ms
    Jan  3 03:16:23.999: INFO: Pod "execpodwjpd7": Phase="Running", Reason="", readiness=true. Elapsed: 2.013038662s
    Jan  3 03:16:23.999: INFO: Pod "execpodwjpd7" satisfied condition "running"
    Jan  3 03:16:25.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-2212 exec execpodwjpd7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jan  3 03:16:25.132: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan  3 03:16:25.132: INFO: stdout: "externalname-service-g6js7"
    Jan  3 03:16:25.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-2212 exec execpodwjpd7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.67.155 80'
    Jan  3 03:16:25.267: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.67.155 80\nConnection to 10.96.67.155 80 port [tcp/http] succeeded!\n"
    Jan  3 03:16:25.267: INFO: stdout: "externalname-service-g6js7"
    Jan  3 03:16:25.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-2212 exec execpodwjpd7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.7.7 30028'
    Jan  3 03:16:25.405: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.7.7 30028\nConnection to 172.21.7.7 30028 port [tcp/*] succeeded!\n"
    Jan  3 03:16:25.405: INFO: stdout: ""
    Jan  3 03:16:26.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-2212 exec execpodwjpd7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.7.7 30028'
    Jan  3 03:16:26.539: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.7.7 30028\nConnection to 172.21.7.7 30028 port [tcp/*] succeeded!\n"
    Jan  3 03:16:26.539: INFO: stdout: "externalname-service-g6js7"
    Jan  3 03:16:26.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-2212 exec execpodwjpd7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.7.13 30028'
    Jan  3 03:16:26.679: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.7.13 30028\nConnection to 172.21.7.13 30028 port [tcp/*] succeeded!\n"
    Jan  3 03:16:26.679: INFO: stdout: ""
    Jan  3 03:16:27.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-2212 exec execpodwjpd7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.7.13 30028'
    Jan  3 03:16:27.813: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.7.13 30028\nConnection to 172.21.7.13 30028 port [tcp/*] succeeded!\n"
    Jan  3 03:16:27.813: INFO: stdout: "externalname-service-g6js7"
    Jan  3 03:16:27.813: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  3 03:16:27.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2212" for this suite. 01/03/23 03:16:27.875
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:16:27.878
Jan  3 03:16:27.878: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename emptydir 01/03/23 03:16:27.879
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:16:27.886
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:16:27.888
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
STEP: Creating Pod 01/03/23 03:16:27.891
Jan  3 03:16:27.914: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-4d4ce9e0-ac84-4418-a12a-937f041f8c34" in namespace "emptydir-8222" to be "running"
Jan  3 03:16:27.978: INFO: Pod "pod-sharedvolume-4d4ce9e0-ac84-4418-a12a-937f041f8c34": Phase="Pending", Reason="", readiness=false. Elapsed: 64.144118ms
Jan  3 03:16:29.984: INFO: Pod "pod-sharedvolume-4d4ce9e0-ac84-4418-a12a-937f041f8c34": Phase="Running", Reason="", readiness=false. Elapsed: 2.069651242s
Jan  3 03:16:29.984: INFO: Pod "pod-sharedvolume-4d4ce9e0-ac84-4418-a12a-937f041f8c34" satisfied condition "running"
STEP: Reading file content from the nginx-container 01/03/23 03:16:29.984
Jan  3 03:16:29.984: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-8222 PodName:pod-sharedvolume-4d4ce9e0-ac84-4418-a12a-937f041f8c34 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 03:16:29.984: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
Jan  3 03:16:29.984: INFO: ExecWithOptions: Clientset creation
Jan  3 03:16:29.984: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-8222/pods/pod-sharedvolume-4d4ce9e0-ac84-4418-a12a-937f041f8c34/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Jan  3 03:16:30.052: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan  3 03:16:30.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8222" for this suite. 01/03/23 03:16:30.055
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","completed":335,"skipped":6420,"failed":0}
------------------------------
• [2.180 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:16:27.878
    Jan  3 03:16:27.878: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename emptydir 01/03/23 03:16:27.879
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:16:27.886
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:16:27.888
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:226
    STEP: Creating Pod 01/03/23 03:16:27.891
    Jan  3 03:16:27.914: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-4d4ce9e0-ac84-4418-a12a-937f041f8c34" in namespace "emptydir-8222" to be "running"
    Jan  3 03:16:27.978: INFO: Pod "pod-sharedvolume-4d4ce9e0-ac84-4418-a12a-937f041f8c34": Phase="Pending", Reason="", readiness=false. Elapsed: 64.144118ms
    Jan  3 03:16:29.984: INFO: Pod "pod-sharedvolume-4d4ce9e0-ac84-4418-a12a-937f041f8c34": Phase="Running", Reason="", readiness=false. Elapsed: 2.069651242s
    Jan  3 03:16:29.984: INFO: Pod "pod-sharedvolume-4d4ce9e0-ac84-4418-a12a-937f041f8c34" satisfied condition "running"
    STEP: Reading file content from the nginx-container 01/03/23 03:16:29.984
    Jan  3 03:16:29.984: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-8222 PodName:pod-sharedvolume-4d4ce9e0-ac84-4418-a12a-937f041f8c34 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 03:16:29.984: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    Jan  3 03:16:29.984: INFO: ExecWithOptions: Clientset creation
    Jan  3 03:16:29.984: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-8222/pods/pod-sharedvolume-4d4ce9e0-ac84-4418-a12a-937f041f8c34/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Jan  3 03:16:30.052: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan  3 03:16:30.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8222" for this suite. 01/03/23 03:16:30.055
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:16:30.058
Jan  3 03:16:30.058: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename security-context-test 01/03/23 03:16:30.059
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:16:30.068
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:16:30.07
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
Jan  3 03:16:30.115: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-aa7ceaed-9f8c-4d8e-8b23-6c20ad8d0f3d" in namespace "security-context-test-6556" to be "Succeeded or Failed"
Jan  3 03:16:30.122: INFO: Pod "busybox-privileged-false-aa7ceaed-9f8c-4d8e-8b23-6c20ad8d0f3d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.052072ms
Jan  3 03:16:32.125: INFO: Pod "busybox-privileged-false-aa7ceaed-9f8c-4d8e-8b23-6c20ad8d0f3d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0097214s
Jan  3 03:16:34.125: INFO: Pod "busybox-privileged-false-aa7ceaed-9f8c-4d8e-8b23-6c20ad8d0f3d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010179681s
Jan  3 03:16:34.125: INFO: Pod "busybox-privileged-false-aa7ceaed-9f8c-4d8e-8b23-6c20ad8d0f3d" satisfied condition "Succeeded or Failed"
Jan  3 03:16:34.128: INFO: Got logs for pod "busybox-privileged-false-aa7ceaed-9f8c-4d8e-8b23-6c20ad8d0f3d": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan  3 03:16:34.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6556" for this suite. 01/03/23 03:16:34.131
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","completed":336,"skipped":6422,"failed":0}
------------------------------
• [4.075 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:490
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:527

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:16:30.058
    Jan  3 03:16:30.058: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename security-context-test 01/03/23 03:16:30.059
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:16:30.068
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:16:30.07
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:527
    Jan  3 03:16:30.115: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-aa7ceaed-9f8c-4d8e-8b23-6c20ad8d0f3d" in namespace "security-context-test-6556" to be "Succeeded or Failed"
    Jan  3 03:16:30.122: INFO: Pod "busybox-privileged-false-aa7ceaed-9f8c-4d8e-8b23-6c20ad8d0f3d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.052072ms
    Jan  3 03:16:32.125: INFO: Pod "busybox-privileged-false-aa7ceaed-9f8c-4d8e-8b23-6c20ad8d0f3d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0097214s
    Jan  3 03:16:34.125: INFO: Pod "busybox-privileged-false-aa7ceaed-9f8c-4d8e-8b23-6c20ad8d0f3d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010179681s
    Jan  3 03:16:34.125: INFO: Pod "busybox-privileged-false-aa7ceaed-9f8c-4d8e-8b23-6c20ad8d0f3d" satisfied condition "Succeeded or Failed"
    Jan  3 03:16:34.128: INFO: Got logs for pod "busybox-privileged-false-aa7ceaed-9f8c-4d8e-8b23-6c20ad8d0f3d": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan  3 03:16:34.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-6556" for this suite. 01/03/23 03:16:34.131
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:16:34.134
Jan  3 03:16:34.134: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename custom-resource-definition 01/03/23 03:16:34.135
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:16:34.143
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:16:34.145
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 01/03/23 03:16:34.147
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 01/03/23 03:16:34.148
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 01/03/23 03:16:34.148
STEP: fetching the /apis/apiextensions.k8s.io discovery document 01/03/23 03:16:34.148
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 01/03/23 03:16:34.149
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 01/03/23 03:16:34.149
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 01/03/23 03:16:34.15
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  3 03:16:34.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7757" for this suite. 01/03/23 03:16:34.156
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","completed":337,"skipped":6437,"failed":0}
------------------------------
• [0.025 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:16:34.134
    Jan  3 03:16:34.134: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename custom-resource-definition 01/03/23 03:16:34.135
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:16:34.143
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:16:34.145
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 01/03/23 03:16:34.147
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 01/03/23 03:16:34.148
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 01/03/23 03:16:34.148
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 01/03/23 03:16:34.148
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 01/03/23 03:16:34.149
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 01/03/23 03:16:34.149
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 01/03/23 03:16:34.15
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  3 03:16:34.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-7757" for this suite. 01/03/23 03:16:34.156
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:16:34.159
Jan  3 03:16:34.159: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename gc 01/03/23 03:16:34.16
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:16:34.167
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:16:34.169
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 01/03/23 03:16:34.19
STEP: create the rc2 01/03/23 03:16:34.194
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 01/03/23 03:16:39.221
STEP: delete the rc simpletest-rc-to-be-deleted 01/03/23 03:16:40.59
STEP: wait for the rc to be deleted 01/03/23 03:16:40.608
Jan  3 03:16:45.620: INFO: 68 pods remaining
Jan  3 03:16:45.620: INFO: 68 pods has nil DeletionTimestamp
Jan  3 03:16:45.620: INFO: 
STEP: Gathering metrics 01/03/23 03:16:50.621
Jan  3 03:16:50.628: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cncf-master" in namespace "kube-system" to be "running and ready"
Jan  3 03:16:50.630: INFO: Pod "kube-controller-manager-cncf-master": Phase="Running", Reason="", readiness=true. Elapsed: 1.574471ms
Jan  3 03:16:50.630: INFO: The phase of Pod kube-controller-manager-cncf-master is Running (Ready = true)
Jan  3 03:16:50.630: INFO: Pod "kube-controller-manager-cncf-master" satisfied condition "running and ready"
Jan  3 03:16:50.685: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan  3 03:16:50.685: INFO: Deleting pod "simpletest-rc-to-be-deleted-256kz" in namespace "gc-951"
Jan  3 03:16:50.760: INFO: Deleting pod "simpletest-rc-to-be-deleted-294mc" in namespace "gc-951"
Jan  3 03:16:50.794: INFO: Deleting pod "simpletest-rc-to-be-deleted-2f8zl" in namespace "gc-951"
Jan  3 03:16:50.823: INFO: Deleting pod "simpletest-rc-to-be-deleted-4hrmn" in namespace "gc-951"
Jan  3 03:16:50.875: INFO: Deleting pod "simpletest-rc-to-be-deleted-4xhp4" in namespace "gc-951"
Jan  3 03:16:50.903: INFO: Deleting pod "simpletest-rc-to-be-deleted-56gz5" in namespace "gc-951"
Jan  3 03:16:50.937: INFO: Deleting pod "simpletest-rc-to-be-deleted-5f26s" in namespace "gc-951"
Jan  3 03:16:51.001: INFO: Deleting pod "simpletest-rc-to-be-deleted-5p6jm" in namespace "gc-951"
Jan  3 03:16:51.023: INFO: Deleting pod "simpletest-rc-to-be-deleted-62wmm" in namespace "gc-951"
Jan  3 03:16:51.040: INFO: Deleting pod "simpletest-rc-to-be-deleted-6dk5h" in namespace "gc-951"
Jan  3 03:16:51.080: INFO: Deleting pod "simpletest-rc-to-be-deleted-6f25r" in namespace "gc-951"
Jan  3 03:16:51.196: INFO: Deleting pod "simpletest-rc-to-be-deleted-6mwww" in namespace "gc-951"
Jan  3 03:16:51.282: INFO: Deleting pod "simpletest-rc-to-be-deleted-74jst" in namespace "gc-951"
Jan  3 03:16:51.321: INFO: Deleting pod "simpletest-rc-to-be-deleted-75lbp" in namespace "gc-951"
Jan  3 03:16:51.362: INFO: Deleting pod "simpletest-rc-to-be-deleted-777kr" in namespace "gc-951"
Jan  3 03:16:51.454: INFO: Deleting pod "simpletest-rc-to-be-deleted-788sl" in namespace "gc-951"
Jan  3 03:16:51.494: INFO: Deleting pod "simpletest-rc-to-be-deleted-7hwz4" in namespace "gc-951"
Jan  3 03:16:51.569: INFO: Deleting pod "simpletest-rc-to-be-deleted-7q9z8" in namespace "gc-951"
Jan  3 03:16:51.607: INFO: Deleting pod "simpletest-rc-to-be-deleted-7t4w2" in namespace "gc-951"
Jan  3 03:16:51.630: INFO: Deleting pod "simpletest-rc-to-be-deleted-852fh" in namespace "gc-951"
Jan  3 03:16:51.685: INFO: Deleting pod "simpletest-rc-to-be-deleted-8b4s6" in namespace "gc-951"
Jan  3 03:16:51.707: INFO: Deleting pod "simpletest-rc-to-be-deleted-8n9tt" in namespace "gc-951"
Jan  3 03:16:51.718: INFO: Deleting pod "simpletest-rc-to-be-deleted-8rbqf" in namespace "gc-951"
Jan  3 03:16:51.741: INFO: Deleting pod "simpletest-rc-to-be-deleted-95xlb" in namespace "gc-951"
Jan  3 03:16:51.812: INFO: Deleting pod "simpletest-rc-to-be-deleted-97qnt" in namespace "gc-951"
Jan  3 03:16:51.839: INFO: Deleting pod "simpletest-rc-to-be-deleted-9gq8s" in namespace "gc-951"
Jan  3 03:16:51.868: INFO: Deleting pod "simpletest-rc-to-be-deleted-9qmjh" in namespace "gc-951"
Jan  3 03:16:51.896: INFO: Deleting pod "simpletest-rc-to-be-deleted-9rdgn" in namespace "gc-951"
Jan  3 03:16:51.966: INFO: Deleting pod "simpletest-rc-to-be-deleted-9zfgq" in namespace "gc-951"
Jan  3 03:16:51.994: INFO: Deleting pod "simpletest-rc-to-be-deleted-bc4k8" in namespace "gc-951"
Jan  3 03:16:52.031: INFO: Deleting pod "simpletest-rc-to-be-deleted-bkqgz" in namespace "gc-951"
Jan  3 03:16:52.097: INFO: Deleting pod "simpletest-rc-to-be-deleted-bwhg2" in namespace "gc-951"
Jan  3 03:16:52.120: INFO: Deleting pod "simpletest-rc-to-be-deleted-c95vh" in namespace "gc-951"
Jan  3 03:16:52.137: INFO: Deleting pod "simpletest-rc-to-be-deleted-d4l8l" in namespace "gc-951"
Jan  3 03:16:52.160: INFO: Deleting pod "simpletest-rc-to-be-deleted-d7tlr" in namespace "gc-951"
Jan  3 03:16:52.180: INFO: Deleting pod "simpletest-rc-to-be-deleted-d8wlq" in namespace "gc-951"
Jan  3 03:16:52.264: INFO: Deleting pod "simpletest-rc-to-be-deleted-dkvkf" in namespace "gc-951"
Jan  3 03:16:52.304: INFO: Deleting pod "simpletest-rc-to-be-deleted-dlxcs" in namespace "gc-951"
Jan  3 03:16:52.442: INFO: Deleting pod "simpletest-rc-to-be-deleted-dwk6x" in namespace "gc-951"
Jan  3 03:16:52.489: INFO: Deleting pod "simpletest-rc-to-be-deleted-f25r7" in namespace "gc-951"
Jan  3 03:16:52.517: INFO: Deleting pod "simpletest-rc-to-be-deleted-fjzb7" in namespace "gc-951"
Jan  3 03:16:52.556: INFO: Deleting pod "simpletest-rc-to-be-deleted-gb5hw" in namespace "gc-951"
Jan  3 03:16:52.580: INFO: Deleting pod "simpletest-rc-to-be-deleted-gqgng" in namespace "gc-951"
Jan  3 03:16:52.654: INFO: Deleting pod "simpletest-rc-to-be-deleted-gsj9x" in namespace "gc-951"
Jan  3 03:16:52.755: INFO: Deleting pod "simpletest-rc-to-be-deleted-gwtkr" in namespace "gc-951"
Jan  3 03:16:52.810: INFO: Deleting pod "simpletest-rc-to-be-deleted-h69fv" in namespace "gc-951"
Jan  3 03:16:52.880: INFO: Deleting pod "simpletest-rc-to-be-deleted-h858l" in namespace "gc-951"
Jan  3 03:16:52.896: INFO: Deleting pod "simpletest-rc-to-be-deleted-hhxtr" in namespace "gc-951"
Jan  3 03:16:52.930: INFO: Deleting pod "simpletest-rc-to-be-deleted-hr9pw" in namespace "gc-951"
Jan  3 03:16:53.035: INFO: Deleting pod "simpletest-rc-to-be-deleted-hv7qf" in namespace "gc-951"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan  3 03:16:53.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-951" for this suite. 01/03/23 03:16:53.073
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","completed":338,"skipped":6442,"failed":0}
------------------------------
• [SLOW TEST] [18.933 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:16:34.159
    Jan  3 03:16:34.159: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename gc 01/03/23 03:16:34.16
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:16:34.167
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:16:34.169
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 01/03/23 03:16:34.19
    STEP: create the rc2 01/03/23 03:16:34.194
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 01/03/23 03:16:39.221
    STEP: delete the rc simpletest-rc-to-be-deleted 01/03/23 03:16:40.59
    STEP: wait for the rc to be deleted 01/03/23 03:16:40.608
    Jan  3 03:16:45.620: INFO: 68 pods remaining
    Jan  3 03:16:45.620: INFO: 68 pods has nil DeletionTimestamp
    Jan  3 03:16:45.620: INFO: 
    STEP: Gathering metrics 01/03/23 03:16:50.621
    Jan  3 03:16:50.628: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cncf-master" in namespace "kube-system" to be "running and ready"
    Jan  3 03:16:50.630: INFO: Pod "kube-controller-manager-cncf-master": Phase="Running", Reason="", readiness=true. Elapsed: 1.574471ms
    Jan  3 03:16:50.630: INFO: The phase of Pod kube-controller-manager-cncf-master is Running (Ready = true)
    Jan  3 03:16:50.630: INFO: Pod "kube-controller-manager-cncf-master" satisfied condition "running and ready"
    Jan  3 03:16:50.685: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jan  3 03:16:50.685: INFO: Deleting pod "simpletest-rc-to-be-deleted-256kz" in namespace "gc-951"
    Jan  3 03:16:50.760: INFO: Deleting pod "simpletest-rc-to-be-deleted-294mc" in namespace "gc-951"
    Jan  3 03:16:50.794: INFO: Deleting pod "simpletest-rc-to-be-deleted-2f8zl" in namespace "gc-951"
    Jan  3 03:16:50.823: INFO: Deleting pod "simpletest-rc-to-be-deleted-4hrmn" in namespace "gc-951"
    Jan  3 03:16:50.875: INFO: Deleting pod "simpletest-rc-to-be-deleted-4xhp4" in namespace "gc-951"
    Jan  3 03:16:50.903: INFO: Deleting pod "simpletest-rc-to-be-deleted-56gz5" in namespace "gc-951"
    Jan  3 03:16:50.937: INFO: Deleting pod "simpletest-rc-to-be-deleted-5f26s" in namespace "gc-951"
    Jan  3 03:16:51.001: INFO: Deleting pod "simpletest-rc-to-be-deleted-5p6jm" in namespace "gc-951"
    Jan  3 03:16:51.023: INFO: Deleting pod "simpletest-rc-to-be-deleted-62wmm" in namespace "gc-951"
    Jan  3 03:16:51.040: INFO: Deleting pod "simpletest-rc-to-be-deleted-6dk5h" in namespace "gc-951"
    Jan  3 03:16:51.080: INFO: Deleting pod "simpletest-rc-to-be-deleted-6f25r" in namespace "gc-951"
    Jan  3 03:16:51.196: INFO: Deleting pod "simpletest-rc-to-be-deleted-6mwww" in namespace "gc-951"
    Jan  3 03:16:51.282: INFO: Deleting pod "simpletest-rc-to-be-deleted-74jst" in namespace "gc-951"
    Jan  3 03:16:51.321: INFO: Deleting pod "simpletest-rc-to-be-deleted-75lbp" in namespace "gc-951"
    Jan  3 03:16:51.362: INFO: Deleting pod "simpletest-rc-to-be-deleted-777kr" in namespace "gc-951"
    Jan  3 03:16:51.454: INFO: Deleting pod "simpletest-rc-to-be-deleted-788sl" in namespace "gc-951"
    Jan  3 03:16:51.494: INFO: Deleting pod "simpletest-rc-to-be-deleted-7hwz4" in namespace "gc-951"
    Jan  3 03:16:51.569: INFO: Deleting pod "simpletest-rc-to-be-deleted-7q9z8" in namespace "gc-951"
    Jan  3 03:16:51.607: INFO: Deleting pod "simpletest-rc-to-be-deleted-7t4w2" in namespace "gc-951"
    Jan  3 03:16:51.630: INFO: Deleting pod "simpletest-rc-to-be-deleted-852fh" in namespace "gc-951"
    Jan  3 03:16:51.685: INFO: Deleting pod "simpletest-rc-to-be-deleted-8b4s6" in namespace "gc-951"
    Jan  3 03:16:51.707: INFO: Deleting pod "simpletest-rc-to-be-deleted-8n9tt" in namespace "gc-951"
    Jan  3 03:16:51.718: INFO: Deleting pod "simpletest-rc-to-be-deleted-8rbqf" in namespace "gc-951"
    Jan  3 03:16:51.741: INFO: Deleting pod "simpletest-rc-to-be-deleted-95xlb" in namespace "gc-951"
    Jan  3 03:16:51.812: INFO: Deleting pod "simpletest-rc-to-be-deleted-97qnt" in namespace "gc-951"
    Jan  3 03:16:51.839: INFO: Deleting pod "simpletest-rc-to-be-deleted-9gq8s" in namespace "gc-951"
    Jan  3 03:16:51.868: INFO: Deleting pod "simpletest-rc-to-be-deleted-9qmjh" in namespace "gc-951"
    Jan  3 03:16:51.896: INFO: Deleting pod "simpletest-rc-to-be-deleted-9rdgn" in namespace "gc-951"
    Jan  3 03:16:51.966: INFO: Deleting pod "simpletest-rc-to-be-deleted-9zfgq" in namespace "gc-951"
    Jan  3 03:16:51.994: INFO: Deleting pod "simpletest-rc-to-be-deleted-bc4k8" in namespace "gc-951"
    Jan  3 03:16:52.031: INFO: Deleting pod "simpletest-rc-to-be-deleted-bkqgz" in namespace "gc-951"
    Jan  3 03:16:52.097: INFO: Deleting pod "simpletest-rc-to-be-deleted-bwhg2" in namespace "gc-951"
    Jan  3 03:16:52.120: INFO: Deleting pod "simpletest-rc-to-be-deleted-c95vh" in namespace "gc-951"
    Jan  3 03:16:52.137: INFO: Deleting pod "simpletest-rc-to-be-deleted-d4l8l" in namespace "gc-951"
    Jan  3 03:16:52.160: INFO: Deleting pod "simpletest-rc-to-be-deleted-d7tlr" in namespace "gc-951"
    Jan  3 03:16:52.180: INFO: Deleting pod "simpletest-rc-to-be-deleted-d8wlq" in namespace "gc-951"
    Jan  3 03:16:52.264: INFO: Deleting pod "simpletest-rc-to-be-deleted-dkvkf" in namespace "gc-951"
    Jan  3 03:16:52.304: INFO: Deleting pod "simpletest-rc-to-be-deleted-dlxcs" in namespace "gc-951"
    Jan  3 03:16:52.442: INFO: Deleting pod "simpletest-rc-to-be-deleted-dwk6x" in namespace "gc-951"
    Jan  3 03:16:52.489: INFO: Deleting pod "simpletest-rc-to-be-deleted-f25r7" in namespace "gc-951"
    Jan  3 03:16:52.517: INFO: Deleting pod "simpletest-rc-to-be-deleted-fjzb7" in namespace "gc-951"
    Jan  3 03:16:52.556: INFO: Deleting pod "simpletest-rc-to-be-deleted-gb5hw" in namespace "gc-951"
    Jan  3 03:16:52.580: INFO: Deleting pod "simpletest-rc-to-be-deleted-gqgng" in namespace "gc-951"
    Jan  3 03:16:52.654: INFO: Deleting pod "simpletest-rc-to-be-deleted-gsj9x" in namespace "gc-951"
    Jan  3 03:16:52.755: INFO: Deleting pod "simpletest-rc-to-be-deleted-gwtkr" in namespace "gc-951"
    Jan  3 03:16:52.810: INFO: Deleting pod "simpletest-rc-to-be-deleted-h69fv" in namespace "gc-951"
    Jan  3 03:16:52.880: INFO: Deleting pod "simpletest-rc-to-be-deleted-h858l" in namespace "gc-951"
    Jan  3 03:16:52.896: INFO: Deleting pod "simpletest-rc-to-be-deleted-hhxtr" in namespace "gc-951"
    Jan  3 03:16:52.930: INFO: Deleting pod "simpletest-rc-to-be-deleted-hr9pw" in namespace "gc-951"
    Jan  3 03:16:53.035: INFO: Deleting pod "simpletest-rc-to-be-deleted-hv7qf" in namespace "gc-951"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan  3 03:16:53.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-951" for this suite. 01/03/23 03:16:53.073
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:16:53.093
Jan  3 03:16:53.093: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename init-container 01/03/23 03:16:53.094
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:16:53.107
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:16:53.109
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
STEP: creating the pod 01/03/23 03:16:53.111
Jan  3 03:16:53.111: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jan  3 03:16:57.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4625" for this suite. 01/03/23 03:16:57.75
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","completed":339,"skipped":6455,"failed":0}
------------------------------
• [4.659 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:16:53.093
    Jan  3 03:16:53.093: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename init-container 01/03/23 03:16:53.094
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:16:53.107
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:16:53.109
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:457
    STEP: creating the pod 01/03/23 03:16:53.111
    Jan  3 03:16:53.111: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan  3 03:16:57.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-4625" for this suite. 01/03/23 03:16:57.75
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:16:57.753
Jan  3 03:16:57.753: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename resourcequota 01/03/23 03:16:57.754
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:16:57.772
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:16:57.774
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
STEP: Creating a ResourceQuota 01/03/23 03:16:57.776
STEP: Getting a ResourceQuota 01/03/23 03:16:57.78
STEP: Updating a ResourceQuota 01/03/23 03:16:57.791
STEP: Verifying a ResourceQuota was modified 01/03/23 03:16:57.797
STEP: Deleting a ResourceQuota 01/03/23 03:16:57.812
STEP: Verifying the deleted ResourceQuota 01/03/23 03:16:57.814
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan  3 03:16:57.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4670" for this suite. 01/03/23 03:16:57.82
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","completed":340,"skipped":6461,"failed":0}
------------------------------
• [0.069 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:16:57.753
    Jan  3 03:16:57.753: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename resourcequota 01/03/23 03:16:57.754
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:16:57.772
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:16:57.774
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:874
    STEP: Creating a ResourceQuota 01/03/23 03:16:57.776
    STEP: Getting a ResourceQuota 01/03/23 03:16:57.78
    STEP: Updating a ResourceQuota 01/03/23 03:16:57.791
    STEP: Verifying a ResourceQuota was modified 01/03/23 03:16:57.797
    STEP: Deleting a ResourceQuota 01/03/23 03:16:57.812
    STEP: Verifying the deleted ResourceQuota 01/03/23 03:16:57.814
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan  3 03:16:57.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-4670" for this suite. 01/03/23 03:16:57.82
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:16:57.823
Jan  3 03:16:57.823: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename projected 01/03/23 03:16:57.823
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:16:57.835
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:16:57.837
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
STEP: Creating secret with name s-test-opt-del-235320f6-c76c-40bb-9d00-d7c576966106 01/03/23 03:16:57.898
STEP: Creating secret with name s-test-opt-upd-b34c0793-3cb5-4e25-ab23-6d6afef8b437 01/03/23 03:16:57.915
STEP: Creating the pod 01/03/23 03:16:57.931
Jan  3 03:16:57.955: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-155dd24d-c51a-4ffe-8eee-e3db353b4877" in namespace "projected-5428" to be "running and ready"
Jan  3 03:16:57.977: INFO: Pod "pod-projected-secrets-155dd24d-c51a-4ffe-8eee-e3db353b4877": Phase="Pending", Reason="", readiness=false. Elapsed: 22.103558ms
Jan  3 03:16:57.977: INFO: The phase of Pod pod-projected-secrets-155dd24d-c51a-4ffe-8eee-e3db353b4877 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 03:16:59.989: INFO: Pod "pod-projected-secrets-155dd24d-c51a-4ffe-8eee-e3db353b4877": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033920911s
Jan  3 03:16:59.989: INFO: The phase of Pod pod-projected-secrets-155dd24d-c51a-4ffe-8eee-e3db353b4877 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 03:17:01.979: INFO: Pod "pod-projected-secrets-155dd24d-c51a-4ffe-8eee-e3db353b4877": Phase="Running", Reason="", readiness=true. Elapsed: 4.024211484s
Jan  3 03:17:01.979: INFO: The phase of Pod pod-projected-secrets-155dd24d-c51a-4ffe-8eee-e3db353b4877 is Running (Ready = true)
Jan  3 03:17:01.979: INFO: Pod "pod-projected-secrets-155dd24d-c51a-4ffe-8eee-e3db353b4877" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-235320f6-c76c-40bb-9d00-d7c576966106 01/03/23 03:17:02.003
STEP: Updating secret s-test-opt-upd-b34c0793-3cb5-4e25-ab23-6d6afef8b437 01/03/23 03:17:02.008
STEP: Creating secret with name s-test-opt-create-cea4f0eb-d674-4f5c-92f3-4b073dd8ee51 01/03/23 03:17:02.024
STEP: waiting to observe update in volume 01/03/23 03:17:02.03
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan  3 03:17:06.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5428" for this suite. 01/03/23 03:17:06.081
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":341,"skipped":6473,"failed":0}
------------------------------
• [SLOW TEST] [8.261 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:16:57.823
    Jan  3 03:16:57.823: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename projected 01/03/23 03:16:57.823
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:16:57.835
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:16:57.837
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:214
    STEP: Creating secret with name s-test-opt-del-235320f6-c76c-40bb-9d00-d7c576966106 01/03/23 03:16:57.898
    STEP: Creating secret with name s-test-opt-upd-b34c0793-3cb5-4e25-ab23-6d6afef8b437 01/03/23 03:16:57.915
    STEP: Creating the pod 01/03/23 03:16:57.931
    Jan  3 03:16:57.955: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-155dd24d-c51a-4ffe-8eee-e3db353b4877" in namespace "projected-5428" to be "running and ready"
    Jan  3 03:16:57.977: INFO: Pod "pod-projected-secrets-155dd24d-c51a-4ffe-8eee-e3db353b4877": Phase="Pending", Reason="", readiness=false. Elapsed: 22.103558ms
    Jan  3 03:16:57.977: INFO: The phase of Pod pod-projected-secrets-155dd24d-c51a-4ffe-8eee-e3db353b4877 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 03:16:59.989: INFO: Pod "pod-projected-secrets-155dd24d-c51a-4ffe-8eee-e3db353b4877": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033920911s
    Jan  3 03:16:59.989: INFO: The phase of Pod pod-projected-secrets-155dd24d-c51a-4ffe-8eee-e3db353b4877 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 03:17:01.979: INFO: Pod "pod-projected-secrets-155dd24d-c51a-4ffe-8eee-e3db353b4877": Phase="Running", Reason="", readiness=true. Elapsed: 4.024211484s
    Jan  3 03:17:01.979: INFO: The phase of Pod pod-projected-secrets-155dd24d-c51a-4ffe-8eee-e3db353b4877 is Running (Ready = true)
    Jan  3 03:17:01.979: INFO: Pod "pod-projected-secrets-155dd24d-c51a-4ffe-8eee-e3db353b4877" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-235320f6-c76c-40bb-9d00-d7c576966106 01/03/23 03:17:02.003
    STEP: Updating secret s-test-opt-upd-b34c0793-3cb5-4e25-ab23-6d6afef8b437 01/03/23 03:17:02.008
    STEP: Creating secret with name s-test-opt-create-cea4f0eb-d674-4f5c-92f3-4b073dd8ee51 01/03/23 03:17:02.024
    STEP: waiting to observe update in volume 01/03/23 03:17:02.03
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan  3 03:17:06.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5428" for this suite. 01/03/23 03:17:06.081
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:17:06.084
Jan  3 03:17:06.084: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename secrets 01/03/23 03:17:06.085
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:17:06.093
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:17:06.095
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
STEP: Creating secret with name secret-test-23c0fbd0-5640-4586-a2a4-effa23bfbba7 01/03/23 03:17:06.097
STEP: Creating a pod to test consume secrets 01/03/23 03:17:06.111
Jan  3 03:17:06.119: INFO: Waiting up to 5m0s for pod "pod-secrets-89ba6581-8ada-44a6-8354-c53af769c1d0" in namespace "secrets-7896" to be "Succeeded or Failed"
Jan  3 03:17:06.134: INFO: Pod "pod-secrets-89ba6581-8ada-44a6-8354-c53af769c1d0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.645733ms
Jan  3 03:17:08.136: INFO: Pod "pod-secrets-89ba6581-8ada-44a6-8354-c53af769c1d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016682241s
Jan  3 03:17:10.154: INFO: Pod "pod-secrets-89ba6581-8ada-44a6-8354-c53af769c1d0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035157914s
Jan  3 03:17:12.136: INFO: Pod "pod-secrets-89ba6581-8ada-44a6-8354-c53af769c1d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016883191s
STEP: Saw pod success 01/03/23 03:17:12.136
Jan  3 03:17:12.136: INFO: Pod "pod-secrets-89ba6581-8ada-44a6-8354-c53af769c1d0" satisfied condition "Succeeded or Failed"
Jan  3 03:17:12.138: INFO: Trying to get logs from node cncf-master pod pod-secrets-89ba6581-8ada-44a6-8354-c53af769c1d0 container secret-volume-test: <nil>
STEP: delete the pod 01/03/23 03:17:12.141
Jan  3 03:17:12.157: INFO: Waiting for pod pod-secrets-89ba6581-8ada-44a6-8354-c53af769c1d0 to disappear
Jan  3 03:17:12.162: INFO: Pod pod-secrets-89ba6581-8ada-44a6-8354-c53af769c1d0 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan  3 03:17:12.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7896" for this suite. 01/03/23 03:17:12.165
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":342,"skipped":6476,"failed":0}
------------------------------
• [SLOW TEST] [6.084 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:17:06.084
    Jan  3 03:17:06.084: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename secrets 01/03/23 03:17:06.085
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:17:06.093
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:17:06.095
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:124
    STEP: Creating secret with name secret-test-23c0fbd0-5640-4586-a2a4-effa23bfbba7 01/03/23 03:17:06.097
    STEP: Creating a pod to test consume secrets 01/03/23 03:17:06.111
    Jan  3 03:17:06.119: INFO: Waiting up to 5m0s for pod "pod-secrets-89ba6581-8ada-44a6-8354-c53af769c1d0" in namespace "secrets-7896" to be "Succeeded or Failed"
    Jan  3 03:17:06.134: INFO: Pod "pod-secrets-89ba6581-8ada-44a6-8354-c53af769c1d0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.645733ms
    Jan  3 03:17:08.136: INFO: Pod "pod-secrets-89ba6581-8ada-44a6-8354-c53af769c1d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016682241s
    Jan  3 03:17:10.154: INFO: Pod "pod-secrets-89ba6581-8ada-44a6-8354-c53af769c1d0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035157914s
    Jan  3 03:17:12.136: INFO: Pod "pod-secrets-89ba6581-8ada-44a6-8354-c53af769c1d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016883191s
    STEP: Saw pod success 01/03/23 03:17:12.136
    Jan  3 03:17:12.136: INFO: Pod "pod-secrets-89ba6581-8ada-44a6-8354-c53af769c1d0" satisfied condition "Succeeded or Failed"
    Jan  3 03:17:12.138: INFO: Trying to get logs from node cncf-master pod pod-secrets-89ba6581-8ada-44a6-8354-c53af769c1d0 container secret-volume-test: <nil>
    STEP: delete the pod 01/03/23 03:17:12.141
    Jan  3 03:17:12.157: INFO: Waiting for pod pod-secrets-89ba6581-8ada-44a6-8354-c53af769c1d0 to disappear
    Jan  3 03:17:12.162: INFO: Pod pod-secrets-89ba6581-8ada-44a6-8354-c53af769c1d0 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan  3 03:17:12.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-7896" for this suite. 01/03/23 03:17:12.165
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:17:12.169
Jan  3 03:17:12.169: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename configmap 01/03/23 03:17:12.17
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:17:12.185
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:17:12.188
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
STEP: Creating configMap with name configmap-test-volume-map-b9e32535-975f-4679-8674-9ccfbfc9efa1 01/03/23 03:17:12.19
STEP: Creating a pod to test consume configMaps 01/03/23 03:17:12.206
Jan  3 03:17:12.263: INFO: Waiting up to 5m0s for pod "pod-configmaps-c8d48550-af8a-4e21-8021-d410fc23d844" in namespace "configmap-3236" to be "Succeeded or Failed"
Jan  3 03:17:12.275: INFO: Pod "pod-configmaps-c8d48550-af8a-4e21-8021-d410fc23d844": Phase="Pending", Reason="", readiness=false. Elapsed: 11.382323ms
Jan  3 03:17:14.286: INFO: Pod "pod-configmaps-c8d48550-af8a-4e21-8021-d410fc23d844": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022272938s
Jan  3 03:17:16.277: INFO: Pod "pod-configmaps-c8d48550-af8a-4e21-8021-d410fc23d844": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013819528s
STEP: Saw pod success 01/03/23 03:17:16.277
Jan  3 03:17:16.277: INFO: Pod "pod-configmaps-c8d48550-af8a-4e21-8021-d410fc23d844" satisfied condition "Succeeded or Failed"
Jan  3 03:17:16.279: INFO: Trying to get logs from node cncf-master pod pod-configmaps-c8d48550-af8a-4e21-8021-d410fc23d844 container agnhost-container: <nil>
STEP: delete the pod 01/03/23 03:17:16.282
Jan  3 03:17:16.306: INFO: Waiting for pod pod-configmaps-c8d48550-af8a-4e21-8021-d410fc23d844 to disappear
Jan  3 03:17:16.317: INFO: Pod pod-configmaps-c8d48550-af8a-4e21-8021-d410fc23d844 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan  3 03:17:16.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3236" for this suite. 01/03/23 03:17:16.32
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":343,"skipped":6490,"failed":0}
------------------------------
• [4.154 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:17:12.169
    Jan  3 03:17:12.169: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename configmap 01/03/23 03:17:12.17
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:17:12.185
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:17:12.188
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:98
    STEP: Creating configMap with name configmap-test-volume-map-b9e32535-975f-4679-8674-9ccfbfc9efa1 01/03/23 03:17:12.19
    STEP: Creating a pod to test consume configMaps 01/03/23 03:17:12.206
    Jan  3 03:17:12.263: INFO: Waiting up to 5m0s for pod "pod-configmaps-c8d48550-af8a-4e21-8021-d410fc23d844" in namespace "configmap-3236" to be "Succeeded or Failed"
    Jan  3 03:17:12.275: INFO: Pod "pod-configmaps-c8d48550-af8a-4e21-8021-d410fc23d844": Phase="Pending", Reason="", readiness=false. Elapsed: 11.382323ms
    Jan  3 03:17:14.286: INFO: Pod "pod-configmaps-c8d48550-af8a-4e21-8021-d410fc23d844": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022272938s
    Jan  3 03:17:16.277: INFO: Pod "pod-configmaps-c8d48550-af8a-4e21-8021-d410fc23d844": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013819528s
    STEP: Saw pod success 01/03/23 03:17:16.277
    Jan  3 03:17:16.277: INFO: Pod "pod-configmaps-c8d48550-af8a-4e21-8021-d410fc23d844" satisfied condition "Succeeded or Failed"
    Jan  3 03:17:16.279: INFO: Trying to get logs from node cncf-master pod pod-configmaps-c8d48550-af8a-4e21-8021-d410fc23d844 container agnhost-container: <nil>
    STEP: delete the pod 01/03/23 03:17:16.282
    Jan  3 03:17:16.306: INFO: Waiting for pod pod-configmaps-c8d48550-af8a-4e21-8021-d410fc23d844 to disappear
    Jan  3 03:17:16.317: INFO: Pod pod-configmaps-c8d48550-af8a-4e21-8021-d410fc23d844 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan  3 03:17:16.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-3236" for this suite. 01/03/23 03:17:16.32
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:17:16.324
Jan  3 03:17:16.324: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename disruption 01/03/23 03:17:16.325
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:17:16.365
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:17:16.367
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:17:16.369
Jan  3 03:17:16.369: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename disruption-2 01/03/23 03:17:16.37
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:17:16.38
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:17:16.382
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
STEP: Waiting for the pdb to be processed 01/03/23 03:17:16.398
STEP: Waiting for the pdb to be processed 01/03/23 03:17:18.417
STEP: Waiting for the pdb to be processed 01/03/23 03:17:18.474
STEP: listing a collection of PDBs across all namespaces 01/03/23 03:17:20.479
STEP: listing a collection of PDBs in namespace disruption-2140 01/03/23 03:17:20.481
STEP: deleting a collection of PDBs 01/03/23 03:17:20.482
STEP: Waiting for the PDB collection to be deleted 01/03/23 03:17:20.49
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:187
Jan  3 03:17:20.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-3440" for this suite. 01/03/23 03:17:20.498
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan  3 03:17:20.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2140" for this suite. 01/03/23 03:17:20.507
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","completed":344,"skipped":6510,"failed":0}
------------------------------
• [4.186 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:77
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:17:16.324
    Jan  3 03:17:16.324: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename disruption 01/03/23 03:17:16.325
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:17:16.365
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:17:16.367
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:17:16.369
    Jan  3 03:17:16.369: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename disruption-2 01/03/23 03:17:16.37
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:17:16.38
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:17:16.382
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:86
    STEP: Waiting for the pdb to be processed 01/03/23 03:17:16.398
    STEP: Waiting for the pdb to be processed 01/03/23 03:17:18.417
    STEP: Waiting for the pdb to be processed 01/03/23 03:17:18.474
    STEP: listing a collection of PDBs across all namespaces 01/03/23 03:17:20.479
    STEP: listing a collection of PDBs in namespace disruption-2140 01/03/23 03:17:20.481
    STEP: deleting a collection of PDBs 01/03/23 03:17:20.482
    STEP: Waiting for the PDB collection to be deleted 01/03/23 03:17:20.49
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:187
    Jan  3 03:17:20.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2-3440" for this suite. 01/03/23 03:17:20.498
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan  3 03:17:20.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2140" for this suite. 01/03/23 03:17:20.507
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:17:20.512
Jan  3 03:17:20.512: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename kubectl 01/03/23 03:17:20.513
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:17:20.548
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:17:20.55
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1698
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/03/23 03:17:20.552
Jan  3 03:17:20.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-729 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
Jan  3 03:17:20.613: INFO: stderr: ""
Jan  3 03:17:20.613: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 01/03/23 03:17:20.613
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1702
Jan  3 03:17:20.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-729 delete pods e2e-test-httpd-pod'
Jan  3 03:17:23.303: INFO: stderr: ""
Jan  3 03:17:23.303: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan  3 03:17:23.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-729" for this suite. 01/03/23 03:17:23.308
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","completed":345,"skipped":6564,"failed":0}
------------------------------
• [2.799 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1695
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1711

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:17:20.512
    Jan  3 03:17:20.512: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename kubectl 01/03/23 03:17:20.513
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:17:20.548
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:17:20.55
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1698
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1711
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/03/23 03:17:20.552
    Jan  3 03:17:20.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-729 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
    Jan  3 03:17:20.613: INFO: stderr: ""
    Jan  3 03:17:20.613: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 01/03/23 03:17:20.613
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1702
    Jan  3 03:17:20.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=kubectl-729 delete pods e2e-test-httpd-pod'
    Jan  3 03:17:23.303: INFO: stderr: ""
    Jan  3 03:17:23.303: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan  3 03:17:23.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-729" for this suite. 01/03/23 03:17:23.308
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:17:23.311
Jan  3 03:17:23.311: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename crd-webhook 01/03/23 03:17:23.311
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:17:23.33
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:17:23.331
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 01/03/23 03:17:23.333
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/03/23 03:17:23.808
STEP: Deploying the custom resource conversion webhook pod 01/03/23 03:17:23.814
STEP: Wait for the deployment to be ready 01/03/23 03:17:23.845
Jan  3 03:17:23.861: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/03/23 03:17:25.889
STEP: Verifying the service has paired with the endpoint 01/03/23 03:17:25.902
Jan  3 03:17:26.903: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Jan  3 03:17:26.906: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Creating a v1 custom resource 01/03/23 03:17:29.479
STEP: v2 custom resource should be converted 01/03/23 03:17:29.491
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  3 03:17:30.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-6440" for this suite. 01/03/23 03:17:30.007
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","completed":346,"skipped":6564,"failed":0}
------------------------------
• [SLOW TEST] [6.770 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:17:23.311
    Jan  3 03:17:23.311: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename crd-webhook 01/03/23 03:17:23.311
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:17:23.33
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:17:23.331
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 01/03/23 03:17:23.333
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/03/23 03:17:23.808
    STEP: Deploying the custom resource conversion webhook pod 01/03/23 03:17:23.814
    STEP: Wait for the deployment to be ready 01/03/23 03:17:23.845
    Jan  3 03:17:23.861: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/03/23 03:17:25.889
    STEP: Verifying the service has paired with the endpoint 01/03/23 03:17:25.902
    Jan  3 03:17:26.903: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Jan  3 03:17:26.906: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Creating a v1 custom resource 01/03/23 03:17:29.479
    STEP: v2 custom resource should be converted 01/03/23 03:17:29.491
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  3 03:17:30.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-6440" for this suite. 01/03/23 03:17:30.007
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:17:30.08
Jan  3 03:17:30.080: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename var-expansion 01/03/23 03:17:30.081
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:17:30.114
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:17:30.115
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
STEP: creating the pod with failed condition 01/03/23 03:17:30.117
Jan  3 03:17:30.142: INFO: Waiting up to 2m0s for pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2" in namespace "var-expansion-8444" to be "running"
Jan  3 03:17:30.154: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 11.459123ms
Jan  3 03:17:32.156: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013781765s
Jan  3 03:17:34.156: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013599391s
Jan  3 03:17:36.159: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016853728s
Jan  3 03:17:38.158: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.015211647s
Jan  3 03:17:40.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.01466225s
Jan  3 03:17:42.156: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 12.014018355s
Jan  3 03:17:44.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 14.014716231s
Jan  3 03:17:46.165: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 16.022670817s
Jan  3 03:17:48.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 18.014835162s
Jan  3 03:17:50.158: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 20.015339193s
Jan  3 03:17:52.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 22.014528232s
Jan  3 03:17:54.158: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 24.015403564s
Jan  3 03:17:56.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 26.014567148s
Jan  3 03:17:58.170: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 28.028039222s
Jan  3 03:18:00.158: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 30.015169895s
Jan  3 03:18:02.256: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 32.113944602s
Jan  3 03:18:04.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 34.014709633s
Jan  3 03:18:06.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 36.014767088s
Jan  3 03:18:08.180: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 38.037938911s
Jan  3 03:18:10.160: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 40.018092183s
Jan  3 03:18:12.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 42.014396655s
Jan  3 03:18:14.158: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 44.015113195s
Jan  3 03:18:16.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 46.014192811s
Jan  3 03:18:18.158: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 48.015495899s
Jan  3 03:18:20.158: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 50.015418154s
Jan  3 03:18:22.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 52.014674239s
Jan  3 03:18:24.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 54.014756101s
Jan  3 03:18:26.156: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 56.014068267s
Jan  3 03:18:28.158: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 58.015326706s
Jan  3 03:18:30.158: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.015258734s
Jan  3 03:18:32.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.014498444s
Jan  3 03:18:34.158: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.015778667s
Jan  3 03:18:36.156: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.013885388s
Jan  3 03:18:38.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.014965102s
Jan  3 03:18:40.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.014351564s
Jan  3 03:18:42.156: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.013624178s
Jan  3 03:18:44.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.014463636s
Jan  3 03:18:46.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.014837805s
Jan  3 03:18:48.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.014992341s
Jan  3 03:18:50.158: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.01528942s
Jan  3 03:18:52.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.014397144s
Jan  3 03:18:54.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.015017276s
Jan  3 03:18:56.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.014602304s
Jan  3 03:18:58.156: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.013859164s
Jan  3 03:19:00.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.014354843s
Jan  3 03:19:02.156: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.014054317s
Jan  3 03:19:04.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.014504251s
Jan  3 03:19:06.156: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.013964026s
Jan  3 03:19:08.192: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.049598759s
Jan  3 03:19:10.158: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.015462574s
Jan  3 03:19:12.156: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.013563792s
Jan  3 03:19:14.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.014731025s
Jan  3 03:19:16.169: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.026342665s
Jan  3 03:19:18.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.014356534s
Jan  3 03:19:20.158: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.015402564s
Jan  3 03:19:22.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.014463071s
Jan  3 03:19:24.156: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.013987975s
Jan  3 03:19:26.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.014283829s
Jan  3 03:19:28.158: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.01512697s
Jan  3 03:19:30.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.015041566s
Jan  3 03:19:30.159: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.017038623s
STEP: updating the pod 01/03/23 03:19:30.159
Jan  3 03:19:30.666: INFO: Successfully updated pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2"
STEP: waiting for pod running 01/03/23 03:19:30.666
Jan  3 03:19:30.666: INFO: Waiting up to 2m0s for pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2" in namespace "var-expansion-8444" to be "running"
Jan  3 03:19:30.714: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 48.31369ms
Jan  3 03:19:32.717: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Running", Reason="", readiness=true. Elapsed: 2.050758313s
Jan  3 03:19:32.717: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2" satisfied condition "running"
STEP: deleting the pod gracefully 01/03/23 03:19:32.717
Jan  3 03:19:32.717: INFO: Deleting pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2" in namespace "var-expansion-8444"
Jan  3 03:19:32.720: INFO: Wait up to 5m0s for pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan  3 03:20:04.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8444" for this suite. 01/03/23 03:20:04.747
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","completed":347,"skipped":6566,"failed":0}
------------------------------
• [SLOW TEST] [154.670 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:17:30.08
    Jan  3 03:17:30.080: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename var-expansion 01/03/23 03:17:30.081
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:17:30.114
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:17:30.115
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:224
    STEP: creating the pod with failed condition 01/03/23 03:17:30.117
    Jan  3 03:17:30.142: INFO: Waiting up to 2m0s for pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2" in namespace "var-expansion-8444" to be "running"
    Jan  3 03:17:30.154: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 11.459123ms
    Jan  3 03:17:32.156: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013781765s
    Jan  3 03:17:34.156: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013599391s
    Jan  3 03:17:36.159: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016853728s
    Jan  3 03:17:38.158: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.015211647s
    Jan  3 03:17:40.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.01466225s
    Jan  3 03:17:42.156: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 12.014018355s
    Jan  3 03:17:44.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 14.014716231s
    Jan  3 03:17:46.165: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 16.022670817s
    Jan  3 03:17:48.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 18.014835162s
    Jan  3 03:17:50.158: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 20.015339193s
    Jan  3 03:17:52.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 22.014528232s
    Jan  3 03:17:54.158: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 24.015403564s
    Jan  3 03:17:56.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 26.014567148s
    Jan  3 03:17:58.170: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 28.028039222s
    Jan  3 03:18:00.158: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 30.015169895s
    Jan  3 03:18:02.256: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 32.113944602s
    Jan  3 03:18:04.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 34.014709633s
    Jan  3 03:18:06.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 36.014767088s
    Jan  3 03:18:08.180: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 38.037938911s
    Jan  3 03:18:10.160: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 40.018092183s
    Jan  3 03:18:12.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 42.014396655s
    Jan  3 03:18:14.158: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 44.015113195s
    Jan  3 03:18:16.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 46.014192811s
    Jan  3 03:18:18.158: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 48.015495899s
    Jan  3 03:18:20.158: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 50.015418154s
    Jan  3 03:18:22.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 52.014674239s
    Jan  3 03:18:24.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 54.014756101s
    Jan  3 03:18:26.156: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 56.014068267s
    Jan  3 03:18:28.158: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 58.015326706s
    Jan  3 03:18:30.158: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.015258734s
    Jan  3 03:18:32.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.014498444s
    Jan  3 03:18:34.158: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.015778667s
    Jan  3 03:18:36.156: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.013885388s
    Jan  3 03:18:38.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.014965102s
    Jan  3 03:18:40.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.014351564s
    Jan  3 03:18:42.156: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.013624178s
    Jan  3 03:18:44.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.014463636s
    Jan  3 03:18:46.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.014837805s
    Jan  3 03:18:48.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.014992341s
    Jan  3 03:18:50.158: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.01528942s
    Jan  3 03:18:52.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.014397144s
    Jan  3 03:18:54.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.015017276s
    Jan  3 03:18:56.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.014602304s
    Jan  3 03:18:58.156: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.013859164s
    Jan  3 03:19:00.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.014354843s
    Jan  3 03:19:02.156: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.014054317s
    Jan  3 03:19:04.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.014504251s
    Jan  3 03:19:06.156: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.013964026s
    Jan  3 03:19:08.192: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.049598759s
    Jan  3 03:19:10.158: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.015462574s
    Jan  3 03:19:12.156: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.013563792s
    Jan  3 03:19:14.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.014731025s
    Jan  3 03:19:16.169: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.026342665s
    Jan  3 03:19:18.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.014356534s
    Jan  3 03:19:20.158: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.015402564s
    Jan  3 03:19:22.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.014463071s
    Jan  3 03:19:24.156: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.013987975s
    Jan  3 03:19:26.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.014283829s
    Jan  3 03:19:28.158: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.01512697s
    Jan  3 03:19:30.157: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.015041566s
    Jan  3 03:19:30.159: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.017038623s
    STEP: updating the pod 01/03/23 03:19:30.159
    Jan  3 03:19:30.666: INFO: Successfully updated pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2"
    STEP: waiting for pod running 01/03/23 03:19:30.666
    Jan  3 03:19:30.666: INFO: Waiting up to 2m0s for pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2" in namespace "var-expansion-8444" to be "running"
    Jan  3 03:19:30.714: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Pending", Reason="", readiness=false. Elapsed: 48.31369ms
    Jan  3 03:19:32.717: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2": Phase="Running", Reason="", readiness=true. Elapsed: 2.050758313s
    Jan  3 03:19:32.717: INFO: Pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2" satisfied condition "running"
    STEP: deleting the pod gracefully 01/03/23 03:19:32.717
    Jan  3 03:19:32.717: INFO: Deleting pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2" in namespace "var-expansion-8444"
    Jan  3 03:19:32.720: INFO: Wait up to 5m0s for pod "var-expansion-bfec52fd-7c49-4ebb-b643-5e12b19542c2" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan  3 03:20:04.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-8444" for this suite. 01/03/23 03:20:04.747
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:20:04.751
Jan  3 03:20:04.751: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename container-runtime 01/03/23 03:20:04.752
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:20:04.777
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:20:04.779
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
STEP: create the container 01/03/23 03:20:04.785
STEP: wait for the container to reach Succeeded 01/03/23 03:20:04.793
STEP: get the container status 01/03/23 03:20:08.836
STEP: the container should be terminated 01/03/23 03:20:08.837
STEP: the termination message should be set 01/03/23 03:20:08.838
Jan  3 03:20:08.838: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 01/03/23 03:20:08.838
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan  3 03:20:08.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7487" for this suite. 01/03/23 03:20:08.874
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":348,"skipped":6577,"failed":0}
------------------------------
• [4.125 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:20:04.751
    Jan  3 03:20:04.751: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename container-runtime 01/03/23 03:20:04.752
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:20:04.777
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:20:04.779
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247
    STEP: create the container 01/03/23 03:20:04.785
    STEP: wait for the container to reach Succeeded 01/03/23 03:20:04.793
    STEP: get the container status 01/03/23 03:20:08.836
    STEP: the container should be terminated 01/03/23 03:20:08.837
    STEP: the termination message should be set 01/03/23 03:20:08.838
    Jan  3 03:20:08.838: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 01/03/23 03:20:08.838
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan  3 03:20:08.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-7487" for this suite. 01/03/23 03:20:08.874
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:20:08.877
Jan  3 03:20:08.877: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename taint-multiple-pods 01/03/23 03:20:08.878
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:20:08.9
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:20:08.901
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:348
Jan  3 03:20:08.903: INFO: Waiting up to 1m0s for all nodes to be ready
Jan  3 03:21:08.950: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
Jan  3 03:21:08.952: INFO: Starting informer...
STEP: Starting pods... 01/03/23 03:21:08.952
Jan  3 03:21:09.164: INFO: Pod1 is running on cncf-master. Tainting Node
Jan  3 03:21:09.372: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-3102" to be "running"
Jan  3 03:21:09.373: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.780488ms
Jan  3 03:21:11.376: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004787885s
Jan  3 03:21:11.376: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Jan  3 03:21:11.376: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-3102" to be "running"
Jan  3 03:21:11.378: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 1.545101ms
Jan  3 03:21:11.378: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Jan  3 03:21:11.378: INFO: Pod2 is running on cncf-master. Tainting Node
STEP: Trying to apply a taint on the Node 01/03/23 03:21:11.378
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/03/23 03:21:11.386
STEP: Waiting for Pod1 and Pod2 to be deleted 01/03/23 03:21:11.44
Jan  3 03:21:18.308: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jan  3 03:21:37.791: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/03/23 03:21:37.801
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:187
Jan  3 03:21:37.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-3102" for this suite. 01/03/23 03:21:37.864
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","completed":349,"skipped":6590,"failed":0}
------------------------------
• [SLOW TEST] [89.018 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:20:08.877
    Jan  3 03:20:08.877: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename taint-multiple-pods 01/03/23 03:20:08.878
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:20:08.9
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:20:08.901
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:348
    Jan  3 03:20:08.903: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan  3 03:21:08.950: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:420
    Jan  3 03:21:08.952: INFO: Starting informer...
    STEP: Starting pods... 01/03/23 03:21:08.952
    Jan  3 03:21:09.164: INFO: Pod1 is running on cncf-master. Tainting Node
    Jan  3 03:21:09.372: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-3102" to be "running"
    Jan  3 03:21:09.373: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.780488ms
    Jan  3 03:21:11.376: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004787885s
    Jan  3 03:21:11.376: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Jan  3 03:21:11.376: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-3102" to be "running"
    Jan  3 03:21:11.378: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 1.545101ms
    Jan  3 03:21:11.378: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Jan  3 03:21:11.378: INFO: Pod2 is running on cncf-master. Tainting Node
    STEP: Trying to apply a taint on the Node 01/03/23 03:21:11.378
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/03/23 03:21:11.386
    STEP: Waiting for Pod1 and Pod2 to be deleted 01/03/23 03:21:11.44
    Jan  3 03:21:18.308: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Jan  3 03:21:37.791: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/03/23 03:21:37.801
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:187
    Jan  3 03:21:37.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-multiple-pods-3102" for this suite. 01/03/23 03:21:37.864
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:21:37.896
Jan  3 03:21:37.896: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename replication-controller 01/03/23 03:21:37.897
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:21:37.941
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:21:37.943
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
STEP: Given a ReplicationController is created 01/03/23 03:21:37.945
STEP: When the matched label of one of its pods change 01/03/23 03:21:37.953
Jan  3 03:21:37.994: INFO: Pod name pod-release: Found 0 pods out of 1
Jan  3 03:21:43.012: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 01/03/23 03:21:43.027
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan  3 03:21:43.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7965" for this suite. 01/03/23 03:21:43.072
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","completed":350,"skipped":6602,"failed":0}
------------------------------
• [SLOW TEST] [5.195 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:21:37.896
    Jan  3 03:21:37.896: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename replication-controller 01/03/23 03:21:37.897
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:21:37.941
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:21:37.943
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:100
    STEP: Given a ReplicationController is created 01/03/23 03:21:37.945
    STEP: When the matched label of one of its pods change 01/03/23 03:21:37.953
    Jan  3 03:21:37.994: INFO: Pod name pod-release: Found 0 pods out of 1
    Jan  3 03:21:43.012: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 01/03/23 03:21:43.027
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan  3 03:21:43.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-7965" for this suite. 01/03/23 03:21:43.072
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:21:43.092
Jan  3 03:21:43.092: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename disruption 01/03/23 03:21:43.092
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:21:43.108
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:21:43.109
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
STEP: Creating a pdb that targets all three pods in a test replica set 01/03/23 03:21:43.111
STEP: Waiting for the pdb to be processed 01/03/23 03:21:43.18
STEP: First trying to evict a pod which shouldn't be evictable 01/03/23 03:21:43.211
STEP: Waiting for all pods to be running 01/03/23 03:21:43.211
Jan  3 03:21:43.216: INFO: pods: 0 < 3
Jan  3 03:21:45.229: INFO: running pods: 2 < 3
Jan  3 03:21:47.229: INFO: running pods: 2 < 3
Jan  3 03:21:49.219: INFO: running pods: 2 < 3
STEP: locating a running pod 01/03/23 03:21:51.219
STEP: Updating the pdb to allow a pod to be evicted 01/03/23 03:21:51.224
STEP: Waiting for the pdb to be processed 01/03/23 03:21:51.228
STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/03/23 03:21:53.239
STEP: Waiting for all pods to be running 01/03/23 03:21:53.239
STEP: Waiting for the pdb to observed all healthy pods 01/03/23 03:21:53.241
STEP: Patching the pdb to disallow a pod to be evicted 01/03/23 03:21:53.256
STEP: Waiting for the pdb to be processed 01/03/23 03:21:53.295
STEP: Waiting for all pods to be running 01/03/23 03:21:55.31
STEP: locating a running pod 01/03/23 03:21:55.312
STEP: Deleting the pdb to allow a pod to be evicted 01/03/23 03:21:55.317
STEP: Waiting for the pdb to be deleted 01/03/23 03:21:55.319
STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/03/23 03:21:55.325
STEP: Waiting for all pods to be running 01/03/23 03:21:55.325
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan  3 03:21:55.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-6316" for this suite. 01/03/23 03:21:55.342
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","completed":351,"skipped":6611,"failed":0}
------------------------------
• [SLOW TEST] [12.292 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:21:43.092
    Jan  3 03:21:43.092: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename disruption 01/03/23 03:21:43.092
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:21:43.108
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:21:43.109
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:346
    STEP: Creating a pdb that targets all three pods in a test replica set 01/03/23 03:21:43.111
    STEP: Waiting for the pdb to be processed 01/03/23 03:21:43.18
    STEP: First trying to evict a pod which shouldn't be evictable 01/03/23 03:21:43.211
    STEP: Waiting for all pods to be running 01/03/23 03:21:43.211
    Jan  3 03:21:43.216: INFO: pods: 0 < 3
    Jan  3 03:21:45.229: INFO: running pods: 2 < 3
    Jan  3 03:21:47.229: INFO: running pods: 2 < 3
    Jan  3 03:21:49.219: INFO: running pods: 2 < 3
    STEP: locating a running pod 01/03/23 03:21:51.219
    STEP: Updating the pdb to allow a pod to be evicted 01/03/23 03:21:51.224
    STEP: Waiting for the pdb to be processed 01/03/23 03:21:51.228
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/03/23 03:21:53.239
    STEP: Waiting for all pods to be running 01/03/23 03:21:53.239
    STEP: Waiting for the pdb to observed all healthy pods 01/03/23 03:21:53.241
    STEP: Patching the pdb to disallow a pod to be evicted 01/03/23 03:21:53.256
    STEP: Waiting for the pdb to be processed 01/03/23 03:21:53.295
    STEP: Waiting for all pods to be running 01/03/23 03:21:55.31
    STEP: locating a running pod 01/03/23 03:21:55.312
    STEP: Deleting the pdb to allow a pod to be evicted 01/03/23 03:21:55.317
    STEP: Waiting for the pdb to be deleted 01/03/23 03:21:55.319
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/03/23 03:21:55.325
    STEP: Waiting for all pods to be running 01/03/23 03:21:55.325
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan  3 03:21:55.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-6316" for this suite. 01/03/23 03:21:55.342
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:21:55.384
Jan  3 03:21:55.384: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename secrets 01/03/23 03:21:55.385
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:21:55.423
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:21:55.425
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
STEP: Creating projection with secret that has name secret-emptykey-test-e194c9fb-f7e0-4fbd-b637-967704c21499 01/03/23 03:21:55.427
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jan  3 03:21:55.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2012" for this suite. 01/03/23 03:21:55.432
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","completed":352,"skipped":6611,"failed":0}
------------------------------
• [0.051 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:21:55.384
    Jan  3 03:21:55.384: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename secrets 01/03/23 03:21:55.385
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:21:55.423
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:21:55.425
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:139
    STEP: Creating projection with secret that has name secret-emptykey-test-e194c9fb-f7e0-4fbd-b637-967704c21499 01/03/23 03:21:55.427
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jan  3 03:21:55.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-2012" for this suite. 01/03/23 03:21:55.432
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:21:55.436
Jan  3 03:21:55.437: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename crd-watch 01/03/23 03:21:55.438
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:21:55.457
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:21:55.46
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Jan  3 03:21:55.462: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Creating first CR  01/03/23 03:21:58.01
Jan  3 03:21:58.013: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-03T03:21:58Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-03T03:21:58Z]] name:name1 resourceVersion:1023310 uid:057211e8-3da0-4c20-9254-fb2b8b6114a6] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 01/03/23 03:22:08.014
Jan  3 03:22:08.018: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-03T03:22:08Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-03T03:22:08Z]] name:name2 resourceVersion:1023408 uid:3a62b661-6aa3-4462-9657-84f2e598aca3] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 01/03/23 03:22:18.019
Jan  3 03:22:18.023: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-03T03:21:58Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-03T03:22:18Z]] name:name1 resourceVersion:1023442 uid:057211e8-3da0-4c20-9254-fb2b8b6114a6] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 01/03/23 03:22:28.025
Jan  3 03:22:28.028: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-03T03:22:08Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-03T03:22:28Z]] name:name2 resourceVersion:1023487 uid:3a62b661-6aa3-4462-9657-84f2e598aca3] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 01/03/23 03:22:38.029
Jan  3 03:22:38.032: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-03T03:21:58Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-03T03:22:18Z]] name:name1 resourceVersion:1023527 uid:057211e8-3da0-4c20-9254-fb2b8b6114a6] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 01/03/23 03:22:48.033
Jan  3 03:22:48.037: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-03T03:22:08Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-03T03:22:28Z]] name:name2 resourceVersion:1023570 uid:3a62b661-6aa3-4462-9657-84f2e598aca3] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  3 03:22:58.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-2334" for this suite. 01/03/23 03:22:58.547
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","completed":353,"skipped":6638,"failed":0}
------------------------------
• [SLOW TEST] [63.113 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:21:55.436
    Jan  3 03:21:55.437: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename crd-watch 01/03/23 03:21:55.438
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:21:55.457
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:21:55.46
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Jan  3 03:21:55.462: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Creating first CR  01/03/23 03:21:58.01
    Jan  3 03:21:58.013: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-03T03:21:58Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-03T03:21:58Z]] name:name1 resourceVersion:1023310 uid:057211e8-3da0-4c20-9254-fb2b8b6114a6] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 01/03/23 03:22:08.014
    Jan  3 03:22:08.018: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-03T03:22:08Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-03T03:22:08Z]] name:name2 resourceVersion:1023408 uid:3a62b661-6aa3-4462-9657-84f2e598aca3] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 01/03/23 03:22:18.019
    Jan  3 03:22:18.023: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-03T03:21:58Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-03T03:22:18Z]] name:name1 resourceVersion:1023442 uid:057211e8-3da0-4c20-9254-fb2b8b6114a6] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 01/03/23 03:22:28.025
    Jan  3 03:22:28.028: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-03T03:22:08Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-03T03:22:28Z]] name:name2 resourceVersion:1023487 uid:3a62b661-6aa3-4462-9657-84f2e598aca3] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 01/03/23 03:22:38.029
    Jan  3 03:22:38.032: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-03T03:21:58Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-03T03:22:18Z]] name:name1 resourceVersion:1023527 uid:057211e8-3da0-4c20-9254-fb2b8b6114a6] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 01/03/23 03:22:48.033
    Jan  3 03:22:48.037: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-03T03:22:08Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-03T03:22:28Z]] name:name2 resourceVersion:1023570 uid:3a62b661-6aa3-4462-9657-84f2e598aca3] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  3 03:22:58.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-watch-2334" for this suite. 01/03/23 03:22:58.547
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:22:58.55
Jan  3 03:22:58.551: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename pod-network-test 01/03/23 03:22:58.551
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:22:58.572
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:22:58.574
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-4361 01/03/23 03:22:58.575
STEP: creating a selector 01/03/23 03:22:58.575
STEP: Creating the service pods in kubernetes 01/03/23 03:22:58.575
Jan  3 03:22:58.575: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan  3 03:22:58.612: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4361" to be "running and ready"
Jan  3 03:22:58.646: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 34.200191ms
Jan  3 03:22:58.646: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan  3 03:23:00.649: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.03738698s
Jan  3 03:23:00.649: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 03:23:02.649: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.037461311s
Jan  3 03:23:02.649: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 03:23:04.648: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.036466248s
Jan  3 03:23:04.648: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 03:23:06.649: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.036702609s
Jan  3 03:23:06.649: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 03:23:08.650: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.038232435s
Jan  3 03:23:08.650: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 03:23:10.649: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.036708556s
Jan  3 03:23:10.649: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 03:23:12.650: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.037973677s
Jan  3 03:23:12.650: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 03:23:14.649: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.036855258s
Jan  3 03:23:14.649: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 03:23:16.649: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.036727042s
Jan  3 03:23:16.649: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 03:23:18.650: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.038449748s
Jan  3 03:23:18.650: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  3 03:23:20.649: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.036662527s
Jan  3 03:23:20.649: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan  3 03:23:20.649: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan  3 03:23:20.650: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4361" to be "running and ready"
Jan  3 03:23:20.652: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.546226ms
Jan  3 03:23:20.652: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan  3 03:23:20.652: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jan  3 03:23:20.653: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-4361" to be "running and ready"
Jan  3 03:23:20.655: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 1.460742ms
Jan  3 03:23:20.655: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jan  3 03:23:20.655: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 01/03/23 03:23:20.656
Jan  3 03:23:20.667: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4361" to be "running"
Jan  3 03:23:20.684: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 16.917326ms
Jan  3 03:23:22.686: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.019044609s
Jan  3 03:23:22.686: INFO: Pod "test-container-pod" satisfied condition "running"
Jan  3 03:23:22.688: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-4361" to be "running"
Jan  3 03:23:22.689: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.360034ms
Jan  3 03:23:22.689: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jan  3 03:23:22.691: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jan  3 03:23:22.691: INFO: Going to poll 10.244.246.84 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Jan  3 03:23:22.692: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.246.84:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4361 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 03:23:22.692: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
Jan  3 03:23:22.693: INFO: ExecWithOptions: Clientset creation
Jan  3 03:23:22.693: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4361/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.246.84%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan  3 03:23:22.775: INFO: Found all 1 expected endpoints: [netserver-0]
Jan  3 03:23:22.775: INFO: Going to poll 10.244.232.50 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Jan  3 03:23:22.776: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.232.50:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4361 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 03:23:22.776: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
Jan  3 03:23:22.777: INFO: ExecWithOptions: Clientset creation
Jan  3 03:23:22.777: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4361/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.232.50%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan  3 03:23:22.847: INFO: Found all 1 expected endpoints: [netserver-1]
Jan  3 03:23:22.847: INFO: Going to poll 10.244.89.122 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Jan  3 03:23:22.849: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.89.122:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4361 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  3 03:23:22.849: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
Jan  3 03:23:22.849: INFO: ExecWithOptions: Clientset creation
Jan  3 03:23:22.849: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4361/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.89.122%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan  3 03:23:22.917: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jan  3 03:23:22.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4361" for this suite. 01/03/23 03:23:22.92
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","completed":354,"skipped":6645,"failed":0}
------------------------------
• [SLOW TEST] [24.372 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:22:58.55
    Jan  3 03:22:58.551: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename pod-network-test 01/03/23 03:22:58.551
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:22:58.572
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:22:58.574
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-4361 01/03/23 03:22:58.575
    STEP: creating a selector 01/03/23 03:22:58.575
    STEP: Creating the service pods in kubernetes 01/03/23 03:22:58.575
    Jan  3 03:22:58.575: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan  3 03:22:58.612: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4361" to be "running and ready"
    Jan  3 03:22:58.646: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 34.200191ms
    Jan  3 03:22:58.646: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 03:23:00.649: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.03738698s
    Jan  3 03:23:00.649: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 03:23:02.649: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.037461311s
    Jan  3 03:23:02.649: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 03:23:04.648: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.036466248s
    Jan  3 03:23:04.648: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 03:23:06.649: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.036702609s
    Jan  3 03:23:06.649: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 03:23:08.650: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.038232435s
    Jan  3 03:23:08.650: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 03:23:10.649: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.036708556s
    Jan  3 03:23:10.649: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 03:23:12.650: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.037973677s
    Jan  3 03:23:12.650: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 03:23:14.649: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.036855258s
    Jan  3 03:23:14.649: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 03:23:16.649: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.036727042s
    Jan  3 03:23:16.649: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 03:23:18.650: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.038449748s
    Jan  3 03:23:18.650: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  3 03:23:20.649: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.036662527s
    Jan  3 03:23:20.649: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan  3 03:23:20.649: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan  3 03:23:20.650: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4361" to be "running and ready"
    Jan  3 03:23:20.652: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.546226ms
    Jan  3 03:23:20.652: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan  3 03:23:20.652: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jan  3 03:23:20.653: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-4361" to be "running and ready"
    Jan  3 03:23:20.655: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 1.460742ms
    Jan  3 03:23:20.655: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jan  3 03:23:20.655: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 01/03/23 03:23:20.656
    Jan  3 03:23:20.667: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4361" to be "running"
    Jan  3 03:23:20.684: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 16.917326ms
    Jan  3 03:23:22.686: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.019044609s
    Jan  3 03:23:22.686: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan  3 03:23:22.688: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-4361" to be "running"
    Jan  3 03:23:22.689: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.360034ms
    Jan  3 03:23:22.689: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jan  3 03:23:22.691: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Jan  3 03:23:22.691: INFO: Going to poll 10.244.246.84 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Jan  3 03:23:22.692: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.246.84:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4361 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 03:23:22.692: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    Jan  3 03:23:22.693: INFO: ExecWithOptions: Clientset creation
    Jan  3 03:23:22.693: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4361/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.246.84%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan  3 03:23:22.775: INFO: Found all 1 expected endpoints: [netserver-0]
    Jan  3 03:23:22.775: INFO: Going to poll 10.244.232.50 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Jan  3 03:23:22.776: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.232.50:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4361 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 03:23:22.776: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    Jan  3 03:23:22.777: INFO: ExecWithOptions: Clientset creation
    Jan  3 03:23:22.777: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4361/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.232.50%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan  3 03:23:22.847: INFO: Found all 1 expected endpoints: [netserver-1]
    Jan  3 03:23:22.847: INFO: Going to poll 10.244.89.122 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Jan  3 03:23:22.849: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.89.122:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4361 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  3 03:23:22.849: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    Jan  3 03:23:22.849: INFO: ExecWithOptions: Clientset creation
    Jan  3 03:23:22.849: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4361/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.89.122%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan  3 03:23:22.917: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jan  3 03:23:22.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-4361" for this suite. 01/03/23 03:23:22.92
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:23:22.922
Jan  3 03:23:22.922: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename emptydir 01/03/23 03:23:22.923
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:23:22.981
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:23:22.983
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
STEP: Creating a pod to test emptydir 0644 on tmpfs 01/03/23 03:23:22.985
Jan  3 03:23:22.999: INFO: Waiting up to 5m0s for pod "pod-f7bb0293-9f67-42c9-8d91-72bb1de39700" in namespace "emptydir-4566" to be "Succeeded or Failed"
Jan  3 03:23:23.005: INFO: Pod "pod-f7bb0293-9f67-42c9-8d91-72bb1de39700": Phase="Pending", Reason="", readiness=false. Elapsed: 5.820685ms
Jan  3 03:23:25.007: INFO: Pod "pod-f7bb0293-9f67-42c9-8d91-72bb1de39700": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008224248s
Jan  3 03:23:27.007: INFO: Pod "pod-f7bb0293-9f67-42c9-8d91-72bb1de39700": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008380415s
STEP: Saw pod success 01/03/23 03:23:27.007
Jan  3 03:23:27.007: INFO: Pod "pod-f7bb0293-9f67-42c9-8d91-72bb1de39700" satisfied condition "Succeeded or Failed"
Jan  3 03:23:27.009: INFO: Trying to get logs from node cncf-master pod pod-f7bb0293-9f67-42c9-8d91-72bb1de39700 container test-container: <nil>
STEP: delete the pod 01/03/23 03:23:27.02
Jan  3 03:23:27.079: INFO: Waiting for pod pod-f7bb0293-9f67-42c9-8d91-72bb1de39700 to disappear
Jan  3 03:23:27.084: INFO: Pod pod-f7bb0293-9f67-42c9-8d91-72bb1de39700 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan  3 03:23:27.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4566" for this suite. 01/03/23 03:23:27.087
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":355,"skipped":6645,"failed":0}
------------------------------
• [4.167 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:23:22.922
    Jan  3 03:23:22.922: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename emptydir 01/03/23 03:23:22.923
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:23:22.981
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:23:22.983
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:96
    STEP: Creating a pod to test emptydir 0644 on tmpfs 01/03/23 03:23:22.985
    Jan  3 03:23:22.999: INFO: Waiting up to 5m0s for pod "pod-f7bb0293-9f67-42c9-8d91-72bb1de39700" in namespace "emptydir-4566" to be "Succeeded or Failed"
    Jan  3 03:23:23.005: INFO: Pod "pod-f7bb0293-9f67-42c9-8d91-72bb1de39700": Phase="Pending", Reason="", readiness=false. Elapsed: 5.820685ms
    Jan  3 03:23:25.007: INFO: Pod "pod-f7bb0293-9f67-42c9-8d91-72bb1de39700": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008224248s
    Jan  3 03:23:27.007: INFO: Pod "pod-f7bb0293-9f67-42c9-8d91-72bb1de39700": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008380415s
    STEP: Saw pod success 01/03/23 03:23:27.007
    Jan  3 03:23:27.007: INFO: Pod "pod-f7bb0293-9f67-42c9-8d91-72bb1de39700" satisfied condition "Succeeded or Failed"
    Jan  3 03:23:27.009: INFO: Trying to get logs from node cncf-master pod pod-f7bb0293-9f67-42c9-8d91-72bb1de39700 container test-container: <nil>
    STEP: delete the pod 01/03/23 03:23:27.02
    Jan  3 03:23:27.079: INFO: Waiting for pod pod-f7bb0293-9f67-42c9-8d91-72bb1de39700 to disappear
    Jan  3 03:23:27.084: INFO: Pod pod-f7bb0293-9f67-42c9-8d91-72bb1de39700 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan  3 03:23:27.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4566" for this suite. 01/03/23 03:23:27.087
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:23:27.09
Jan  3 03:23:27.090: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename webhook 01/03/23 03:23:27.091
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:23:27.102
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:23:27.104
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/03/23 03:23:27.13
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/23 03:23:27.626
STEP: Deploying the webhook pod 01/03/23 03:23:27.631
STEP: Wait for the deployment to be ready 01/03/23 03:23:27.666
Jan  3 03:23:27.689: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan  3 03:23:29.696: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 3, 3, 23, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 23, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 3, 23, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 23, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/03/23 03:23:31.698
STEP: Verifying the service has paired with the endpoint 01/03/23 03:23:31.716
Jan  3 03:23:32.717: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
STEP: Listing all of the created validation webhooks 01/03/23 03:23:32.876
STEP: Creating a configMap that does not comply to the validation webhook rules 01/03/23 03:23:32.91
STEP: Deleting the collection of validation webhooks 01/03/23 03:23:32.931
STEP: Creating a configMap that does not comply to the validation webhook rules 01/03/23 03:23:33.008
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan  3 03:23:33.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7502" for this suite. 01/03/23 03:23:33.065
STEP: Destroying namespace "webhook-7502-markers" for this suite. 01/03/23 03:23:33.07
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","completed":356,"skipped":6651,"failed":0}
------------------------------
• [SLOW TEST] [6.091 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:23:27.09
    Jan  3 03:23:27.090: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename webhook 01/03/23 03:23:27.091
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:23:27.102
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:23:27.104
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/03/23 03:23:27.13
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/03/23 03:23:27.626
    STEP: Deploying the webhook pod 01/03/23 03:23:27.631
    STEP: Wait for the deployment to be ready 01/03/23 03:23:27.666
    Jan  3 03:23:27.689: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan  3 03:23:29.696: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 3, 3, 23, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 23, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 3, 3, 23, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 3, 3, 23, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/03/23 03:23:31.698
    STEP: Verifying the service has paired with the endpoint 01/03/23 03:23:31.716
    Jan  3 03:23:32.717: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:581
    STEP: Listing all of the created validation webhooks 01/03/23 03:23:32.876
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/03/23 03:23:32.91
    STEP: Deleting the collection of validation webhooks 01/03/23 03:23:32.931
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/03/23 03:23:33.008
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan  3 03:23:33.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7502" for this suite. 01/03/23 03:23:33.065
    STEP: Destroying namespace "webhook-7502-markers" for this suite. 01/03/23 03:23:33.07
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:23:33.182
Jan  3 03:23:33.182: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename events 01/03/23 03:23:33.183
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:23:33.201
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:23:33.203
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 01/03/23 03:23:33.205
STEP: get a list of Events with a label in the current namespace 01/03/23 03:23:33.253
STEP: delete a list of events 01/03/23 03:23:33.271
Jan  3 03:23:33.271: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 01/03/23 03:23:33.322
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Jan  3 03:23:33.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8906" for this suite. 01/03/23 03:23:33.334
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","completed":357,"skipped":6655,"failed":0}
------------------------------
• [0.158 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:23:33.182
    Jan  3 03:23:33.182: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename events 01/03/23 03:23:33.183
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:23:33.201
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:23:33.203
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 01/03/23 03:23:33.205
    STEP: get a list of Events with a label in the current namespace 01/03/23 03:23:33.253
    STEP: delete a list of events 01/03/23 03:23:33.271
    Jan  3 03:23:33.271: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 01/03/23 03:23:33.322
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Jan  3 03:23:33.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-8906" for this suite. 01/03/23 03:23:33.334
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:23:33.339
Jan  3 03:23:33.339: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename secrets 01/03/23 03:23:33.34
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:23:33.362
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:23:33.364
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
STEP: Creating secret with name secret-test-map-d1aef5e7-7d7a-4a3e-b319-e8c3c0411ea2 01/03/23 03:23:33.367
STEP: Creating a pod to test consume secrets 01/03/23 03:23:33.37
Jan  3 03:23:33.390: INFO: Waiting up to 5m0s for pod "pod-secrets-d0b1f492-684c-4e95-93c4-93abe56046bf" in namespace "secrets-954" to be "Succeeded or Failed"
Jan  3 03:23:33.396: INFO: Pod "pod-secrets-d0b1f492-684c-4e95-93c4-93abe56046bf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.484333ms
Jan  3 03:23:35.399: INFO: Pod "pod-secrets-d0b1f492-684c-4e95-93c4-93abe56046bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008993712s
Jan  3 03:23:37.398: INFO: Pod "pod-secrets-d0b1f492-684c-4e95-93c4-93abe56046bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007851759s
STEP: Saw pod success 01/03/23 03:23:37.398
Jan  3 03:23:37.398: INFO: Pod "pod-secrets-d0b1f492-684c-4e95-93c4-93abe56046bf" satisfied condition "Succeeded or Failed"
Jan  3 03:23:37.407: INFO: Trying to get logs from node cncf-master pod pod-secrets-d0b1f492-684c-4e95-93c4-93abe56046bf container secret-volume-test: <nil>
STEP: delete the pod 01/03/23 03:23:37.415
Jan  3 03:23:37.451: INFO: Waiting for pod pod-secrets-d0b1f492-684c-4e95-93c4-93abe56046bf to disappear
Jan  3 03:23:37.453: INFO: Pod pod-secrets-d0b1f492-684c-4e95-93c4-93abe56046bf no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan  3 03:23:37.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-954" for this suite. 01/03/23 03:23:37.456
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":358,"skipped":6655,"failed":0}
------------------------------
• [4.119 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:23:33.339
    Jan  3 03:23:33.339: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename secrets 01/03/23 03:23:33.34
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:23:33.362
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:23:33.364
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:88
    STEP: Creating secret with name secret-test-map-d1aef5e7-7d7a-4a3e-b319-e8c3c0411ea2 01/03/23 03:23:33.367
    STEP: Creating a pod to test consume secrets 01/03/23 03:23:33.37
    Jan  3 03:23:33.390: INFO: Waiting up to 5m0s for pod "pod-secrets-d0b1f492-684c-4e95-93c4-93abe56046bf" in namespace "secrets-954" to be "Succeeded or Failed"
    Jan  3 03:23:33.396: INFO: Pod "pod-secrets-d0b1f492-684c-4e95-93c4-93abe56046bf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.484333ms
    Jan  3 03:23:35.399: INFO: Pod "pod-secrets-d0b1f492-684c-4e95-93c4-93abe56046bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008993712s
    Jan  3 03:23:37.398: INFO: Pod "pod-secrets-d0b1f492-684c-4e95-93c4-93abe56046bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007851759s
    STEP: Saw pod success 01/03/23 03:23:37.398
    Jan  3 03:23:37.398: INFO: Pod "pod-secrets-d0b1f492-684c-4e95-93c4-93abe56046bf" satisfied condition "Succeeded or Failed"
    Jan  3 03:23:37.407: INFO: Trying to get logs from node cncf-master pod pod-secrets-d0b1f492-684c-4e95-93c4-93abe56046bf container secret-volume-test: <nil>
    STEP: delete the pod 01/03/23 03:23:37.415
    Jan  3 03:23:37.451: INFO: Waiting for pod pod-secrets-d0b1f492-684c-4e95-93c4-93abe56046bf to disappear
    Jan  3 03:23:37.453: INFO: Pod pod-secrets-d0b1f492-684c-4e95-93c4-93abe56046bf no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan  3 03:23:37.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-954" for this suite. 01/03/23 03:23:37.456
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:23:37.458
Jan  3 03:23:37.459: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename services 01/03/23 03:23:37.459
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:23:37.47
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:23:37.472
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
STEP: creating service in namespace services-180 01/03/23 03:23:37.475
STEP: creating service affinity-nodeport-transition in namespace services-180 01/03/23 03:23:37.475
STEP: creating replication controller affinity-nodeport-transition in namespace services-180 01/03/23 03:23:37.534
I0103 03:23:37.539563      23 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-180, replica count: 3
I0103 03:23:40.590513      23 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  3 03:23:40.596: INFO: Creating new exec pod
Jan  3 03:23:40.601: INFO: Waiting up to 5m0s for pod "execpod-affinitynvwbt" in namespace "services-180" to be "running"
Jan  3 03:23:40.631: INFO: Pod "execpod-affinitynvwbt": Phase="Pending", Reason="", readiness=false. Elapsed: 30.454743ms
Jan  3 03:23:42.643: INFO: Pod "execpod-affinitynvwbt": Phase="Running", Reason="", readiness=true. Elapsed: 2.042525783s
Jan  3 03:23:42.643: INFO: Pod "execpod-affinitynvwbt" satisfied condition "running"
Jan  3 03:23:43.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-180 exec execpod-affinitynvwbt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Jan  3 03:23:43.782: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jan  3 03:23:43.782: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  3 03:23:43.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-180 exec execpod-affinitynvwbt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.222.182 80'
Jan  3 03:23:43.923: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.222.182 80\nConnection to 10.96.222.182 80 port [tcp/http] succeeded!\n"
Jan  3 03:23:43.923: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  3 03:23:43.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-180 exec execpod-affinitynvwbt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.7.7 31355'
Jan  3 03:23:44.045: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.7.7 31355\nConnection to 172.21.7.7 31355 port [tcp/*] succeeded!\n"
Jan  3 03:23:44.045: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  3 03:23:44.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-180 exec execpod-affinitynvwbt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.7.13 31355'
Jan  3 03:23:44.173: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.7.13 31355\nConnection to 172.21.7.13 31355 port [tcp/*] succeeded!\n"
Jan  3 03:23:44.173: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan  3 03:23:44.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-180 exec execpod-affinitynvwbt -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.7.5:31355/ ; done'
Jan  3 03:23:44.394: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n"
Jan  3 03:23:44.394: INFO: stdout: "\naffinity-nodeport-transition-qf562\naffinity-nodeport-transition-jmmjb\naffinity-nodeport-transition-qf562\naffinity-nodeport-transition-cdr8n\naffinity-nodeport-transition-cdr8n\naffinity-nodeport-transition-jmmjb\naffinity-nodeport-transition-jmmjb\naffinity-nodeport-transition-cdr8n\naffinity-nodeport-transition-qf562\naffinity-nodeport-transition-qf562\naffinity-nodeport-transition-cdr8n\naffinity-nodeport-transition-jmmjb\naffinity-nodeport-transition-cdr8n\naffinity-nodeport-transition-cdr8n\naffinity-nodeport-transition-cdr8n\naffinity-nodeport-transition-qf562"
Jan  3 03:23:44.394: INFO: Received response from host: affinity-nodeport-transition-qf562
Jan  3 03:23:44.394: INFO: Received response from host: affinity-nodeport-transition-jmmjb
Jan  3 03:23:44.394: INFO: Received response from host: affinity-nodeport-transition-qf562
Jan  3 03:23:44.394: INFO: Received response from host: affinity-nodeport-transition-cdr8n
Jan  3 03:23:44.394: INFO: Received response from host: affinity-nodeport-transition-cdr8n
Jan  3 03:23:44.394: INFO: Received response from host: affinity-nodeport-transition-jmmjb
Jan  3 03:23:44.394: INFO: Received response from host: affinity-nodeport-transition-jmmjb
Jan  3 03:23:44.394: INFO: Received response from host: affinity-nodeport-transition-cdr8n
Jan  3 03:23:44.394: INFO: Received response from host: affinity-nodeport-transition-qf562
Jan  3 03:23:44.394: INFO: Received response from host: affinity-nodeport-transition-qf562
Jan  3 03:23:44.394: INFO: Received response from host: affinity-nodeport-transition-cdr8n
Jan  3 03:23:44.394: INFO: Received response from host: affinity-nodeport-transition-jmmjb
Jan  3 03:23:44.394: INFO: Received response from host: affinity-nodeport-transition-cdr8n
Jan  3 03:23:44.394: INFO: Received response from host: affinity-nodeport-transition-cdr8n
Jan  3 03:23:44.394: INFO: Received response from host: affinity-nodeport-transition-cdr8n
Jan  3 03:23:44.394: INFO: Received response from host: affinity-nodeport-transition-qf562
Jan  3 03:23:44.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-180 exec execpod-affinitynvwbt -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.7.5:31355/ ; done'
Jan  3 03:23:44.586: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n"
Jan  3 03:23:44.586: INFO: stdout: "\naffinity-nodeport-transition-qf562\naffinity-nodeport-transition-qf562\naffinity-nodeport-transition-qf562\naffinity-nodeport-transition-qf562\naffinity-nodeport-transition-qf562\naffinity-nodeport-transition-qf562\naffinity-nodeport-transition-qf562\naffinity-nodeport-transition-qf562\naffinity-nodeport-transition-qf562\naffinity-nodeport-transition-qf562\naffinity-nodeport-transition-qf562\naffinity-nodeport-transition-qf562\naffinity-nodeport-transition-qf562\naffinity-nodeport-transition-qf562\naffinity-nodeport-transition-qf562\naffinity-nodeport-transition-qf562"
Jan  3 03:23:44.586: INFO: Received response from host: affinity-nodeport-transition-qf562
Jan  3 03:23:44.586: INFO: Received response from host: affinity-nodeport-transition-qf562
Jan  3 03:23:44.586: INFO: Received response from host: affinity-nodeport-transition-qf562
Jan  3 03:23:44.586: INFO: Received response from host: affinity-nodeport-transition-qf562
Jan  3 03:23:44.586: INFO: Received response from host: affinity-nodeport-transition-qf562
Jan  3 03:23:44.586: INFO: Received response from host: affinity-nodeport-transition-qf562
Jan  3 03:23:44.586: INFO: Received response from host: affinity-nodeport-transition-qf562
Jan  3 03:23:44.586: INFO: Received response from host: affinity-nodeport-transition-qf562
Jan  3 03:23:44.586: INFO: Received response from host: affinity-nodeport-transition-qf562
Jan  3 03:23:44.586: INFO: Received response from host: affinity-nodeport-transition-qf562
Jan  3 03:23:44.586: INFO: Received response from host: affinity-nodeport-transition-qf562
Jan  3 03:23:44.586: INFO: Received response from host: affinity-nodeport-transition-qf562
Jan  3 03:23:44.586: INFO: Received response from host: affinity-nodeport-transition-qf562
Jan  3 03:23:44.586: INFO: Received response from host: affinity-nodeport-transition-qf562
Jan  3 03:23:44.586: INFO: Received response from host: affinity-nodeport-transition-qf562
Jan  3 03:23:44.586: INFO: Received response from host: affinity-nodeport-transition-qf562
Jan  3 03:23:44.586: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-180, will wait for the garbage collector to delete the pods 01/03/23 03:23:44.615
Jan  3 03:23:44.679: INFO: Deleting ReplicationController affinity-nodeport-transition took: 2.454174ms
Jan  3 03:23:44.779: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.543115ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan  3 03:23:47.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-180" for this suite. 01/03/23 03:23:47.279
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","completed":359,"skipped":6663,"failed":0}
------------------------------
• [SLOW TEST] [9.823 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:23:37.458
    Jan  3 03:23:37.459: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename services 01/03/23 03:23:37.459
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:23:37.47
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:23:37.472
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2237
    STEP: creating service in namespace services-180 01/03/23 03:23:37.475
    STEP: creating service affinity-nodeport-transition in namespace services-180 01/03/23 03:23:37.475
    STEP: creating replication controller affinity-nodeport-transition in namespace services-180 01/03/23 03:23:37.534
    I0103 03:23:37.539563      23 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-180, replica count: 3
    I0103 03:23:40.590513      23 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  3 03:23:40.596: INFO: Creating new exec pod
    Jan  3 03:23:40.601: INFO: Waiting up to 5m0s for pod "execpod-affinitynvwbt" in namespace "services-180" to be "running"
    Jan  3 03:23:40.631: INFO: Pod "execpod-affinitynvwbt": Phase="Pending", Reason="", readiness=false. Elapsed: 30.454743ms
    Jan  3 03:23:42.643: INFO: Pod "execpod-affinitynvwbt": Phase="Running", Reason="", readiness=true. Elapsed: 2.042525783s
    Jan  3 03:23:42.643: INFO: Pod "execpod-affinitynvwbt" satisfied condition "running"
    Jan  3 03:23:43.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-180 exec execpod-affinitynvwbt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
    Jan  3 03:23:43.782: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Jan  3 03:23:43.782: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  3 03:23:43.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-180 exec execpod-affinitynvwbt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.222.182 80'
    Jan  3 03:23:43.923: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.222.182 80\nConnection to 10.96.222.182 80 port [tcp/http] succeeded!\n"
    Jan  3 03:23:43.923: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  3 03:23:43.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-180 exec execpod-affinitynvwbt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.7.7 31355'
    Jan  3 03:23:44.045: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.7.7 31355\nConnection to 172.21.7.7 31355 port [tcp/*] succeeded!\n"
    Jan  3 03:23:44.045: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  3 03:23:44.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-180 exec execpod-affinitynvwbt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.7.13 31355'
    Jan  3 03:23:44.173: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.7.13 31355\nConnection to 172.21.7.13 31355 port [tcp/*] succeeded!\n"
    Jan  3 03:23:44.173: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan  3 03:23:44.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-180 exec execpod-affinitynvwbt -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.7.5:31355/ ; done'
    Jan  3 03:23:44.394: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n"
    Jan  3 03:23:44.394: INFO: stdout: "\naffinity-nodeport-transition-qf562\naffinity-nodeport-transition-jmmjb\naffinity-nodeport-transition-qf562\naffinity-nodeport-transition-cdr8n\naffinity-nodeport-transition-cdr8n\naffinity-nodeport-transition-jmmjb\naffinity-nodeport-transition-jmmjb\naffinity-nodeport-transition-cdr8n\naffinity-nodeport-transition-qf562\naffinity-nodeport-transition-qf562\naffinity-nodeport-transition-cdr8n\naffinity-nodeport-transition-jmmjb\naffinity-nodeport-transition-cdr8n\naffinity-nodeport-transition-cdr8n\naffinity-nodeport-transition-cdr8n\naffinity-nodeport-transition-qf562"
    Jan  3 03:23:44.394: INFO: Received response from host: affinity-nodeport-transition-qf562
    Jan  3 03:23:44.394: INFO: Received response from host: affinity-nodeport-transition-jmmjb
    Jan  3 03:23:44.394: INFO: Received response from host: affinity-nodeport-transition-qf562
    Jan  3 03:23:44.394: INFO: Received response from host: affinity-nodeport-transition-cdr8n
    Jan  3 03:23:44.394: INFO: Received response from host: affinity-nodeport-transition-cdr8n
    Jan  3 03:23:44.394: INFO: Received response from host: affinity-nodeport-transition-jmmjb
    Jan  3 03:23:44.394: INFO: Received response from host: affinity-nodeport-transition-jmmjb
    Jan  3 03:23:44.394: INFO: Received response from host: affinity-nodeport-transition-cdr8n
    Jan  3 03:23:44.394: INFO: Received response from host: affinity-nodeport-transition-qf562
    Jan  3 03:23:44.394: INFO: Received response from host: affinity-nodeport-transition-qf562
    Jan  3 03:23:44.394: INFO: Received response from host: affinity-nodeport-transition-cdr8n
    Jan  3 03:23:44.394: INFO: Received response from host: affinity-nodeport-transition-jmmjb
    Jan  3 03:23:44.394: INFO: Received response from host: affinity-nodeport-transition-cdr8n
    Jan  3 03:23:44.394: INFO: Received response from host: affinity-nodeport-transition-cdr8n
    Jan  3 03:23:44.394: INFO: Received response from host: affinity-nodeport-transition-cdr8n
    Jan  3 03:23:44.394: INFO: Received response from host: affinity-nodeport-transition-qf562
    Jan  3 03:23:44.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2125277514 --namespace=services-180 exec execpod-affinitynvwbt -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.7.5:31355/ ; done'
    Jan  3 03:23:44.586: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.5:31355/\n"
    Jan  3 03:23:44.586: INFO: stdout: "\naffinity-nodeport-transition-qf562\naffinity-nodeport-transition-qf562\naffinity-nodeport-transition-qf562\naffinity-nodeport-transition-qf562\naffinity-nodeport-transition-qf562\naffinity-nodeport-transition-qf562\naffinity-nodeport-transition-qf562\naffinity-nodeport-transition-qf562\naffinity-nodeport-transition-qf562\naffinity-nodeport-transition-qf562\naffinity-nodeport-transition-qf562\naffinity-nodeport-transition-qf562\naffinity-nodeport-transition-qf562\naffinity-nodeport-transition-qf562\naffinity-nodeport-transition-qf562\naffinity-nodeport-transition-qf562"
    Jan  3 03:23:44.586: INFO: Received response from host: affinity-nodeport-transition-qf562
    Jan  3 03:23:44.586: INFO: Received response from host: affinity-nodeport-transition-qf562
    Jan  3 03:23:44.586: INFO: Received response from host: affinity-nodeport-transition-qf562
    Jan  3 03:23:44.586: INFO: Received response from host: affinity-nodeport-transition-qf562
    Jan  3 03:23:44.586: INFO: Received response from host: affinity-nodeport-transition-qf562
    Jan  3 03:23:44.586: INFO: Received response from host: affinity-nodeport-transition-qf562
    Jan  3 03:23:44.586: INFO: Received response from host: affinity-nodeport-transition-qf562
    Jan  3 03:23:44.586: INFO: Received response from host: affinity-nodeport-transition-qf562
    Jan  3 03:23:44.586: INFO: Received response from host: affinity-nodeport-transition-qf562
    Jan  3 03:23:44.586: INFO: Received response from host: affinity-nodeport-transition-qf562
    Jan  3 03:23:44.586: INFO: Received response from host: affinity-nodeport-transition-qf562
    Jan  3 03:23:44.586: INFO: Received response from host: affinity-nodeport-transition-qf562
    Jan  3 03:23:44.586: INFO: Received response from host: affinity-nodeport-transition-qf562
    Jan  3 03:23:44.586: INFO: Received response from host: affinity-nodeport-transition-qf562
    Jan  3 03:23:44.586: INFO: Received response from host: affinity-nodeport-transition-qf562
    Jan  3 03:23:44.586: INFO: Received response from host: affinity-nodeport-transition-qf562
    Jan  3 03:23:44.586: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-180, will wait for the garbage collector to delete the pods 01/03/23 03:23:44.615
    Jan  3 03:23:44.679: INFO: Deleting ReplicationController affinity-nodeport-transition took: 2.454174ms
    Jan  3 03:23:44.779: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.543115ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan  3 03:23:47.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-180" for this suite. 01/03/23 03:23:47.279
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:23:47.283
Jan  3 03:23:47.283: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename container-lifecycle-hook 01/03/23 03:23:47.284
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:23:47.301
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:23:47.302
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 01/03/23 03:23:47.347
Jan  3 03:23:47.355: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8800" to be "running and ready"
Jan  3 03:23:47.358: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.647511ms
Jan  3 03:23:47.358: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan  3 03:23:49.360: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.004734201s
Jan  3 03:23:49.360: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan  3 03:23:49.360: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
STEP: create the pod with lifecycle hook 01/03/23 03:23:49.361
Jan  3 03:23:49.366: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-8800" to be "running and ready"
Jan  3 03:23:49.370: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.784079ms
Jan  3 03:23:49.370: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan  3 03:23:51.372: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.006255456s
Jan  3 03:23:51.372: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Jan  3 03:23:51.372: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 01/03/23 03:23:51.374
STEP: delete the pod with lifecycle hook 01/03/23 03:23:51.378
Jan  3 03:23:51.380: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan  3 03:23:51.403: INFO: Pod pod-with-poststart-http-hook still exists
Jan  3 03:23:53.404: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan  3 03:23:53.420: INFO: Pod pod-with-poststart-http-hook still exists
Jan  3 03:23:55.404: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan  3 03:23:55.406: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jan  3 03:23:55.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8800" for this suite. 01/03/23 03:23:55.409
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","completed":360,"skipped":6674,"failed":0}
------------------------------
• [SLOW TEST] [8.129 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:23:47.283
    Jan  3 03:23:47.283: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/03/23 03:23:47.284
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:23:47.301
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:23:47.302
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 01/03/23 03:23:47.347
    Jan  3 03:23:47.355: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8800" to be "running and ready"
    Jan  3 03:23:47.358: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.647511ms
    Jan  3 03:23:47.358: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 03:23:49.360: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.004734201s
    Jan  3 03:23:49.360: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan  3 03:23:49.360: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:130
    STEP: create the pod with lifecycle hook 01/03/23 03:23:49.361
    Jan  3 03:23:49.366: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-8800" to be "running and ready"
    Jan  3 03:23:49.370: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.784079ms
    Jan  3 03:23:49.370: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan  3 03:23:51.372: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.006255456s
    Jan  3 03:23:51.372: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Jan  3 03:23:51.372: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 01/03/23 03:23:51.374
    STEP: delete the pod with lifecycle hook 01/03/23 03:23:51.378
    Jan  3 03:23:51.380: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan  3 03:23:51.403: INFO: Pod pod-with-poststart-http-hook still exists
    Jan  3 03:23:53.404: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan  3 03:23:53.420: INFO: Pod pod-with-poststart-http-hook still exists
    Jan  3 03:23:55.404: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan  3 03:23:55.406: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jan  3 03:23:55.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-8800" for this suite. 01/03/23 03:23:55.409
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:23:55.412
Jan  3 03:23:55.412: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename sysctl 01/03/23 03:23:55.413
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:23:55.422
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:23:55.424
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 01/03/23 03:23:55.426
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Jan  3 03:23:55.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-1084" for this suite. 01/03/23 03:23:55.443
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":361,"skipped":6676,"failed":0}
------------------------------
• [0.039 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:23:55.412
    Jan  3 03:23:55.412: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename sysctl 01/03/23 03:23:55.413
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:23:55.422
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:23:55.424
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 01/03/23 03:23:55.426
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan  3 03:23:55.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-1084" for this suite. 01/03/23 03:23:55.443
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/03/23 03:23:55.452
Jan  3 03:23:55.452: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
STEP: Building a namespace api object, basename projected 01/03/23 03:23:55.453
STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:23:55.494
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:23:55.496
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
STEP: Creating a pod to test downward API volume plugin 01/03/23 03:23:55.498
Jan  3 03:23:55.504: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c33cd09e-4380-4dc7-9904-3e2e9b7730c3" in namespace "projected-465" to be "Succeeded or Failed"
Jan  3 03:23:55.513: INFO: Pod "downwardapi-volume-c33cd09e-4380-4dc7-9904-3e2e9b7730c3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.111893ms
Jan  3 03:23:57.552: INFO: Pod "downwardapi-volume-c33cd09e-4380-4dc7-9904-3e2e9b7730c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047934677s
Jan  3 03:23:59.517: INFO: Pod "downwardapi-volume-c33cd09e-4380-4dc7-9904-3e2e9b7730c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01346413s
STEP: Saw pod success 01/03/23 03:23:59.517
Jan  3 03:23:59.517: INFO: Pod "downwardapi-volume-c33cd09e-4380-4dc7-9904-3e2e9b7730c3" satisfied condition "Succeeded or Failed"
Jan  3 03:23:59.519: INFO: Trying to get logs from node cncf-master pod downwardapi-volume-c33cd09e-4380-4dc7-9904-3e2e9b7730c3 container client-container: <nil>
STEP: delete the pod 01/03/23 03:23:59.522
Jan  3 03:23:59.542: INFO: Waiting for pod downwardapi-volume-c33cd09e-4380-4dc7-9904-3e2e9b7730c3 to disappear
Jan  3 03:23:59.554: INFO: Pod downwardapi-volume-c33cd09e-4380-4dc7-9904-3e2e9b7730c3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan  3 03:23:59.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-465" for this suite. 01/03/23 03:23:59.557
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":362,"skipped":6695,"failed":0}
------------------------------
• [4.107 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/03/23 03:23:55.452
    Jan  3 03:23:55.452: INFO: >>> kubeConfig: /tmp/kubeconfig-2125277514
    STEP: Building a namespace api object, basename projected 01/03/23 03:23:55.453
    STEP: Waiting for a default service account to be provisioned in namespace 01/03/23 03:23:55.494
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/03/23 03:23:55.496
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:83
    STEP: Creating a pod to test downward API volume plugin 01/03/23 03:23:55.498
    Jan  3 03:23:55.504: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c33cd09e-4380-4dc7-9904-3e2e9b7730c3" in namespace "projected-465" to be "Succeeded or Failed"
    Jan  3 03:23:55.513: INFO: Pod "downwardapi-volume-c33cd09e-4380-4dc7-9904-3e2e9b7730c3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.111893ms
    Jan  3 03:23:57.552: INFO: Pod "downwardapi-volume-c33cd09e-4380-4dc7-9904-3e2e9b7730c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047934677s
    Jan  3 03:23:59.517: INFO: Pod "downwardapi-volume-c33cd09e-4380-4dc7-9904-3e2e9b7730c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01346413s
    STEP: Saw pod success 01/03/23 03:23:59.517
    Jan  3 03:23:59.517: INFO: Pod "downwardapi-volume-c33cd09e-4380-4dc7-9904-3e2e9b7730c3" satisfied condition "Succeeded or Failed"
    Jan  3 03:23:59.519: INFO: Trying to get logs from node cncf-master pod downwardapi-volume-c33cd09e-4380-4dc7-9904-3e2e9b7730c3 container client-container: <nil>
    STEP: delete the pod 01/03/23 03:23:59.522
    Jan  3 03:23:59.542: INFO: Waiting for pod downwardapi-volume-c33cd09e-4380-4dc7-9904-3e2e9b7730c3 to disappear
    Jan  3 03:23:59.554: INFO: Pod downwardapi-volume-c33cd09e-4380-4dc7-9904-3e2e9b7730c3 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan  3 03:23:59.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-465" for this suite. 01/03/23 03:23:59.557
    [ReportAfterEach] TOP-LEVEL
      test/e2e/e2e_test.go:142
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S[ReportAfterEach] TOP-LEVEL
  test/e2e/e2e_test.go:142
S
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
{"msg":"Test Suite completed","completed":362,"skipped":6705,"failed":0}
Jan  3 03:23:59.560: INFO: Running AfterSuite actions on all nodes
Jan  3 03:23:59.560: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
Jan  3 03:23:59.560: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
Jan  3 03:23:59.560: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
Jan  3 03:23:59.560: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Jan  3 03:23:59.560: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Jan  3 03:23:59.560: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Jan  3 03:23:59.560: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
Jan  3 03:23:59.560: INFO: Running AfterSuite actions on node 1
Jan  3 03:23:59.560: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Jan  3 03:23:59.560: INFO: Running AfterSuite actions on all nodes
    Jan  3 03:23:59.560: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
    Jan  3 03:23:59.560: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
    Jan  3 03:23:59.560: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
    Jan  3 03:23:59.560: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
    Jan  3 03:23:59.560: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
    Jan  3 03:23:59.560: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
    Jan  3 03:23:59.560: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Jan  3 03:23:59.560: INFO: Running AfterSuite actions on node 1
    Jan  3 03:23:59.560: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:146
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:146
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:559
------------------------------
[ReportAfterSuite] PASSED [0.062 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:559
  << End Captured GinkgoWriter Output
------------------------------

Ran 362 of 7067 Specs in 5828.663 seconds
SUCCESS! -- 362 Passed | 0 Failed | 0 Pending | 6705 Skipped
PASS

Ginkgo ran 1 suite in 1h37m8.898675795s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.1.4[0m

