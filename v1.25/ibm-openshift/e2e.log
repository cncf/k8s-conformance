I0302 01:07:37.850828      20 e2e.go:116] Starting e2e run "bb38eda3-bda3-4c13-b1e5-e09c1a12bd1d" on Ginkgo node 1
Mar  2 01:07:37.879: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1677719257 - will randomize all specs

Will run 362 of 7067 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
{"msg":"Test Suite starting","completed":0,"skipped":0,"failed":0}
Mar  2 01:07:38.182: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
E0302 01:07:38.183740      20 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Mar  2 01:07:38.183: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
E0302 01:07:38.183740      20 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Mar  2 01:07:38.232: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Mar  2 01:07:38.309: INFO: 14 / 14 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Mar  2 01:07:38.310: INFO: expected 5 pod replicas in namespace 'kube-system', 5 are Running and Ready.
Mar  2 01:07:38.310: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Mar  2 01:07:38.328: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-keepalived-watcher' (0 seconds elapsed)
Mar  2 01:07:38.328: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibmcloud-block-storage-driver' (0 seconds elapsed)
Mar  2 01:07:38.328: INFO: e2e test version: v1.25.4
Mar  2 01:07:38.371: INFO: kube-apiserver version: v1.25.4+a34b9e9
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
Mar  2 01:07:38.371: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
Mar  2 01:07:38.392: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.211 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Mar  2 01:07:38.182: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    Mar  2 01:07:38.183: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    E0302 01:07:38.183740      20 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
    Mar  2 01:07:38.232: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Mar  2 01:07:38.309: INFO: 14 / 14 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Mar  2 01:07:38.310: INFO: expected 5 pod replicas in namespace 'kube-system', 5 are Running and Ready.
    Mar  2 01:07:38.310: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Mar  2 01:07:38.328: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-keepalived-watcher' (0 seconds elapsed)
    Mar  2 01:07:38.328: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibmcloud-block-storage-driver' (0 seconds elapsed)
    Mar  2 01:07:38.328: INFO: e2e test version: v1.25.4
    Mar  2 01:07:38.371: INFO: kube-apiserver version: v1.25.4+a34b9e9
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Mar  2 01:07:38.371: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    Mar  2 01:07:38.392: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:07:38.447
Mar  2 01:07:38.448: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename emptydir-wrapper 03/02/23 01:07:38.45
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:07:38.554
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:07:38.565
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Mar  2 01:07:38.662: INFO: Waiting up to 5m0s for pod "pod-secrets-95c46426-9b4d-40f1-835e-b1c42ad35b9c" in namespace "emptydir-wrapper-702" to be "running and ready"
Mar  2 01:07:38.684: INFO: Pod "pod-secrets-95c46426-9b4d-40f1-835e-b1c42ad35b9c": Phase="Pending", Reason="", readiness=false. Elapsed: 21.877397ms
Mar  2 01:07:38.684: INFO: The phase of Pod pod-secrets-95c46426-9b4d-40f1-835e-b1c42ad35b9c is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:07:40.700: INFO: Pod "pod-secrets-95c46426-9b4d-40f1-835e-b1c42ad35b9c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037832932s
Mar  2 01:07:40.700: INFO: The phase of Pod pod-secrets-95c46426-9b4d-40f1-835e-b1c42ad35b9c is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:07:42.710: INFO: Pod "pod-secrets-95c46426-9b4d-40f1-835e-b1c42ad35b9c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047378402s
Mar  2 01:07:42.710: INFO: The phase of Pod pod-secrets-95c46426-9b4d-40f1-835e-b1c42ad35b9c is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:07:44.717: INFO: Pod "pod-secrets-95c46426-9b4d-40f1-835e-b1c42ad35b9c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.054384697s
Mar  2 01:07:44.717: INFO: The phase of Pod pod-secrets-95c46426-9b4d-40f1-835e-b1c42ad35b9c is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:07:46.745: INFO: Pod "pod-secrets-95c46426-9b4d-40f1-835e-b1c42ad35b9c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.082199051s
Mar  2 01:07:46.745: INFO: The phase of Pod pod-secrets-95c46426-9b4d-40f1-835e-b1c42ad35b9c is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:07:48.701: INFO: Pod "pod-secrets-95c46426-9b4d-40f1-835e-b1c42ad35b9c": Phase="Running", Reason="", readiness=true. Elapsed: 10.038761074s
Mar  2 01:07:48.701: INFO: The phase of Pod pod-secrets-95c46426-9b4d-40f1-835e-b1c42ad35b9c is Running (Ready = true)
Mar  2 01:07:48.701: INFO: Pod "pod-secrets-95c46426-9b4d-40f1-835e-b1c42ad35b9c" satisfied condition "running and ready"
STEP: Cleaning up the secret 03/02/23 01:07:48.721
STEP: Cleaning up the configmap 03/02/23 01:07:48.745
STEP: Cleaning up the pod 03/02/23 01:07:48.769
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Mar  2 01:07:48.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-702" for this suite. 03/02/23 01:07:48.847
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","completed":1,"skipped":86,"failed":0}
------------------------------
• [SLOW TEST] [10.429 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:07:38.447
    Mar  2 01:07:38.448: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename emptydir-wrapper 03/02/23 01:07:38.45
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:07:38.554
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:07:38.565
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Mar  2 01:07:38.662: INFO: Waiting up to 5m0s for pod "pod-secrets-95c46426-9b4d-40f1-835e-b1c42ad35b9c" in namespace "emptydir-wrapper-702" to be "running and ready"
    Mar  2 01:07:38.684: INFO: Pod "pod-secrets-95c46426-9b4d-40f1-835e-b1c42ad35b9c": Phase="Pending", Reason="", readiness=false. Elapsed: 21.877397ms
    Mar  2 01:07:38.684: INFO: The phase of Pod pod-secrets-95c46426-9b4d-40f1-835e-b1c42ad35b9c is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 01:07:40.700: INFO: Pod "pod-secrets-95c46426-9b4d-40f1-835e-b1c42ad35b9c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037832932s
    Mar  2 01:07:40.700: INFO: The phase of Pod pod-secrets-95c46426-9b4d-40f1-835e-b1c42ad35b9c is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 01:07:42.710: INFO: Pod "pod-secrets-95c46426-9b4d-40f1-835e-b1c42ad35b9c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047378402s
    Mar  2 01:07:42.710: INFO: The phase of Pod pod-secrets-95c46426-9b4d-40f1-835e-b1c42ad35b9c is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 01:07:44.717: INFO: Pod "pod-secrets-95c46426-9b4d-40f1-835e-b1c42ad35b9c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.054384697s
    Mar  2 01:07:44.717: INFO: The phase of Pod pod-secrets-95c46426-9b4d-40f1-835e-b1c42ad35b9c is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 01:07:46.745: INFO: Pod "pod-secrets-95c46426-9b4d-40f1-835e-b1c42ad35b9c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.082199051s
    Mar  2 01:07:46.745: INFO: The phase of Pod pod-secrets-95c46426-9b4d-40f1-835e-b1c42ad35b9c is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 01:07:48.701: INFO: Pod "pod-secrets-95c46426-9b4d-40f1-835e-b1c42ad35b9c": Phase="Running", Reason="", readiness=true. Elapsed: 10.038761074s
    Mar  2 01:07:48.701: INFO: The phase of Pod pod-secrets-95c46426-9b4d-40f1-835e-b1c42ad35b9c is Running (Ready = true)
    Mar  2 01:07:48.701: INFO: Pod "pod-secrets-95c46426-9b4d-40f1-835e-b1c42ad35b9c" satisfied condition "running and ready"
    STEP: Cleaning up the secret 03/02/23 01:07:48.721
    STEP: Cleaning up the configmap 03/02/23 01:07:48.745
    STEP: Cleaning up the pod 03/02/23 01:07:48.769
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Mar  2 01:07:48.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-702" for this suite. 03/02/23 01:07:48.847
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:07:48.881
Mar  2 01:07:48.881: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename kubectl 03/02/23 01:07:48.882
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:07:48.943
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:07:48.967
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
STEP: creating all guestbook components 03/02/23 01:07:48.98
Mar  2 01:07:48.980: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Mar  2 01:07:48.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-9171 create -f -'
Mar  2 01:07:50.065: INFO: stderr: ""
Mar  2 01:07:50.065: INFO: stdout: "service/agnhost-replica created\n"
Mar  2 01:07:50.065: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Mar  2 01:07:50.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-9171 create -f -'
Mar  2 01:07:50.909: INFO: stderr: ""
Mar  2 01:07:50.909: INFO: stdout: "service/agnhost-primary created\n"
Mar  2 01:07:50.910: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Mar  2 01:07:50.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-9171 create -f -'
Mar  2 01:07:51.692: INFO: stderr: ""
Mar  2 01:07:51.692: INFO: stdout: "service/frontend created\n"
Mar  2 01:07:51.692: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Mar  2 01:07:51.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-9171 create -f -'
Mar  2 01:07:52.588: INFO: stderr: ""
Mar  2 01:07:52.588: INFO: stdout: "deployment.apps/frontend created\n"
Mar  2 01:07:52.588: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar  2 01:07:52.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-9171 create -f -'
Mar  2 01:07:53.503: INFO: stderr: ""
Mar  2 01:07:53.503: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Mar  2 01:07:53.503: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar  2 01:07:53.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-9171 create -f -'
Mar  2 01:07:54.591: INFO: stderr: ""
Mar  2 01:07:54.591: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 03/02/23 01:07:54.591
Mar  2 01:07:54.591: INFO: Waiting for all frontend pods to be Running.
Mar  2 01:08:04.645: INFO: Waiting for frontend to serve content.
Mar  2 01:08:04.710: INFO: Trying to add a new entry to the guestbook.
Mar  2 01:08:04.782: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 03/02/23 01:08:04.805
Mar  2 01:08:04.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-9171 delete --grace-period=0 --force -f -'
Mar  2 01:08:04.968: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 01:08:04.969: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 03/02/23 01:08:04.969
Mar  2 01:08:04.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-9171 delete --grace-period=0 --force -f -'
Mar  2 01:08:05.185: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 01:08:05.185: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 03/02/23 01:08:05.186
Mar  2 01:08:05.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-9171 delete --grace-period=0 --force -f -'
Mar  2 01:08:05.462: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 01:08:05.462: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 03/02/23 01:08:05.462
Mar  2 01:08:05.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-9171 delete --grace-period=0 --force -f -'
Mar  2 01:08:05.654: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 01:08:05.654: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 03/02/23 01:08:05.654
Mar  2 01:08:05.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-9171 delete --grace-period=0 --force -f -'
Mar  2 01:08:05.920: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 01:08:05.920: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 03/02/23 01:08:05.92
Mar  2 01:08:05.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-9171 delete --grace-period=0 --force -f -'
Mar  2 01:08:06.092: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 01:08:06.092: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  2 01:08:06.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9171" for this suite. 03/02/23 01:08:06.171
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","completed":2,"skipped":122,"failed":0}
------------------------------
• [SLOW TEST] [17.342 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:367
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:07:48.881
    Mar  2 01:07:48.881: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename kubectl 03/02/23 01:07:48.882
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:07:48.943
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:07:48.967
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:392
    STEP: creating all guestbook components 03/02/23 01:07:48.98
    Mar  2 01:07:48.980: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Mar  2 01:07:48.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-9171 create -f -'
    Mar  2 01:07:50.065: INFO: stderr: ""
    Mar  2 01:07:50.065: INFO: stdout: "service/agnhost-replica created\n"
    Mar  2 01:07:50.065: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Mar  2 01:07:50.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-9171 create -f -'
    Mar  2 01:07:50.909: INFO: stderr: ""
    Mar  2 01:07:50.909: INFO: stdout: "service/agnhost-primary created\n"
    Mar  2 01:07:50.910: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Mar  2 01:07:50.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-9171 create -f -'
    Mar  2 01:07:51.692: INFO: stderr: ""
    Mar  2 01:07:51.692: INFO: stdout: "service/frontend created\n"
    Mar  2 01:07:51.692: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Mar  2 01:07:51.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-9171 create -f -'
    Mar  2 01:07:52.588: INFO: stderr: ""
    Mar  2 01:07:52.588: INFO: stdout: "deployment.apps/frontend created\n"
    Mar  2 01:07:52.588: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Mar  2 01:07:52.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-9171 create -f -'
    Mar  2 01:07:53.503: INFO: stderr: ""
    Mar  2 01:07:53.503: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Mar  2 01:07:53.503: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Mar  2 01:07:53.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-9171 create -f -'
    Mar  2 01:07:54.591: INFO: stderr: ""
    Mar  2 01:07:54.591: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 03/02/23 01:07:54.591
    Mar  2 01:07:54.591: INFO: Waiting for all frontend pods to be Running.
    Mar  2 01:08:04.645: INFO: Waiting for frontend to serve content.
    Mar  2 01:08:04.710: INFO: Trying to add a new entry to the guestbook.
    Mar  2 01:08:04.782: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 03/02/23 01:08:04.805
    Mar  2 01:08:04.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-9171 delete --grace-period=0 --force -f -'
    Mar  2 01:08:04.968: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar  2 01:08:04.969: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 03/02/23 01:08:04.969
    Mar  2 01:08:04.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-9171 delete --grace-period=0 --force -f -'
    Mar  2 01:08:05.185: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar  2 01:08:05.185: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 03/02/23 01:08:05.186
    Mar  2 01:08:05.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-9171 delete --grace-period=0 --force -f -'
    Mar  2 01:08:05.462: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar  2 01:08:05.462: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 03/02/23 01:08:05.462
    Mar  2 01:08:05.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-9171 delete --grace-period=0 --force -f -'
    Mar  2 01:08:05.654: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar  2 01:08:05.654: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 03/02/23 01:08:05.654
    Mar  2 01:08:05.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-9171 delete --grace-period=0 --force -f -'
    Mar  2 01:08:05.920: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar  2 01:08:05.920: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 03/02/23 01:08:05.92
    Mar  2 01:08:05.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-9171 delete --grace-period=0 --force -f -'
    Mar  2 01:08:06.092: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar  2 01:08:06.092: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  2 01:08:06.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9171" for this suite. 03/02/23 01:08:06.171
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:08:06.226
Mar  2 01:08:06.226: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename endpointslice 03/02/23 01:08:06.227
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:08:06.325
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:08:06.368
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
STEP: getting /apis 03/02/23 01:08:06.408
STEP: getting /apis/discovery.k8s.io 03/02/23 01:08:06.448
STEP: getting /apis/discovery.k8s.iov1 03/02/23 01:08:06.467
STEP: creating 03/02/23 01:08:06.473
STEP: getting 03/02/23 01:08:06.599
STEP: listing 03/02/23 01:08:06.62
STEP: watching 03/02/23 01:08:06.633
Mar  2 01:08:06.633: INFO: starting watch
STEP: cluster-wide listing 03/02/23 01:08:06.638
STEP: cluster-wide watching 03/02/23 01:08:06.665
Mar  2 01:08:06.665: INFO: starting watch
STEP: patching 03/02/23 01:08:06.67
STEP: updating 03/02/23 01:08:06.72
Mar  2 01:08:06.760: INFO: waiting for watch events with expected annotations
Mar  2 01:08:06.761: INFO: saw patched and updated annotations
STEP: deleting 03/02/23 01:08:06.761
STEP: deleting a collection 03/02/23 01:08:06.829
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Mar  2 01:08:06.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-2030" for this suite. 03/02/23 01:08:06.953
{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","completed":3,"skipped":166,"failed":0}
------------------------------
• [0.750 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:08:06.226
    Mar  2 01:08:06.226: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename endpointslice 03/02/23 01:08:06.227
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:08:06.325
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:08:06.368
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:352
    STEP: getting /apis 03/02/23 01:08:06.408
    STEP: getting /apis/discovery.k8s.io 03/02/23 01:08:06.448
    STEP: getting /apis/discovery.k8s.iov1 03/02/23 01:08:06.467
    STEP: creating 03/02/23 01:08:06.473
    STEP: getting 03/02/23 01:08:06.599
    STEP: listing 03/02/23 01:08:06.62
    STEP: watching 03/02/23 01:08:06.633
    Mar  2 01:08:06.633: INFO: starting watch
    STEP: cluster-wide listing 03/02/23 01:08:06.638
    STEP: cluster-wide watching 03/02/23 01:08:06.665
    Mar  2 01:08:06.665: INFO: starting watch
    STEP: patching 03/02/23 01:08:06.67
    STEP: updating 03/02/23 01:08:06.72
    Mar  2 01:08:06.760: INFO: waiting for watch events with expected annotations
    Mar  2 01:08:06.761: INFO: saw patched and updated annotations
    STEP: deleting 03/02/23 01:08:06.761
    STEP: deleting a collection 03/02/23 01:08:06.829
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Mar  2 01:08:06.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-2030" for this suite. 03/02/23 01:08:06.953
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:08:06.98
Mar  2 01:08:06.980: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename kubectl 03/02/23 01:08:06.981
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:08:07.057
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:08:07.07
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
Mar  2 01:08:07.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-7479 create -f -'
Mar  2 01:08:08.201: INFO: stderr: ""
Mar  2 01:08:08.201: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Mar  2 01:08:08.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-7479 create -f -'
Mar  2 01:08:08.976: INFO: stderr: ""
Mar  2 01:08:08.976: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 03/02/23 01:08:08.976
Mar  2 01:08:10.033: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 01:08:10.033: INFO: Found 0 / 1
Mar  2 01:08:10.993: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 01:08:10.993: INFO: Found 1 / 1
Mar  2 01:08:10.993: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar  2 01:08:11.008: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 01:08:11.008: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  2 01:08:11.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-7479 describe pod agnhost-primary-f2k7c'
Mar  2 01:08:11.191: INFO: stderr: ""
Mar  2 01:08:11.191: INFO: stdout: "Name:             agnhost-primary-f2k7c\nNamespace:        kubectl-7479\nPriority:         0\nService Account:  default\nNode:             10.132.92.143/10.132.92.143\nStart Time:       Thu, 02 Mar 2023 01:08:08 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: a7de7e9556c77d0af0610ec9fdf9d3bb972fc898704cb8295b2339c11f064912\n                  cni.projectcalico.org/podIP: 172.30.156.121/32\n                  cni.projectcalico.org/podIPs: 172.30.156.121/32\n                  k8s.v1.cni.cncf.io/network-status:\n                    [{\n                        \"name\": \"k8s-pod-network\",\n                        \"ips\": [\n                            \"172.30.156.121\"\n                        ],\n                        \"default\": true,\n                        \"dns\": {}\n                    }]\n                  k8s.v1.cni.cncf.io/networks-status:\n                    [{\n                        \"name\": \"k8s-pod-network\",\n                        \"ips\": [\n                            \"172.30.156.121\"\n                        ],\n                        \"default\": true,\n                        \"dns\": {}\n                    }]\n                  openshift.io/scc: anyuid\nStatus:           Running\nIP:               172.30.156.121\nIPs:\n  IP:           172.30.156.121\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://e51e70ca94df38d6a5fdbc0e205688660acc447e1418060bb90e77e7265eeb94\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 02 Mar 2023 01:08:10 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-kn47b (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-kn47b:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\n    ConfigMapName:           openshift-service-ca.crt\n    ConfigMapOptional:       <nil>\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason          Age   From               Message\n  ----    ------          ----  ----               -------\n  Normal  Scheduled       3s    default-scheduler  Successfully assigned kubectl-7479/agnhost-primary-f2k7c to 10.132.92.143\n  Normal  AddedInterface  2s    multus             Add eth0 [172.30.156.121/32] from k8s-pod-network\n  Normal  Pulled          2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created         1s    kubelet            Created container agnhost-primary\n  Normal  Started         1s    kubelet            Started container agnhost-primary\n"
Mar  2 01:08:11.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-7479 describe rc agnhost-primary'
Mar  2 01:08:11.477: INFO: stderr: ""
Mar  2 01:08:11.477: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-7479\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-f2k7c\n"
Mar  2 01:08:11.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-7479 describe service agnhost-primary'
Mar  2 01:08:11.643: INFO: stderr: ""
Mar  2 01:08:11.643: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-7479\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                172.21.245.250\nIPs:               172.21.245.250\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.30.156.121:6379\nSession Affinity:  None\nEvents:            <none>\n"
Mar  2 01:08:11.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-7479 describe node 10.132.92.143'
Mar  2 01:08:12.282: INFO: stderr: ""
Mar  2 01:08:12.282: INFO: stdout: "Name:               10.132.92.143\nRoles:              master,worker\nLabels:             arch=amd64\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=jp-tok\n                    failure-domain.beta.kubernetes.io/zone=tok02\n                    ibm-cloud.kubernetes.io/encrypted-docker-data=true\n                    ibm-cloud.kubernetes.io/external-ip=161.202.69.45\n                    ibm-cloud.kubernetes.io/iaas-provider=softlayer\n                    ibm-cloud.kubernetes.io/internal-ip=10.132.92.143\n                    ibm-cloud.kubernetes.io/machine-type=b3c.4x16.encrypted\n                    ibm-cloud.kubernetes.io/os=REDHAT_8_64\n                    ibm-cloud.kubernetes.io/region=jp-tok\n                    ibm-cloud.kubernetes.io/sgx-enabled=false\n                    ibm-cloud.kubernetes.io/worker-id=kube-cfvstrkt0sgh86rm4olg-kubee2epvgx-default-000003db\n                    ibm-cloud.kubernetes.io/worker-pool-id=cfvstrkt0sgh86rm4olg-c22e046\n                    ibm-cloud.kubernetes.io/worker-pool-name=default\n                    ibm-cloud.kubernetes.io/worker-version=4.12.4_1528_openshift\n                    ibm-cloud.kubernetes.io/zone=tok02\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.132.92.143\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\n                    node-role.kubernetes.io/worker=\n                    node.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    node.openshift.io/os_id=rhel\n                    privateVLAN=2723046\n                    publicVLAN=2723044\n                    topology.kubernetes.io/region=jp-tok\n                    topology.kubernetes.io/zone=tok02\nAnnotations:        projectcalico.org/IPv4Address: 10.132.92.143/26\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.30.156.64\nCreationTimestamp:  Wed, 01 Mar 2023 22:48:16 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  10.132.92.143\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 02 Mar 2023 01:08:07 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 01 Mar 2023 22:52:23 +0000   Wed, 01 Mar 2023 22:52:23 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Thu, 02 Mar 2023 01:07:51 +0000   Wed, 01 Mar 2023 22:48:16 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 02 Mar 2023 01:07:51 +0000   Wed, 01 Mar 2023 22:48:16 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 02 Mar 2023 01:07:51 +0000   Wed, 01 Mar 2023 22:48:16 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 02 Mar 2023 01:07:51 +0000   Wed, 01 Mar 2023 22:53:12 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.132.92.143\n  ExternalIP:  161.202.69.45\n  Hostname:    10.132.92.143\nCapacity:\n  cpu:                4\n  ephemeral-storage:  102609848Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16386532Ki\n  pods:               110\nAllocatable:\n  cpu:                3910m\n  ephemeral-storage:  93913280025\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             13597156Ki\n  pods:               110\nSystem Info:\n  Machine ID:                             e30506b8459c455d9f41ee57150f5bc0\n  System UUID:                            8932f301-ad60-0271-eb6a-4b9fa1a88a23\n  Boot ID:                                d46fbfa0-0e67-4a20-8b4a-1e7d5596ca16\n  Kernel Version:                         4.18.0-425.13.1.el8_7.x86_64\n  OS Image:                               Red Hat Enterprise Linux 8.7 (Ootpa)\n  Operating System:                       linux\n  Architecture:                           amd64\n  Container Runtime Version:              cri-o://1.25.2-6.rhaos4.12.git3c4e50c.el8\n  Kubelet Version:                        v1.25.4+a34b9e9\n  Kube-Proxy Version:                     v1.25.4+a34b9e9\nPodCIDR:                                  172.30.1.0/24\nPodCIDRs:                                 172.30.1.0/24\nProviderID:                               ibm://fee034388aa6435883a1f720010ab3a2///cfvstrkt0sgh86rm4olg/kube-cfvstrkt0sgh86rm4olg-kubee2epvgx-default-000003db\nNon-terminated Pods:                      (31 in total)\n  Namespace                               Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                               ----                                                       ------------  ----------  ---------------  -------------  ---\n  calico-system                           calico-node-99tft                                          250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         138m\n  calico-system                           calico-typha-b7f8b755-xlnll                                250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         138m\n  ibm-system                              ibm-cloud-provider-ip-169-56-11-242-558f6ff69-jtkgc        5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         132m\n  kube-system                             ibm-keepalived-watcher-dp97c                               5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         139m\n  kube-system                             ibm-master-proxy-static-10.132.92.143                      26m (0%)      300m (7%)   32001024 (0%)    512M (3%)      139m\n  kube-system                             ibmcloud-block-storage-driver-zfc8r                        50m (1%)      300m (7%)   100Mi (0%)       300Mi (2%)     139m\n  kube-system                             vpn-f6c799ddd-kvwzk                                        5m (0%)       0 (0%)      5Mi (0%)         0 (0%)         129m\n  kubectl-7479                            agnhost-primary-f2k7c                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         4s\n  openshift-cluster-node-tuning-operator  tuned-w95xr                                                10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         132m\n  openshift-console                       console-6c8dcd4bdd-wsgwn                                   10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         128m\n  openshift-dns                           dns-default-zr27n                                          60m (1%)      0 (0%)      110Mi (0%)       0 (0%)         132m\n  openshift-dns                           node-resolver-7wklz                                        5m (0%)       0 (0%)      21Mi (0%)        0 (0%)         132m\n  openshift-image-registry                node-ca-q4vcl                                              10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         132m\n  openshift-ingress-canary                ingress-canary-rwxvb                                       10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         132m\n  openshift-ingress                       router-default-68bc8785b7-zkcbh                            100m (2%)     0 (0%)      256Mi (1%)       0 (0%)         132m\n  openshift-kube-proxy                    openshift-kube-proxy-zdts7                                 110m (2%)     0 (0%)      220Mi (1%)       0 (0%)         139m\n  openshift-monitoring                    alertmanager-main-1                                        9m (0%)       0 (0%)      120Mi (0%)       0 (0%)         130m\n  openshift-monitoring                    kube-state-metrics-554994774b-mttch                        4m (0%)       0 (0%)      110Mi (0%)       0 (0%)         131m\n  openshift-monitoring                    node-exporter-z52nh                                        9m (0%)       0 (0%)      47Mi (0%)        0 (0%)         131m\n  openshift-monitoring                    prometheus-adapter-95d69f68c-2rgvr                         1m (0%)       0 (0%)      40Mi (0%)        0 (0%)         130m\n  openshift-monitoring                    prometheus-k8s-1                                           75m (1%)      0 (0%)      1104Mi (8%)      0 (0%)         130m\n  openshift-monitoring                    prometheus-operator-admission-webhook-6d5fbffb86-7xdt5     5m (0%)       0 (0%)      30Mi (0%)        0 (0%)         132m\n  openshift-monitoring                    telemeter-client-769c487d5b-fv8s4                          3m (0%)       0 (0%)      70Mi (0%)        0 (0%)         130m\n  openshift-monitoring                    thanos-querier-6cd8656bbb-6ffxj                            15m (0%)      0 (0%)      92Mi (0%)        0 (0%)         131m\n  openshift-multus                        multus-48w86                                               10m (0%)      0 (0%)      65Mi (0%)        0 (0%)         139m\n  openshift-multus                        multus-additional-cni-plugins-d2kv5                        10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         139m\n  openshift-multus                        multus-admission-controller-6f984f76c7-mtmdg               20m (0%)      0 (0%)      70Mi (0%)        0 (0%)         132m\n  openshift-multus                        network-metrics-daemon-7jfj4                               20m (0%)      0 (0%)      120Mi (0%)       0 (0%)         139m\n  openshift-network-diagnostics           network-check-target-w7x6h                                 10m (0%)      0 (0%)      15Mi (0%)        0 (0%)         139m\n  sonobuoy                                sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         67s\n  sonobuoy                                sonobuoy-systemd-logs-daemon-set-7266eb5e626146f7-4c6tp    0 (0%)        0 (0%)      0 (0%)           0 (0%)         58s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests         Limits\n  --------           --------         ------\n  cpu                1097m (28%)      600m (15%)\n  memory             3067411Ki (22%)  826572800 (5%)\n  ephemeral-storage  0 (0%)           0 (0%)\n  hugepages-1Gi      0 (0%)           0 (0%)\n  hugepages-2Mi      0 (0%)           0 (0%)\nEvents:\n  Type    Reason                   Age                  From                   Message\n  ----    ------                   ----                 ----                   -------\n  Normal  Starting                 138m                 kube-proxy             \n  Normal  Starting                 139m                 kubelet                Starting kubelet.\n  Normal  NodeAllocatableEnforced  139m                 kubelet                Updated Node Allocatable limit across pods\n  Normal  NodeHasSufficientMemory  139m (x7 over 139m)  kubelet                Node 10.132.92.143 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    139m (x7 over 139m)  kubelet                Node 10.132.92.143 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     139m (x7 over 139m)  kubelet                Node 10.132.92.143 status is now: NodeHasSufficientPID\n  Normal  RegisteredNode           139m                 node-controller        Node 10.132.92.143 event: Registered Node 10.132.92.143 in Controller\n  Normal  Synced                   139m                 cloud-node-controller  Node synced successfully\n  Normal  NodeReady                135m                 kubelet                Node 10.132.92.143 status is now: NodeReady\n  Normal  RegisteredNode           132m                 node-controller        Node 10.132.92.143 event: Registered Node 10.132.92.143 in Controller\n"
Mar  2 01:08:12.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-7479 describe namespace kubectl-7479'
Mar  2 01:08:12.478: INFO: stderr: ""
Mar  2 01:08:12.478: INFO: stdout: "Name:         kubectl-7479\nLabels:       e2e-framework=kubectl\n              e2e-run=bb38eda3-bda3-4c13-b1e5-e09c1a12bd1d\n              kubernetes.io/metadata.name=kubectl-7479\n              pod-security.kubernetes.io/audit=privileged\n              pod-security.kubernetes.io/audit-version=v1.24\n              pod-security.kubernetes.io/enforce=baseline\n              pod-security.kubernetes.io/warn=privileged\n              pod-security.kubernetes.io/warn-version=v1.24\nAnnotations:  openshift.io/sa.scc.mcs: s0:c30,c10\n              openshift.io/sa.scc.supplemental-groups: 1000890000/10000\n              openshift.io/sa.scc.uid-range: 1000890000/10000\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  2 01:08:12.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7479" for this suite. 03/02/23 01:08:12.497
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","completed":4,"skipped":199,"failed":0}
------------------------------
• [SLOW TEST] [5.538 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1268
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1274

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:08:06.98
    Mar  2 01:08:06.980: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename kubectl 03/02/23 01:08:06.981
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:08:07.057
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:08:07.07
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1274
    Mar  2 01:08:07.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-7479 create -f -'
    Mar  2 01:08:08.201: INFO: stderr: ""
    Mar  2 01:08:08.201: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Mar  2 01:08:08.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-7479 create -f -'
    Mar  2 01:08:08.976: INFO: stderr: ""
    Mar  2 01:08:08.976: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 03/02/23 01:08:08.976
    Mar  2 01:08:10.033: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar  2 01:08:10.033: INFO: Found 0 / 1
    Mar  2 01:08:10.993: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar  2 01:08:10.993: INFO: Found 1 / 1
    Mar  2 01:08:10.993: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Mar  2 01:08:11.008: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar  2 01:08:11.008: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Mar  2 01:08:11.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-7479 describe pod agnhost-primary-f2k7c'
    Mar  2 01:08:11.191: INFO: stderr: ""
    Mar  2 01:08:11.191: INFO: stdout: "Name:             agnhost-primary-f2k7c\nNamespace:        kubectl-7479\nPriority:         0\nService Account:  default\nNode:             10.132.92.143/10.132.92.143\nStart Time:       Thu, 02 Mar 2023 01:08:08 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: a7de7e9556c77d0af0610ec9fdf9d3bb972fc898704cb8295b2339c11f064912\n                  cni.projectcalico.org/podIP: 172.30.156.121/32\n                  cni.projectcalico.org/podIPs: 172.30.156.121/32\n                  k8s.v1.cni.cncf.io/network-status:\n                    [{\n                        \"name\": \"k8s-pod-network\",\n                        \"ips\": [\n                            \"172.30.156.121\"\n                        ],\n                        \"default\": true,\n                        \"dns\": {}\n                    }]\n                  k8s.v1.cni.cncf.io/networks-status:\n                    [{\n                        \"name\": \"k8s-pod-network\",\n                        \"ips\": [\n                            \"172.30.156.121\"\n                        ],\n                        \"default\": true,\n                        \"dns\": {}\n                    }]\n                  openshift.io/scc: anyuid\nStatus:           Running\nIP:               172.30.156.121\nIPs:\n  IP:           172.30.156.121\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://e51e70ca94df38d6a5fdbc0e205688660acc447e1418060bb90e77e7265eeb94\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 02 Mar 2023 01:08:10 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-kn47b (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-kn47b:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\n    ConfigMapName:           openshift-service-ca.crt\n    ConfigMapOptional:       <nil>\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason          Age   From               Message\n  ----    ------          ----  ----               -------\n  Normal  Scheduled       3s    default-scheduler  Successfully assigned kubectl-7479/agnhost-primary-f2k7c to 10.132.92.143\n  Normal  AddedInterface  2s    multus             Add eth0 [172.30.156.121/32] from k8s-pod-network\n  Normal  Pulled          2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created         1s    kubelet            Created container agnhost-primary\n  Normal  Started         1s    kubelet            Started container agnhost-primary\n"
    Mar  2 01:08:11.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-7479 describe rc agnhost-primary'
    Mar  2 01:08:11.477: INFO: stderr: ""
    Mar  2 01:08:11.477: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-7479\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-f2k7c\n"
    Mar  2 01:08:11.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-7479 describe service agnhost-primary'
    Mar  2 01:08:11.643: INFO: stderr: ""
    Mar  2 01:08:11.643: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-7479\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                172.21.245.250\nIPs:               172.21.245.250\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.30.156.121:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Mar  2 01:08:11.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-7479 describe node 10.132.92.143'
    Mar  2 01:08:12.282: INFO: stderr: ""
    Mar  2 01:08:12.282: INFO: stdout: "Name:               10.132.92.143\nRoles:              master,worker\nLabels:             arch=amd64\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=jp-tok\n                    failure-domain.beta.kubernetes.io/zone=tok02\n                    ibm-cloud.kubernetes.io/encrypted-docker-data=true\n                    ibm-cloud.kubernetes.io/external-ip=161.202.69.45\n                    ibm-cloud.kubernetes.io/iaas-provider=softlayer\n                    ibm-cloud.kubernetes.io/internal-ip=10.132.92.143\n                    ibm-cloud.kubernetes.io/machine-type=b3c.4x16.encrypted\n                    ibm-cloud.kubernetes.io/os=REDHAT_8_64\n                    ibm-cloud.kubernetes.io/region=jp-tok\n                    ibm-cloud.kubernetes.io/sgx-enabled=false\n                    ibm-cloud.kubernetes.io/worker-id=kube-cfvstrkt0sgh86rm4olg-kubee2epvgx-default-000003db\n                    ibm-cloud.kubernetes.io/worker-pool-id=cfvstrkt0sgh86rm4olg-c22e046\n                    ibm-cloud.kubernetes.io/worker-pool-name=default\n                    ibm-cloud.kubernetes.io/worker-version=4.12.4_1528_openshift\n                    ibm-cloud.kubernetes.io/zone=tok02\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.132.92.143\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\n                    node-role.kubernetes.io/worker=\n                    node.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    node.openshift.io/os_id=rhel\n                    privateVLAN=2723046\n                    publicVLAN=2723044\n                    topology.kubernetes.io/region=jp-tok\n                    topology.kubernetes.io/zone=tok02\nAnnotations:        projectcalico.org/IPv4Address: 10.132.92.143/26\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.30.156.64\nCreationTimestamp:  Wed, 01 Mar 2023 22:48:16 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  10.132.92.143\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 02 Mar 2023 01:08:07 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 01 Mar 2023 22:52:23 +0000   Wed, 01 Mar 2023 22:52:23 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Thu, 02 Mar 2023 01:07:51 +0000   Wed, 01 Mar 2023 22:48:16 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 02 Mar 2023 01:07:51 +0000   Wed, 01 Mar 2023 22:48:16 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 02 Mar 2023 01:07:51 +0000   Wed, 01 Mar 2023 22:48:16 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 02 Mar 2023 01:07:51 +0000   Wed, 01 Mar 2023 22:53:12 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.132.92.143\n  ExternalIP:  161.202.69.45\n  Hostname:    10.132.92.143\nCapacity:\n  cpu:                4\n  ephemeral-storage:  102609848Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16386532Ki\n  pods:               110\nAllocatable:\n  cpu:                3910m\n  ephemeral-storage:  93913280025\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             13597156Ki\n  pods:               110\nSystem Info:\n  Machine ID:                             e30506b8459c455d9f41ee57150f5bc0\n  System UUID:                            8932f301-ad60-0271-eb6a-4b9fa1a88a23\n  Boot ID:                                d46fbfa0-0e67-4a20-8b4a-1e7d5596ca16\n  Kernel Version:                         4.18.0-425.13.1.el8_7.x86_64\n  OS Image:                               Red Hat Enterprise Linux 8.7 (Ootpa)\n  Operating System:                       linux\n  Architecture:                           amd64\n  Container Runtime Version:              cri-o://1.25.2-6.rhaos4.12.git3c4e50c.el8\n  Kubelet Version:                        v1.25.4+a34b9e9\n  Kube-Proxy Version:                     v1.25.4+a34b9e9\nPodCIDR:                                  172.30.1.0/24\nPodCIDRs:                                 172.30.1.0/24\nProviderID:                               ibm://fee034388aa6435883a1f720010ab3a2///cfvstrkt0sgh86rm4olg/kube-cfvstrkt0sgh86rm4olg-kubee2epvgx-default-000003db\nNon-terminated Pods:                      (31 in total)\n  Namespace                               Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                               ----                                                       ------------  ----------  ---------------  -------------  ---\n  calico-system                           calico-node-99tft                                          250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         138m\n  calico-system                           calico-typha-b7f8b755-xlnll                                250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         138m\n  ibm-system                              ibm-cloud-provider-ip-169-56-11-242-558f6ff69-jtkgc        5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         132m\n  kube-system                             ibm-keepalived-watcher-dp97c                               5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         139m\n  kube-system                             ibm-master-proxy-static-10.132.92.143                      26m (0%)      300m (7%)   32001024 (0%)    512M (3%)      139m\n  kube-system                             ibmcloud-block-storage-driver-zfc8r                        50m (1%)      300m (7%)   100Mi (0%)       300Mi (2%)     139m\n  kube-system                             vpn-f6c799ddd-kvwzk                                        5m (0%)       0 (0%)      5Mi (0%)         0 (0%)         129m\n  kubectl-7479                            agnhost-primary-f2k7c                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         4s\n  openshift-cluster-node-tuning-operator  tuned-w95xr                                                10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         132m\n  openshift-console                       console-6c8dcd4bdd-wsgwn                                   10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         128m\n  openshift-dns                           dns-default-zr27n                                          60m (1%)      0 (0%)      110Mi (0%)       0 (0%)         132m\n  openshift-dns                           node-resolver-7wklz                                        5m (0%)       0 (0%)      21Mi (0%)        0 (0%)         132m\n  openshift-image-registry                node-ca-q4vcl                                              10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         132m\n  openshift-ingress-canary                ingress-canary-rwxvb                                       10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         132m\n  openshift-ingress                       router-default-68bc8785b7-zkcbh                            100m (2%)     0 (0%)      256Mi (1%)       0 (0%)         132m\n  openshift-kube-proxy                    openshift-kube-proxy-zdts7                                 110m (2%)     0 (0%)      220Mi (1%)       0 (0%)         139m\n  openshift-monitoring                    alertmanager-main-1                                        9m (0%)       0 (0%)      120Mi (0%)       0 (0%)         130m\n  openshift-monitoring                    kube-state-metrics-554994774b-mttch                        4m (0%)       0 (0%)      110Mi (0%)       0 (0%)         131m\n  openshift-monitoring                    node-exporter-z52nh                                        9m (0%)       0 (0%)      47Mi (0%)        0 (0%)         131m\n  openshift-monitoring                    prometheus-adapter-95d69f68c-2rgvr                         1m (0%)       0 (0%)      40Mi (0%)        0 (0%)         130m\n  openshift-monitoring                    prometheus-k8s-1                                           75m (1%)      0 (0%)      1104Mi (8%)      0 (0%)         130m\n  openshift-monitoring                    prometheus-operator-admission-webhook-6d5fbffb86-7xdt5     5m (0%)       0 (0%)      30Mi (0%)        0 (0%)         132m\n  openshift-monitoring                    telemeter-client-769c487d5b-fv8s4                          3m (0%)       0 (0%)      70Mi (0%)        0 (0%)         130m\n  openshift-monitoring                    thanos-querier-6cd8656bbb-6ffxj                            15m (0%)      0 (0%)      92Mi (0%)        0 (0%)         131m\n  openshift-multus                        multus-48w86                                               10m (0%)      0 (0%)      65Mi (0%)        0 (0%)         139m\n  openshift-multus                        multus-additional-cni-plugins-d2kv5                        10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         139m\n  openshift-multus                        multus-admission-controller-6f984f76c7-mtmdg               20m (0%)      0 (0%)      70Mi (0%)        0 (0%)         132m\n  openshift-multus                        network-metrics-daemon-7jfj4                               20m (0%)      0 (0%)      120Mi (0%)       0 (0%)         139m\n  openshift-network-diagnostics           network-check-target-w7x6h                                 10m (0%)      0 (0%)      15Mi (0%)        0 (0%)         139m\n  sonobuoy                                sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         67s\n  sonobuoy                                sonobuoy-systemd-logs-daemon-set-7266eb5e626146f7-4c6tp    0 (0%)        0 (0%)      0 (0%)           0 (0%)         58s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests         Limits\n  --------           --------         ------\n  cpu                1097m (28%)      600m (15%)\n  memory             3067411Ki (22%)  826572800 (5%)\n  ephemeral-storage  0 (0%)           0 (0%)\n  hugepages-1Gi      0 (0%)           0 (0%)\n  hugepages-2Mi      0 (0%)           0 (0%)\nEvents:\n  Type    Reason                   Age                  From                   Message\n  ----    ------                   ----                 ----                   -------\n  Normal  Starting                 138m                 kube-proxy             \n  Normal  Starting                 139m                 kubelet                Starting kubelet.\n  Normal  NodeAllocatableEnforced  139m                 kubelet                Updated Node Allocatable limit across pods\n  Normal  NodeHasSufficientMemory  139m (x7 over 139m)  kubelet                Node 10.132.92.143 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    139m (x7 over 139m)  kubelet                Node 10.132.92.143 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     139m (x7 over 139m)  kubelet                Node 10.132.92.143 status is now: NodeHasSufficientPID\n  Normal  RegisteredNode           139m                 node-controller        Node 10.132.92.143 event: Registered Node 10.132.92.143 in Controller\n  Normal  Synced                   139m                 cloud-node-controller  Node synced successfully\n  Normal  NodeReady                135m                 kubelet                Node 10.132.92.143 status is now: NodeReady\n  Normal  RegisteredNode           132m                 node-controller        Node 10.132.92.143 event: Registered Node 10.132.92.143 in Controller\n"
    Mar  2 01:08:12.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-7479 describe namespace kubectl-7479'
    Mar  2 01:08:12.478: INFO: stderr: ""
    Mar  2 01:08:12.478: INFO: stdout: "Name:         kubectl-7479\nLabels:       e2e-framework=kubectl\n              e2e-run=bb38eda3-bda3-4c13-b1e5-e09c1a12bd1d\n              kubernetes.io/metadata.name=kubectl-7479\n              pod-security.kubernetes.io/audit=privileged\n              pod-security.kubernetes.io/audit-version=v1.24\n              pod-security.kubernetes.io/enforce=baseline\n              pod-security.kubernetes.io/warn=privileged\n              pod-security.kubernetes.io/warn-version=v1.24\nAnnotations:  openshift.io/sa.scc.mcs: s0:c30,c10\n              openshift.io/sa.scc.supplemental-groups: 1000890000/10000\n              openshift.io/sa.scc.uid-range: 1000890000/10000\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  2 01:08:12.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7479" for this suite. 03/02/23 01:08:12.497
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:08:12.523
Mar  2 01:08:12.523: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename deployment 03/02/23 01:08:12.524
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:08:12.569
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:08:12.577
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Mar  2 01:08:12.585: INFO: Creating deployment "test-recreate-deployment"
W0302 01:08:12.602689      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Mar  2 01:08:12.602: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Mar  2 01:08:12.661: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Mar  2 01:08:14.706: INFO: Waiting deployment "test-recreate-deployment" to complete
Mar  2 01:08:14.731: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 1, 8, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 8, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 8, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 8, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-7d8b6f647f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 01:08:16.742: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Mar  2 01:08:16.772: INFO: Updating deployment test-recreate-deployment
Mar  2 01:08:16.772: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar  2 01:08:16.989: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-810  68aaf961-e495-4b49-b10d-8574be48744e 67771 2 2023-03-02 01:08:12 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-02 01:08:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 01:08:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001ff3f18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-02 01:08:16 +0000 UTC,LastTransitionTime:2023-03-02 01:08:16 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-03-02 01:08:16 +0000 UTC,LastTransitionTime:2023-03-02 01:08:12 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Mar  2 01:08:17.027: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-810  9abce8b5-b967-4638-b253-4697b6a6063b 67770 1 2023-03-02 01:08:16 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 68aaf961-e495-4b49-b10d-8574be48744e 0xc002628030 0xc002628031}] [] [{kube-controller-manager Update apps/v1 2023-03-02 01:08:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68aaf961-e495-4b49-b10d-8574be48744e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 01:08:16 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0026280c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  2 01:08:17.027: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Mar  2 01:08:17.027: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-810  1ec3f37a-4a1a-46ef-b019-a3de6580d876 67760 2 2023-03-02 01:08:12 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 68aaf961-e495-4b49-b10d-8574be48744e 0xc001e39f17 0xc001e39f18}] [] [{kube-controller-manager Update apps/v1 2023-03-02 01:08:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68aaf961-e495-4b49-b10d-8574be48744e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 01:08:16 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001e39fc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  2 01:08:17.044: INFO: Pod "test-recreate-deployment-9d58999df-zvtxb" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-9d58999df-zvtxb test-recreate-deployment-9d58999df- deployment-810  92c0456e-a955-4c49-9b4a-894e7e5356fa 67772 0 2023-03-02 01:08:16 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df 9abce8b5-b967-4638-b253-4697b6a6063b 0xc002281f97 0xc002281f98}] [] [{kube-controller-manager Update v1 2023-03-02 01:08:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9abce8b5-b967-4638-b253-4697b6a6063b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-02 01:08:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vtpp7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vtpp7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.132.92.143,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c30,c15,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-5h4lq,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:08:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:08:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:08:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:08:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.132.92.143,PodIP:,StartTime:2023-03-02 01:08:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar  2 01:08:17.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-810" for this suite. 03/02/23 01:08:17.074
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","completed":5,"skipped":239,"failed":0}
------------------------------
• [4.571 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:08:12.523
    Mar  2 01:08:12.523: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename deployment 03/02/23 01:08:12.524
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:08:12.569
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:08:12.577
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Mar  2 01:08:12.585: INFO: Creating deployment "test-recreate-deployment"
    W0302 01:08:12.602689      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Mar  2 01:08:12.602: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Mar  2 01:08:12.661: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Mar  2 01:08:14.706: INFO: Waiting deployment "test-recreate-deployment" to complete
    Mar  2 01:08:14.731: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 1, 8, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 8, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 8, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 8, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-7d8b6f647f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  2 01:08:16.742: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Mar  2 01:08:16.772: INFO: Updating deployment test-recreate-deployment
    Mar  2 01:08:16.772: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar  2 01:08:16.989: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-810  68aaf961-e495-4b49-b10d-8574be48744e 67771 2 2023-03-02 01:08:12 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-02 01:08:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 01:08:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001ff3f18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-02 01:08:16 +0000 UTC,LastTransitionTime:2023-03-02 01:08:16 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-03-02 01:08:16 +0000 UTC,LastTransitionTime:2023-03-02 01:08:12 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Mar  2 01:08:17.027: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-810  9abce8b5-b967-4638-b253-4697b6a6063b 67770 1 2023-03-02 01:08:16 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 68aaf961-e495-4b49-b10d-8574be48744e 0xc002628030 0xc002628031}] [] [{kube-controller-manager Update apps/v1 2023-03-02 01:08:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68aaf961-e495-4b49-b10d-8574be48744e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 01:08:16 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0026280c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar  2 01:08:17.027: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Mar  2 01:08:17.027: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-810  1ec3f37a-4a1a-46ef-b019-a3de6580d876 67760 2 2023-03-02 01:08:12 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 68aaf961-e495-4b49-b10d-8574be48744e 0xc001e39f17 0xc001e39f18}] [] [{kube-controller-manager Update apps/v1 2023-03-02 01:08:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68aaf961-e495-4b49-b10d-8574be48744e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 01:08:16 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001e39fc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar  2 01:08:17.044: INFO: Pod "test-recreate-deployment-9d58999df-zvtxb" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-9d58999df-zvtxb test-recreate-deployment-9d58999df- deployment-810  92c0456e-a955-4c49-9b4a-894e7e5356fa 67772 0 2023-03-02 01:08:16 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df 9abce8b5-b967-4638-b253-4697b6a6063b 0xc002281f97 0xc002281f98}] [] [{kube-controller-manager Update v1 2023-03-02 01:08:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9abce8b5-b967-4638-b253-4697b6a6063b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-02 01:08:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vtpp7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vtpp7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.132.92.143,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c30,c15,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-5h4lq,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:08:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:08:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:08:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:08:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.132.92.143,PodIP:,StartTime:2023-03-02 01:08:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar  2 01:08:17.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-810" for this suite. 03/02/23 01:08:17.074
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:08:17.095
Mar  2 01:08:17.095: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename secrets 03/02/23 01:08:17.097
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:08:17.148
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:08:17.156
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
STEP: Creating secret with name secret-test-ef1e0354-45ad-43f4-999c-4ad439374971 03/02/23 01:08:17.195
STEP: Creating a pod to test consume secrets 03/02/23 01:08:17.21
Mar  2 01:08:17.269: INFO: Waiting up to 5m0s for pod "pod-secrets-a04d31d5-b3f3-4476-a93d-ef35c72d6ed1" in namespace "secrets-2091" to be "Succeeded or Failed"
Mar  2 01:08:17.290: INFO: Pod "pod-secrets-a04d31d5-b3f3-4476-a93d-ef35c72d6ed1": Phase="Pending", Reason="", readiness=false. Elapsed: 20.374056ms
Mar  2 01:08:19.318: INFO: Pod "pod-secrets-a04d31d5-b3f3-4476-a93d-ef35c72d6ed1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048441123s
Mar  2 01:08:21.310: INFO: Pod "pod-secrets-a04d31d5-b3f3-4476-a93d-ef35c72d6ed1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040300207s
Mar  2 01:08:23.309: INFO: Pod "pod-secrets-a04d31d5-b3f3-4476-a93d-ef35c72d6ed1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.040087288s
Mar  2 01:08:25.309: INFO: Pod "pod-secrets-a04d31d5-b3f3-4476-a93d-ef35c72d6ed1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.040001008s
STEP: Saw pod success 03/02/23 01:08:25.309
Mar  2 01:08:25.310: INFO: Pod "pod-secrets-a04d31d5-b3f3-4476-a93d-ef35c72d6ed1" satisfied condition "Succeeded or Failed"
Mar  2 01:08:25.337: INFO: Trying to get logs from node 10.132.92.143 pod pod-secrets-a04d31d5-b3f3-4476-a93d-ef35c72d6ed1 container secret-env-test: <nil>
STEP: delete the pod 03/02/23 01:08:25.429
Mar  2 01:08:25.469: INFO: Waiting for pod pod-secrets-a04d31d5-b3f3-4476-a93d-ef35c72d6ed1 to disappear
Mar  2 01:08:25.489: INFO: Pod pod-secrets-a04d31d5-b3f3-4476-a93d-ef35c72d6ed1 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Mar  2 01:08:25.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2091" for this suite. 03/02/23 01:08:25.507
{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","completed":6,"skipped":248,"failed":0}
------------------------------
• [SLOW TEST] [8.431 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:08:17.095
    Mar  2 01:08:17.095: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename secrets 03/02/23 01:08:17.097
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:08:17.148
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:08:17.156
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:45
    STEP: Creating secret with name secret-test-ef1e0354-45ad-43f4-999c-4ad439374971 03/02/23 01:08:17.195
    STEP: Creating a pod to test consume secrets 03/02/23 01:08:17.21
    Mar  2 01:08:17.269: INFO: Waiting up to 5m0s for pod "pod-secrets-a04d31d5-b3f3-4476-a93d-ef35c72d6ed1" in namespace "secrets-2091" to be "Succeeded or Failed"
    Mar  2 01:08:17.290: INFO: Pod "pod-secrets-a04d31d5-b3f3-4476-a93d-ef35c72d6ed1": Phase="Pending", Reason="", readiness=false. Elapsed: 20.374056ms
    Mar  2 01:08:19.318: INFO: Pod "pod-secrets-a04d31d5-b3f3-4476-a93d-ef35c72d6ed1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048441123s
    Mar  2 01:08:21.310: INFO: Pod "pod-secrets-a04d31d5-b3f3-4476-a93d-ef35c72d6ed1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040300207s
    Mar  2 01:08:23.309: INFO: Pod "pod-secrets-a04d31d5-b3f3-4476-a93d-ef35c72d6ed1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.040087288s
    Mar  2 01:08:25.309: INFO: Pod "pod-secrets-a04d31d5-b3f3-4476-a93d-ef35c72d6ed1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.040001008s
    STEP: Saw pod success 03/02/23 01:08:25.309
    Mar  2 01:08:25.310: INFO: Pod "pod-secrets-a04d31d5-b3f3-4476-a93d-ef35c72d6ed1" satisfied condition "Succeeded or Failed"
    Mar  2 01:08:25.337: INFO: Trying to get logs from node 10.132.92.143 pod pod-secrets-a04d31d5-b3f3-4476-a93d-ef35c72d6ed1 container secret-env-test: <nil>
    STEP: delete the pod 03/02/23 01:08:25.429
    Mar  2 01:08:25.469: INFO: Waiting for pod pod-secrets-a04d31d5-b3f3-4476-a93d-ef35c72d6ed1 to disappear
    Mar  2 01:08:25.489: INFO: Pod pod-secrets-a04d31d5-b3f3-4476-a93d-ef35c72d6ed1 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Mar  2 01:08:25.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-2091" for this suite. 03/02/23 01:08:25.507
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:08:25.529
Mar  2 01:08:25.529: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename events 03/02/23 01:08:25.53
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:08:25.614
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:08:25.653
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 03/02/23 01:08:25.692
STEP: listing events in all namespaces 03/02/23 01:08:25.75
STEP: listing events in test namespace 03/02/23 01:08:25.845
STEP: listing events with field selection filtering on source 03/02/23 01:08:25.854
STEP: listing events with field selection filtering on reportingController 03/02/23 01:08:25.892
STEP: getting the test event 03/02/23 01:08:25.902
STEP: patching the test event 03/02/23 01:08:25.911
STEP: getting the test event 03/02/23 01:08:25.97
STEP: updating the test event 03/02/23 01:08:25.982
STEP: getting the test event 03/02/23 01:08:26.002
STEP: deleting the test event 03/02/23 01:08:26.012
STEP: listing events in all namespaces 03/02/23 01:08:26.038
STEP: listing events in test namespace 03/02/23 01:08:26.112
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Mar  2 01:08:26.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3885" for this suite. 03/02/23 01:08:26.172
{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","completed":7,"skipped":278,"failed":0}
------------------------------
• [0.663 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:08:25.529
    Mar  2 01:08:25.529: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename events 03/02/23 01:08:25.53
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:08:25.614
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:08:25.653
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 03/02/23 01:08:25.692
    STEP: listing events in all namespaces 03/02/23 01:08:25.75
    STEP: listing events in test namespace 03/02/23 01:08:25.845
    STEP: listing events with field selection filtering on source 03/02/23 01:08:25.854
    STEP: listing events with field selection filtering on reportingController 03/02/23 01:08:25.892
    STEP: getting the test event 03/02/23 01:08:25.902
    STEP: patching the test event 03/02/23 01:08:25.911
    STEP: getting the test event 03/02/23 01:08:25.97
    STEP: updating the test event 03/02/23 01:08:25.982
    STEP: getting the test event 03/02/23 01:08:26.002
    STEP: deleting the test event 03/02/23 01:08:26.012
    STEP: listing events in all namespaces 03/02/23 01:08:26.038
    STEP: listing events in test namespace 03/02/23 01:08:26.112
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Mar  2 01:08:26.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-3885" for this suite. 03/02/23 01:08:26.172
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:08:26.193
Mar  2 01:08:26.194: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename var-expansion 03/02/23 01:08:26.194
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:08:26.254
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:08:26.294
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
Mar  2 01:08:26.518: INFO: Waiting up to 2m0s for pod "var-expansion-a460046a-8bb0-45fc-9378-9fe7c329be79" in namespace "var-expansion-2319" to be "container 0 failed with reason CreateContainerConfigError"
Mar  2 01:08:26.564: INFO: Pod "var-expansion-a460046a-8bb0-45fc-9378-9fe7c329be79": Phase="Pending", Reason="", readiness=false. Elapsed: 45.960292ms
Mar  2 01:08:28.579: INFO: Pod "var-expansion-a460046a-8bb0-45fc-9378-9fe7c329be79": Phase="Pending", Reason="", readiness=false. Elapsed: 2.061171914s
Mar  2 01:08:28.579: INFO: Pod "var-expansion-a460046a-8bb0-45fc-9378-9fe7c329be79" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Mar  2 01:08:28.579: INFO: Deleting pod "var-expansion-a460046a-8bb0-45fc-9378-9fe7c329be79" in namespace "var-expansion-2319"
Mar  2 01:08:28.608: INFO: Wait up to 5m0s for pod "var-expansion-a460046a-8bb0-45fc-9378-9fe7c329be79" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar  2 01:08:32.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2319" for this suite. 03/02/23 01:08:32.738
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","completed":8,"skipped":289,"failed":0}
------------------------------
• [SLOW TEST] [6.596 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:08:26.193
    Mar  2 01:08:26.194: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename var-expansion 03/02/23 01:08:26.194
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:08:26.254
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:08:26.294
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:151
    Mar  2 01:08:26.518: INFO: Waiting up to 2m0s for pod "var-expansion-a460046a-8bb0-45fc-9378-9fe7c329be79" in namespace "var-expansion-2319" to be "container 0 failed with reason CreateContainerConfigError"
    Mar  2 01:08:26.564: INFO: Pod "var-expansion-a460046a-8bb0-45fc-9378-9fe7c329be79": Phase="Pending", Reason="", readiness=false. Elapsed: 45.960292ms
    Mar  2 01:08:28.579: INFO: Pod "var-expansion-a460046a-8bb0-45fc-9378-9fe7c329be79": Phase="Pending", Reason="", readiness=false. Elapsed: 2.061171914s
    Mar  2 01:08:28.579: INFO: Pod "var-expansion-a460046a-8bb0-45fc-9378-9fe7c329be79" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Mar  2 01:08:28.579: INFO: Deleting pod "var-expansion-a460046a-8bb0-45fc-9378-9fe7c329be79" in namespace "var-expansion-2319"
    Mar  2 01:08:28.608: INFO: Wait up to 5m0s for pod "var-expansion-a460046a-8bb0-45fc-9378-9fe7c329be79" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar  2 01:08:32.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-2319" for this suite. 03/02/23 01:08:32.738
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:08:32.791
Mar  2 01:08:32.794: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename resourcequota 03/02/23 01:08:32.795
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:08:32.877
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:08:32.908
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
STEP: Creating a ResourceQuota with best effort scope 03/02/23 01:08:32.919
STEP: Ensuring ResourceQuota status is calculated 03/02/23 01:08:32.951
STEP: Creating a ResourceQuota with not best effort scope 03/02/23 01:08:34.963
STEP: Ensuring ResourceQuota status is calculated 03/02/23 01:08:34.981
STEP: Creating a best-effort pod 03/02/23 01:08:36.992
STEP: Ensuring resource quota with best effort scope captures the pod usage 03/02/23 01:08:37.077
STEP: Ensuring resource quota with not best effort ignored the pod usage 03/02/23 01:08:39.093
STEP: Deleting the pod 03/02/23 01:08:41.105
STEP: Ensuring resource quota status released the pod usage 03/02/23 01:08:41.181
STEP: Creating a not best-effort pod 03/02/23 01:08:43.193
STEP: Ensuring resource quota with not best effort scope captures the pod usage 03/02/23 01:08:43.236
STEP: Ensuring resource quota with best effort scope ignored the pod usage 03/02/23 01:08:45.277
STEP: Deleting the pod 03/02/23 01:08:47.289
STEP: Ensuring resource quota status released the pod usage 03/02/23 01:08:47.35
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar  2 01:08:49.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9658" for this suite. 03/02/23 01:08:49.381
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","completed":9,"skipped":321,"failed":0}
------------------------------
• [SLOW TEST] [16.610 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:08:32.791
    Mar  2 01:08:32.794: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename resourcequota 03/02/23 01:08:32.795
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:08:32.877
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:08:32.908
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:793
    STEP: Creating a ResourceQuota with best effort scope 03/02/23 01:08:32.919
    STEP: Ensuring ResourceQuota status is calculated 03/02/23 01:08:32.951
    STEP: Creating a ResourceQuota with not best effort scope 03/02/23 01:08:34.963
    STEP: Ensuring ResourceQuota status is calculated 03/02/23 01:08:34.981
    STEP: Creating a best-effort pod 03/02/23 01:08:36.992
    STEP: Ensuring resource quota with best effort scope captures the pod usage 03/02/23 01:08:37.077
    STEP: Ensuring resource quota with not best effort ignored the pod usage 03/02/23 01:08:39.093
    STEP: Deleting the pod 03/02/23 01:08:41.105
    STEP: Ensuring resource quota status released the pod usage 03/02/23 01:08:41.181
    STEP: Creating a not best-effort pod 03/02/23 01:08:43.193
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 03/02/23 01:08:43.236
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 03/02/23 01:08:45.277
    STEP: Deleting the pod 03/02/23 01:08:47.289
    STEP: Ensuring resource quota status released the pod usage 03/02/23 01:08:47.35
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar  2 01:08:49.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-9658" for this suite. 03/02/23 01:08:49.381
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:08:49.411
Mar  2 01:08:49.411: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename cronjob 03/02/23 01:08:49.413
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:08:49.487
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:08:49.498
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 03/02/23 01:08:49.508
W0302 01:08:49.535701      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring no jobs are scheduled 03/02/23 01:08:49.535
STEP: Ensuring no job exists by listing jobs explicitly 03/02/23 01:13:49.556
STEP: Removing cronjob 03/02/23 01:13:49.57
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Mar  2 01:13:49.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-2763" for this suite. 03/02/23 01:13:49.607
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","completed":10,"skipped":359,"failed":0}
------------------------------
• [SLOW TEST] [300.213 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:08:49.411
    Mar  2 01:08:49.411: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename cronjob 03/02/23 01:08:49.413
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:08:49.487
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:08:49.498
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 03/02/23 01:08:49.508
    W0302 01:08:49.535701      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring no jobs are scheduled 03/02/23 01:08:49.535
    STEP: Ensuring no job exists by listing jobs explicitly 03/02/23 01:13:49.556
    STEP: Removing cronjob 03/02/23 01:13:49.57
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Mar  2 01:13:49.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-2763" for this suite. 03/02/23 01:13:49.607
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:13:49.628
Mar  2 01:13:49.628: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename daemonsets 03/02/23 01:13:49.629
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:13:49.683
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:13:49.694
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
Mar  2 01:13:49.811: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 03/02/23 01:13:49.831
Mar  2 01:13:49.864: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 01:13:49.864: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
Mar  2 01:13:50.911: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 01:13:50.911: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
Mar  2 01:13:51.919: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 01:13:51.919: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
Mar  2 01:13:52.902: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 01:13:52.902: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
Mar  2 01:13:53.908: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 01:13:53.908: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
Mar  2 01:13:54.920: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 01:13:54.920: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
Mar  2 01:13:55.899: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 01:13:55.899: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
Mar  2 01:13:56.935: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 01:13:56.935: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
Mar  2 01:13:57.937: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 01:13:57.937: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
Mar  2 01:13:58.923: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 01:13:58.923: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
Mar  2 01:13:59.902: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar  2 01:13:59.902: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image. 03/02/23 01:13:59.964
STEP: Check that daemon pods images are updated. 03/02/23 01:14:00.005
Mar  2 01:14:00.022: INFO: Wrong image for pod: daemon-set-j9kz6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 01:14:00.022: INFO: Wrong image for pod: daemon-set-ljj9z. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 01:14:00.022: INFO: Wrong image for pod: daemon-set-vhv9d. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 01:14:01.062: INFO: Wrong image for pod: daemon-set-j9kz6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 01:14:01.062: INFO: Wrong image for pod: daemon-set-vhv9d. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 01:14:02.089: INFO: Wrong image for pod: daemon-set-j9kz6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 01:14:02.090: INFO: Wrong image for pod: daemon-set-vhv9d. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 01:14:03.062: INFO: Wrong image for pod: daemon-set-j9kz6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 01:14:03.062: INFO: Wrong image for pod: daemon-set-vhv9d. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 01:14:04.061: INFO: Wrong image for pod: daemon-set-j9kz6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 01:14:04.061: INFO: Pod daemon-set-qqgbc is not available
Mar  2 01:14:04.061: INFO: Wrong image for pod: daemon-set-vhv9d. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 01:14:05.069: INFO: Wrong image for pod: daemon-set-j9kz6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 01:14:05.069: INFO: Pod daemon-set-qqgbc is not available
Mar  2 01:14:05.069: INFO: Wrong image for pod: daemon-set-vhv9d. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 01:14:06.060: INFO: Wrong image for pod: daemon-set-j9kz6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 01:14:07.073: INFO: Wrong image for pod: daemon-set-j9kz6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 01:14:08.065: INFO: Wrong image for pod: daemon-set-j9kz6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 01:14:08.065: INFO: Pod daemon-set-jfbxx is not available
Mar  2 01:14:09.090: INFO: Wrong image for pod: daemon-set-j9kz6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 01:14:09.090: INFO: Pod daemon-set-jfbxx is not available
Mar  2 01:14:10.092: INFO: Wrong image for pod: daemon-set-j9kz6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 01:14:10.092: INFO: Pod daemon-set-jfbxx is not available
Mar  2 01:14:12.060: INFO: Pod daemon-set-lhsdj is not available
STEP: Check that daemon pods are still running on every node of the cluster. 03/02/23 01:14:12.08
Mar  2 01:14:12.114: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar  2 01:14:12.114: INFO: Node 10.132.92.186 is running 0 daemon pod, expected 1
Mar  2 01:14:13.151: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar  2 01:14:13.151: INFO: Node 10.132.92.186 is running 0 daemon pod, expected 1
Mar  2 01:14:14.176: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar  2 01:14:14.176: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 03/02/23 01:14:14.375
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1753, will wait for the garbage collector to delete the pods 03/02/23 01:14:14.375
Mar  2 01:14:14.516: INFO: Deleting DaemonSet.extensions daemon-set took: 69.083458ms
Mar  2 01:14:14.716: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.396256ms
Mar  2 01:14:17.831: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 01:14:17.831: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar  2 01:14:17.845: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"70220"},"items":null}

Mar  2 01:14:17.859: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"70220"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Mar  2 01:14:17.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1753" for this suite. 03/02/23 01:14:17.95
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","completed":11,"skipped":389,"failed":0}
------------------------------
• [SLOW TEST] [28.338 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:13:49.628
    Mar  2 01:13:49.628: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename daemonsets 03/02/23 01:13:49.629
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:13:49.683
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:13:49.694
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:373
    Mar  2 01:13:49.811: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 03/02/23 01:13:49.831
    Mar  2 01:13:49.864: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 01:13:49.864: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
    Mar  2 01:13:50.911: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 01:13:50.911: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
    Mar  2 01:13:51.919: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 01:13:51.919: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
    Mar  2 01:13:52.902: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 01:13:52.902: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
    Mar  2 01:13:53.908: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 01:13:53.908: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
    Mar  2 01:13:54.920: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 01:13:54.920: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
    Mar  2 01:13:55.899: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 01:13:55.899: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
    Mar  2 01:13:56.935: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 01:13:56.935: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
    Mar  2 01:13:57.937: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 01:13:57.937: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
    Mar  2 01:13:58.923: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 01:13:58.923: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
    Mar  2 01:13:59.902: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar  2 01:13:59.902: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Update daemon pods image. 03/02/23 01:13:59.964
    STEP: Check that daemon pods images are updated. 03/02/23 01:14:00.005
    Mar  2 01:14:00.022: INFO: Wrong image for pod: daemon-set-j9kz6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  2 01:14:00.022: INFO: Wrong image for pod: daemon-set-ljj9z. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  2 01:14:00.022: INFO: Wrong image for pod: daemon-set-vhv9d. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  2 01:14:01.062: INFO: Wrong image for pod: daemon-set-j9kz6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  2 01:14:01.062: INFO: Wrong image for pod: daemon-set-vhv9d. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  2 01:14:02.089: INFO: Wrong image for pod: daemon-set-j9kz6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  2 01:14:02.090: INFO: Wrong image for pod: daemon-set-vhv9d. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  2 01:14:03.062: INFO: Wrong image for pod: daemon-set-j9kz6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  2 01:14:03.062: INFO: Wrong image for pod: daemon-set-vhv9d. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  2 01:14:04.061: INFO: Wrong image for pod: daemon-set-j9kz6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  2 01:14:04.061: INFO: Pod daemon-set-qqgbc is not available
    Mar  2 01:14:04.061: INFO: Wrong image for pod: daemon-set-vhv9d. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  2 01:14:05.069: INFO: Wrong image for pod: daemon-set-j9kz6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  2 01:14:05.069: INFO: Pod daemon-set-qqgbc is not available
    Mar  2 01:14:05.069: INFO: Wrong image for pod: daemon-set-vhv9d. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  2 01:14:06.060: INFO: Wrong image for pod: daemon-set-j9kz6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  2 01:14:07.073: INFO: Wrong image for pod: daemon-set-j9kz6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  2 01:14:08.065: INFO: Wrong image for pod: daemon-set-j9kz6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  2 01:14:08.065: INFO: Pod daemon-set-jfbxx is not available
    Mar  2 01:14:09.090: INFO: Wrong image for pod: daemon-set-j9kz6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  2 01:14:09.090: INFO: Pod daemon-set-jfbxx is not available
    Mar  2 01:14:10.092: INFO: Wrong image for pod: daemon-set-j9kz6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar  2 01:14:10.092: INFO: Pod daemon-set-jfbxx is not available
    Mar  2 01:14:12.060: INFO: Pod daemon-set-lhsdj is not available
    STEP: Check that daemon pods are still running on every node of the cluster. 03/02/23 01:14:12.08
    Mar  2 01:14:12.114: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar  2 01:14:12.114: INFO: Node 10.132.92.186 is running 0 daemon pod, expected 1
    Mar  2 01:14:13.151: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar  2 01:14:13.151: INFO: Node 10.132.92.186 is running 0 daemon pod, expected 1
    Mar  2 01:14:14.176: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar  2 01:14:14.176: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 03/02/23 01:14:14.375
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1753, will wait for the garbage collector to delete the pods 03/02/23 01:14:14.375
    Mar  2 01:14:14.516: INFO: Deleting DaemonSet.extensions daemon-set took: 69.083458ms
    Mar  2 01:14:14.716: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.396256ms
    Mar  2 01:14:17.831: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 01:14:17.831: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar  2 01:14:17.845: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"70220"},"items":null}

    Mar  2 01:14:17.859: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"70220"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Mar  2 01:14:17.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-1753" for this suite. 03/02/23 01:14:17.95
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:14:17.967
Mar  2 01:14:17.967: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename var-expansion 03/02/23 01:14:17.968
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:14:18.017
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:14:18.031
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
STEP: Creating a pod to test substitution in container's command 03/02/23 01:14:18.049
Mar  2 01:14:18.132: INFO: Waiting up to 5m0s for pod "var-expansion-5322343c-8bd3-4f03-aeb7-c8052304d7c0" in namespace "var-expansion-7686" to be "Succeeded or Failed"
Mar  2 01:14:18.152: INFO: Pod "var-expansion-5322343c-8bd3-4f03-aeb7-c8052304d7c0": Phase="Pending", Reason="", readiness=false. Elapsed: 19.349145ms
Mar  2 01:14:20.206: INFO: Pod "var-expansion-5322343c-8bd3-4f03-aeb7-c8052304d7c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.07379209s
Mar  2 01:14:22.193: INFO: Pod "var-expansion-5322343c-8bd3-4f03-aeb7-c8052304d7c0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.060986532s
Mar  2 01:14:24.182: INFO: Pod "var-expansion-5322343c-8bd3-4f03-aeb7-c8052304d7c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.050081528s
STEP: Saw pod success 03/02/23 01:14:24.182
Mar  2 01:14:24.183: INFO: Pod "var-expansion-5322343c-8bd3-4f03-aeb7-c8052304d7c0" satisfied condition "Succeeded or Failed"
Mar  2 01:14:24.197: INFO: Trying to get logs from node 10.132.92.143 pod var-expansion-5322343c-8bd3-4f03-aeb7-c8052304d7c0 container dapi-container: <nil>
STEP: delete the pod 03/02/23 01:14:24.257
Mar  2 01:14:24.450: INFO: Waiting for pod var-expansion-5322343c-8bd3-4f03-aeb7-c8052304d7c0 to disappear
Mar  2 01:14:24.471: INFO: Pod var-expansion-5322343c-8bd3-4f03-aeb7-c8052304d7c0 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar  2 01:14:24.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7686" for this suite. 03/02/23 01:14:24.494
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","completed":12,"skipped":392,"failed":0}
------------------------------
• [SLOW TEST] [6.543 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:14:17.967
    Mar  2 01:14:17.967: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename var-expansion 03/02/23 01:14:17.968
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:14:18.017
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:14:18.031
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:72
    STEP: Creating a pod to test substitution in container's command 03/02/23 01:14:18.049
    Mar  2 01:14:18.132: INFO: Waiting up to 5m0s for pod "var-expansion-5322343c-8bd3-4f03-aeb7-c8052304d7c0" in namespace "var-expansion-7686" to be "Succeeded or Failed"
    Mar  2 01:14:18.152: INFO: Pod "var-expansion-5322343c-8bd3-4f03-aeb7-c8052304d7c0": Phase="Pending", Reason="", readiness=false. Elapsed: 19.349145ms
    Mar  2 01:14:20.206: INFO: Pod "var-expansion-5322343c-8bd3-4f03-aeb7-c8052304d7c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.07379209s
    Mar  2 01:14:22.193: INFO: Pod "var-expansion-5322343c-8bd3-4f03-aeb7-c8052304d7c0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.060986532s
    Mar  2 01:14:24.182: INFO: Pod "var-expansion-5322343c-8bd3-4f03-aeb7-c8052304d7c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.050081528s
    STEP: Saw pod success 03/02/23 01:14:24.182
    Mar  2 01:14:24.183: INFO: Pod "var-expansion-5322343c-8bd3-4f03-aeb7-c8052304d7c0" satisfied condition "Succeeded or Failed"
    Mar  2 01:14:24.197: INFO: Trying to get logs from node 10.132.92.143 pod var-expansion-5322343c-8bd3-4f03-aeb7-c8052304d7c0 container dapi-container: <nil>
    STEP: delete the pod 03/02/23 01:14:24.257
    Mar  2 01:14:24.450: INFO: Waiting for pod var-expansion-5322343c-8bd3-4f03-aeb7-c8052304d7c0 to disappear
    Mar  2 01:14:24.471: INFO: Pod var-expansion-5322343c-8bd3-4f03-aeb7-c8052304d7c0 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar  2 01:14:24.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-7686" for this suite. 03/02/23 01:14:24.494
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:14:24.511
Mar  2 01:14:24.511: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename replication-controller 03/02/23 01:14:24.513
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:14:24.562
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:14:24.578
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
STEP: Creating replication controller my-hostname-basic-5e3b680b-be1f-400e-9d37-7f68db282469 03/02/23 01:14:24.608
Mar  2 01:14:24.654: INFO: Pod name my-hostname-basic-5e3b680b-be1f-400e-9d37-7f68db282469: Found 0 pods out of 1
Mar  2 01:14:29.687: INFO: Pod name my-hostname-basic-5e3b680b-be1f-400e-9d37-7f68db282469: Found 1 pods out of 1
Mar  2 01:14:29.687: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-5e3b680b-be1f-400e-9d37-7f68db282469" are running
Mar  2 01:14:29.687: INFO: Waiting up to 5m0s for pod "my-hostname-basic-5e3b680b-be1f-400e-9d37-7f68db282469-27778" in namespace "replication-controller-1965" to be "running"
Mar  2 01:14:29.706: INFO: Pod "my-hostname-basic-5e3b680b-be1f-400e-9d37-7f68db282469-27778": Phase="Running", Reason="", readiness=true. Elapsed: 19.016702ms
Mar  2 01:14:29.706: INFO: Pod "my-hostname-basic-5e3b680b-be1f-400e-9d37-7f68db282469-27778" satisfied condition "running"
Mar  2 01:14:29.706: INFO: Pod "my-hostname-basic-5e3b680b-be1f-400e-9d37-7f68db282469-27778" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-02 01:14:24 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-02 01:14:27 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-02 01:14:27 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-02 01:14:24 +0000 UTC Reason: Message:}])
Mar  2 01:14:29.706: INFO: Trying to dial the pod
Mar  2 01:14:34.772: INFO: Controller my-hostname-basic-5e3b680b-be1f-400e-9d37-7f68db282469: Got expected result from replica 1 [my-hostname-basic-5e3b680b-be1f-400e-9d37-7f68db282469-27778]: "my-hostname-basic-5e3b680b-be1f-400e-9d37-7f68db282469-27778", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Mar  2 01:14:34.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1965" for this suite. 03/02/23 01:14:34.796
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","completed":13,"skipped":402,"failed":0}
------------------------------
• [SLOW TEST] [10.303 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:14:24.511
    Mar  2 01:14:24.511: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename replication-controller 03/02/23 01:14:24.513
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:14:24.562
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:14:24.578
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:66
    STEP: Creating replication controller my-hostname-basic-5e3b680b-be1f-400e-9d37-7f68db282469 03/02/23 01:14:24.608
    Mar  2 01:14:24.654: INFO: Pod name my-hostname-basic-5e3b680b-be1f-400e-9d37-7f68db282469: Found 0 pods out of 1
    Mar  2 01:14:29.687: INFO: Pod name my-hostname-basic-5e3b680b-be1f-400e-9d37-7f68db282469: Found 1 pods out of 1
    Mar  2 01:14:29.687: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-5e3b680b-be1f-400e-9d37-7f68db282469" are running
    Mar  2 01:14:29.687: INFO: Waiting up to 5m0s for pod "my-hostname-basic-5e3b680b-be1f-400e-9d37-7f68db282469-27778" in namespace "replication-controller-1965" to be "running"
    Mar  2 01:14:29.706: INFO: Pod "my-hostname-basic-5e3b680b-be1f-400e-9d37-7f68db282469-27778": Phase="Running", Reason="", readiness=true. Elapsed: 19.016702ms
    Mar  2 01:14:29.706: INFO: Pod "my-hostname-basic-5e3b680b-be1f-400e-9d37-7f68db282469-27778" satisfied condition "running"
    Mar  2 01:14:29.706: INFO: Pod "my-hostname-basic-5e3b680b-be1f-400e-9d37-7f68db282469-27778" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-02 01:14:24 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-02 01:14:27 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-02 01:14:27 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-02 01:14:24 +0000 UTC Reason: Message:}])
    Mar  2 01:14:29.706: INFO: Trying to dial the pod
    Mar  2 01:14:34.772: INFO: Controller my-hostname-basic-5e3b680b-be1f-400e-9d37-7f68db282469: Got expected result from replica 1 [my-hostname-basic-5e3b680b-be1f-400e-9d37-7f68db282469-27778]: "my-hostname-basic-5e3b680b-be1f-400e-9d37-7f68db282469-27778", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Mar  2 01:14:34.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-1965" for this suite. 03/02/23 01:14:34.796
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:14:34.815
Mar  2 01:14:34.815: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename disruption 03/02/23 01:14:34.818
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:14:34.881
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:14:34.892
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
STEP: creating the pdb 03/02/23 01:14:34.958
STEP: Waiting for the pdb to be processed 03/02/23 01:14:35.043
STEP: updating the pdb 03/02/23 01:14:37.103
STEP: Waiting for the pdb to be processed 03/02/23 01:14:37.137
STEP: patching the pdb 03/02/23 01:14:39.172
STEP: Waiting for the pdb to be processed 03/02/23 01:14:39.207
STEP: Waiting for the pdb to be deleted 03/02/23 01:14:41.308
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Mar  2 01:14:41.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9957" for this suite. 03/02/23 01:14:41.347
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","completed":14,"skipped":414,"failed":0}
------------------------------
• [SLOW TEST] [6.554 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:14:34.815
    Mar  2 01:14:34.815: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename disruption 03/02/23 01:14:34.818
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:14:34.881
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:14:34.892
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:107
    STEP: creating the pdb 03/02/23 01:14:34.958
    STEP: Waiting for the pdb to be processed 03/02/23 01:14:35.043
    STEP: updating the pdb 03/02/23 01:14:37.103
    STEP: Waiting for the pdb to be processed 03/02/23 01:14:37.137
    STEP: patching the pdb 03/02/23 01:14:39.172
    STEP: Waiting for the pdb to be processed 03/02/23 01:14:39.207
    STEP: Waiting for the pdb to be deleted 03/02/23 01:14:41.308
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Mar  2 01:14:41.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-9957" for this suite. 03/02/23 01:14:41.347
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:14:41.373
Mar  2 01:14:41.373: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename server-version 03/02/23 01:14:41.375
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:14:41.462
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:14:41.472
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 03/02/23 01:14:41.5
STEP: Confirm major version 03/02/23 01:14:41.505
Mar  2 01:14:41.505: INFO: Major version: 1
STEP: Confirm minor version 03/02/23 01:14:41.505
Mar  2 01:14:41.505: INFO: cleanMinorVersion: 25
Mar  2 01:14:41.505: INFO: Minor version: 25
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:187
Mar  2 01:14:41.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-4929" for this suite. 03/02/23 01:14:41.54
{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","completed":15,"skipped":418,"failed":0}
------------------------------
• [0.201 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:14:41.373
    Mar  2 01:14:41.373: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename server-version 03/02/23 01:14:41.375
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:14:41.462
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:14:41.472
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 03/02/23 01:14:41.5
    STEP: Confirm major version 03/02/23 01:14:41.505
    Mar  2 01:14:41.505: INFO: Major version: 1
    STEP: Confirm minor version 03/02/23 01:14:41.505
    Mar  2 01:14:41.505: INFO: cleanMinorVersion: 25
    Mar  2 01:14:41.505: INFO: Minor version: 25
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:187
    Mar  2 01:14:41.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "server-version-4929" for this suite. 03/02/23 01:14:41.54
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:14:41.574
Mar  2 01:14:41.574: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename projected 03/02/23 01:14:41.575
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:14:41.627
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:14:41.689
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
STEP: Creating a pod to test downward API volume plugin 03/02/23 01:14:41.697
Mar  2 01:14:41.757: INFO: Waiting up to 5m0s for pod "downwardapi-volume-67b19265-3010-4a48-8847-4073507908fe" in namespace "projected-3282" to be "Succeeded or Failed"
Mar  2 01:14:41.772: INFO: Pod "downwardapi-volume-67b19265-3010-4a48-8847-4073507908fe": Phase="Pending", Reason="", readiness=false. Elapsed: 15.005487ms
Mar  2 01:14:43.789: INFO: Pod "downwardapi-volume-67b19265-3010-4a48-8847-4073507908fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032320728s
Mar  2 01:14:45.790: INFO: Pod "downwardapi-volume-67b19265-3010-4a48-8847-4073507908fe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032490468s
Mar  2 01:14:47.785: INFO: Pod "downwardapi-volume-67b19265-3010-4a48-8847-4073507908fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028152952s
STEP: Saw pod success 03/02/23 01:14:47.785
Mar  2 01:14:47.786: INFO: Pod "downwardapi-volume-67b19265-3010-4a48-8847-4073507908fe" satisfied condition "Succeeded or Failed"
Mar  2 01:14:47.799: INFO: Trying to get logs from node 10.132.92.143 pod downwardapi-volume-67b19265-3010-4a48-8847-4073507908fe container client-container: <nil>
STEP: delete the pod 03/02/23 01:14:47.831
Mar  2 01:14:47.867: INFO: Waiting for pod downwardapi-volume-67b19265-3010-4a48-8847-4073507908fe to disappear
Mar  2 01:14:47.880: INFO: Pod downwardapi-volume-67b19265-3010-4a48-8847-4073507908fe no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar  2 01:14:47.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3282" for this suite. 03/02/23 01:14:47.897
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":16,"skipped":419,"failed":0}
------------------------------
• [SLOW TEST] [6.336 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:14:41.574
    Mar  2 01:14:41.574: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename projected 03/02/23 01:14:41.575
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:14:41.627
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:14:41.689
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:67
    STEP: Creating a pod to test downward API volume plugin 03/02/23 01:14:41.697
    Mar  2 01:14:41.757: INFO: Waiting up to 5m0s for pod "downwardapi-volume-67b19265-3010-4a48-8847-4073507908fe" in namespace "projected-3282" to be "Succeeded or Failed"
    Mar  2 01:14:41.772: INFO: Pod "downwardapi-volume-67b19265-3010-4a48-8847-4073507908fe": Phase="Pending", Reason="", readiness=false. Elapsed: 15.005487ms
    Mar  2 01:14:43.789: INFO: Pod "downwardapi-volume-67b19265-3010-4a48-8847-4073507908fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032320728s
    Mar  2 01:14:45.790: INFO: Pod "downwardapi-volume-67b19265-3010-4a48-8847-4073507908fe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032490468s
    Mar  2 01:14:47.785: INFO: Pod "downwardapi-volume-67b19265-3010-4a48-8847-4073507908fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028152952s
    STEP: Saw pod success 03/02/23 01:14:47.785
    Mar  2 01:14:47.786: INFO: Pod "downwardapi-volume-67b19265-3010-4a48-8847-4073507908fe" satisfied condition "Succeeded or Failed"
    Mar  2 01:14:47.799: INFO: Trying to get logs from node 10.132.92.143 pod downwardapi-volume-67b19265-3010-4a48-8847-4073507908fe container client-container: <nil>
    STEP: delete the pod 03/02/23 01:14:47.831
    Mar  2 01:14:47.867: INFO: Waiting for pod downwardapi-volume-67b19265-3010-4a48-8847-4073507908fe to disappear
    Mar  2 01:14:47.880: INFO: Pod downwardapi-volume-67b19265-3010-4a48-8847-4073507908fe no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar  2 01:14:47.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3282" for this suite. 03/02/23 01:14:47.897
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:14:47.911
Mar  2 01:14:47.911: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename services 03/02/23 01:14:47.912
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:14:48.031
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:14:48.044
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
STEP: creating service in namespace services-2809 03/02/23 01:14:48.059
Mar  2 01:14:48.185: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-2809" to be "running and ready"
Mar  2 01:14:48.236: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 51.007487ms
Mar  2 01:14:48.236: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:14:50.272: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.08660136s
Mar  2 01:14:50.272: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Mar  2 01:14:50.272: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Mar  2 01:14:50.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-2809 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Mar  2 01:14:50.720: INFO: rc: 7
Mar  2 01:14:50.780: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Mar  2 01:14:50.799: INFO: Pod kube-proxy-mode-detector no longer exists
Mar  2 01:14:50.799: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-2809 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
Command stdout:

stderr:
+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
command terminated with exit code 7

error:
exit status 7
STEP: creating service affinity-nodeport-timeout in namespace services-2809 03/02/23 01:14:50.799
STEP: creating replication controller affinity-nodeport-timeout in namespace services-2809 03/02/23 01:14:50.872
I0302 01:14:50.918482      20 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-2809, replica count: 3
I0302 01:14:53.970221      20 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 01:14:54.043: INFO: Creating new exec pod
Mar  2 01:14:54.086: INFO: Waiting up to 5m0s for pod "execpod-affinity7p8t9" in namespace "services-2809" to be "running"
Mar  2 01:14:54.130: INFO: Pod "execpod-affinity7p8t9": Phase="Pending", Reason="", readiness=false. Elapsed: 43.70139ms
Mar  2 01:14:56.147: INFO: Pod "execpod-affinity7p8t9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.061512893s
Mar  2 01:14:58.145: INFO: Pod "execpod-affinity7p8t9": Phase="Running", Reason="", readiness=true. Elapsed: 4.058859556s
Mar  2 01:14:58.145: INFO: Pod "execpod-affinity7p8t9" satisfied condition "running"
Mar  2 01:14:59.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-2809 exec execpod-affinity7p8t9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Mar  2 01:14:59.500: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Mar  2 01:14:59.500: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 01:14:59.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-2809 exec execpod-affinity7p8t9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.56.115 80'
Mar  2 01:14:59.926: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.56.115 80\nConnection to 172.21.56.115 80 port [tcp/http] succeeded!\n"
Mar  2 01:14:59.927: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 01:14:59.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-2809 exec execpod-affinity7p8t9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.132.92.186 31721'
Mar  2 01:15:00.329: INFO: stderr: "+ + echonc -v hostName -t\n -w 2 10.132.92.186 31721\nConnection to 10.132.92.186 31721 port [tcp/*] succeeded!\n"
Mar  2 01:15:00.329: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 01:15:00.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-2809 exec execpod-affinity7p8t9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.132.92.188 31721'
Mar  2 01:15:00.706: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.132.92.188 31721\nConnection to 10.132.92.188 31721 port [tcp/*] succeeded!\n"
Mar  2 01:15:00.707: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 01:15:00.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-2809 exec execpod-affinity7p8t9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.132.92.143:31721/ ; done'
Mar  2 01:15:01.263: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31721/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31721/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31721/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31721/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31721/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31721/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31721/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31721/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31721/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31721/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31721/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31721/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31721/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31721/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31721/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31721/\n"
Mar  2 01:15:01.263: INFO: stdout: "\naffinity-nodeport-timeout-hcqbw\naffinity-nodeport-timeout-hcqbw\naffinity-nodeport-timeout-hcqbw\naffinity-nodeport-timeout-hcqbw\naffinity-nodeport-timeout-hcqbw\naffinity-nodeport-timeout-hcqbw\naffinity-nodeport-timeout-hcqbw\naffinity-nodeport-timeout-hcqbw\naffinity-nodeport-timeout-hcqbw\naffinity-nodeport-timeout-hcqbw\naffinity-nodeport-timeout-hcqbw\naffinity-nodeport-timeout-hcqbw\naffinity-nodeport-timeout-hcqbw\naffinity-nodeport-timeout-hcqbw\naffinity-nodeport-timeout-hcqbw\naffinity-nodeport-timeout-hcqbw"
Mar  2 01:15:01.264: INFO: Received response from host: affinity-nodeport-timeout-hcqbw
Mar  2 01:15:01.264: INFO: Received response from host: affinity-nodeport-timeout-hcqbw
Mar  2 01:15:01.264: INFO: Received response from host: affinity-nodeport-timeout-hcqbw
Mar  2 01:15:01.264: INFO: Received response from host: affinity-nodeport-timeout-hcqbw
Mar  2 01:15:01.264: INFO: Received response from host: affinity-nodeport-timeout-hcqbw
Mar  2 01:15:01.264: INFO: Received response from host: affinity-nodeport-timeout-hcqbw
Mar  2 01:15:01.264: INFO: Received response from host: affinity-nodeport-timeout-hcqbw
Mar  2 01:15:01.264: INFO: Received response from host: affinity-nodeport-timeout-hcqbw
Mar  2 01:15:01.264: INFO: Received response from host: affinity-nodeport-timeout-hcqbw
Mar  2 01:15:01.264: INFO: Received response from host: affinity-nodeport-timeout-hcqbw
Mar  2 01:15:01.264: INFO: Received response from host: affinity-nodeport-timeout-hcqbw
Mar  2 01:15:01.264: INFO: Received response from host: affinity-nodeport-timeout-hcqbw
Mar  2 01:15:01.264: INFO: Received response from host: affinity-nodeport-timeout-hcqbw
Mar  2 01:15:01.264: INFO: Received response from host: affinity-nodeport-timeout-hcqbw
Mar  2 01:15:01.264: INFO: Received response from host: affinity-nodeport-timeout-hcqbw
Mar  2 01:15:01.264: INFO: Received response from host: affinity-nodeport-timeout-hcqbw
Mar  2 01:15:01.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-2809 exec execpod-affinity7p8t9 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.132.92.143:31721/'
Mar  2 01:15:01.788: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.132.92.143:31721/\n"
Mar  2 01:15:01.788: INFO: stdout: "affinity-nodeport-timeout-hcqbw"
Mar  2 01:15:21.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-2809 exec execpod-affinity7p8t9 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.132.92.143:31721/'
Mar  2 01:15:22.177: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.132.92.143:31721/\n"
Mar  2 01:15:22.177: INFO: stdout: "affinity-nodeport-timeout-xt64s"
Mar  2 01:15:22.177: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-2809, will wait for the garbage collector to delete the pods 03/02/23 01:15:22.234
Mar  2 01:15:22.345: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 42.207507ms
Mar  2 01:15:22.446: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 101.374973ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  2 01:15:25.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2809" for this suite. 03/02/23 01:15:25.836
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","completed":17,"skipped":427,"failed":0}
------------------------------
• [SLOW TEST] [37.945 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:14:47.911
    Mar  2 01:14:47.911: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename services 03/02/23 01:14:47.912
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:14:48.031
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:14:48.044
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2221
    STEP: creating service in namespace services-2809 03/02/23 01:14:48.059
    Mar  2 01:14:48.185: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-2809" to be "running and ready"
    Mar  2 01:14:48.236: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 51.007487ms
    Mar  2 01:14:48.236: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 01:14:50.272: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.08660136s
    Mar  2 01:14:50.272: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Mar  2 01:14:50.272: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Mar  2 01:14:50.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-2809 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Mar  2 01:14:50.720: INFO: rc: 7
    Mar  2 01:14:50.780: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Mar  2 01:14:50.799: INFO: Pod kube-proxy-mode-detector no longer exists
    Mar  2 01:14:50.799: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-2809 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
    Command stdout:

    stderr:
    + curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
    command terminated with exit code 7

    error:
    exit status 7
    STEP: creating service affinity-nodeport-timeout in namespace services-2809 03/02/23 01:14:50.799
    STEP: creating replication controller affinity-nodeport-timeout in namespace services-2809 03/02/23 01:14:50.872
    I0302 01:14:50.918482      20 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-2809, replica count: 3
    I0302 01:14:53.970221      20 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar  2 01:14:54.043: INFO: Creating new exec pod
    Mar  2 01:14:54.086: INFO: Waiting up to 5m0s for pod "execpod-affinity7p8t9" in namespace "services-2809" to be "running"
    Mar  2 01:14:54.130: INFO: Pod "execpod-affinity7p8t9": Phase="Pending", Reason="", readiness=false. Elapsed: 43.70139ms
    Mar  2 01:14:56.147: INFO: Pod "execpod-affinity7p8t9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.061512893s
    Mar  2 01:14:58.145: INFO: Pod "execpod-affinity7p8t9": Phase="Running", Reason="", readiness=true. Elapsed: 4.058859556s
    Mar  2 01:14:58.145: INFO: Pod "execpod-affinity7p8t9" satisfied condition "running"
    Mar  2 01:14:59.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-2809 exec execpod-affinity7p8t9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
    Mar  2 01:14:59.500: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
    Mar  2 01:14:59.500: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  2 01:14:59.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-2809 exec execpod-affinity7p8t9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.56.115 80'
    Mar  2 01:14:59.926: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.56.115 80\nConnection to 172.21.56.115 80 port [tcp/http] succeeded!\n"
    Mar  2 01:14:59.927: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  2 01:14:59.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-2809 exec execpod-affinity7p8t9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.132.92.186 31721'
    Mar  2 01:15:00.329: INFO: stderr: "+ + echonc -v hostName -t\n -w 2 10.132.92.186 31721\nConnection to 10.132.92.186 31721 port [tcp/*] succeeded!\n"
    Mar  2 01:15:00.329: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  2 01:15:00.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-2809 exec execpod-affinity7p8t9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.132.92.188 31721'
    Mar  2 01:15:00.706: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.132.92.188 31721\nConnection to 10.132.92.188 31721 port [tcp/*] succeeded!\n"
    Mar  2 01:15:00.707: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  2 01:15:00.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-2809 exec execpod-affinity7p8t9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.132.92.143:31721/ ; done'
    Mar  2 01:15:01.263: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31721/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31721/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31721/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31721/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31721/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31721/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31721/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31721/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31721/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31721/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31721/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31721/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31721/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31721/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31721/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31721/\n"
    Mar  2 01:15:01.263: INFO: stdout: "\naffinity-nodeport-timeout-hcqbw\naffinity-nodeport-timeout-hcqbw\naffinity-nodeport-timeout-hcqbw\naffinity-nodeport-timeout-hcqbw\naffinity-nodeport-timeout-hcqbw\naffinity-nodeport-timeout-hcqbw\naffinity-nodeport-timeout-hcqbw\naffinity-nodeport-timeout-hcqbw\naffinity-nodeport-timeout-hcqbw\naffinity-nodeport-timeout-hcqbw\naffinity-nodeport-timeout-hcqbw\naffinity-nodeport-timeout-hcqbw\naffinity-nodeport-timeout-hcqbw\naffinity-nodeport-timeout-hcqbw\naffinity-nodeport-timeout-hcqbw\naffinity-nodeport-timeout-hcqbw"
    Mar  2 01:15:01.264: INFO: Received response from host: affinity-nodeport-timeout-hcqbw
    Mar  2 01:15:01.264: INFO: Received response from host: affinity-nodeport-timeout-hcqbw
    Mar  2 01:15:01.264: INFO: Received response from host: affinity-nodeport-timeout-hcqbw
    Mar  2 01:15:01.264: INFO: Received response from host: affinity-nodeport-timeout-hcqbw
    Mar  2 01:15:01.264: INFO: Received response from host: affinity-nodeport-timeout-hcqbw
    Mar  2 01:15:01.264: INFO: Received response from host: affinity-nodeport-timeout-hcqbw
    Mar  2 01:15:01.264: INFO: Received response from host: affinity-nodeport-timeout-hcqbw
    Mar  2 01:15:01.264: INFO: Received response from host: affinity-nodeport-timeout-hcqbw
    Mar  2 01:15:01.264: INFO: Received response from host: affinity-nodeport-timeout-hcqbw
    Mar  2 01:15:01.264: INFO: Received response from host: affinity-nodeport-timeout-hcqbw
    Mar  2 01:15:01.264: INFO: Received response from host: affinity-nodeport-timeout-hcqbw
    Mar  2 01:15:01.264: INFO: Received response from host: affinity-nodeport-timeout-hcqbw
    Mar  2 01:15:01.264: INFO: Received response from host: affinity-nodeport-timeout-hcqbw
    Mar  2 01:15:01.264: INFO: Received response from host: affinity-nodeport-timeout-hcqbw
    Mar  2 01:15:01.264: INFO: Received response from host: affinity-nodeport-timeout-hcqbw
    Mar  2 01:15:01.264: INFO: Received response from host: affinity-nodeport-timeout-hcqbw
    Mar  2 01:15:01.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-2809 exec execpod-affinity7p8t9 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.132.92.143:31721/'
    Mar  2 01:15:01.788: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.132.92.143:31721/\n"
    Mar  2 01:15:01.788: INFO: stdout: "affinity-nodeport-timeout-hcqbw"
    Mar  2 01:15:21.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-2809 exec execpod-affinity7p8t9 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.132.92.143:31721/'
    Mar  2 01:15:22.177: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.132.92.143:31721/\n"
    Mar  2 01:15:22.177: INFO: stdout: "affinity-nodeport-timeout-xt64s"
    Mar  2 01:15:22.177: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-2809, will wait for the garbage collector to delete the pods 03/02/23 01:15:22.234
    Mar  2 01:15:22.345: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 42.207507ms
    Mar  2 01:15:22.446: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 101.374973ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  2 01:15:25.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2809" for this suite. 03/02/23 01:15:25.836
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:15:25.858
Mar  2 01:15:25.858: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename statefulset 03/02/23 01:15:25.859
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:15:25.91
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:15:25.921
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-1294 03/02/23 01:15:25.93
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
STEP: Creating stateful set ss in namespace statefulset-1294 03/02/23 01:15:25.952
W0302 01:15:25.980792      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1294 03/02/23 01:15:25.981
Mar  2 01:15:26.014: INFO: Found 0 stateful pods, waiting for 1
Mar  2 01:15:36.030: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 03/02/23 01:15:36.03
Mar  2 01:15:36.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=statefulset-1294 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 01:15:36.432: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 01:15:36.432: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 01:15:36.432: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 01:15:36.458: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar  2 01:15:46.498: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 01:15:46.498: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 01:15:46.571: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Mar  2 01:15:46.571: INFO: ss-0  10.132.92.143  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:26 +0000 UTC  }]
Mar  2 01:15:46.571: INFO: 
Mar  2 01:15:46.571: INFO: StatefulSet ss has not reached scale 3, at 1
Mar  2 01:15:47.640: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.976111216s
Mar  2 01:15:48.670: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.912643837s
Mar  2 01:15:49.692: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.885055249s
Mar  2 01:15:50.709: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.861817778s
Mar  2 01:15:51.729: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.845014983s
Mar  2 01:15:52.748: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.825164469s
Mar  2 01:15:53.766: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.807024001s
Mar  2 01:15:54.803: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.789115663s
Mar  2 01:15:55.821: INFO: Verifying statefulset ss doesn't scale past 3 for another 751.476123ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1294 03/02/23 01:15:56.821
Mar  2 01:15:56.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=statefulset-1294 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 01:15:57.301: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  2 01:15:57.301: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 01:15:57.301: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 01:15:57.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=statefulset-1294 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 01:15:57.636: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar  2 01:15:57.636: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 01:15:57.636: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 01:15:57.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=statefulset-1294 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 01:15:58.073: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar  2 01:15:58.073: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 01:15:58.073: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 01:15:58.092: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 01:15:58.092: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 01:15:58.092: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 03/02/23 01:15:58.092
Mar  2 01:15:58.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=statefulset-1294 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 01:15:58.413: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 01:15:58.413: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 01:15:58.413: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 01:15:58.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=statefulset-1294 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 01:15:58.755: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 01:15:58.755: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 01:15:58.755: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 01:15:58.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=statefulset-1294 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 01:15:59.099: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 01:15:59.099: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 01:15:59.099: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 01:15:59.099: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 01:15:59.116: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Mar  2 01:16:09.151: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 01:16:09.151: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 01:16:09.151: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 01:16:09.235: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Mar  2 01:16:09.235: INFO: ss-0  10.132.92.143  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:26 +0000 UTC  }]
Mar  2 01:16:09.235: INFO: ss-1  10.132.92.188  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:46 +0000 UTC  }]
Mar  2 01:16:09.235: INFO: ss-2  10.132.92.186  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:16:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:16:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:46 +0000 UTC  }]
Mar  2 01:16:09.235: INFO: 
Mar  2 01:16:09.235: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  2 01:16:10.258: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Mar  2 01:16:10.258: INFO: ss-0  10.132.92.143  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:26 +0000 UTC  }]
Mar  2 01:16:10.258: INFO: ss-1  10.132.92.188  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:46 +0000 UTC  }]
Mar  2 01:16:10.258: INFO: ss-2  10.132.92.186  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:16:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:16:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:46 +0000 UTC  }]
Mar  2 01:16:10.258: INFO: 
Mar  2 01:16:10.258: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  2 01:16:11.280: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Mar  2 01:16:11.280: INFO: ss-1  10.132.92.188  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:46 +0000 UTC  }]
Mar  2 01:16:11.280: INFO: ss-2  10.132.92.186  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:16:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:16:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:46 +0000 UTC  }]
Mar  2 01:16:11.280: INFO: 
Mar  2 01:16:11.280: INFO: StatefulSet ss has not reached scale 0, at 2
Mar  2 01:16:12.295: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.937499836s
Mar  2 01:16:13.310: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.922568474s
Mar  2 01:16:14.328: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.907856906s
Mar  2 01:16:15.343: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.888392883s
Mar  2 01:16:16.379: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.873802402s
Mar  2 01:16:17.397: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.837820611s
Mar  2 01:16:18.413: INFO: Verifying statefulset ss doesn't scale past 0 for another 820.769083ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1294 03/02/23 01:16:19.413
Mar  2 01:16:19.430: INFO: Scaling statefulset ss to 0
Mar  2 01:16:19.474: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar  2 01:16:19.488: INFO: Deleting all statefulset in ns statefulset-1294
Mar  2 01:16:19.502: INFO: Scaling statefulset ss to 0
Mar  2 01:16:19.557: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 01:16:19.571: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar  2 01:16:19.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1294" for this suite. 03/02/23 01:16:19.713
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","completed":18,"skipped":446,"failed":0}
------------------------------
• [SLOW TEST] [53.872 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:695

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:15:25.858
    Mar  2 01:15:25.858: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename statefulset 03/02/23 01:15:25.859
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:15:25.91
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:15:25.921
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-1294 03/02/23 01:15:25.93
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:695
    STEP: Creating stateful set ss in namespace statefulset-1294 03/02/23 01:15:25.952
    W0302 01:15:25.980792      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1294 03/02/23 01:15:25.981
    Mar  2 01:15:26.014: INFO: Found 0 stateful pods, waiting for 1
    Mar  2 01:15:36.030: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 03/02/23 01:15:36.03
    Mar  2 01:15:36.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=statefulset-1294 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar  2 01:15:36.432: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar  2 01:15:36.432: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar  2 01:15:36.432: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar  2 01:15:36.458: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Mar  2 01:15:46.498: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Mar  2 01:15:46.498: INFO: Waiting for statefulset status.replicas updated to 0
    Mar  2 01:15:46.571: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
    Mar  2 01:15:46.571: INFO: ss-0  10.132.92.143  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:26 +0000 UTC  }]
    Mar  2 01:15:46.571: INFO: 
    Mar  2 01:15:46.571: INFO: StatefulSet ss has not reached scale 3, at 1
    Mar  2 01:15:47.640: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.976111216s
    Mar  2 01:15:48.670: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.912643837s
    Mar  2 01:15:49.692: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.885055249s
    Mar  2 01:15:50.709: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.861817778s
    Mar  2 01:15:51.729: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.845014983s
    Mar  2 01:15:52.748: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.825164469s
    Mar  2 01:15:53.766: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.807024001s
    Mar  2 01:15:54.803: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.789115663s
    Mar  2 01:15:55.821: INFO: Verifying statefulset ss doesn't scale past 3 for another 751.476123ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1294 03/02/23 01:15:56.821
    Mar  2 01:15:56.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=statefulset-1294 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar  2 01:15:57.301: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar  2 01:15:57.301: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar  2 01:15:57.301: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar  2 01:15:57.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=statefulset-1294 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar  2 01:15:57.636: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Mar  2 01:15:57.636: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar  2 01:15:57.636: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar  2 01:15:57.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=statefulset-1294 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar  2 01:15:58.073: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Mar  2 01:15:58.073: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar  2 01:15:58.073: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar  2 01:15:58.092: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar  2 01:15:58.092: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar  2 01:15:58.092: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 03/02/23 01:15:58.092
    Mar  2 01:15:58.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=statefulset-1294 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar  2 01:15:58.413: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar  2 01:15:58.413: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar  2 01:15:58.413: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar  2 01:15:58.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=statefulset-1294 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar  2 01:15:58.755: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar  2 01:15:58.755: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar  2 01:15:58.755: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar  2 01:15:58.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=statefulset-1294 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar  2 01:15:59.099: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar  2 01:15:59.099: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar  2 01:15:59.099: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar  2 01:15:59.099: INFO: Waiting for statefulset status.replicas updated to 0
    Mar  2 01:15:59.116: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
    Mar  2 01:16:09.151: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Mar  2 01:16:09.151: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Mar  2 01:16:09.151: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Mar  2 01:16:09.235: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
    Mar  2 01:16:09.235: INFO: ss-0  10.132.92.143  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:26 +0000 UTC  }]
    Mar  2 01:16:09.235: INFO: ss-1  10.132.92.188  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:46 +0000 UTC  }]
    Mar  2 01:16:09.235: INFO: ss-2  10.132.92.186  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:16:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:16:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:46 +0000 UTC  }]
    Mar  2 01:16:09.235: INFO: 
    Mar  2 01:16:09.235: INFO: StatefulSet ss has not reached scale 0, at 3
    Mar  2 01:16:10.258: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
    Mar  2 01:16:10.258: INFO: ss-0  10.132.92.143  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:26 +0000 UTC  }]
    Mar  2 01:16:10.258: INFO: ss-1  10.132.92.188  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:46 +0000 UTC  }]
    Mar  2 01:16:10.258: INFO: ss-2  10.132.92.186  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:16:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:16:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:46 +0000 UTC  }]
    Mar  2 01:16:10.258: INFO: 
    Mar  2 01:16:10.258: INFO: StatefulSet ss has not reached scale 0, at 3
    Mar  2 01:16:11.280: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
    Mar  2 01:16:11.280: INFO: ss-1  10.132.92.188  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:46 +0000 UTC  }]
    Mar  2 01:16:11.280: INFO: ss-2  10.132.92.186  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:16:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:16:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:15:46 +0000 UTC  }]
    Mar  2 01:16:11.280: INFO: 
    Mar  2 01:16:11.280: INFO: StatefulSet ss has not reached scale 0, at 2
    Mar  2 01:16:12.295: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.937499836s
    Mar  2 01:16:13.310: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.922568474s
    Mar  2 01:16:14.328: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.907856906s
    Mar  2 01:16:15.343: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.888392883s
    Mar  2 01:16:16.379: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.873802402s
    Mar  2 01:16:17.397: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.837820611s
    Mar  2 01:16:18.413: INFO: Verifying statefulset ss doesn't scale past 0 for another 820.769083ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1294 03/02/23 01:16:19.413
    Mar  2 01:16:19.430: INFO: Scaling statefulset ss to 0
    Mar  2 01:16:19.474: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar  2 01:16:19.488: INFO: Deleting all statefulset in ns statefulset-1294
    Mar  2 01:16:19.502: INFO: Scaling statefulset ss to 0
    Mar  2 01:16:19.557: INFO: Waiting for statefulset status.replicas updated to 0
    Mar  2 01:16:19.571: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar  2 01:16:19.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-1294" for this suite. 03/02/23 01:16:19.713
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:16:19.736
Mar  2 01:16:19.736: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename pods 03/02/23 01:16:19.737
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:16:19.802
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:16:19.812
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
STEP: creating the pod 03/02/23 01:16:19.822
STEP: submitting the pod to kubernetes 03/02/23 01:16:19.822
Mar  2 01:16:19.898: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-4cc47fe4-d9c2-4b86-81f8-a055ab3c5e37" in namespace "pods-9792" to be "running and ready"
Mar  2 01:16:19.943: INFO: Pod "pod-update-activedeadlineseconds-4cc47fe4-d9c2-4b86-81f8-a055ab3c5e37": Phase="Pending", Reason="", readiness=false. Elapsed: 44.572074ms
Mar  2 01:16:19.943: INFO: The phase of Pod pod-update-activedeadlineseconds-4cc47fe4-d9c2-4b86-81f8-a055ab3c5e37 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:16:21.958: INFO: Pod "pod-update-activedeadlineseconds-4cc47fe4-d9c2-4b86-81f8-a055ab3c5e37": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059747662s
Mar  2 01:16:21.958: INFO: The phase of Pod pod-update-activedeadlineseconds-4cc47fe4-d9c2-4b86-81f8-a055ab3c5e37 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:16:23.957: INFO: Pod "pod-update-activedeadlineseconds-4cc47fe4-d9c2-4b86-81f8-a055ab3c5e37": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05905517s
Mar  2 01:16:23.957: INFO: The phase of Pod pod-update-activedeadlineseconds-4cc47fe4-d9c2-4b86-81f8-a055ab3c5e37 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:16:25.956: INFO: Pod "pod-update-activedeadlineseconds-4cc47fe4-d9c2-4b86-81f8-a055ab3c5e37": Phase="Running", Reason="", readiness=true. Elapsed: 6.058095231s
Mar  2 01:16:25.956: INFO: The phase of Pod pod-update-activedeadlineseconds-4cc47fe4-d9c2-4b86-81f8-a055ab3c5e37 is Running (Ready = true)
Mar  2 01:16:25.956: INFO: Pod "pod-update-activedeadlineseconds-4cc47fe4-d9c2-4b86-81f8-a055ab3c5e37" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 03/02/23 01:16:25.98
STEP: updating the pod 03/02/23 01:16:26.003
Mar  2 01:16:26.714: INFO: Successfully updated pod "pod-update-activedeadlineseconds-4cc47fe4-d9c2-4b86-81f8-a055ab3c5e37"
Mar  2 01:16:26.714: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-4cc47fe4-d9c2-4b86-81f8-a055ab3c5e37" in namespace "pods-9792" to be "terminated with reason DeadlineExceeded"
Mar  2 01:16:26.819: INFO: Pod "pod-update-activedeadlineseconds-4cc47fe4-d9c2-4b86-81f8-a055ab3c5e37": Phase="Running", Reason="", readiness=false. Elapsed: 105.344453ms
Mar  2 01:16:28.885: INFO: Pod "pod-update-activedeadlineseconds-4cc47fe4-d9c2-4b86-81f8-a055ab3c5e37": Phase="Running", Reason="", readiness=false. Elapsed: 2.171309423s
Mar  2 01:16:30.864: INFO: Pod "pod-update-activedeadlineseconds-4cc47fe4-d9c2-4b86-81f8-a055ab3c5e37": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.150233394s
Mar  2 01:16:30.864: INFO: Pod "pod-update-activedeadlineseconds-4cc47fe4-d9c2-4b86-81f8-a055ab3c5e37" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar  2 01:16:30.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9792" for this suite. 03/02/23 01:16:30.883
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","completed":19,"skipped":476,"failed":0}
------------------------------
• [SLOW TEST] [11.165 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:16:19.736
    Mar  2 01:16:19.736: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename pods 03/02/23 01:16:19.737
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:16:19.802
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:16:19.812
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:397
    STEP: creating the pod 03/02/23 01:16:19.822
    STEP: submitting the pod to kubernetes 03/02/23 01:16:19.822
    Mar  2 01:16:19.898: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-4cc47fe4-d9c2-4b86-81f8-a055ab3c5e37" in namespace "pods-9792" to be "running and ready"
    Mar  2 01:16:19.943: INFO: Pod "pod-update-activedeadlineseconds-4cc47fe4-d9c2-4b86-81f8-a055ab3c5e37": Phase="Pending", Reason="", readiness=false. Elapsed: 44.572074ms
    Mar  2 01:16:19.943: INFO: The phase of Pod pod-update-activedeadlineseconds-4cc47fe4-d9c2-4b86-81f8-a055ab3c5e37 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 01:16:21.958: INFO: Pod "pod-update-activedeadlineseconds-4cc47fe4-d9c2-4b86-81f8-a055ab3c5e37": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059747662s
    Mar  2 01:16:21.958: INFO: The phase of Pod pod-update-activedeadlineseconds-4cc47fe4-d9c2-4b86-81f8-a055ab3c5e37 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 01:16:23.957: INFO: Pod "pod-update-activedeadlineseconds-4cc47fe4-d9c2-4b86-81f8-a055ab3c5e37": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05905517s
    Mar  2 01:16:23.957: INFO: The phase of Pod pod-update-activedeadlineseconds-4cc47fe4-d9c2-4b86-81f8-a055ab3c5e37 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 01:16:25.956: INFO: Pod "pod-update-activedeadlineseconds-4cc47fe4-d9c2-4b86-81f8-a055ab3c5e37": Phase="Running", Reason="", readiness=true. Elapsed: 6.058095231s
    Mar  2 01:16:25.956: INFO: The phase of Pod pod-update-activedeadlineseconds-4cc47fe4-d9c2-4b86-81f8-a055ab3c5e37 is Running (Ready = true)
    Mar  2 01:16:25.956: INFO: Pod "pod-update-activedeadlineseconds-4cc47fe4-d9c2-4b86-81f8-a055ab3c5e37" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 03/02/23 01:16:25.98
    STEP: updating the pod 03/02/23 01:16:26.003
    Mar  2 01:16:26.714: INFO: Successfully updated pod "pod-update-activedeadlineseconds-4cc47fe4-d9c2-4b86-81f8-a055ab3c5e37"
    Mar  2 01:16:26.714: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-4cc47fe4-d9c2-4b86-81f8-a055ab3c5e37" in namespace "pods-9792" to be "terminated with reason DeadlineExceeded"
    Mar  2 01:16:26.819: INFO: Pod "pod-update-activedeadlineseconds-4cc47fe4-d9c2-4b86-81f8-a055ab3c5e37": Phase="Running", Reason="", readiness=false. Elapsed: 105.344453ms
    Mar  2 01:16:28.885: INFO: Pod "pod-update-activedeadlineseconds-4cc47fe4-d9c2-4b86-81f8-a055ab3c5e37": Phase="Running", Reason="", readiness=false. Elapsed: 2.171309423s
    Mar  2 01:16:30.864: INFO: Pod "pod-update-activedeadlineseconds-4cc47fe4-d9c2-4b86-81f8-a055ab3c5e37": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.150233394s
    Mar  2 01:16:30.864: INFO: Pod "pod-update-activedeadlineseconds-4cc47fe4-d9c2-4b86-81f8-a055ab3c5e37" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar  2 01:16:30.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-9792" for this suite. 03/02/23 01:16:30.883
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:16:30.905
Mar  2 01:16:30.905: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename emptydir 03/02/23 01:16:30.906
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:16:30.985
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:16:30.995
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
STEP: Creating a pod to test emptydir volume type on tmpfs 03/02/23 01:16:31.012
Mar  2 01:16:31.064: INFO: Waiting up to 5m0s for pod "pod-f918d840-b6dd-4f13-8424-4c2f5d7f6474" in namespace "emptydir-4371" to be "Succeeded or Failed"
Mar  2 01:16:31.081: INFO: Pod "pod-f918d840-b6dd-4f13-8424-4c2f5d7f6474": Phase="Pending", Reason="", readiness=false. Elapsed: 17.574914ms
Mar  2 01:16:33.103: INFO: Pod "pod-f918d840-b6dd-4f13-8424-4c2f5d7f6474": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038922433s
Mar  2 01:16:35.098: INFO: Pod "pod-f918d840-b6dd-4f13-8424-4c2f5d7f6474": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033762845s
Mar  2 01:16:37.098: INFO: Pod "pod-f918d840-b6dd-4f13-8424-4c2f5d7f6474": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.033863071s
STEP: Saw pod success 03/02/23 01:16:37.098
Mar  2 01:16:37.098: INFO: Pod "pod-f918d840-b6dd-4f13-8424-4c2f5d7f6474" satisfied condition "Succeeded or Failed"
Mar  2 01:16:37.111: INFO: Trying to get logs from node 10.132.92.143 pod pod-f918d840-b6dd-4f13-8424-4c2f5d7f6474 container test-container: <nil>
STEP: delete the pod 03/02/23 01:16:37.17
Mar  2 01:16:37.260: INFO: Waiting for pod pod-f918d840-b6dd-4f13-8424-4c2f5d7f6474 to disappear
Mar  2 01:16:37.274: INFO: Pod pod-f918d840-b6dd-4f13-8424-4c2f5d7f6474 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  2 01:16:37.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4371" for this suite. 03/02/23 01:16:37.297
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":20,"skipped":489,"failed":0}
------------------------------
• [SLOW TEST] [6.409 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:16:30.905
    Mar  2 01:16:30.905: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename emptydir 03/02/23 01:16:30.906
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:16:30.985
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:16:30.995
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:86
    STEP: Creating a pod to test emptydir volume type on tmpfs 03/02/23 01:16:31.012
    Mar  2 01:16:31.064: INFO: Waiting up to 5m0s for pod "pod-f918d840-b6dd-4f13-8424-4c2f5d7f6474" in namespace "emptydir-4371" to be "Succeeded or Failed"
    Mar  2 01:16:31.081: INFO: Pod "pod-f918d840-b6dd-4f13-8424-4c2f5d7f6474": Phase="Pending", Reason="", readiness=false. Elapsed: 17.574914ms
    Mar  2 01:16:33.103: INFO: Pod "pod-f918d840-b6dd-4f13-8424-4c2f5d7f6474": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038922433s
    Mar  2 01:16:35.098: INFO: Pod "pod-f918d840-b6dd-4f13-8424-4c2f5d7f6474": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033762845s
    Mar  2 01:16:37.098: INFO: Pod "pod-f918d840-b6dd-4f13-8424-4c2f5d7f6474": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.033863071s
    STEP: Saw pod success 03/02/23 01:16:37.098
    Mar  2 01:16:37.098: INFO: Pod "pod-f918d840-b6dd-4f13-8424-4c2f5d7f6474" satisfied condition "Succeeded or Failed"
    Mar  2 01:16:37.111: INFO: Trying to get logs from node 10.132.92.143 pod pod-f918d840-b6dd-4f13-8424-4c2f5d7f6474 container test-container: <nil>
    STEP: delete the pod 03/02/23 01:16:37.17
    Mar  2 01:16:37.260: INFO: Waiting for pod pod-f918d840-b6dd-4f13-8424-4c2f5d7f6474 to disappear
    Mar  2 01:16:37.274: INFO: Pod pod-f918d840-b6dd-4f13-8424-4c2f5d7f6474 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  2 01:16:37.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4371" for this suite. 03/02/23 01:16:37.297
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:16:37.316
Mar  2 01:16:37.316: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename events 03/02/23 01:16:37.318
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:16:37.369
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:16:37.381
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 03/02/23 01:16:37.393
STEP: get a list of Events with a label in the current namespace 03/02/23 01:16:37.447
STEP: delete a list of events 03/02/23 01:16:37.462
Mar  2 01:16:37.462: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 03/02/23 01:16:37.533
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Mar  2 01:16:37.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1024" for this suite. 03/02/23 01:16:37.564
{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","completed":21,"skipped":491,"failed":0}
------------------------------
• [0.266 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:16:37.316
    Mar  2 01:16:37.316: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename events 03/02/23 01:16:37.318
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:16:37.369
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:16:37.381
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 03/02/23 01:16:37.393
    STEP: get a list of Events with a label in the current namespace 03/02/23 01:16:37.447
    STEP: delete a list of events 03/02/23 01:16:37.462
    Mar  2 01:16:37.462: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 03/02/23 01:16:37.533
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Mar  2 01:16:37.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-1024" for this suite. 03/02/23 01:16:37.564
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:16:37.584
Mar  2 01:16:37.584: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename secrets 03/02/23 01:16:37.586
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:16:37.632
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:16:37.642
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
Mar  2 01:16:37.679: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-58f16550-daa8-4227-be92-72c22065d187 03/02/23 01:16:37.679
STEP: Creating secret with name s-test-opt-upd-f602435c-dcb9-4b80-8cf7-a614ab52405d 03/02/23 01:16:37.693
STEP: Creating the pod 03/02/23 01:16:37.706
Mar  2 01:16:37.766: INFO: Waiting up to 5m0s for pod "pod-secrets-25c75f17-93b7-45e4-a0d9-b8efe89175a1" in namespace "secrets-5027" to be "running and ready"
Mar  2 01:16:37.783: INFO: Pod "pod-secrets-25c75f17-93b7-45e4-a0d9-b8efe89175a1": Phase="Pending", Reason="", readiness=false. Elapsed: 16.725868ms
Mar  2 01:16:37.783: INFO: The phase of Pod pod-secrets-25c75f17-93b7-45e4-a0d9-b8efe89175a1 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:16:39.799: INFO: Pod "pod-secrets-25c75f17-93b7-45e4-a0d9-b8efe89175a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032714332s
Mar  2 01:16:39.799: INFO: The phase of Pod pod-secrets-25c75f17-93b7-45e4-a0d9-b8efe89175a1 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:16:41.801: INFO: Pod "pod-secrets-25c75f17-93b7-45e4-a0d9-b8efe89175a1": Phase="Running", Reason="", readiness=true. Elapsed: 4.034504835s
Mar  2 01:16:41.801: INFO: The phase of Pod pod-secrets-25c75f17-93b7-45e4-a0d9-b8efe89175a1 is Running (Ready = true)
Mar  2 01:16:41.801: INFO: Pod "pod-secrets-25c75f17-93b7-45e4-a0d9-b8efe89175a1" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-58f16550-daa8-4227-be92-72c22065d187 03/02/23 01:16:41.951
STEP: Updating secret s-test-opt-upd-f602435c-dcb9-4b80-8cf7-a614ab52405d 03/02/23 01:16:41.969
STEP: Creating secret with name s-test-opt-create-0c12dd79-045b-49ce-bfac-fc990d54857f 03/02/23 01:16:41.985
STEP: waiting to observe update in volume 03/02/23 01:16:42
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar  2 01:17:49.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5027" for this suite. 03/02/23 01:17:49.615
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":22,"skipped":504,"failed":0}
------------------------------
• [SLOW TEST] [72.058 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:16:37.584
    Mar  2 01:16:37.584: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename secrets 03/02/23 01:16:37.586
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:16:37.632
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:16:37.642
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:204
    Mar  2 01:16:37.679: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating secret with name s-test-opt-del-58f16550-daa8-4227-be92-72c22065d187 03/02/23 01:16:37.679
    STEP: Creating secret with name s-test-opt-upd-f602435c-dcb9-4b80-8cf7-a614ab52405d 03/02/23 01:16:37.693
    STEP: Creating the pod 03/02/23 01:16:37.706
    Mar  2 01:16:37.766: INFO: Waiting up to 5m0s for pod "pod-secrets-25c75f17-93b7-45e4-a0d9-b8efe89175a1" in namespace "secrets-5027" to be "running and ready"
    Mar  2 01:16:37.783: INFO: Pod "pod-secrets-25c75f17-93b7-45e4-a0d9-b8efe89175a1": Phase="Pending", Reason="", readiness=false. Elapsed: 16.725868ms
    Mar  2 01:16:37.783: INFO: The phase of Pod pod-secrets-25c75f17-93b7-45e4-a0d9-b8efe89175a1 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 01:16:39.799: INFO: Pod "pod-secrets-25c75f17-93b7-45e4-a0d9-b8efe89175a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032714332s
    Mar  2 01:16:39.799: INFO: The phase of Pod pod-secrets-25c75f17-93b7-45e4-a0d9-b8efe89175a1 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 01:16:41.801: INFO: Pod "pod-secrets-25c75f17-93b7-45e4-a0d9-b8efe89175a1": Phase="Running", Reason="", readiness=true. Elapsed: 4.034504835s
    Mar  2 01:16:41.801: INFO: The phase of Pod pod-secrets-25c75f17-93b7-45e4-a0d9-b8efe89175a1 is Running (Ready = true)
    Mar  2 01:16:41.801: INFO: Pod "pod-secrets-25c75f17-93b7-45e4-a0d9-b8efe89175a1" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-58f16550-daa8-4227-be92-72c22065d187 03/02/23 01:16:41.951
    STEP: Updating secret s-test-opt-upd-f602435c-dcb9-4b80-8cf7-a614ab52405d 03/02/23 01:16:41.969
    STEP: Creating secret with name s-test-opt-create-0c12dd79-045b-49ce-bfac-fc990d54857f 03/02/23 01:16:41.985
    STEP: waiting to observe update in volume 03/02/23 01:16:42
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar  2 01:17:49.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5027" for this suite. 03/02/23 01:17:49.615
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:17:49.651
Mar  2 01:17:49.651: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename downward-api 03/02/23 01:17:49.652
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:17:49.7
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:17:49.71
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
STEP: Creating a pod to test downward api env vars 03/02/23 01:17:49.73
Mar  2 01:17:49.791: INFO: Waiting up to 5m0s for pod "downward-api-4041e6d1-a7b8-48ae-8afa-02866063212d" in namespace "downward-api-2291" to be "Succeeded or Failed"
Mar  2 01:17:49.810: INFO: Pod "downward-api-4041e6d1-a7b8-48ae-8afa-02866063212d": Phase="Pending", Reason="", readiness=false. Elapsed: 18.385927ms
Mar  2 01:17:51.844: INFO: Pod "downward-api-4041e6d1-a7b8-48ae-8afa-02866063212d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052871012s
Mar  2 01:17:53.824: INFO: Pod "downward-api-4041e6d1-a7b8-48ae-8afa-02866063212d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032680431s
Mar  2 01:17:55.827: INFO: Pod "downward-api-4041e6d1-a7b8-48ae-8afa-02866063212d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.035370988s
Mar  2 01:17:57.826: INFO: Pod "downward-api-4041e6d1-a7b8-48ae-8afa-02866063212d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.0343848s
STEP: Saw pod success 03/02/23 01:17:57.826
Mar  2 01:17:57.826: INFO: Pod "downward-api-4041e6d1-a7b8-48ae-8afa-02866063212d" satisfied condition "Succeeded or Failed"
Mar  2 01:17:57.843: INFO: Trying to get logs from node 10.132.92.188 pod downward-api-4041e6d1-a7b8-48ae-8afa-02866063212d container dapi-container: <nil>
STEP: delete the pod 03/02/23 01:17:57.918
Mar  2 01:17:57.972: INFO: Waiting for pod downward-api-4041e6d1-a7b8-48ae-8afa-02866063212d to disappear
Mar  2 01:17:57.986: INFO: Pod downward-api-4041e6d1-a7b8-48ae-8afa-02866063212d no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Mar  2 01:17:57.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2291" for this suite. 03/02/23 01:17:58.004
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","completed":23,"skipped":561,"failed":0}
------------------------------
• [SLOW TEST] [8.372 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:17:49.651
    Mar  2 01:17:49.651: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename downward-api 03/02/23 01:17:49.652
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:17:49.7
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:17:49.71
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:43
    STEP: Creating a pod to test downward api env vars 03/02/23 01:17:49.73
    Mar  2 01:17:49.791: INFO: Waiting up to 5m0s for pod "downward-api-4041e6d1-a7b8-48ae-8afa-02866063212d" in namespace "downward-api-2291" to be "Succeeded or Failed"
    Mar  2 01:17:49.810: INFO: Pod "downward-api-4041e6d1-a7b8-48ae-8afa-02866063212d": Phase="Pending", Reason="", readiness=false. Elapsed: 18.385927ms
    Mar  2 01:17:51.844: INFO: Pod "downward-api-4041e6d1-a7b8-48ae-8afa-02866063212d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052871012s
    Mar  2 01:17:53.824: INFO: Pod "downward-api-4041e6d1-a7b8-48ae-8afa-02866063212d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032680431s
    Mar  2 01:17:55.827: INFO: Pod "downward-api-4041e6d1-a7b8-48ae-8afa-02866063212d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.035370988s
    Mar  2 01:17:57.826: INFO: Pod "downward-api-4041e6d1-a7b8-48ae-8afa-02866063212d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.0343848s
    STEP: Saw pod success 03/02/23 01:17:57.826
    Mar  2 01:17:57.826: INFO: Pod "downward-api-4041e6d1-a7b8-48ae-8afa-02866063212d" satisfied condition "Succeeded or Failed"
    Mar  2 01:17:57.843: INFO: Trying to get logs from node 10.132.92.188 pod downward-api-4041e6d1-a7b8-48ae-8afa-02866063212d container dapi-container: <nil>
    STEP: delete the pod 03/02/23 01:17:57.918
    Mar  2 01:17:57.972: INFO: Waiting for pod downward-api-4041e6d1-a7b8-48ae-8afa-02866063212d to disappear
    Mar  2 01:17:57.986: INFO: Pod downward-api-4041e6d1-a7b8-48ae-8afa-02866063212d no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Mar  2 01:17:57.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2291" for this suite. 03/02/23 01:17:58.004
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:17:58.026
Mar  2 01:17:58.027: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename gc 03/02/23 01:17:58.028
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:17:58.085
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:17:58.102
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 03/02/23 01:17:58.13
STEP: delete the rc 03/02/23 01:18:03.159
STEP: wait for the rc to be deleted 03/02/23 01:18:03.195
Mar  2 01:18:04.284: INFO: 22 pods remaining
Mar  2 01:18:04.284: INFO: 19 pods has nil DeletionTimestamp
Mar  2 01:18:04.284: INFO: 
STEP: Gathering metrics 03/02/23 01:18:05.221
W0302 01:18:05.246066      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Mar  2 01:18:05.246: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Mar  2 01:18:05.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4338" for this suite. 03/02/23 01:18:05.264
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","completed":24,"skipped":572,"failed":0}
------------------------------
• [SLOW TEST] [7.261 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:17:58.026
    Mar  2 01:17:58.027: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename gc 03/02/23 01:17:58.028
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:17:58.085
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:17:58.102
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 03/02/23 01:17:58.13
    STEP: delete the rc 03/02/23 01:18:03.159
    STEP: wait for the rc to be deleted 03/02/23 01:18:03.195
    Mar  2 01:18:04.284: INFO: 22 pods remaining
    Mar  2 01:18:04.284: INFO: 19 pods has nil DeletionTimestamp
    Mar  2 01:18:04.284: INFO: 
    STEP: Gathering metrics 03/02/23 01:18:05.221
    W0302 01:18:05.246066      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Mar  2 01:18:05.246: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Mar  2 01:18:05.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-4338" for this suite. 03/02/23 01:18:05.264
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:18:05.294
Mar  2 01:18:05.294: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename emptydir 03/02/23 01:18:05.296
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:18:05.364
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:18:05.376
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
STEP: Creating a pod to test emptydir 0777 on tmpfs 03/02/23 01:18:05.392
W0302 01:18:05.475413      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Mar  2 01:18:05.475: INFO: Waiting up to 5m0s for pod "pod-48d9455d-dd65-474e-94c6-36b97c7c611d" in namespace "emptydir-5852" to be "Succeeded or Failed"
Mar  2 01:18:05.497: INFO: Pod "pod-48d9455d-dd65-474e-94c6-36b97c7c611d": Phase="Pending", Reason="", readiness=false. Elapsed: 22.20348ms
Mar  2 01:18:07.520: INFO: Pod "pod-48d9455d-dd65-474e-94c6-36b97c7c611d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04463817s
Mar  2 01:18:09.515: INFO: Pod "pod-48d9455d-dd65-474e-94c6-36b97c7c611d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039892136s
Mar  2 01:18:11.529: INFO: Pod "pod-48d9455d-dd65-474e-94c6-36b97c7c611d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.053871109s
Mar  2 01:18:13.517: INFO: Pod "pod-48d9455d-dd65-474e-94c6-36b97c7c611d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.041640367s
Mar  2 01:18:15.514: INFO: Pod "pod-48d9455d-dd65-474e-94c6-36b97c7c611d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.038621567s
Mar  2 01:18:17.515: INFO: Pod "pod-48d9455d-dd65-474e-94c6-36b97c7c611d": Phase="Pending", Reason="", readiness=false. Elapsed: 12.040239212s
Mar  2 01:18:19.513: INFO: Pod "pod-48d9455d-dd65-474e-94c6-36b97c7c611d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.037724157s
STEP: Saw pod success 03/02/23 01:18:19.514
Mar  2 01:18:19.514: INFO: Pod "pod-48d9455d-dd65-474e-94c6-36b97c7c611d" satisfied condition "Succeeded or Failed"
Mar  2 01:18:19.528: INFO: Trying to get logs from node 10.132.92.143 pod pod-48d9455d-dd65-474e-94c6-36b97c7c611d container test-container: <nil>
STEP: delete the pod 03/02/23 01:18:19.559
Mar  2 01:18:19.599: INFO: Waiting for pod pod-48d9455d-dd65-474e-94c6-36b97c7c611d to disappear
Mar  2 01:18:19.622: INFO: Pod pod-48d9455d-dd65-474e-94c6-36b97c7c611d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  2 01:18:19.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5852" for this suite. 03/02/23 01:18:19.641
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":25,"skipped":594,"failed":0}
------------------------------
• [SLOW TEST] [14.364 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:18:05.294
    Mar  2 01:18:05.294: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename emptydir 03/02/23 01:18:05.296
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:18:05.364
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:18:05.376
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:146
    STEP: Creating a pod to test emptydir 0777 on tmpfs 03/02/23 01:18:05.392
    W0302 01:18:05.475413      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Mar  2 01:18:05.475: INFO: Waiting up to 5m0s for pod "pod-48d9455d-dd65-474e-94c6-36b97c7c611d" in namespace "emptydir-5852" to be "Succeeded or Failed"
    Mar  2 01:18:05.497: INFO: Pod "pod-48d9455d-dd65-474e-94c6-36b97c7c611d": Phase="Pending", Reason="", readiness=false. Elapsed: 22.20348ms
    Mar  2 01:18:07.520: INFO: Pod "pod-48d9455d-dd65-474e-94c6-36b97c7c611d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04463817s
    Mar  2 01:18:09.515: INFO: Pod "pod-48d9455d-dd65-474e-94c6-36b97c7c611d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039892136s
    Mar  2 01:18:11.529: INFO: Pod "pod-48d9455d-dd65-474e-94c6-36b97c7c611d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.053871109s
    Mar  2 01:18:13.517: INFO: Pod "pod-48d9455d-dd65-474e-94c6-36b97c7c611d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.041640367s
    Mar  2 01:18:15.514: INFO: Pod "pod-48d9455d-dd65-474e-94c6-36b97c7c611d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.038621567s
    Mar  2 01:18:17.515: INFO: Pod "pod-48d9455d-dd65-474e-94c6-36b97c7c611d": Phase="Pending", Reason="", readiness=false. Elapsed: 12.040239212s
    Mar  2 01:18:19.513: INFO: Pod "pod-48d9455d-dd65-474e-94c6-36b97c7c611d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.037724157s
    STEP: Saw pod success 03/02/23 01:18:19.514
    Mar  2 01:18:19.514: INFO: Pod "pod-48d9455d-dd65-474e-94c6-36b97c7c611d" satisfied condition "Succeeded or Failed"
    Mar  2 01:18:19.528: INFO: Trying to get logs from node 10.132.92.143 pod pod-48d9455d-dd65-474e-94c6-36b97c7c611d container test-container: <nil>
    STEP: delete the pod 03/02/23 01:18:19.559
    Mar  2 01:18:19.599: INFO: Waiting for pod pod-48d9455d-dd65-474e-94c6-36b97c7c611d to disappear
    Mar  2 01:18:19.622: INFO: Pod pod-48d9455d-dd65-474e-94c6-36b97c7c611d no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  2 01:18:19.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5852" for this suite. 03/02/23 01:18:19.641
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:18:19.659
Mar  2 01:18:19.659: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename controllerrevisions 03/02/23 01:18:19.66
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:18:19.722
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:18:19.733
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-5dlf8-daemon-set" 03/02/23 01:18:19.874
STEP: Check that daemon pods launch on every node of the cluster. 03/02/23 01:18:19.897
Mar  2 01:18:19.927: INFO: Number of nodes with available pods controlled by daemonset e2e-5dlf8-daemon-set: 0
Mar  2 01:18:19.927: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
Mar  2 01:18:20.976: INFO: Number of nodes with available pods controlled by daemonset e2e-5dlf8-daemon-set: 0
Mar  2 01:18:20.976: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
Mar  2 01:18:21.965: INFO: Number of nodes with available pods controlled by daemonset e2e-5dlf8-daemon-set: 0
Mar  2 01:18:21.965: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
Mar  2 01:18:22.964: INFO: Number of nodes with available pods controlled by daemonset e2e-5dlf8-daemon-set: 3
Mar  2 01:18:22.964: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-5dlf8-daemon-set
STEP: Confirm DaemonSet "e2e-5dlf8-daemon-set" successfully created with "daemonset-name=e2e-5dlf8-daemon-set" label 03/02/23 01:18:22.977
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-5dlf8-daemon-set" 03/02/23 01:18:23.014
Mar  2 01:18:23.039: INFO: Located ControllerRevision: "e2e-5dlf8-daemon-set-6cb9496d58"
STEP: Patching ControllerRevision "e2e-5dlf8-daemon-set-6cb9496d58" 03/02/23 01:18:23.052
Mar  2 01:18:23.075: INFO: e2e-5dlf8-daemon-set-6cb9496d58 has been patched
STEP: Create a new ControllerRevision 03/02/23 01:18:23.075
Mar  2 01:18:23.095: INFO: Created ControllerRevision: e2e-5dlf8-daemon-set-79877587fb
STEP: Confirm that there are two ControllerRevisions 03/02/23 01:18:23.095
Mar  2 01:18:23.096: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar  2 01:18:23.110: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-5dlf8-daemon-set-6cb9496d58" 03/02/23 01:18:23.11
STEP: Confirm that there is only one ControllerRevision 03/02/23 01:18:23.136
Mar  2 01:18:23.137: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar  2 01:18:23.151: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-5dlf8-daemon-set-79877587fb" 03/02/23 01:18:23.166
Mar  2 01:18:23.205: INFO: e2e-5dlf8-daemon-set-79877587fb has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 03/02/23 01:18:23.205
W0302 01:18:23.229357      20 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 03/02/23 01:18:23.229
Mar  2 01:18:23.229: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar  2 01:18:24.244: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar  2 01:18:24.262: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-5dlf8-daemon-set-79877587fb=updated" 03/02/23 01:18:24.262
STEP: Confirm that there is only one ControllerRevision 03/02/23 01:18:24.303
Mar  2 01:18:24.303: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar  2 01:18:24.319: INFO: Found 1 ControllerRevisions
Mar  2 01:18:24.333: INFO: ControllerRevision "e2e-5dlf8-daemon-set-85469fd6bf" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-5dlf8-daemon-set" 03/02/23 01:18:24.349
STEP: deleting DaemonSet.extensions e2e-5dlf8-daemon-set in namespace controllerrevisions-3602, will wait for the garbage collector to delete the pods 03/02/23 01:18:24.349
Mar  2 01:18:24.439: INFO: Deleting DaemonSet.extensions e2e-5dlf8-daemon-set took: 24.383756ms
Mar  2 01:18:24.540: INFO: Terminating DaemonSet.extensions e2e-5dlf8-daemon-set pods took: 101.254832ms
Mar  2 01:18:26.856: INFO: Number of nodes with available pods controlled by daemonset e2e-5dlf8-daemon-set: 0
Mar  2 01:18:26.856: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-5dlf8-daemon-set
Mar  2 01:18:26.870: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"74201"},"items":null}

Mar  2 01:18:26.883: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"74201"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:187
Mar  2 01:18:26.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "controllerrevisions-3602" for this suite. 03/02/23 01:18:27.013
{"msg":"PASSED [sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]","completed":26,"skipped":600,"failed":0}
------------------------------
• [SLOW TEST] [7.379 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:18:19.659
    Mar  2 01:18:19.659: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename controllerrevisions 03/02/23 01:18:19.66
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:18:19.722
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:18:19.733
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-5dlf8-daemon-set" 03/02/23 01:18:19.874
    STEP: Check that daemon pods launch on every node of the cluster. 03/02/23 01:18:19.897
    Mar  2 01:18:19.927: INFO: Number of nodes with available pods controlled by daemonset e2e-5dlf8-daemon-set: 0
    Mar  2 01:18:19.927: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
    Mar  2 01:18:20.976: INFO: Number of nodes with available pods controlled by daemonset e2e-5dlf8-daemon-set: 0
    Mar  2 01:18:20.976: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
    Mar  2 01:18:21.965: INFO: Number of nodes with available pods controlled by daemonset e2e-5dlf8-daemon-set: 0
    Mar  2 01:18:21.965: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
    Mar  2 01:18:22.964: INFO: Number of nodes with available pods controlled by daemonset e2e-5dlf8-daemon-set: 3
    Mar  2 01:18:22.964: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-5dlf8-daemon-set
    STEP: Confirm DaemonSet "e2e-5dlf8-daemon-set" successfully created with "daemonset-name=e2e-5dlf8-daemon-set" label 03/02/23 01:18:22.977
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-5dlf8-daemon-set" 03/02/23 01:18:23.014
    Mar  2 01:18:23.039: INFO: Located ControllerRevision: "e2e-5dlf8-daemon-set-6cb9496d58"
    STEP: Patching ControllerRevision "e2e-5dlf8-daemon-set-6cb9496d58" 03/02/23 01:18:23.052
    Mar  2 01:18:23.075: INFO: e2e-5dlf8-daemon-set-6cb9496d58 has been patched
    STEP: Create a new ControllerRevision 03/02/23 01:18:23.075
    Mar  2 01:18:23.095: INFO: Created ControllerRevision: e2e-5dlf8-daemon-set-79877587fb
    STEP: Confirm that there are two ControllerRevisions 03/02/23 01:18:23.095
    Mar  2 01:18:23.096: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar  2 01:18:23.110: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-5dlf8-daemon-set-6cb9496d58" 03/02/23 01:18:23.11
    STEP: Confirm that there is only one ControllerRevision 03/02/23 01:18:23.136
    Mar  2 01:18:23.137: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar  2 01:18:23.151: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-5dlf8-daemon-set-79877587fb" 03/02/23 01:18:23.166
    Mar  2 01:18:23.205: INFO: e2e-5dlf8-daemon-set-79877587fb has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 03/02/23 01:18:23.205
    W0302 01:18:23.229357      20 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 03/02/23 01:18:23.229
    Mar  2 01:18:23.229: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar  2 01:18:24.244: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar  2 01:18:24.262: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-5dlf8-daemon-set-79877587fb=updated" 03/02/23 01:18:24.262
    STEP: Confirm that there is only one ControllerRevision 03/02/23 01:18:24.303
    Mar  2 01:18:24.303: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar  2 01:18:24.319: INFO: Found 1 ControllerRevisions
    Mar  2 01:18:24.333: INFO: ControllerRevision "e2e-5dlf8-daemon-set-85469fd6bf" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-5dlf8-daemon-set" 03/02/23 01:18:24.349
    STEP: deleting DaemonSet.extensions e2e-5dlf8-daemon-set in namespace controllerrevisions-3602, will wait for the garbage collector to delete the pods 03/02/23 01:18:24.349
    Mar  2 01:18:24.439: INFO: Deleting DaemonSet.extensions e2e-5dlf8-daemon-set took: 24.383756ms
    Mar  2 01:18:24.540: INFO: Terminating DaemonSet.extensions e2e-5dlf8-daemon-set pods took: 101.254832ms
    Mar  2 01:18:26.856: INFO: Number of nodes with available pods controlled by daemonset e2e-5dlf8-daemon-set: 0
    Mar  2 01:18:26.856: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-5dlf8-daemon-set
    Mar  2 01:18:26.870: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"74201"},"items":null}

    Mar  2 01:18:26.883: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"74201"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:187
    Mar  2 01:18:26.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "controllerrevisions-3602" for this suite. 03/02/23 01:18:27.013
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:18:27.042
Mar  2 01:18:27.042: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename svcaccounts 03/02/23 01:18:27.044
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:18:27.097
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:18:27.11
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
Mar  2 01:18:27.257: INFO: created pod pod-service-account-defaultsa
Mar  2 01:18:27.257: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Mar  2 01:18:27.296: INFO: created pod pod-service-account-mountsa
Mar  2 01:18:27.296: INFO: pod pod-service-account-mountsa service account token volume mount: true
Mar  2 01:18:27.330: INFO: created pod pod-service-account-nomountsa
Mar  2 01:18:27.330: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Mar  2 01:18:27.367: INFO: created pod pod-service-account-defaultsa-mountspec
Mar  2 01:18:27.368: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Mar  2 01:18:27.402: INFO: created pod pod-service-account-mountsa-mountspec
Mar  2 01:18:27.402: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Mar  2 01:18:27.439: INFO: created pod pod-service-account-nomountsa-mountspec
Mar  2 01:18:27.440: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Mar  2 01:18:27.483: INFO: created pod pod-service-account-defaultsa-nomountspec
Mar  2 01:18:27.483: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Mar  2 01:18:27.562: INFO: created pod pod-service-account-mountsa-nomountspec
Mar  2 01:18:27.562: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Mar  2 01:18:27.614: INFO: created pod pod-service-account-nomountsa-nomountspec
Mar  2 01:18:27.614: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Mar  2 01:18:27.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9111" for this suite. 03/02/23 01:18:27.636
{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","completed":27,"skipped":654,"failed":0}
------------------------------
• [0.614 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:18:27.042
    Mar  2 01:18:27.042: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename svcaccounts 03/02/23 01:18:27.044
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:18:27.097
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:18:27.11
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:158
    Mar  2 01:18:27.257: INFO: created pod pod-service-account-defaultsa
    Mar  2 01:18:27.257: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Mar  2 01:18:27.296: INFO: created pod pod-service-account-mountsa
    Mar  2 01:18:27.296: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Mar  2 01:18:27.330: INFO: created pod pod-service-account-nomountsa
    Mar  2 01:18:27.330: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Mar  2 01:18:27.367: INFO: created pod pod-service-account-defaultsa-mountspec
    Mar  2 01:18:27.368: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Mar  2 01:18:27.402: INFO: created pod pod-service-account-mountsa-mountspec
    Mar  2 01:18:27.402: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Mar  2 01:18:27.439: INFO: created pod pod-service-account-nomountsa-mountspec
    Mar  2 01:18:27.440: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Mar  2 01:18:27.483: INFO: created pod pod-service-account-defaultsa-nomountspec
    Mar  2 01:18:27.483: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Mar  2 01:18:27.562: INFO: created pod pod-service-account-mountsa-nomountspec
    Mar  2 01:18:27.562: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Mar  2 01:18:27.614: INFO: created pod pod-service-account-nomountsa-nomountspec
    Mar  2 01:18:27.614: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Mar  2 01:18:27.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-9111" for this suite. 03/02/23 01:18:27.636
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:18:27.661
Mar  2 01:18:27.662: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename runtimeclass 03/02/23 01:18:27.663
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:18:27.726
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:18:27.739
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 03/02/23 01:18:27.749
STEP: getting /apis/node.k8s.io 03/02/23 01:18:27.76
STEP: getting /apis/node.k8s.io/v1 03/02/23 01:18:27.766
STEP: creating 03/02/23 01:18:27.77
STEP: watching 03/02/23 01:18:27.91
Mar  2 01:18:27.910: INFO: starting watch
STEP: getting 03/02/23 01:18:27.944
STEP: listing 03/02/23 01:18:27.963
STEP: patching 03/02/23 01:18:27.981
STEP: updating 03/02/23 01:18:28.003
Mar  2 01:18:28.028: INFO: waiting for watch events with expected annotations
STEP: deleting 03/02/23 01:18:28.028
STEP: deleting a collection 03/02/23 01:18:28.092
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Mar  2 01:18:28.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-5819" for this suite. 03/02/23 01:18:28.199
{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","completed":28,"skipped":666,"failed":0}
------------------------------
• [0.558 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:18:27.661
    Mar  2 01:18:27.662: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename runtimeclass 03/02/23 01:18:27.663
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:18:27.726
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:18:27.739
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 03/02/23 01:18:27.749
    STEP: getting /apis/node.k8s.io 03/02/23 01:18:27.76
    STEP: getting /apis/node.k8s.io/v1 03/02/23 01:18:27.766
    STEP: creating 03/02/23 01:18:27.77
    STEP: watching 03/02/23 01:18:27.91
    Mar  2 01:18:27.910: INFO: starting watch
    STEP: getting 03/02/23 01:18:27.944
    STEP: listing 03/02/23 01:18:27.963
    STEP: patching 03/02/23 01:18:27.981
    STEP: updating 03/02/23 01:18:28.003
    Mar  2 01:18:28.028: INFO: waiting for watch events with expected annotations
    STEP: deleting 03/02/23 01:18:28.028
    STEP: deleting a collection 03/02/23 01:18:28.092
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Mar  2 01:18:28.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-5819" for this suite. 03/02/23 01:18:28.199
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:18:28.224
Mar  2 01:18:28.225: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename configmap 03/02/23 01:18:28.225
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:18:28.274
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:18:28.289
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
STEP: Creating configMap that has name configmap-test-emptyKey-8af29414-d73e-470c-871b-800106a1a133 03/02/23 01:18:28.305
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Mar  2 01:18:28.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2299" for this suite. 03/02/23 01:18:28.335
{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","completed":29,"skipped":681,"failed":0}
------------------------------
• [0.136 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:18:28.224
    Mar  2 01:18:28.225: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename configmap 03/02/23 01:18:28.225
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:18:28.274
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:18:28.289
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:137
    STEP: Creating configMap that has name configmap-test-emptyKey-8af29414-d73e-470c-871b-800106a1a133 03/02/23 01:18:28.305
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  2 01:18:28.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2299" for this suite. 03/02/23 01:18:28.335
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:18:28.384
Mar  2 01:18:28.393: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename proxy 03/02/23 01:18:28.396
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:18:28.448
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:18:28.458
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Mar  2 01:18:28.474: INFO: Creating pod...
Mar  2 01:18:28.598: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-3002" to be "running"
Mar  2 01:18:28.645: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 46.786813ms
Mar  2 01:18:30.661: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.062005223s
Mar  2 01:18:32.663: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.063956382s
Mar  2 01:18:32.663: INFO: Pod "agnhost" satisfied condition "running"
Mar  2 01:18:32.663: INFO: Creating service...
Mar  2 01:18:32.699: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3002/pods/agnhost/proxy/some/path/with/DELETE
Mar  2 01:18:32.730: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar  2 01:18:32.730: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3002/pods/agnhost/proxy/some/path/with/GET
Mar  2 01:18:32.752: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Mar  2 01:18:32.752: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3002/pods/agnhost/proxy/some/path/with/HEAD
Mar  2 01:18:32.780: INFO: http.Client request:HEAD | StatusCode:200
Mar  2 01:18:32.780: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3002/pods/agnhost/proxy/some/path/with/OPTIONS
Mar  2 01:18:32.801: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar  2 01:18:32.801: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3002/pods/agnhost/proxy/some/path/with/PATCH
Mar  2 01:18:32.821: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar  2 01:18:32.821: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3002/pods/agnhost/proxy/some/path/with/POST
Mar  2 01:18:32.841: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar  2 01:18:32.841: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3002/pods/agnhost/proxy/some/path/with/PUT
Mar  2 01:18:32.872: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Mar  2 01:18:32.872: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3002/services/test-service/proxy/some/path/with/DELETE
Mar  2 01:18:32.941: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar  2 01:18:32.941: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3002/services/test-service/proxy/some/path/with/GET
Mar  2 01:18:32.971: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Mar  2 01:18:32.972: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3002/services/test-service/proxy/some/path/with/HEAD
Mar  2 01:18:33.212: INFO: http.Client request:HEAD | StatusCode:200
Mar  2 01:18:33.213: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3002/services/test-service/proxy/some/path/with/OPTIONS
Mar  2 01:18:33.240: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar  2 01:18:33.240: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3002/services/test-service/proxy/some/path/with/PATCH
Mar  2 01:18:33.266: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar  2 01:18:33.266: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3002/services/test-service/proxy/some/path/with/POST
Mar  2 01:18:33.289: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar  2 01:18:33.289: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3002/services/test-service/proxy/some/path/with/PUT
Mar  2 01:18:33.314: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Mar  2 01:18:33.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3002" for this suite. 03/02/23 01:18:33.336
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","completed":30,"skipped":686,"failed":0}
------------------------------
• [4.971 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:18:28.384
    Mar  2 01:18:28.393: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename proxy 03/02/23 01:18:28.396
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:18:28.448
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:18:28.458
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Mar  2 01:18:28.474: INFO: Creating pod...
    Mar  2 01:18:28.598: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-3002" to be "running"
    Mar  2 01:18:28.645: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 46.786813ms
    Mar  2 01:18:30.661: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.062005223s
    Mar  2 01:18:32.663: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.063956382s
    Mar  2 01:18:32.663: INFO: Pod "agnhost" satisfied condition "running"
    Mar  2 01:18:32.663: INFO: Creating service...
    Mar  2 01:18:32.699: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3002/pods/agnhost/proxy/some/path/with/DELETE
    Mar  2 01:18:32.730: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Mar  2 01:18:32.730: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3002/pods/agnhost/proxy/some/path/with/GET
    Mar  2 01:18:32.752: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Mar  2 01:18:32.752: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3002/pods/agnhost/proxy/some/path/with/HEAD
    Mar  2 01:18:32.780: INFO: http.Client request:HEAD | StatusCode:200
    Mar  2 01:18:32.780: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3002/pods/agnhost/proxy/some/path/with/OPTIONS
    Mar  2 01:18:32.801: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Mar  2 01:18:32.801: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3002/pods/agnhost/proxy/some/path/with/PATCH
    Mar  2 01:18:32.821: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Mar  2 01:18:32.821: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3002/pods/agnhost/proxy/some/path/with/POST
    Mar  2 01:18:32.841: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Mar  2 01:18:32.841: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3002/pods/agnhost/proxy/some/path/with/PUT
    Mar  2 01:18:32.872: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Mar  2 01:18:32.872: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3002/services/test-service/proxy/some/path/with/DELETE
    Mar  2 01:18:32.941: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Mar  2 01:18:32.941: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3002/services/test-service/proxy/some/path/with/GET
    Mar  2 01:18:32.971: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Mar  2 01:18:32.972: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3002/services/test-service/proxy/some/path/with/HEAD
    Mar  2 01:18:33.212: INFO: http.Client request:HEAD | StatusCode:200
    Mar  2 01:18:33.213: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3002/services/test-service/proxy/some/path/with/OPTIONS
    Mar  2 01:18:33.240: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Mar  2 01:18:33.240: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3002/services/test-service/proxy/some/path/with/PATCH
    Mar  2 01:18:33.266: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Mar  2 01:18:33.266: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3002/services/test-service/proxy/some/path/with/POST
    Mar  2 01:18:33.289: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Mar  2 01:18:33.289: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3002/services/test-service/proxy/some/path/with/PUT
    Mar  2 01:18:33.314: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Mar  2 01:18:33.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-3002" for this suite. 03/02/23 01:18:33.336
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:18:33.356
Mar  2 01:18:33.356: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename projected 03/02/23 01:18:33.357
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:18:33.406
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:18:33.416
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
STEP: Creating projection with secret that has name projected-secret-test-030b3253-e435-4cb2-9af3-4184c4344f18 03/02/23 01:18:33.457
STEP: Creating a pod to test consume secrets 03/02/23 01:18:33.496
Mar  2 01:18:33.610: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8ebeabee-cb33-44c1-ab61-b7c716a0837a" in namespace "projected-6478" to be "Succeeded or Failed"
Mar  2 01:18:33.637: INFO: Pod "pod-projected-secrets-8ebeabee-cb33-44c1-ab61-b7c716a0837a": Phase="Pending", Reason="", readiness=false. Elapsed: 26.043416ms
Mar  2 01:18:35.654: INFO: Pod "pod-projected-secrets-8ebeabee-cb33-44c1-ab61-b7c716a0837a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043799293s
Mar  2 01:18:37.653: INFO: Pod "pod-projected-secrets-8ebeabee-cb33-44c1-ab61-b7c716a0837a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042149602s
Mar  2 01:18:39.651: INFO: Pod "pod-projected-secrets-8ebeabee-cb33-44c1-ab61-b7c716a0837a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040978703s
STEP: Saw pod success 03/02/23 01:18:39.652
Mar  2 01:18:39.652: INFO: Pod "pod-projected-secrets-8ebeabee-cb33-44c1-ab61-b7c716a0837a" satisfied condition "Succeeded or Failed"
Mar  2 01:18:39.667: INFO: Trying to get logs from node 10.132.92.186 pod pod-projected-secrets-8ebeabee-cb33-44c1-ab61-b7c716a0837a container projected-secret-volume-test: <nil>
STEP: delete the pod 03/02/23 01:18:39.732
Mar  2 01:18:39.788: INFO: Waiting for pod pod-projected-secrets-8ebeabee-cb33-44c1-ab61-b7c716a0837a to disappear
Mar  2 01:18:39.802: INFO: Pod pod-projected-secrets-8ebeabee-cb33-44c1-ab61-b7c716a0837a no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Mar  2 01:18:39.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6478" for this suite. 03/02/23 01:18:39.823
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":31,"skipped":691,"failed":0}
------------------------------
• [SLOW TEST] [6.487 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:18:33.356
    Mar  2 01:18:33.356: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename projected 03/02/23 01:18:33.357
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:18:33.406
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:18:33.416
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:55
    STEP: Creating projection with secret that has name projected-secret-test-030b3253-e435-4cb2-9af3-4184c4344f18 03/02/23 01:18:33.457
    STEP: Creating a pod to test consume secrets 03/02/23 01:18:33.496
    Mar  2 01:18:33.610: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8ebeabee-cb33-44c1-ab61-b7c716a0837a" in namespace "projected-6478" to be "Succeeded or Failed"
    Mar  2 01:18:33.637: INFO: Pod "pod-projected-secrets-8ebeabee-cb33-44c1-ab61-b7c716a0837a": Phase="Pending", Reason="", readiness=false. Elapsed: 26.043416ms
    Mar  2 01:18:35.654: INFO: Pod "pod-projected-secrets-8ebeabee-cb33-44c1-ab61-b7c716a0837a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043799293s
    Mar  2 01:18:37.653: INFO: Pod "pod-projected-secrets-8ebeabee-cb33-44c1-ab61-b7c716a0837a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042149602s
    Mar  2 01:18:39.651: INFO: Pod "pod-projected-secrets-8ebeabee-cb33-44c1-ab61-b7c716a0837a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040978703s
    STEP: Saw pod success 03/02/23 01:18:39.652
    Mar  2 01:18:39.652: INFO: Pod "pod-projected-secrets-8ebeabee-cb33-44c1-ab61-b7c716a0837a" satisfied condition "Succeeded or Failed"
    Mar  2 01:18:39.667: INFO: Trying to get logs from node 10.132.92.186 pod pod-projected-secrets-8ebeabee-cb33-44c1-ab61-b7c716a0837a container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/02/23 01:18:39.732
    Mar  2 01:18:39.788: INFO: Waiting for pod pod-projected-secrets-8ebeabee-cb33-44c1-ab61-b7c716a0837a to disappear
    Mar  2 01:18:39.802: INFO: Pod pod-projected-secrets-8ebeabee-cb33-44c1-ab61-b7c716a0837a no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Mar  2 01:18:39.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6478" for this suite. 03/02/23 01:18:39.823
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:18:39.851
Mar  2 01:18:39.851: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename statefulset 03/02/23 01:18:39.852
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:18:39.895
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:18:39.906
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-2735 03/02/23 01:18:39.917
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
Mar  2 01:18:40.002: INFO: Found 0 stateful pods, waiting for 1
Mar  2 01:18:50.019: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 03/02/23 01:18:50.048
W0302 01:18:50.084829      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Mar  2 01:18:50.120: INFO: Found 1 stateful pods, waiting for 2
Mar  2 01:19:00.138: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 01:19:00.138: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 03/02/23 01:19:00.172
STEP: Delete all of the StatefulSets 03/02/23 01:19:00.191
STEP: Verify that StatefulSets have been deleted 03/02/23 01:19:00.226
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar  2 01:19:00.243: INFO: Deleting all statefulset in ns statefulset-2735
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar  2 01:19:00.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2735" for this suite. 03/02/23 01:19:00.32
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","completed":32,"skipped":762,"failed":0}
------------------------------
• [SLOW TEST] [20.486 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:906

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:18:39.851
    Mar  2 01:18:39.851: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename statefulset 03/02/23 01:18:39.852
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:18:39.895
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:18:39.906
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-2735 03/02/23 01:18:39.917
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:906
    Mar  2 01:18:40.002: INFO: Found 0 stateful pods, waiting for 1
    Mar  2 01:18:50.019: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 03/02/23 01:18:50.048
    W0302 01:18:50.084829      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Mar  2 01:18:50.120: INFO: Found 1 stateful pods, waiting for 2
    Mar  2 01:19:00.138: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar  2 01:19:00.138: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 03/02/23 01:19:00.172
    STEP: Delete all of the StatefulSets 03/02/23 01:19:00.191
    STEP: Verify that StatefulSets have been deleted 03/02/23 01:19:00.226
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar  2 01:19:00.243: INFO: Deleting all statefulset in ns statefulset-2735
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar  2 01:19:00.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-2735" for this suite. 03/02/23 01:19:00.32
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:19:00.338
Mar  2 01:19:00.339: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename configmap 03/02/23 01:19:00.34
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:19:00.404
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:19:00.422
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
STEP: Creating configMap with name configmap-test-volume-ba35a16c-c40f-4638-97ee-d0e3c7c07225 03/02/23 01:19:00.455
STEP: Creating a pod to test consume configMaps 03/02/23 01:19:00.527
Mar  2 01:19:00.617: INFO: Waiting up to 5m0s for pod "pod-configmaps-ff562abd-8c50-47c6-b8f3-828661d8d298" in namespace "configmap-1271" to be "Succeeded or Failed"
Mar  2 01:19:00.668: INFO: Pod "pod-configmaps-ff562abd-8c50-47c6-b8f3-828661d8d298": Phase="Pending", Reason="", readiness=false. Elapsed: 51.159335ms
Mar  2 01:19:02.685: INFO: Pod "pod-configmaps-ff562abd-8c50-47c6-b8f3-828661d8d298": Phase="Pending", Reason="", readiness=false. Elapsed: 2.067670176s
Mar  2 01:19:04.688: INFO: Pod "pod-configmaps-ff562abd-8c50-47c6-b8f3-828661d8d298": Phase="Pending", Reason="", readiness=false. Elapsed: 4.07110328s
Mar  2 01:19:06.685: INFO: Pod "pod-configmaps-ff562abd-8c50-47c6-b8f3-828661d8d298": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.067626923s
STEP: Saw pod success 03/02/23 01:19:06.685
Mar  2 01:19:06.685: INFO: Pod "pod-configmaps-ff562abd-8c50-47c6-b8f3-828661d8d298" satisfied condition "Succeeded or Failed"
Mar  2 01:19:06.698: INFO: Trying to get logs from node 10.132.92.143 pod pod-configmaps-ff562abd-8c50-47c6-b8f3-828661d8d298 container configmap-volume-test: <nil>
STEP: delete the pod 03/02/23 01:19:06.791
Mar  2 01:19:06.881: INFO: Waiting for pod pod-configmaps-ff562abd-8c50-47c6-b8f3-828661d8d298 to disappear
Mar  2 01:19:06.895: INFO: Pod pod-configmaps-ff562abd-8c50-47c6-b8f3-828661d8d298 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar  2 01:19:06.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1271" for this suite. 03/02/23 01:19:06.946
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":33,"skipped":766,"failed":0}
------------------------------
• [SLOW TEST] [6.624 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:19:00.338
    Mar  2 01:19:00.339: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename configmap 03/02/23 01:19:00.34
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:19:00.404
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:19:00.422
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:422
    STEP: Creating configMap with name configmap-test-volume-ba35a16c-c40f-4638-97ee-d0e3c7c07225 03/02/23 01:19:00.455
    STEP: Creating a pod to test consume configMaps 03/02/23 01:19:00.527
    Mar  2 01:19:00.617: INFO: Waiting up to 5m0s for pod "pod-configmaps-ff562abd-8c50-47c6-b8f3-828661d8d298" in namespace "configmap-1271" to be "Succeeded or Failed"
    Mar  2 01:19:00.668: INFO: Pod "pod-configmaps-ff562abd-8c50-47c6-b8f3-828661d8d298": Phase="Pending", Reason="", readiness=false. Elapsed: 51.159335ms
    Mar  2 01:19:02.685: INFO: Pod "pod-configmaps-ff562abd-8c50-47c6-b8f3-828661d8d298": Phase="Pending", Reason="", readiness=false. Elapsed: 2.067670176s
    Mar  2 01:19:04.688: INFO: Pod "pod-configmaps-ff562abd-8c50-47c6-b8f3-828661d8d298": Phase="Pending", Reason="", readiness=false. Elapsed: 4.07110328s
    Mar  2 01:19:06.685: INFO: Pod "pod-configmaps-ff562abd-8c50-47c6-b8f3-828661d8d298": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.067626923s
    STEP: Saw pod success 03/02/23 01:19:06.685
    Mar  2 01:19:06.685: INFO: Pod "pod-configmaps-ff562abd-8c50-47c6-b8f3-828661d8d298" satisfied condition "Succeeded or Failed"
    Mar  2 01:19:06.698: INFO: Trying to get logs from node 10.132.92.143 pod pod-configmaps-ff562abd-8c50-47c6-b8f3-828661d8d298 container configmap-volume-test: <nil>
    STEP: delete the pod 03/02/23 01:19:06.791
    Mar  2 01:19:06.881: INFO: Waiting for pod pod-configmaps-ff562abd-8c50-47c6-b8f3-828661d8d298 to disappear
    Mar  2 01:19:06.895: INFO: Pod pod-configmaps-ff562abd-8c50-47c6-b8f3-828661d8d298 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  2 01:19:06.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1271" for this suite. 03/02/23 01:19:06.946
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:19:06.964
Mar  2 01:19:06.964: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename container-lifecycle-hook 03/02/23 01:19:06.966
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:19:07.027
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:19:07.035
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 03/02/23 01:19:07.131
Mar  2 01:19:07.260: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1779" to be "running and ready"
Mar  2 01:19:07.325: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 65.059131ms
Mar  2 01:19:07.325: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:19:09.340: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.080443s
Mar  2 01:19:09.340: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:19:11.344: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.084042294s
Mar  2 01:19:11.344: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Mar  2 01:19:11.344: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
STEP: create the pod with lifecycle hook 03/02/23 01:19:11.36
Mar  2 01:19:11.398: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-1779" to be "running and ready"
Mar  2 01:19:11.446: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 48.411017ms
Mar  2 01:19:11.446: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:19:13.461: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.063411091s
Mar  2 01:19:13.461: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Mar  2 01:19:13.461: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 03/02/23 01:19:13.475
Mar  2 01:19:13.502: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  2 01:19:13.548: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  2 01:19:15.550: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  2 01:19:15.565: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  2 01:19:17.550: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  2 01:19:17.565: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 03/02/23 01:19:17.565
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Mar  2 01:19:17.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1779" for this suite. 03/02/23 01:19:17.619
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","completed":34,"skipped":776,"failed":0}
------------------------------
• [SLOW TEST] [10.675 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:114

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:19:06.964
    Mar  2 01:19:06.964: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename container-lifecycle-hook 03/02/23 01:19:06.966
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:19:07.027
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:19:07.035
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 03/02/23 01:19:07.131
    Mar  2 01:19:07.260: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1779" to be "running and ready"
    Mar  2 01:19:07.325: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 65.059131ms
    Mar  2 01:19:07.325: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 01:19:09.340: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.080443s
    Mar  2 01:19:09.340: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 01:19:11.344: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.084042294s
    Mar  2 01:19:11.344: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Mar  2 01:19:11.344: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:114
    STEP: create the pod with lifecycle hook 03/02/23 01:19:11.36
    Mar  2 01:19:11.398: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-1779" to be "running and ready"
    Mar  2 01:19:11.446: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 48.411017ms
    Mar  2 01:19:11.446: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 01:19:13.461: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.063411091s
    Mar  2 01:19:13.461: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Mar  2 01:19:13.461: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 03/02/23 01:19:13.475
    Mar  2 01:19:13.502: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Mar  2 01:19:13.548: INFO: Pod pod-with-prestop-exec-hook still exists
    Mar  2 01:19:15.550: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Mar  2 01:19:15.565: INFO: Pod pod-with-prestop-exec-hook still exists
    Mar  2 01:19:17.550: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Mar  2 01:19:17.565: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 03/02/23 01:19:17.565
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Mar  2 01:19:17.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-1779" for this suite. 03/02/23 01:19:17.619
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:19:17.641
Mar  2 01:19:17.642: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename gc 03/02/23 01:19:17.643
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:19:17.69
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:19:17.704
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
W0302 01:19:17.790341      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Mar  2 01:19:17.924: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"df92e99b-d9bb-4485-93b8-9d685b136af1", Controller:(*bool)(0xc0038bca52), BlockOwnerDeletion:(*bool)(0xc0038bca53)}}
Mar  2 01:19:17.964: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"5dfecb1c-4fef-4257-8ab9-93cac9addb51", Controller:(*bool)(0xc0038bcd72), BlockOwnerDeletion:(*bool)(0xc0038bcd73)}}
Mar  2 01:19:18.013: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"39ea83dc-db36-4b6f-9833-c6a3397b192c", Controller:(*bool)(0xc002629386), BlockOwnerDeletion:(*bool)(0xc002629387)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Mar  2 01:19:23.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5870" for this suite. 03/02/23 01:19:23.089
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","completed":35,"skipped":782,"failed":0}
------------------------------
• [SLOW TEST] [5.467 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:19:17.641
    Mar  2 01:19:17.642: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename gc 03/02/23 01:19:17.643
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:19:17.69
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:19:17.704
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    W0302 01:19:17.790341      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Mar  2 01:19:17.924: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"df92e99b-d9bb-4485-93b8-9d685b136af1", Controller:(*bool)(0xc0038bca52), BlockOwnerDeletion:(*bool)(0xc0038bca53)}}
    Mar  2 01:19:17.964: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"5dfecb1c-4fef-4257-8ab9-93cac9addb51", Controller:(*bool)(0xc0038bcd72), BlockOwnerDeletion:(*bool)(0xc0038bcd73)}}
    Mar  2 01:19:18.013: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"39ea83dc-db36-4b6f-9833-c6a3397b192c", Controller:(*bool)(0xc002629386), BlockOwnerDeletion:(*bool)(0xc002629387)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Mar  2 01:19:23.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-5870" for this suite. 03/02/23 01:19:23.089
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:19:23.11
Mar  2 01:19:23.110: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename subpath 03/02/23 01:19:23.111
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:19:23.187
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:19:23.198
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/02/23 01:19:23.209
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-8jl2 03/02/23 01:19:23.251
STEP: Creating a pod to test atomic-volume-subpath 03/02/23 01:19:23.252
Mar  2 01:19:23.308: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-8jl2" in namespace "subpath-486" to be "Succeeded or Failed"
Mar  2 01:19:23.323: INFO: Pod "pod-subpath-test-configmap-8jl2": Phase="Pending", Reason="", readiness=false. Elapsed: 15.680535ms
Mar  2 01:19:25.342: INFO: Pod "pod-subpath-test-configmap-8jl2": Phase="Running", Reason="", readiness=true. Elapsed: 2.034463596s
Mar  2 01:19:27.343: INFO: Pod "pod-subpath-test-configmap-8jl2": Phase="Running", Reason="", readiness=true. Elapsed: 4.035223855s
Mar  2 01:19:29.353: INFO: Pod "pod-subpath-test-configmap-8jl2": Phase="Running", Reason="", readiness=true. Elapsed: 6.045293087s
Mar  2 01:19:31.342: INFO: Pod "pod-subpath-test-configmap-8jl2": Phase="Running", Reason="", readiness=true. Elapsed: 8.033831342s
Mar  2 01:19:33.340: INFO: Pod "pod-subpath-test-configmap-8jl2": Phase="Running", Reason="", readiness=true. Elapsed: 10.03189166s
Mar  2 01:19:35.338: INFO: Pod "pod-subpath-test-configmap-8jl2": Phase="Running", Reason="", readiness=true. Elapsed: 12.03048628s
Mar  2 01:19:37.346: INFO: Pod "pod-subpath-test-configmap-8jl2": Phase="Running", Reason="", readiness=true. Elapsed: 14.038683393s
Mar  2 01:19:39.344: INFO: Pod "pod-subpath-test-configmap-8jl2": Phase="Running", Reason="", readiness=true. Elapsed: 16.036069574s
Mar  2 01:19:41.341: INFO: Pod "pod-subpath-test-configmap-8jl2": Phase="Running", Reason="", readiness=true. Elapsed: 18.033269353s
Mar  2 01:19:43.339: INFO: Pod "pod-subpath-test-configmap-8jl2": Phase="Running", Reason="", readiness=true. Elapsed: 20.030848015s
Mar  2 01:19:45.338: INFO: Pod "pod-subpath-test-configmap-8jl2": Phase="Running", Reason="", readiness=false. Elapsed: 22.030129728s
Mar  2 01:19:47.339: INFO: Pod "pod-subpath-test-configmap-8jl2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.031650412s
STEP: Saw pod success 03/02/23 01:19:47.339
Mar  2 01:19:47.340: INFO: Pod "pod-subpath-test-configmap-8jl2" satisfied condition "Succeeded or Failed"
Mar  2 01:19:47.355: INFO: Trying to get logs from node 10.132.92.143 pod pod-subpath-test-configmap-8jl2 container test-container-subpath-configmap-8jl2: <nil>
STEP: delete the pod 03/02/23 01:19:47.407
Mar  2 01:19:47.446: INFO: Waiting for pod pod-subpath-test-configmap-8jl2 to disappear
Mar  2 01:19:47.460: INFO: Pod pod-subpath-test-configmap-8jl2 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-8jl2 03/02/23 01:19:47.46
Mar  2 01:19:47.460: INFO: Deleting pod "pod-subpath-test-configmap-8jl2" in namespace "subpath-486"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Mar  2 01:19:47.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-486" for this suite. 03/02/23 01:19:47.493
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]","completed":36,"skipped":791,"failed":0}
------------------------------
• [SLOW TEST] [24.402 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:19:23.11
    Mar  2 01:19:23.110: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename subpath 03/02/23 01:19:23.111
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:19:23.187
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:19:23.198
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/02/23 01:19:23.209
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-8jl2 03/02/23 01:19:23.251
    STEP: Creating a pod to test atomic-volume-subpath 03/02/23 01:19:23.252
    Mar  2 01:19:23.308: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-8jl2" in namespace "subpath-486" to be "Succeeded or Failed"
    Mar  2 01:19:23.323: INFO: Pod "pod-subpath-test-configmap-8jl2": Phase="Pending", Reason="", readiness=false. Elapsed: 15.680535ms
    Mar  2 01:19:25.342: INFO: Pod "pod-subpath-test-configmap-8jl2": Phase="Running", Reason="", readiness=true. Elapsed: 2.034463596s
    Mar  2 01:19:27.343: INFO: Pod "pod-subpath-test-configmap-8jl2": Phase="Running", Reason="", readiness=true. Elapsed: 4.035223855s
    Mar  2 01:19:29.353: INFO: Pod "pod-subpath-test-configmap-8jl2": Phase="Running", Reason="", readiness=true. Elapsed: 6.045293087s
    Mar  2 01:19:31.342: INFO: Pod "pod-subpath-test-configmap-8jl2": Phase="Running", Reason="", readiness=true. Elapsed: 8.033831342s
    Mar  2 01:19:33.340: INFO: Pod "pod-subpath-test-configmap-8jl2": Phase="Running", Reason="", readiness=true. Elapsed: 10.03189166s
    Mar  2 01:19:35.338: INFO: Pod "pod-subpath-test-configmap-8jl2": Phase="Running", Reason="", readiness=true. Elapsed: 12.03048628s
    Mar  2 01:19:37.346: INFO: Pod "pod-subpath-test-configmap-8jl2": Phase="Running", Reason="", readiness=true. Elapsed: 14.038683393s
    Mar  2 01:19:39.344: INFO: Pod "pod-subpath-test-configmap-8jl2": Phase="Running", Reason="", readiness=true. Elapsed: 16.036069574s
    Mar  2 01:19:41.341: INFO: Pod "pod-subpath-test-configmap-8jl2": Phase="Running", Reason="", readiness=true. Elapsed: 18.033269353s
    Mar  2 01:19:43.339: INFO: Pod "pod-subpath-test-configmap-8jl2": Phase="Running", Reason="", readiness=true. Elapsed: 20.030848015s
    Mar  2 01:19:45.338: INFO: Pod "pod-subpath-test-configmap-8jl2": Phase="Running", Reason="", readiness=false. Elapsed: 22.030129728s
    Mar  2 01:19:47.339: INFO: Pod "pod-subpath-test-configmap-8jl2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.031650412s
    STEP: Saw pod success 03/02/23 01:19:47.339
    Mar  2 01:19:47.340: INFO: Pod "pod-subpath-test-configmap-8jl2" satisfied condition "Succeeded or Failed"
    Mar  2 01:19:47.355: INFO: Trying to get logs from node 10.132.92.143 pod pod-subpath-test-configmap-8jl2 container test-container-subpath-configmap-8jl2: <nil>
    STEP: delete the pod 03/02/23 01:19:47.407
    Mar  2 01:19:47.446: INFO: Waiting for pod pod-subpath-test-configmap-8jl2 to disappear
    Mar  2 01:19:47.460: INFO: Pod pod-subpath-test-configmap-8jl2 no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-8jl2 03/02/23 01:19:47.46
    Mar  2 01:19:47.460: INFO: Deleting pod "pod-subpath-test-configmap-8jl2" in namespace "subpath-486"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Mar  2 01:19:47.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-486" for this suite. 03/02/23 01:19:47.493
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:19:47.517
Mar  2 01:19:47.517: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename replication-controller 03/02/23 01:19:47.519
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:19:47.573
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:19:47.584
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
STEP: Given a Pod with a 'name' label pod-adoption is created 03/02/23 01:19:47.593
Mar  2 01:19:47.659: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-9213" to be "running and ready"
Mar  2 01:19:47.687: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 28.088671ms
Mar  2 01:19:47.687: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:19:49.703: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044169096s
Mar  2 01:19:49.703: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:19:51.739: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 4.080407232s
Mar  2 01:19:51.739: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Mar  2 01:19:51.739: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 03/02/23 01:19:51.754
STEP: Then the orphan pod is adopted 03/02/23 01:19:51.773
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Mar  2 01:19:52.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9213" for this suite. 03/02/23 01:19:52.867
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","completed":37,"skipped":822,"failed":0}
------------------------------
• [SLOW TEST] [5.403 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:19:47.517
    Mar  2 01:19:47.517: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename replication-controller 03/02/23 01:19:47.519
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:19:47.573
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:19:47.584
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:91
    STEP: Given a Pod with a 'name' label pod-adoption is created 03/02/23 01:19:47.593
    Mar  2 01:19:47.659: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-9213" to be "running and ready"
    Mar  2 01:19:47.687: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 28.088671ms
    Mar  2 01:19:47.687: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 01:19:49.703: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044169096s
    Mar  2 01:19:49.703: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 01:19:51.739: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 4.080407232s
    Mar  2 01:19:51.739: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Mar  2 01:19:51.739: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 03/02/23 01:19:51.754
    STEP: Then the orphan pod is adopted 03/02/23 01:19:51.773
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Mar  2 01:19:52.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-9213" for this suite. 03/02/23 01:19:52.867
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:19:52.923
Mar  2 01:19:52.923: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename aggregator 03/02/23 01:19:52.924
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:19:52.976
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:19:52.99
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Mar  2 01:19:53.000: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 03/02/23 01:19:53.001
Mar  2 01:19:53.739: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Mar  2 01:19:55.958: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 01:19:57.967: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 01:19:59.996: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 01:20:01.968: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 01:20:03.969: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 01:20:05.970: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 01:20:07.968: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 01:20:09.968: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 01:20:11.969: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 01:20:13.969: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 01:20:15.970: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 01:20:17.969: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 01:20:20.153: INFO: Waited 162.962052ms for the sample-apiserver to be ready to handle requests.
I0302 01:20:21.362053      20 request.go:682] Waited for 1.047048645s due to client-side throttling, not priority and fairness, request: GET:https://172.21.0.1:443/apis/performance.openshift.io/v2
STEP: Read Status for v1alpha1.wardle.example.com 03/02/23 01:20:21.672
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 03/02/23 01:20:21.681
STEP: List APIServices 03/02/23 01:20:21.719
Mar  2 01:20:21.786: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:187
Mar  2 01:20:22.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-8187" for this suite. 03/02/23 01:20:22.631
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","completed":38,"skipped":829,"failed":0}
------------------------------
• [SLOW TEST] [29.748 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:19:52.923
    Mar  2 01:19:52.923: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename aggregator 03/02/23 01:19:52.924
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:19:52.976
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:19:52.99
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Mar  2 01:19:53.000: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 03/02/23 01:19:53.001
    Mar  2 01:19:53.739: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Mar  2 01:19:55.958: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  2 01:19:57.967: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  2 01:19:59.996: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  2 01:20:01.968: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  2 01:20:03.969: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  2 01:20:05.970: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  2 01:20:07.968: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  2 01:20:09.968: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  2 01:20:11.969: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  2 01:20:13.969: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  2 01:20:15.970: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  2 01:20:17.969: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 19, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  2 01:20:20.153: INFO: Waited 162.962052ms for the sample-apiserver to be ready to handle requests.
    I0302 01:20:21.362053      20 request.go:682] Waited for 1.047048645s due to client-side throttling, not priority and fairness, request: GET:https://172.21.0.1:443/apis/performance.openshift.io/v2
    STEP: Read Status for v1alpha1.wardle.example.com 03/02/23 01:20:21.672
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 03/02/23 01:20:21.681
    STEP: List APIServices 03/02/23 01:20:21.719
    Mar  2 01:20:21.786: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:187
    Mar  2 01:20:22.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "aggregator-8187" for this suite. 03/02/23 01:20:22.631
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:20:22.672
Mar  2 01:20:22.672: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename crd-publish-openapi 03/02/23 01:20:22.673
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:20:22.724
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:20:22.731
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
Mar  2 01:20:22.792: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/02/23 01:20:39.358
Mar  2 01:20:39.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-8917 --namespace=crd-publish-openapi-8917 create -f -'
Mar  2 01:20:41.600: INFO: stderr: ""
Mar  2 01:20:41.600: INFO: stdout: "e2e-test-crd-publish-openapi-6306-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar  2 01:20:41.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-8917 --namespace=crd-publish-openapi-8917 delete e2e-test-crd-publish-openapi-6306-crds test-cr'
Mar  2 01:20:41.770: INFO: stderr: ""
Mar  2 01:20:41.770: INFO: stdout: "e2e-test-crd-publish-openapi-6306-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Mar  2 01:20:41.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-8917 --namespace=crd-publish-openapi-8917 apply -f -'
Mar  2 01:20:43.780: INFO: stderr: ""
Mar  2 01:20:43.780: INFO: stdout: "e2e-test-crd-publish-openapi-6306-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar  2 01:20:43.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-8917 --namespace=crd-publish-openapi-8917 delete e2e-test-crd-publish-openapi-6306-crds test-cr'
Mar  2 01:20:44.000: INFO: stderr: ""
Mar  2 01:20:44.000: INFO: stdout: "e2e-test-crd-publish-openapi-6306-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 03/02/23 01:20:44
Mar  2 01:20:44.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-8917 explain e2e-test-crd-publish-openapi-6306-crds'
Mar  2 01:20:44.724: INFO: stderr: ""
Mar  2 01:20:44.724: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6306-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 01:20:59.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8917" for this suite. 03/02/23 01:20:59.634
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","completed":39,"skipped":831,"failed":0}
------------------------------
• [SLOW TEST] [36.994 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:20:22.672
    Mar  2 01:20:22.672: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename crd-publish-openapi 03/02/23 01:20:22.673
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:20:22.724
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:20:22.731
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:152
    Mar  2 01:20:22.792: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/02/23 01:20:39.358
    Mar  2 01:20:39.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-8917 --namespace=crd-publish-openapi-8917 create -f -'
    Mar  2 01:20:41.600: INFO: stderr: ""
    Mar  2 01:20:41.600: INFO: stdout: "e2e-test-crd-publish-openapi-6306-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Mar  2 01:20:41.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-8917 --namespace=crd-publish-openapi-8917 delete e2e-test-crd-publish-openapi-6306-crds test-cr'
    Mar  2 01:20:41.770: INFO: stderr: ""
    Mar  2 01:20:41.770: INFO: stdout: "e2e-test-crd-publish-openapi-6306-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Mar  2 01:20:41.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-8917 --namespace=crd-publish-openapi-8917 apply -f -'
    Mar  2 01:20:43.780: INFO: stderr: ""
    Mar  2 01:20:43.780: INFO: stdout: "e2e-test-crd-publish-openapi-6306-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Mar  2 01:20:43.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-8917 --namespace=crd-publish-openapi-8917 delete e2e-test-crd-publish-openapi-6306-crds test-cr'
    Mar  2 01:20:44.000: INFO: stderr: ""
    Mar  2 01:20:44.000: INFO: stdout: "e2e-test-crd-publish-openapi-6306-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 03/02/23 01:20:44
    Mar  2 01:20:44.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-8917 explain e2e-test-crd-publish-openapi-6306-crds'
    Mar  2 01:20:44.724: INFO: stderr: ""
    Mar  2 01:20:44.724: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6306-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 01:20:59.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-8917" for this suite. 03/02/23 01:20:59.634
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:20:59.669
Mar  2 01:20:59.669: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename webhook 03/02/23 01:20:59.67
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:20:59.735
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:20:59.751
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/02/23 01:20:59.836
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 01:21:00.429
STEP: Deploying the webhook pod 03/02/23 01:21:00.46
STEP: Wait for the deployment to be ready 03/02/23 01:21:00.503
Mar  2 01:21:00.538: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar  2 01:21:02.624: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 1, 21, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 21, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 21, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 21, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/02/23 01:21:04.639
STEP: Verifying the service has paired with the endpoint 03/02/23 01:21:04.712
Mar  2 01:21:05.714: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
STEP: Registering the webhook via the AdmissionRegistration API 03/02/23 01:21:05.775
STEP: create a pod 03/02/23 01:21:05.831
Mar  2 01:21:05.888: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-4805" to be "running"
Mar  2 01:21:05.902: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.408114ms
Mar  2 01:21:07.918: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.029882891s
Mar  2 01:21:07.918: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 03/02/23 01:21:07.918
Mar  2 01:21:07.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=webhook-4805 attach --namespace=webhook-4805 to-be-attached-pod -i -c=container1'
Mar  2 01:21:08.163: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 01:21:08.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4805" for this suite. 03/02/23 01:21:08.214
STEP: Destroying namespace "webhook-4805-markers" for this suite. 03/02/23 01:21:08.244
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","completed":40,"skipped":844,"failed":0}
------------------------------
• [SLOW TEST] [8.830 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:20:59.669
    Mar  2 01:20:59.669: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename webhook 03/02/23 01:20:59.67
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:20:59.735
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:20:59.751
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/02/23 01:20:59.836
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 01:21:00.429
    STEP: Deploying the webhook pod 03/02/23 01:21:00.46
    STEP: Wait for the deployment to be ready 03/02/23 01:21:00.503
    Mar  2 01:21:00.538: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Mar  2 01:21:02.624: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 1, 21, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 21, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 21, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 21, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/02/23 01:21:04.639
    STEP: Verifying the service has paired with the endpoint 03/02/23 01:21:04.712
    Mar  2 01:21:05.714: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:208
    STEP: Registering the webhook via the AdmissionRegistration API 03/02/23 01:21:05.775
    STEP: create a pod 03/02/23 01:21:05.831
    Mar  2 01:21:05.888: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-4805" to be "running"
    Mar  2 01:21:05.902: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.408114ms
    Mar  2 01:21:07.918: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.029882891s
    Mar  2 01:21:07.918: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 03/02/23 01:21:07.918
    Mar  2 01:21:07.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=webhook-4805 attach --namespace=webhook-4805 to-be-attached-pod -i -c=container1'
    Mar  2 01:21:08.163: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 01:21:08.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4805" for this suite. 03/02/23 01:21:08.214
    STEP: Destroying namespace "webhook-4805-markers" for this suite. 03/02/23 01:21:08.244
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:21:08.501
Mar  2 01:21:08.501: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename disruption 03/02/23 01:21:08.502
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:21:08.639
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:21:08.652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:21:08.711
Mar  2 01:21:08.711: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename disruption-2 03/02/23 01:21:08.713
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:21:08.84
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:21:08.853
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
STEP: Waiting for the pdb to be processed 03/02/23 01:21:08.909
STEP: Waiting for the pdb to be processed 03/02/23 01:21:10.959
STEP: Waiting for the pdb to be processed 03/02/23 01:21:13.019
STEP: listing a collection of PDBs across all namespaces 03/02/23 01:21:15.068
STEP: listing a collection of PDBs in namespace disruption-8840 03/02/23 01:21:15.088
STEP: deleting a collection of PDBs 03/02/23 01:21:15.109
STEP: Waiting for the PDB collection to be deleted 03/02/23 01:21:15.152
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:187
Mar  2 01:21:15.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-342" for this suite. 03/02/23 01:21:15.197
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Mar  2 01:21:15.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-8840" for this suite. 03/02/23 01:21:15.301
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","completed":41,"skipped":857,"failed":0}
------------------------------
• [SLOW TEST] [6.835 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:77
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:21:08.501
    Mar  2 01:21:08.501: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename disruption 03/02/23 01:21:08.502
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:21:08.639
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:21:08.652
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:21:08.711
    Mar  2 01:21:08.711: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename disruption-2 03/02/23 01:21:08.713
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:21:08.84
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:21:08.853
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:86
    STEP: Waiting for the pdb to be processed 03/02/23 01:21:08.909
    STEP: Waiting for the pdb to be processed 03/02/23 01:21:10.959
    STEP: Waiting for the pdb to be processed 03/02/23 01:21:13.019
    STEP: listing a collection of PDBs across all namespaces 03/02/23 01:21:15.068
    STEP: listing a collection of PDBs in namespace disruption-8840 03/02/23 01:21:15.088
    STEP: deleting a collection of PDBs 03/02/23 01:21:15.109
    STEP: Waiting for the PDB collection to be deleted 03/02/23 01:21:15.152
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:187
    Mar  2 01:21:15.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2-342" for this suite. 03/02/23 01:21:15.197
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Mar  2 01:21:15.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-8840" for this suite. 03/02/23 01:21:15.301
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:21:15.338
Mar  2 01:21:15.339: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename certificates 03/02/23 01:21:15.34
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:21:15.395
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:21:15.41
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 03/02/23 01:21:16.109
STEP: getting /apis/certificates.k8s.io 03/02/23 01:21:16.12
STEP: getting /apis/certificates.k8s.io/v1 03/02/23 01:21:16.132
STEP: creating 03/02/23 01:21:16.138
STEP: getting 03/02/23 01:21:16.205
STEP: listing 03/02/23 01:21:16.225
STEP: watching 03/02/23 01:21:16.247
Mar  2 01:21:16.248: INFO: starting watch
STEP: patching 03/02/23 01:21:16.253
STEP: updating 03/02/23 01:21:16.274
Mar  2 01:21:16.315: INFO: waiting for watch events with expected annotations
Mar  2 01:21:16.316: INFO: saw patched and updated annotations
STEP: getting /approval 03/02/23 01:21:16.316
STEP: patching /approval 03/02/23 01:21:16.349
STEP: updating /approval 03/02/23 01:21:16.392
STEP: getting /status 03/02/23 01:21:16.428
STEP: patching /status 03/02/23 01:21:16.467
STEP: updating /status 03/02/23 01:21:16.493
STEP: deleting 03/02/23 01:21:16.52
STEP: deleting a collection 03/02/23 01:21:16.579
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 01:21:16.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-6272" for this suite. 03/02/23 01:21:16.756
{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","completed":42,"skipped":865,"failed":0}
------------------------------
• [1.470 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:21:15.338
    Mar  2 01:21:15.339: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename certificates 03/02/23 01:21:15.34
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:21:15.395
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:21:15.41
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 03/02/23 01:21:16.109
    STEP: getting /apis/certificates.k8s.io 03/02/23 01:21:16.12
    STEP: getting /apis/certificates.k8s.io/v1 03/02/23 01:21:16.132
    STEP: creating 03/02/23 01:21:16.138
    STEP: getting 03/02/23 01:21:16.205
    STEP: listing 03/02/23 01:21:16.225
    STEP: watching 03/02/23 01:21:16.247
    Mar  2 01:21:16.248: INFO: starting watch
    STEP: patching 03/02/23 01:21:16.253
    STEP: updating 03/02/23 01:21:16.274
    Mar  2 01:21:16.315: INFO: waiting for watch events with expected annotations
    Mar  2 01:21:16.316: INFO: saw patched and updated annotations
    STEP: getting /approval 03/02/23 01:21:16.316
    STEP: patching /approval 03/02/23 01:21:16.349
    STEP: updating /approval 03/02/23 01:21:16.392
    STEP: getting /status 03/02/23 01:21:16.428
    STEP: patching /status 03/02/23 01:21:16.467
    STEP: updating /status 03/02/23 01:21:16.493
    STEP: deleting 03/02/23 01:21:16.52
    STEP: deleting a collection 03/02/23 01:21:16.579
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 01:21:16.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "certificates-6272" for this suite. 03/02/23 01:21:16.756
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:21:16.81
Mar  2 01:21:16.810: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename pod-network-test 03/02/23 01:21:16.811
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:21:16.864
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:21:16.876
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-9590 03/02/23 01:21:16.889
STEP: creating a selector 03/02/23 01:21:16.889
STEP: Creating the service pods in kubernetes 03/02/23 01:21:16.889
Mar  2 01:21:16.889: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar  2 01:21:17.150: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9590" to be "running and ready"
Mar  2 01:21:17.169: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 19.434845ms
Mar  2 01:21:17.169: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:21:19.185: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.035721575s
Mar  2 01:21:19.185: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 01:21:21.189: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.039108404s
Mar  2 01:21:21.189: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 01:21:23.186: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.036448923s
Mar  2 01:21:23.186: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 01:21:25.202: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.052738921s
Mar  2 01:21:25.203: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 01:21:27.272: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.122114537s
Mar  2 01:21:27.317: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 01:21:29.186: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.036459816s
Mar  2 01:21:29.186: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 01:21:31.191: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.041479082s
Mar  2 01:21:31.191: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 01:21:33.185: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.035152102s
Mar  2 01:21:33.185: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 01:21:35.190: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.040638219s
Mar  2 01:21:35.190: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 01:21:37.184: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.034208491s
Mar  2 01:21:37.184: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 01:21:39.218: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.068116187s
Mar  2 01:21:39.218: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Mar  2 01:21:39.218: INFO: Pod "netserver-0" satisfied condition "running and ready"
Mar  2 01:21:39.232: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9590" to be "running and ready"
Mar  2 01:21:39.247: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 14.202462ms
Mar  2 01:21:39.247: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Mar  2 01:21:39.247: INFO: Pod "netserver-1" satisfied condition "running and ready"
Mar  2 01:21:39.260: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-9590" to be "running and ready"
Mar  2 01:21:39.282: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 22.214646ms
Mar  2 01:21:39.282: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Mar  2 01:21:39.282: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 03/02/23 01:21:39.31
Mar  2 01:21:39.357: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9590" to be "running"
Mar  2 01:21:39.372: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 15.089187ms
Mar  2 01:21:41.387: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03030119s
Mar  2 01:21:43.387: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.029934745s
Mar  2 01:21:43.387: INFO: Pod "test-container-pod" satisfied condition "running"
Mar  2 01:21:43.401: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar  2 01:21:43.401: INFO: Breadth first check of 172.30.156.123 on host 10.132.92.143...
Mar  2 01:21:43.415: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.156.124:9080/dial?request=hostname&protocol=udp&host=172.30.156.123&port=8081&tries=1'] Namespace:pod-network-test-9590 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 01:21:43.416: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
Mar  2 01:21:43.416: INFO: ExecWithOptions: Clientset creation
Mar  2 01:21:43.416: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-9590/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.156.124%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.156.123%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar  2 01:21:43.611: INFO: Waiting for responses: map[]
Mar  2 01:21:43.611: INFO: reached 172.30.156.123 after 0/1 tries
Mar  2 01:21:43.611: INFO: Breadth first check of 172.30.62.239 on host 10.132.92.186...
Mar  2 01:21:43.633: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.156.124:9080/dial?request=hostname&protocol=udp&host=172.30.62.239&port=8081&tries=1'] Namespace:pod-network-test-9590 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 01:21:43.633: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
Mar  2 01:21:43.634: INFO: ExecWithOptions: Clientset creation
Mar  2 01:21:43.634: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-9590/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.156.124%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.62.239%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar  2 01:21:43.883: INFO: Waiting for responses: map[]
Mar  2 01:21:43.883: INFO: reached 172.30.62.239 after 0/1 tries
Mar  2 01:21:43.883: INFO: Breadth first check of 172.30.201.230 on host 10.132.92.188...
Mar  2 01:21:43.898: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.156.124:9080/dial?request=hostname&protocol=udp&host=172.30.201.230&port=8081&tries=1'] Namespace:pod-network-test-9590 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 01:21:43.898: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
Mar  2 01:21:43.900: INFO: ExecWithOptions: Clientset creation
Mar  2 01:21:43.900: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-9590/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.156.124%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.201.230%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar  2 01:21:44.144: INFO: Waiting for responses: map[]
Mar  2 01:21:44.144: INFO: reached 172.30.201.230 after 0/1 tries
Mar  2 01:21:44.144: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Mar  2 01:21:44.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9590" for this suite. 03/02/23 01:21:44.175
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","completed":43,"skipped":888,"failed":0}
------------------------------
• [SLOW TEST] [27.442 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:21:16.81
    Mar  2 01:21:16.810: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename pod-network-test 03/02/23 01:21:16.811
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:21:16.864
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:21:16.876
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-9590 03/02/23 01:21:16.889
    STEP: creating a selector 03/02/23 01:21:16.889
    STEP: Creating the service pods in kubernetes 03/02/23 01:21:16.889
    Mar  2 01:21:16.889: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Mar  2 01:21:17.150: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9590" to be "running and ready"
    Mar  2 01:21:17.169: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 19.434845ms
    Mar  2 01:21:17.169: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 01:21:19.185: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.035721575s
    Mar  2 01:21:19.185: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 01:21:21.189: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.039108404s
    Mar  2 01:21:21.189: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 01:21:23.186: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.036448923s
    Mar  2 01:21:23.186: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 01:21:25.202: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.052738921s
    Mar  2 01:21:25.203: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 01:21:27.272: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.122114537s
    Mar  2 01:21:27.317: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 01:21:29.186: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.036459816s
    Mar  2 01:21:29.186: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 01:21:31.191: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.041479082s
    Mar  2 01:21:31.191: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 01:21:33.185: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.035152102s
    Mar  2 01:21:33.185: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 01:21:35.190: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.040638219s
    Mar  2 01:21:35.190: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 01:21:37.184: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.034208491s
    Mar  2 01:21:37.184: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 01:21:39.218: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.068116187s
    Mar  2 01:21:39.218: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Mar  2 01:21:39.218: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Mar  2 01:21:39.232: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9590" to be "running and ready"
    Mar  2 01:21:39.247: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 14.202462ms
    Mar  2 01:21:39.247: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Mar  2 01:21:39.247: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Mar  2 01:21:39.260: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-9590" to be "running and ready"
    Mar  2 01:21:39.282: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 22.214646ms
    Mar  2 01:21:39.282: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Mar  2 01:21:39.282: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 03/02/23 01:21:39.31
    Mar  2 01:21:39.357: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9590" to be "running"
    Mar  2 01:21:39.372: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 15.089187ms
    Mar  2 01:21:41.387: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03030119s
    Mar  2 01:21:43.387: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.029934745s
    Mar  2 01:21:43.387: INFO: Pod "test-container-pod" satisfied condition "running"
    Mar  2 01:21:43.401: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Mar  2 01:21:43.401: INFO: Breadth first check of 172.30.156.123 on host 10.132.92.143...
    Mar  2 01:21:43.415: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.156.124:9080/dial?request=hostname&protocol=udp&host=172.30.156.123&port=8081&tries=1'] Namespace:pod-network-test-9590 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 01:21:43.416: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    Mar  2 01:21:43.416: INFO: ExecWithOptions: Clientset creation
    Mar  2 01:21:43.416: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-9590/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.156.124%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.156.123%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar  2 01:21:43.611: INFO: Waiting for responses: map[]
    Mar  2 01:21:43.611: INFO: reached 172.30.156.123 after 0/1 tries
    Mar  2 01:21:43.611: INFO: Breadth first check of 172.30.62.239 on host 10.132.92.186...
    Mar  2 01:21:43.633: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.156.124:9080/dial?request=hostname&protocol=udp&host=172.30.62.239&port=8081&tries=1'] Namespace:pod-network-test-9590 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 01:21:43.633: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    Mar  2 01:21:43.634: INFO: ExecWithOptions: Clientset creation
    Mar  2 01:21:43.634: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-9590/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.156.124%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.62.239%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar  2 01:21:43.883: INFO: Waiting for responses: map[]
    Mar  2 01:21:43.883: INFO: reached 172.30.62.239 after 0/1 tries
    Mar  2 01:21:43.883: INFO: Breadth first check of 172.30.201.230 on host 10.132.92.188...
    Mar  2 01:21:43.898: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.156.124:9080/dial?request=hostname&protocol=udp&host=172.30.201.230&port=8081&tries=1'] Namespace:pod-network-test-9590 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 01:21:43.898: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    Mar  2 01:21:43.900: INFO: ExecWithOptions: Clientset creation
    Mar  2 01:21:43.900: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-9590/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.156.124%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.201.230%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar  2 01:21:44.144: INFO: Waiting for responses: map[]
    Mar  2 01:21:44.144: INFO: reached 172.30.201.230 after 0/1 tries
    Mar  2 01:21:44.144: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Mar  2 01:21:44.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-9590" for this suite. 03/02/23 01:21:44.175
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:21:44.254
Mar  2 01:21:44.254: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename services 03/02/23 01:21:44.255
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:21:44.324
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:21:44.338
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
STEP: creating service nodeport-test with type=NodePort in namespace services-4749 03/02/23 01:21:44.356
STEP: creating replication controller nodeport-test in namespace services-4749 03/02/23 01:21:44.432
I0302 01:21:44.474494      20 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-4749, replica count: 2
I0302 01:21:47.525786      20 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 01:21:47.525: INFO: Creating new exec pod
Mar  2 01:21:47.590: INFO: Waiting up to 5m0s for pod "execpodtqz4g" in namespace "services-4749" to be "running"
Mar  2 01:21:47.604: INFO: Pod "execpodtqz4g": Phase="Pending", Reason="", readiness=false. Elapsed: 13.996091ms
Mar  2 01:21:49.619: INFO: Pod "execpodtqz4g": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029314252s
Mar  2 01:21:51.634: INFO: Pod "execpodtqz4g": Phase="Running", Reason="", readiness=true. Elapsed: 4.044258944s
Mar  2 01:21:51.634: INFO: Pod "execpodtqz4g" satisfied condition "running"
Mar  2 01:21:52.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-4749 exec execpodtqz4g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Mar  2 01:21:53.100: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Mar  2 01:21:53.100: INFO: stdout: "nodeport-test-dfqpl"
Mar  2 01:21:53.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-4749 exec execpodtqz4g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.111.148 80'
Mar  2 01:21:53.450: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.111.148 80\nConnection to 172.21.111.148 80 port [tcp/http] succeeded!\n"
Mar  2 01:21:53.450: INFO: stdout: ""
Mar  2 01:21:54.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-4749 exec execpodtqz4g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.111.148 80'
Mar  2 01:21:54.895: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.111.148 80\nConnection to 172.21.111.148 80 port [tcp/http] succeeded!\n"
Mar  2 01:21:54.895: INFO: stdout: ""
Mar  2 01:21:55.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-4749 exec execpodtqz4g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.111.148 80'
Mar  2 01:21:55.820: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.111.148 80\nConnection to 172.21.111.148 80 port [tcp/http] succeeded!\n"
Mar  2 01:21:55.820: INFO: stdout: "nodeport-test-dfqpl"
Mar  2 01:21:55.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-4749 exec execpodtqz4g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.132.92.188 30902'
Mar  2 01:21:56.178: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.132.92.188 30902\nConnection to 10.132.92.188 30902 port [tcp/*] succeeded!\n"
Mar  2 01:21:56.178: INFO: stdout: "nodeport-test-dfqpl"
Mar  2 01:21:56.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-4749 exec execpodtqz4g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.132.92.186 30902'
Mar  2 01:21:56.566: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.132.92.186 30902\nConnection to 10.132.92.186 30902 port [tcp/*] succeeded!\n"
Mar  2 01:21:56.566: INFO: stdout: "nodeport-test-dfqpl"
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  2 01:21:56.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4749" for this suite. 03/02/23 01:21:56.616
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","completed":44,"skipped":898,"failed":0}
------------------------------
• [SLOW TEST] [12.386 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:21:44.254
    Mar  2 01:21:44.254: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename services 03/02/23 01:21:44.255
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:21:44.324
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:21:44.338
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1268
    STEP: creating service nodeport-test with type=NodePort in namespace services-4749 03/02/23 01:21:44.356
    STEP: creating replication controller nodeport-test in namespace services-4749 03/02/23 01:21:44.432
    I0302 01:21:44.474494      20 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-4749, replica count: 2
    I0302 01:21:47.525786      20 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar  2 01:21:47.525: INFO: Creating new exec pod
    Mar  2 01:21:47.590: INFO: Waiting up to 5m0s for pod "execpodtqz4g" in namespace "services-4749" to be "running"
    Mar  2 01:21:47.604: INFO: Pod "execpodtqz4g": Phase="Pending", Reason="", readiness=false. Elapsed: 13.996091ms
    Mar  2 01:21:49.619: INFO: Pod "execpodtqz4g": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029314252s
    Mar  2 01:21:51.634: INFO: Pod "execpodtqz4g": Phase="Running", Reason="", readiness=true. Elapsed: 4.044258944s
    Mar  2 01:21:51.634: INFO: Pod "execpodtqz4g" satisfied condition "running"
    Mar  2 01:21:52.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-4749 exec execpodtqz4g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Mar  2 01:21:53.100: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Mar  2 01:21:53.100: INFO: stdout: "nodeport-test-dfqpl"
    Mar  2 01:21:53.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-4749 exec execpodtqz4g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.111.148 80'
    Mar  2 01:21:53.450: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.111.148 80\nConnection to 172.21.111.148 80 port [tcp/http] succeeded!\n"
    Mar  2 01:21:53.450: INFO: stdout: ""
    Mar  2 01:21:54.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-4749 exec execpodtqz4g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.111.148 80'
    Mar  2 01:21:54.895: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.111.148 80\nConnection to 172.21.111.148 80 port [tcp/http] succeeded!\n"
    Mar  2 01:21:54.895: INFO: stdout: ""
    Mar  2 01:21:55.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-4749 exec execpodtqz4g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.111.148 80'
    Mar  2 01:21:55.820: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.111.148 80\nConnection to 172.21.111.148 80 port [tcp/http] succeeded!\n"
    Mar  2 01:21:55.820: INFO: stdout: "nodeport-test-dfqpl"
    Mar  2 01:21:55.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-4749 exec execpodtqz4g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.132.92.188 30902'
    Mar  2 01:21:56.178: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.132.92.188 30902\nConnection to 10.132.92.188 30902 port [tcp/*] succeeded!\n"
    Mar  2 01:21:56.178: INFO: stdout: "nodeport-test-dfqpl"
    Mar  2 01:21:56.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-4749 exec execpodtqz4g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.132.92.186 30902'
    Mar  2 01:21:56.566: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.132.92.186 30902\nConnection to 10.132.92.186 30902 port [tcp/*] succeeded!\n"
    Mar  2 01:21:56.566: INFO: stdout: "nodeport-test-dfqpl"
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  2 01:21:56.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4749" for this suite. 03/02/23 01:21:56.616
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:21:56.641
Mar  2 01:21:56.641: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename secrets 03/02/23 01:21:56.643
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:21:56.766
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:21:56.78
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
STEP: creating a secret 03/02/23 01:21:56.795
STEP: listing secrets in all namespaces to ensure that there are more than zero 03/02/23 01:21:56.839
STEP: patching the secret 03/02/23 01:21:57.163
STEP: deleting the secret using a LabelSelector 03/02/23 01:21:57.202
STEP: listing secrets in all namespaces, searching for label name and value in patch 03/02/23 01:21:57.25
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Mar  2 01:21:57.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1" for this suite. 03/02/23 01:21:57.541
{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","completed":45,"skipped":901,"failed":0}
------------------------------
• [0.923 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:21:56.641
    Mar  2 01:21:56.641: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename secrets 03/02/23 01:21:56.643
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:21:56.766
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:21:56.78
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:153
    STEP: creating a secret 03/02/23 01:21:56.795
    STEP: listing secrets in all namespaces to ensure that there are more than zero 03/02/23 01:21:56.839
    STEP: patching the secret 03/02/23 01:21:57.163
    STEP: deleting the secret using a LabelSelector 03/02/23 01:21:57.202
    STEP: listing secrets in all namespaces, searching for label name and value in patch 03/02/23 01:21:57.25
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Mar  2 01:21:57.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-1" for this suite. 03/02/23 01:21:57.541
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:21:57.567
Mar  2 01:21:57.567: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename runtimeclass 03/02/23 01:21:57.568
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:21:57.703
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:21:57.715
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Mar  2 01:21:57.853: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-4163 to be scheduled
Mar  2 01:21:57.868: INFO: 1 pods are not scheduled: [runtimeclass-4163/test-runtimeclass-runtimeclass-4163-preconfigured-handler-7466s(09d60253-a4d5-4c27-aa2b-e24f5c2ef197)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Mar  2 01:21:59.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-4163" for this suite. 03/02/23 01:21:59.929
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]","completed":46,"skipped":923,"failed":0}
------------------------------
• [2.386 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:21:57.567
    Mar  2 01:21:57.567: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename runtimeclass 03/02/23 01:21:57.568
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:21:57.703
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:21:57.715
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Mar  2 01:21:57.853: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-4163 to be scheduled
    Mar  2 01:21:57.868: INFO: 1 pods are not scheduled: [runtimeclass-4163/test-runtimeclass-runtimeclass-4163-preconfigured-handler-7466s(09d60253-a4d5-4c27-aa2b-e24f5c2ef197)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Mar  2 01:21:59.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-4163" for this suite. 03/02/23 01:21:59.929
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:21:59.953
Mar  2 01:21:59.954: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename downward-api 03/02/23 01:21:59.955
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:22:00.021
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:22:00.036
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
STEP: Creating a pod to test downward api env vars 03/02/23 01:22:00.048
Mar  2 01:22:00.126: INFO: Waiting up to 5m0s for pod "downward-api-9830e107-d99f-4f30-ab02-bb3de28a6b19" in namespace "downward-api-9590" to be "Succeeded or Failed"
Mar  2 01:22:00.143: INFO: Pod "downward-api-9830e107-d99f-4f30-ab02-bb3de28a6b19": Phase="Pending", Reason="", readiness=false. Elapsed: 17.69352ms
Mar  2 01:22:02.183: INFO: Pod "downward-api-9830e107-d99f-4f30-ab02-bb3de28a6b19": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057887707s
Mar  2 01:22:04.158: INFO: Pod "downward-api-9830e107-d99f-4f30-ab02-bb3de28a6b19": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032035318s
Mar  2 01:22:06.163: INFO: Pod "downward-api-9830e107-d99f-4f30-ab02-bb3de28a6b19": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037067164s
STEP: Saw pod success 03/02/23 01:22:06.163
Mar  2 01:22:06.163: INFO: Pod "downward-api-9830e107-d99f-4f30-ab02-bb3de28a6b19" satisfied condition "Succeeded or Failed"
Mar  2 01:22:06.176: INFO: Trying to get logs from node 10.132.92.143 pod downward-api-9830e107-d99f-4f30-ab02-bb3de28a6b19 container dapi-container: <nil>
STEP: delete the pod 03/02/23 01:22:06.308
Mar  2 01:22:06.363: INFO: Waiting for pod downward-api-9830e107-d99f-4f30-ab02-bb3de28a6b19 to disappear
Mar  2 01:22:06.396: INFO: Pod downward-api-9830e107-d99f-4f30-ab02-bb3de28a6b19 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Mar  2 01:22:06.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9590" for this suite. 03/02/23 01:22:06.427
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","completed":47,"skipped":934,"failed":0}
------------------------------
• [SLOW TEST] [6.544 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:21:59.953
    Mar  2 01:21:59.954: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename downward-api 03/02/23 01:21:59.955
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:22:00.021
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:22:00.036
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:89
    STEP: Creating a pod to test downward api env vars 03/02/23 01:22:00.048
    Mar  2 01:22:00.126: INFO: Waiting up to 5m0s for pod "downward-api-9830e107-d99f-4f30-ab02-bb3de28a6b19" in namespace "downward-api-9590" to be "Succeeded or Failed"
    Mar  2 01:22:00.143: INFO: Pod "downward-api-9830e107-d99f-4f30-ab02-bb3de28a6b19": Phase="Pending", Reason="", readiness=false. Elapsed: 17.69352ms
    Mar  2 01:22:02.183: INFO: Pod "downward-api-9830e107-d99f-4f30-ab02-bb3de28a6b19": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057887707s
    Mar  2 01:22:04.158: INFO: Pod "downward-api-9830e107-d99f-4f30-ab02-bb3de28a6b19": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032035318s
    Mar  2 01:22:06.163: INFO: Pod "downward-api-9830e107-d99f-4f30-ab02-bb3de28a6b19": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037067164s
    STEP: Saw pod success 03/02/23 01:22:06.163
    Mar  2 01:22:06.163: INFO: Pod "downward-api-9830e107-d99f-4f30-ab02-bb3de28a6b19" satisfied condition "Succeeded or Failed"
    Mar  2 01:22:06.176: INFO: Trying to get logs from node 10.132.92.143 pod downward-api-9830e107-d99f-4f30-ab02-bb3de28a6b19 container dapi-container: <nil>
    STEP: delete the pod 03/02/23 01:22:06.308
    Mar  2 01:22:06.363: INFO: Waiting for pod downward-api-9830e107-d99f-4f30-ab02-bb3de28a6b19 to disappear
    Mar  2 01:22:06.396: INFO: Pod downward-api-9830e107-d99f-4f30-ab02-bb3de28a6b19 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Mar  2 01:22:06.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9590" for this suite. 03/02/23 01:22:06.427
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:22:06.502
Mar  2 01:22:06.502: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename projected 03/02/23 01:22:06.504
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:22:06.653
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:22:06.675
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
STEP: Creating the pod 03/02/23 01:22:06.7
Mar  2 01:22:06.861: INFO: Waiting up to 5m0s for pod "labelsupdate1d511b6e-a6d4-4729-beb4-356d600ddd26" in namespace "projected-4902" to be "running and ready"
Mar  2 01:22:06.875: INFO: Pod "labelsupdate1d511b6e-a6d4-4729-beb4-356d600ddd26": Phase="Pending", Reason="", readiness=false. Elapsed: 13.871256ms
Mar  2 01:22:06.875: INFO: The phase of Pod labelsupdate1d511b6e-a6d4-4729-beb4-356d600ddd26 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:22:08.890: INFO: Pod "labelsupdate1d511b6e-a6d4-4729-beb4-356d600ddd26": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029247121s
Mar  2 01:22:08.890: INFO: The phase of Pod labelsupdate1d511b6e-a6d4-4729-beb4-356d600ddd26 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:22:10.890: INFO: Pod "labelsupdate1d511b6e-a6d4-4729-beb4-356d600ddd26": Phase="Running", Reason="", readiness=true. Elapsed: 4.029476266s
Mar  2 01:22:10.890: INFO: The phase of Pod labelsupdate1d511b6e-a6d4-4729-beb4-356d600ddd26 is Running (Ready = true)
Mar  2 01:22:10.890: INFO: Pod "labelsupdate1d511b6e-a6d4-4729-beb4-356d600ddd26" satisfied condition "running and ready"
Mar  2 01:22:11.521: INFO: Successfully updated pod "labelsupdate1d511b6e-a6d4-4729-beb4-356d600ddd26"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar  2 01:22:13.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4902" for this suite. 03/02/23 01:22:13.624
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","completed":48,"skipped":966,"failed":0}
------------------------------
• [SLOW TEST] [7.145 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:22:06.502
    Mar  2 01:22:06.502: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename projected 03/02/23 01:22:06.504
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:22:06.653
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:22:06.675
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:129
    STEP: Creating the pod 03/02/23 01:22:06.7
    Mar  2 01:22:06.861: INFO: Waiting up to 5m0s for pod "labelsupdate1d511b6e-a6d4-4729-beb4-356d600ddd26" in namespace "projected-4902" to be "running and ready"
    Mar  2 01:22:06.875: INFO: Pod "labelsupdate1d511b6e-a6d4-4729-beb4-356d600ddd26": Phase="Pending", Reason="", readiness=false. Elapsed: 13.871256ms
    Mar  2 01:22:06.875: INFO: The phase of Pod labelsupdate1d511b6e-a6d4-4729-beb4-356d600ddd26 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 01:22:08.890: INFO: Pod "labelsupdate1d511b6e-a6d4-4729-beb4-356d600ddd26": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029247121s
    Mar  2 01:22:08.890: INFO: The phase of Pod labelsupdate1d511b6e-a6d4-4729-beb4-356d600ddd26 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 01:22:10.890: INFO: Pod "labelsupdate1d511b6e-a6d4-4729-beb4-356d600ddd26": Phase="Running", Reason="", readiness=true. Elapsed: 4.029476266s
    Mar  2 01:22:10.890: INFO: The phase of Pod labelsupdate1d511b6e-a6d4-4729-beb4-356d600ddd26 is Running (Ready = true)
    Mar  2 01:22:10.890: INFO: Pod "labelsupdate1d511b6e-a6d4-4729-beb4-356d600ddd26" satisfied condition "running and ready"
    Mar  2 01:22:11.521: INFO: Successfully updated pod "labelsupdate1d511b6e-a6d4-4729-beb4-356d600ddd26"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar  2 01:22:13.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4902" for this suite. 03/02/23 01:22:13.624
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:22:13.649
Mar  2 01:22:13.649: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename projected 03/02/23 01:22:13.65
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:22:13.715
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:22:13.728
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
STEP: Creating a pod to test downward API volume plugin 03/02/23 01:22:13.742
Mar  2 01:22:13.816: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b9ee525b-698c-4414-880f-f56f2d185688" in namespace "projected-2000" to be "Succeeded or Failed"
Mar  2 01:22:13.851: INFO: Pod "downwardapi-volume-b9ee525b-698c-4414-880f-f56f2d185688": Phase="Pending", Reason="", readiness=false. Elapsed: 35.131209ms
Mar  2 01:22:15.867: INFO: Pod "downwardapi-volume-b9ee525b-698c-4414-880f-f56f2d185688": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05049997s
Mar  2 01:22:17.866: INFO: Pod "downwardapi-volume-b9ee525b-698c-4414-880f-f56f2d185688": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050262329s
Mar  2 01:22:19.871: INFO: Pod "downwardapi-volume-b9ee525b-698c-4414-880f-f56f2d185688": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.054329117s
STEP: Saw pod success 03/02/23 01:22:19.871
Mar  2 01:22:19.871: INFO: Pod "downwardapi-volume-b9ee525b-698c-4414-880f-f56f2d185688" satisfied condition "Succeeded or Failed"
Mar  2 01:22:19.885: INFO: Trying to get logs from node 10.132.92.143 pod downwardapi-volume-b9ee525b-698c-4414-880f-f56f2d185688 container client-container: <nil>
STEP: delete the pod 03/02/23 01:22:19.938
Mar  2 01:22:19.985: INFO: Waiting for pod downwardapi-volume-b9ee525b-698c-4414-880f-f56f2d185688 to disappear
Mar  2 01:22:20.003: INFO: Pod downwardapi-volume-b9ee525b-698c-4414-880f-f56f2d185688 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar  2 01:22:20.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2000" for this suite. 03/02/23 01:22:20.024
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","completed":49,"skipped":993,"failed":0}
------------------------------
• [SLOW TEST] [6.405 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:22:13.649
    Mar  2 01:22:13.649: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename projected 03/02/23 01:22:13.65
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:22:13.715
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:22:13.728
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:220
    STEP: Creating a pod to test downward API volume plugin 03/02/23 01:22:13.742
    Mar  2 01:22:13.816: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b9ee525b-698c-4414-880f-f56f2d185688" in namespace "projected-2000" to be "Succeeded or Failed"
    Mar  2 01:22:13.851: INFO: Pod "downwardapi-volume-b9ee525b-698c-4414-880f-f56f2d185688": Phase="Pending", Reason="", readiness=false. Elapsed: 35.131209ms
    Mar  2 01:22:15.867: INFO: Pod "downwardapi-volume-b9ee525b-698c-4414-880f-f56f2d185688": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05049997s
    Mar  2 01:22:17.866: INFO: Pod "downwardapi-volume-b9ee525b-698c-4414-880f-f56f2d185688": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050262329s
    Mar  2 01:22:19.871: INFO: Pod "downwardapi-volume-b9ee525b-698c-4414-880f-f56f2d185688": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.054329117s
    STEP: Saw pod success 03/02/23 01:22:19.871
    Mar  2 01:22:19.871: INFO: Pod "downwardapi-volume-b9ee525b-698c-4414-880f-f56f2d185688" satisfied condition "Succeeded or Failed"
    Mar  2 01:22:19.885: INFO: Trying to get logs from node 10.132.92.143 pod downwardapi-volume-b9ee525b-698c-4414-880f-f56f2d185688 container client-container: <nil>
    STEP: delete the pod 03/02/23 01:22:19.938
    Mar  2 01:22:19.985: INFO: Waiting for pod downwardapi-volume-b9ee525b-698c-4414-880f-f56f2d185688 to disappear
    Mar  2 01:22:20.003: INFO: Pod downwardapi-volume-b9ee525b-698c-4414-880f-f56f2d185688 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar  2 01:22:20.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2000" for this suite. 03/02/23 01:22:20.024
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:22:20.057
Mar  2 01:22:20.057: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename container-lifecycle-hook 03/02/23 01:22:20.058
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:22:20.104
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:22:20.116
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 03/02/23 01:22:20.16
Mar  2 01:22:20.239: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6272" to be "running and ready"
Mar  2 01:22:20.261: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 21.613386ms
Mar  2 01:22:20.261: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:22:22.277: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037721422s
Mar  2 01:22:22.277: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:22:24.278: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.038955371s
Mar  2 01:22:24.279: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Mar  2 01:22:24.279: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
STEP: create the pod with lifecycle hook 03/02/23 01:22:24.292
Mar  2 01:22:24.358: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-6272" to be "running and ready"
Mar  2 01:22:24.374: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 15.992642ms
Mar  2 01:22:24.375: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:22:26.390: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.031103501s
Mar  2 01:22:26.390: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Mar  2 01:22:26.390: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 03/02/23 01:22:26.405
STEP: delete the pod with lifecycle hook 03/02/23 01:22:26.465
Mar  2 01:22:26.493: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  2 01:22:26.507: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  2 01:22:28.507: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  2 01:22:28.529: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  2 01:22:30.508: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  2 01:22:30.522: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Mar  2 01:22:30.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6272" for this suite. 03/02/23 01:22:30.544
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","completed":50,"skipped":1012,"failed":0}
------------------------------
• [SLOW TEST] [10.512 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:22:20.057
    Mar  2 01:22:20.057: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename container-lifecycle-hook 03/02/23 01:22:20.058
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:22:20.104
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:22:20.116
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 03/02/23 01:22:20.16
    Mar  2 01:22:20.239: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6272" to be "running and ready"
    Mar  2 01:22:20.261: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 21.613386ms
    Mar  2 01:22:20.261: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 01:22:22.277: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037721422s
    Mar  2 01:22:22.277: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 01:22:24.278: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.038955371s
    Mar  2 01:22:24.279: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Mar  2 01:22:24.279: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:97
    STEP: create the pod with lifecycle hook 03/02/23 01:22:24.292
    Mar  2 01:22:24.358: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-6272" to be "running and ready"
    Mar  2 01:22:24.374: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 15.992642ms
    Mar  2 01:22:24.375: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 01:22:26.390: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.031103501s
    Mar  2 01:22:26.390: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Mar  2 01:22:26.390: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 03/02/23 01:22:26.405
    STEP: delete the pod with lifecycle hook 03/02/23 01:22:26.465
    Mar  2 01:22:26.493: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Mar  2 01:22:26.507: INFO: Pod pod-with-poststart-exec-hook still exists
    Mar  2 01:22:28.507: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Mar  2 01:22:28.529: INFO: Pod pod-with-poststart-exec-hook still exists
    Mar  2 01:22:30.508: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Mar  2 01:22:30.522: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Mar  2 01:22:30.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-6272" for this suite. 03/02/23 01:22:30.544
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:22:30.571
Mar  2 01:22:30.572: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename disruption 03/02/23 01:22:30.576
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:22:30.653
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:22:30.666
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
STEP: Waiting for the pdb to be processed 03/02/23 01:22:30.709
STEP: Updating PodDisruptionBudget status 03/02/23 01:22:32.759
STEP: Waiting for all pods to be running 03/02/23 01:22:32.82
Mar  2 01:22:32.842: INFO: running pods: 0 < 1
Mar  2 01:22:34.861: INFO: running pods: 0 < 1
STEP: locating a running pod 03/02/23 01:22:36.858
STEP: Waiting for the pdb to be processed 03/02/23 01:22:36.904
STEP: Patching PodDisruptionBudget status 03/02/23 01:22:36.93
STEP: Waiting for the pdb to be processed 03/02/23 01:22:36.961
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Mar  2 01:22:36.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-8394" for this suite. 03/02/23 01:22:37.01
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","completed":51,"skipped":1020,"failed":0}
------------------------------
• [SLOW TEST] [6.464 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:22:30.571
    Mar  2 01:22:30.572: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename disruption 03/02/23 01:22:30.576
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:22:30.653
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:22:30.666
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:163
    STEP: Waiting for the pdb to be processed 03/02/23 01:22:30.709
    STEP: Updating PodDisruptionBudget status 03/02/23 01:22:32.759
    STEP: Waiting for all pods to be running 03/02/23 01:22:32.82
    Mar  2 01:22:32.842: INFO: running pods: 0 < 1
    Mar  2 01:22:34.861: INFO: running pods: 0 < 1
    STEP: locating a running pod 03/02/23 01:22:36.858
    STEP: Waiting for the pdb to be processed 03/02/23 01:22:36.904
    STEP: Patching PodDisruptionBudget status 03/02/23 01:22:36.93
    STEP: Waiting for the pdb to be processed 03/02/23 01:22:36.961
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Mar  2 01:22:36.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-8394" for this suite. 03/02/23 01:22:37.01
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:22:37.043
Mar  2 01:22:37.043: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename disruption 03/02/23 01:22:37.045
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:22:37.107
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:22:37.121
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
STEP: Creating a pdb that targets all three pods in a test replica set 03/02/23 01:22:37.138
STEP: Waiting for the pdb to be processed 03/02/23 01:22:37.177
STEP: First trying to evict a pod which shouldn't be evictable 03/02/23 01:22:39.238
STEP: Waiting for all pods to be running 03/02/23 01:22:39.239
Mar  2 01:22:39.272: INFO: pods: 0 < 3
Mar  2 01:22:41.294: INFO: running pods: 0 < 3
Mar  2 01:22:43.288: INFO: running pods: 2 < 3
STEP: locating a running pod 03/02/23 01:22:45.289
STEP: Updating the pdb to allow a pod to be evicted 03/02/23 01:22:45.334
STEP: Waiting for the pdb to be processed 03/02/23 01:22:45.366
STEP: Trying to evict the same pod we tried earlier which should now be evictable 03/02/23 01:22:47.393
STEP: Waiting for all pods to be running 03/02/23 01:22:47.393
STEP: Waiting for the pdb to observed all healthy pods 03/02/23 01:22:47.411
STEP: Patching the pdb to disallow a pod to be evicted 03/02/23 01:22:47.485
STEP: Waiting for the pdb to be processed 03/02/23 01:22:47.518
STEP: Waiting for all pods to be running 03/02/23 01:22:49.545
Mar  2 01:22:49.564: INFO: running pods: 2 < 3
STEP: locating a running pod 03/02/23 01:22:51.586
STEP: Deleting the pdb to allow a pod to be evicted 03/02/23 01:22:51.631
STEP: Waiting for the pdb to be deleted 03/02/23 01:22:51.651
STEP: Trying to evict the same pod we tried earlier which should now be evictable 03/02/23 01:22:51.664
STEP: Waiting for all pods to be running 03/02/23 01:22:51.664
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Mar  2 01:22:51.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-4395" for this suite. 03/02/23 01:22:51.76
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","completed":52,"skipped":1036,"failed":0}
------------------------------
• [SLOW TEST] [14.753 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:22:37.043
    Mar  2 01:22:37.043: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename disruption 03/02/23 01:22:37.045
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:22:37.107
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:22:37.121
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:346
    STEP: Creating a pdb that targets all three pods in a test replica set 03/02/23 01:22:37.138
    STEP: Waiting for the pdb to be processed 03/02/23 01:22:37.177
    STEP: First trying to evict a pod which shouldn't be evictable 03/02/23 01:22:39.238
    STEP: Waiting for all pods to be running 03/02/23 01:22:39.239
    Mar  2 01:22:39.272: INFO: pods: 0 < 3
    Mar  2 01:22:41.294: INFO: running pods: 0 < 3
    Mar  2 01:22:43.288: INFO: running pods: 2 < 3
    STEP: locating a running pod 03/02/23 01:22:45.289
    STEP: Updating the pdb to allow a pod to be evicted 03/02/23 01:22:45.334
    STEP: Waiting for the pdb to be processed 03/02/23 01:22:45.366
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 03/02/23 01:22:47.393
    STEP: Waiting for all pods to be running 03/02/23 01:22:47.393
    STEP: Waiting for the pdb to observed all healthy pods 03/02/23 01:22:47.411
    STEP: Patching the pdb to disallow a pod to be evicted 03/02/23 01:22:47.485
    STEP: Waiting for the pdb to be processed 03/02/23 01:22:47.518
    STEP: Waiting for all pods to be running 03/02/23 01:22:49.545
    Mar  2 01:22:49.564: INFO: running pods: 2 < 3
    STEP: locating a running pod 03/02/23 01:22:51.586
    STEP: Deleting the pdb to allow a pod to be evicted 03/02/23 01:22:51.631
    STEP: Waiting for the pdb to be deleted 03/02/23 01:22:51.651
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 03/02/23 01:22:51.664
    STEP: Waiting for all pods to be running 03/02/23 01:22:51.664
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Mar  2 01:22:51.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-4395" for this suite. 03/02/23 01:22:51.76
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:22:51.797
Mar  2 01:22:51.797: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename resourcequota 03/02/23 01:22:51.798
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:22:51.869
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:22:51.883
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
STEP: Counting existing ResourceQuota 03/02/23 01:22:51.899
STEP: Creating a ResourceQuota 03/02/23 01:22:56.93
STEP: Ensuring resource quota status is calculated 03/02/23 01:22:56.956
STEP: Creating a ReplicationController 03/02/23 01:22:58.969
STEP: Ensuring resource quota status captures replication controller creation 03/02/23 01:22:59.071
STEP: Deleting a ReplicationController 03/02/23 01:23:01.086
STEP: Ensuring resource quota status released usage 03/02/23 01:23:01.116
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar  2 01:23:03.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3564" for this suite. 03/02/23 01:23:03.158
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","completed":53,"skipped":1065,"failed":0}
------------------------------
• [SLOW TEST] [11.386 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:22:51.797
    Mar  2 01:22:51.797: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename resourcequota 03/02/23 01:22:51.798
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:22:51.869
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:22:51.883
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:382
    STEP: Counting existing ResourceQuota 03/02/23 01:22:51.899
    STEP: Creating a ResourceQuota 03/02/23 01:22:56.93
    STEP: Ensuring resource quota status is calculated 03/02/23 01:22:56.956
    STEP: Creating a ReplicationController 03/02/23 01:22:58.969
    STEP: Ensuring resource quota status captures replication controller creation 03/02/23 01:22:59.071
    STEP: Deleting a ReplicationController 03/02/23 01:23:01.086
    STEP: Ensuring resource quota status released usage 03/02/23 01:23:01.116
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar  2 01:23:03.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-3564" for this suite. 03/02/23 01:23:03.158
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:23:03.186
Mar  2 01:23:03.187: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename services 03/02/23 01:23:03.188
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:23:03.248
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:23:03.266
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
STEP: creating service multi-endpoint-test in namespace services-7029 03/02/23 01:23:03.279
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7029 to expose endpoints map[] 03/02/23 01:23:03.325
Mar  2 01:23:03.378: INFO: successfully validated that service multi-endpoint-test in namespace services-7029 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-7029 03/02/23 01:23:03.378
Mar  2 01:23:03.438: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-7029" to be "running and ready"
Mar  2 01:23:03.456: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.440007ms
Mar  2 01:23:03.456: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:23:05.473: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035660893s
Mar  2 01:23:05.473: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:23:07.522: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.083973065s
Mar  2 01:23:07.522: INFO: The phase of Pod pod1 is Running (Ready = true)
Mar  2 01:23:07.522: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7029 to expose endpoints map[pod1:[100]] 03/02/23 01:23:07.537
Mar  2 01:23:07.580: INFO: successfully validated that service multi-endpoint-test in namespace services-7029 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-7029 03/02/23 01:23:07.58
Mar  2 01:23:07.666: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-7029" to be "running and ready"
Mar  2 01:23:07.679: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 12.865667ms
Mar  2 01:23:07.679: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:23:09.695: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.028968982s
Mar  2 01:23:09.695: INFO: The phase of Pod pod2 is Running (Ready = true)
Mar  2 01:23:09.695: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7029 to expose endpoints map[pod1:[100] pod2:[101]] 03/02/23 01:23:09.709
Mar  2 01:23:09.774: INFO: successfully validated that service multi-endpoint-test in namespace services-7029 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 03/02/23 01:23:09.774
Mar  2 01:23:09.774: INFO: Creating new exec pod
Mar  2 01:23:09.815: INFO: Waiting up to 5m0s for pod "execpodg6jxq" in namespace "services-7029" to be "running"
Mar  2 01:23:09.838: INFO: Pod "execpodg6jxq": Phase="Pending", Reason="", readiness=false. Elapsed: 22.931253ms
Mar  2 01:23:11.860: INFO: Pod "execpodg6jxq": Phase="Running", Reason="", readiness=true. Elapsed: 2.045319952s
Mar  2 01:23:11.860: INFO: Pod "execpodg6jxq" satisfied condition "running"
Mar  2 01:23:12.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-7029 exec execpodg6jxq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Mar  2 01:23:13.200: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Mar  2 01:23:13.200: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 01:23:13.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-7029 exec execpodg6jxq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.150.141 80'
Mar  2 01:23:13.529: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.150.141 80\nConnection to 172.21.150.141 80 port [tcp/http] succeeded!\n"
Mar  2 01:23:13.529: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 01:23:13.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-7029 exec execpodg6jxq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Mar  2 01:23:14.083: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Mar  2 01:23:14.083: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 01:23:14.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-7029 exec execpodg6jxq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.150.141 81'
Mar  2 01:23:14.461: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.150.141 81\nConnection to 172.21.150.141 81 port [tcp/*] succeeded!\n"
Mar  2 01:23:14.461: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-7029 03/02/23 01:23:14.462
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7029 to expose endpoints map[pod2:[101]] 03/02/23 01:23:14.512
Mar  2 01:23:14.571: INFO: successfully validated that service multi-endpoint-test in namespace services-7029 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-7029 03/02/23 01:23:14.571
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7029 to expose endpoints map[] 03/02/23 01:23:14.609
Mar  2 01:23:14.660: INFO: successfully validated that service multi-endpoint-test in namespace services-7029 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  2 01:23:14.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7029" for this suite. 03/02/23 01:23:14.775
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","completed":54,"skipped":1071,"failed":0}
------------------------------
• [SLOW TEST] [11.616 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:23:03.186
    Mar  2 01:23:03.187: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename services 03/02/23 01:23:03.188
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:23:03.248
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:23:03.266
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:852
    STEP: creating service multi-endpoint-test in namespace services-7029 03/02/23 01:23:03.279
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7029 to expose endpoints map[] 03/02/23 01:23:03.325
    Mar  2 01:23:03.378: INFO: successfully validated that service multi-endpoint-test in namespace services-7029 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-7029 03/02/23 01:23:03.378
    Mar  2 01:23:03.438: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-7029" to be "running and ready"
    Mar  2 01:23:03.456: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.440007ms
    Mar  2 01:23:03.456: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 01:23:05.473: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035660893s
    Mar  2 01:23:05.473: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 01:23:07.522: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.083973065s
    Mar  2 01:23:07.522: INFO: The phase of Pod pod1 is Running (Ready = true)
    Mar  2 01:23:07.522: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7029 to expose endpoints map[pod1:[100]] 03/02/23 01:23:07.537
    Mar  2 01:23:07.580: INFO: successfully validated that service multi-endpoint-test in namespace services-7029 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-7029 03/02/23 01:23:07.58
    Mar  2 01:23:07.666: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-7029" to be "running and ready"
    Mar  2 01:23:07.679: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 12.865667ms
    Mar  2 01:23:07.679: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 01:23:09.695: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.028968982s
    Mar  2 01:23:09.695: INFO: The phase of Pod pod2 is Running (Ready = true)
    Mar  2 01:23:09.695: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7029 to expose endpoints map[pod1:[100] pod2:[101]] 03/02/23 01:23:09.709
    Mar  2 01:23:09.774: INFO: successfully validated that service multi-endpoint-test in namespace services-7029 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 03/02/23 01:23:09.774
    Mar  2 01:23:09.774: INFO: Creating new exec pod
    Mar  2 01:23:09.815: INFO: Waiting up to 5m0s for pod "execpodg6jxq" in namespace "services-7029" to be "running"
    Mar  2 01:23:09.838: INFO: Pod "execpodg6jxq": Phase="Pending", Reason="", readiness=false. Elapsed: 22.931253ms
    Mar  2 01:23:11.860: INFO: Pod "execpodg6jxq": Phase="Running", Reason="", readiness=true. Elapsed: 2.045319952s
    Mar  2 01:23:11.860: INFO: Pod "execpodg6jxq" satisfied condition "running"
    Mar  2 01:23:12.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-7029 exec execpodg6jxq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
    Mar  2 01:23:13.200: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Mar  2 01:23:13.200: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  2 01:23:13.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-7029 exec execpodg6jxq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.150.141 80'
    Mar  2 01:23:13.529: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.150.141 80\nConnection to 172.21.150.141 80 port [tcp/http] succeeded!\n"
    Mar  2 01:23:13.529: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  2 01:23:13.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-7029 exec execpodg6jxq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
    Mar  2 01:23:14.083: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Mar  2 01:23:14.083: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  2 01:23:14.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-7029 exec execpodg6jxq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.150.141 81'
    Mar  2 01:23:14.461: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.150.141 81\nConnection to 172.21.150.141 81 port [tcp/*] succeeded!\n"
    Mar  2 01:23:14.461: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-7029 03/02/23 01:23:14.462
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7029 to expose endpoints map[pod2:[101]] 03/02/23 01:23:14.512
    Mar  2 01:23:14.571: INFO: successfully validated that service multi-endpoint-test in namespace services-7029 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-7029 03/02/23 01:23:14.571
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7029 to expose endpoints map[] 03/02/23 01:23:14.609
    Mar  2 01:23:14.660: INFO: successfully validated that service multi-endpoint-test in namespace services-7029 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  2 01:23:14.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7029" for this suite. 03/02/23 01:23:14.775
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:23:14.803
Mar  2 01:23:14.803: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename deployment 03/02/23 01:23:14.805
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:23:14.86
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:23:14.875
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Mar  2 01:23:14.949: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Mar  2 01:23:19.971: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/02/23 01:23:19.971
Mar  2 01:23:19.971: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 03/02/23 01:23:20.014
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar  2 01:23:20.056: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-6578  c3184808-9268-4a95-89c1-bd6cd3665bec 78639 1 2023-03-02 01:23:19 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-03-02 01:23:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003571db8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Mar  2 01:23:20.072: INFO: New ReplicaSet "test-cleanup-deployment-69cb9c5497" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-69cb9c5497  deployment-6578  bd441ac9-20c0-4b68-b690-dad015293d46 78644 1 2023-03-02 01:23:20 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment c3184808-9268-4a95-89c1-bd6cd3665bec 0xc003b60207 0xc003b60208}] [] [{kube-controller-manager Update apps/v1 2023-03-02 01:23:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c3184808-9268-4a95-89c1-bd6cd3665bec\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 69cb9c5497,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003b60298 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  2 01:23:20.072: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Mar  2 01:23:20.073: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-6578  779ea8a7-3edb-4691-895d-6606eca732c2 78643 1 2023-03-02 01:23:14 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment c3184808-9268-4a95-89c1-bd6cd3665bec 0xc003b600d7 0xc003b600d8}] [] [{e2e.test Update apps/v1 2023-03-02 01:23:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 01:23:16 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-03-02 01:23:20 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"c3184808-9268-4a95-89c1-bd6cd3665bec\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003b60198 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar  2 01:23:20.099: INFO: Pod "test-cleanup-controller-dtqs2" is available:
&Pod{ObjectMeta:{test-cleanup-controller-dtqs2 test-cleanup-controller- deployment-6578  23747d5a-2199-494f-a81d-c93db3fd9e7b 78611 0 2023-03-02 01:23:14 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:aa5c7060e87cbc29afbe5be583c5cfabe771e082dfe1c9e35d3bcd0c33eaa4d8 cni.projectcalico.org/podIP:172.30.156.127/32 cni.projectcalico.org/podIPs:172.30.156.127/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.156.127"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.156.127"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-cleanup-controller 779ea8a7-3edb-4691-895d-6606eca732c2 0xc003ca6887 0xc003ca6888}] [] [{kube-controller-manager Update v1 2023-03-02 01:23:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"779ea8a7-3edb-4691-895d-6606eca732c2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 01:23:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-02 01:23:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.156.127\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-03-02 01:23:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f85zw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f85zw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.132.92.143,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c38,c7,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:23:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:23:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:23:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:23:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.132.92.143,PodIP:172.30.156.127,StartTime:2023-03-02 01:23:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 01:23:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://4d59f844c07fc2f7fdf3cdf8ba2968960725301b65c8b7b4264b13c670e9588a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.156.127,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar  2 01:23:20.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6578" for this suite. 03/02/23 01:23:20.132
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","completed":55,"skipped":1081,"failed":0}
------------------------------
• [SLOW TEST] [5.358 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:23:14.803
    Mar  2 01:23:14.803: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename deployment 03/02/23 01:23:14.805
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:23:14.86
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:23:14.875
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Mar  2 01:23:14.949: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Mar  2 01:23:19.971: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/02/23 01:23:19.971
    Mar  2 01:23:19.971: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 03/02/23 01:23:20.014
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar  2 01:23:20.056: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-6578  c3184808-9268-4a95-89c1-bd6cd3665bec 78639 1 2023-03-02 01:23:19 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-03-02 01:23:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003571db8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Mar  2 01:23:20.072: INFO: New ReplicaSet "test-cleanup-deployment-69cb9c5497" of Deployment "test-cleanup-deployment":
    &ReplicaSet{ObjectMeta:{test-cleanup-deployment-69cb9c5497  deployment-6578  bd441ac9-20c0-4b68-b690-dad015293d46 78644 1 2023-03-02 01:23:20 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment c3184808-9268-4a95-89c1-bd6cd3665bec 0xc003b60207 0xc003b60208}] [] [{kube-controller-manager Update apps/v1 2023-03-02 01:23:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c3184808-9268-4a95-89c1-bd6cd3665bec\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 69cb9c5497,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003b60298 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar  2 01:23:20.072: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    Mar  2 01:23:20.073: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-6578  779ea8a7-3edb-4691-895d-6606eca732c2 78643 1 2023-03-02 01:23:14 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment c3184808-9268-4a95-89c1-bd6cd3665bec 0xc003b600d7 0xc003b600d8}] [] [{e2e.test Update apps/v1 2023-03-02 01:23:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 01:23:16 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-03-02 01:23:20 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"c3184808-9268-4a95-89c1-bd6cd3665bec\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003b60198 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar  2 01:23:20.099: INFO: Pod "test-cleanup-controller-dtqs2" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-dtqs2 test-cleanup-controller- deployment-6578  23747d5a-2199-494f-a81d-c93db3fd9e7b 78611 0 2023-03-02 01:23:14 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:aa5c7060e87cbc29afbe5be583c5cfabe771e082dfe1c9e35d3bcd0c33eaa4d8 cni.projectcalico.org/podIP:172.30.156.127/32 cni.projectcalico.org/podIPs:172.30.156.127/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.156.127"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.156.127"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-cleanup-controller 779ea8a7-3edb-4691-895d-6606eca732c2 0xc003ca6887 0xc003ca6888}] [] [{kube-controller-manager Update v1 2023-03-02 01:23:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"779ea8a7-3edb-4691-895d-6606eca732c2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 01:23:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-02 01:23:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.156.127\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-03-02 01:23:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f85zw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f85zw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.132.92.143,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c38,c7,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:23:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:23:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:23:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:23:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.132.92.143,PodIP:172.30.156.127,StartTime:2023-03-02 01:23:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 01:23:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://4d59f844c07fc2f7fdf3cdf8ba2968960725301b65c8b7b4264b13c670e9588a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.156.127,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar  2 01:23:20.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-6578" for this suite. 03/02/23 01:23:20.132
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:23:20.163
Mar  2 01:23:20.163: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename services 03/02/23 01:23:20.164
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:23:20.224
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:23:20.239
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
STEP: creating a collection of services 03/02/23 01:23:20.254
Mar  2 01:23:20.254: INFO: Creating e2e-svc-a-b8rw8
Mar  2 01:23:20.314: INFO: Creating e2e-svc-b-5485p
Mar  2 01:23:20.355: INFO: Creating e2e-svc-c-6rbls
STEP: deleting service collection 03/02/23 01:23:20.406
Mar  2 01:23:20.531: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  2 01:23:20.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4264" for this suite. 03/02/23 01:23:20.556
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","completed":56,"skipped":1100,"failed":0}
------------------------------
• [0.417 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:23:20.163
    Mar  2 01:23:20.163: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename services 03/02/23 01:23:20.164
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:23:20.224
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:23:20.239
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3641
    STEP: creating a collection of services 03/02/23 01:23:20.254
    Mar  2 01:23:20.254: INFO: Creating e2e-svc-a-b8rw8
    Mar  2 01:23:20.314: INFO: Creating e2e-svc-b-5485p
    Mar  2 01:23:20.355: INFO: Creating e2e-svc-c-6rbls
    STEP: deleting service collection 03/02/23 01:23:20.406
    Mar  2 01:23:20.531: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  2 01:23:20.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4264" for this suite. 03/02/23 01:23:20.556
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:23:20.58
Mar  2 01:23:20.580: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename container-probe 03/02/23 01:23:20.582
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:23:20.64
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:23:20.653
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar  2 01:24:20.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4179" for this suite. 03/02/23 01:24:20.759
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","completed":57,"skipped":1107,"failed":0}
------------------------------
• [SLOW TEST] [60.210 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:23:20.58
    Mar  2 01:23:20.580: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename container-probe 03/02/23 01:23:20.582
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:23:20.64
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:23:20.653
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:104
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar  2 01:24:20.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-4179" for this suite. 03/02/23 01:24:20.759
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:24:20.791
Mar  2 01:24:20.791: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename crd-publish-openapi 03/02/23 01:24:20.792
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:24:20.843
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:24:20.857
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
Mar  2 01:24:20.870: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/02/23 01:24:36.515
Mar  2 01:24:36.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-348 --namespace=crd-publish-openapi-348 create -f -'
Mar  2 01:24:39.920: INFO: stderr: ""
Mar  2 01:24:39.920: INFO: stdout: "e2e-test-crd-publish-openapi-5482-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar  2 01:24:39.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-348 --namespace=crd-publish-openapi-348 delete e2e-test-crd-publish-openapi-5482-crds test-cr'
Mar  2 01:24:40.136: INFO: stderr: ""
Mar  2 01:24:40.136: INFO: stdout: "e2e-test-crd-publish-openapi-5482-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Mar  2 01:24:40.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-348 --namespace=crd-publish-openapi-348 apply -f -'
Mar  2 01:24:42.940: INFO: stderr: ""
Mar  2 01:24:42.940: INFO: stdout: "e2e-test-crd-publish-openapi-5482-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar  2 01:24:42.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-348 --namespace=crd-publish-openapi-348 delete e2e-test-crd-publish-openapi-5482-crds test-cr'
Mar  2 01:24:43.089: INFO: stderr: ""
Mar  2 01:24:43.089: INFO: stdout: "e2e-test-crd-publish-openapi-5482-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 03/02/23 01:24:43.089
Mar  2 01:24:43.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-348 explain e2e-test-crd-publish-openapi-5482-crds'
Mar  2 01:24:44.833: INFO: stderr: ""
Mar  2 01:24:44.833: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5482-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 01:24:59.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-348" for this suite. 03/02/23 01:24:59.372
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","completed":58,"skipped":1107,"failed":0}
------------------------------
• [SLOW TEST] [38.600 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:24:20.791
    Mar  2 01:24:20.791: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename crd-publish-openapi 03/02/23 01:24:20.792
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:24:20.843
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:24:20.857
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:193
    Mar  2 01:24:20.870: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/02/23 01:24:36.515
    Mar  2 01:24:36.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-348 --namespace=crd-publish-openapi-348 create -f -'
    Mar  2 01:24:39.920: INFO: stderr: ""
    Mar  2 01:24:39.920: INFO: stdout: "e2e-test-crd-publish-openapi-5482-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Mar  2 01:24:39.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-348 --namespace=crd-publish-openapi-348 delete e2e-test-crd-publish-openapi-5482-crds test-cr'
    Mar  2 01:24:40.136: INFO: stderr: ""
    Mar  2 01:24:40.136: INFO: stdout: "e2e-test-crd-publish-openapi-5482-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Mar  2 01:24:40.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-348 --namespace=crd-publish-openapi-348 apply -f -'
    Mar  2 01:24:42.940: INFO: stderr: ""
    Mar  2 01:24:42.940: INFO: stdout: "e2e-test-crd-publish-openapi-5482-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Mar  2 01:24:42.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-348 --namespace=crd-publish-openapi-348 delete e2e-test-crd-publish-openapi-5482-crds test-cr'
    Mar  2 01:24:43.089: INFO: stderr: ""
    Mar  2 01:24:43.089: INFO: stdout: "e2e-test-crd-publish-openapi-5482-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 03/02/23 01:24:43.089
    Mar  2 01:24:43.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-348 explain e2e-test-crd-publish-openapi-5482-crds'
    Mar  2 01:24:44.833: INFO: stderr: ""
    Mar  2 01:24:44.833: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5482-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 01:24:59.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-348" for this suite. 03/02/23 01:24:59.372
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:24:59.392
Mar  2 01:24:59.392: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename kubectl 03/02/23 01:24:59.393
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:24:59.442
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:24:59.457
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
STEP: creating Agnhost RC 03/02/23 01:24:59.469
Mar  2 01:24:59.469: INFO: namespace kubectl-8911
Mar  2 01:24:59.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-8911 create -f -'
Mar  2 01:25:01.339: INFO: stderr: ""
Mar  2 01:25:01.339: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 03/02/23 01:25:01.339
Mar  2 01:25:02.365: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 01:25:02.365: INFO: Found 0 / 1
Mar  2 01:25:03.395: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 01:25:03.395: INFO: Found 0 / 1
Mar  2 01:25:04.356: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 01:25:04.356: INFO: Found 1 / 1
Mar  2 01:25:04.356: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar  2 01:25:04.372: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 01:25:04.372: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  2 01:25:04.372: INFO: wait on agnhost-primary startup in kubectl-8911 
Mar  2 01:25:04.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-8911 logs agnhost-primary-knl88 agnhost-primary'
Mar  2 01:25:04.575: INFO: stderr: ""
Mar  2 01:25:04.575: INFO: stdout: "Paused\n"
STEP: exposing RC 03/02/23 01:25:04.575
Mar  2 01:25:04.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-8911 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Mar  2 01:25:04.828: INFO: stderr: ""
Mar  2 01:25:04.828: INFO: stdout: "service/rm2 exposed\n"
Mar  2 01:25:04.846: INFO: Service rm2 in namespace kubectl-8911 found.
STEP: exposing service 03/02/23 01:25:06.872
Mar  2 01:25:06.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-8911 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Mar  2 01:25:07.053: INFO: stderr: ""
Mar  2 01:25:07.053: INFO: stdout: "service/rm3 exposed\n"
Mar  2 01:25:07.074: INFO: Service rm3 in namespace kubectl-8911 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  2 01:25:09.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8911" for this suite. 03/02/23 01:25:09.151
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","completed":59,"skipped":1115,"failed":0}
------------------------------
• [SLOW TEST] [9.776 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1407
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:24:59.392
    Mar  2 01:24:59.392: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename kubectl 03/02/23 01:24:59.393
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:24:59.442
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:24:59.457
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1413
    STEP: creating Agnhost RC 03/02/23 01:24:59.469
    Mar  2 01:24:59.469: INFO: namespace kubectl-8911
    Mar  2 01:24:59.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-8911 create -f -'
    Mar  2 01:25:01.339: INFO: stderr: ""
    Mar  2 01:25:01.339: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 03/02/23 01:25:01.339
    Mar  2 01:25:02.365: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar  2 01:25:02.365: INFO: Found 0 / 1
    Mar  2 01:25:03.395: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar  2 01:25:03.395: INFO: Found 0 / 1
    Mar  2 01:25:04.356: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar  2 01:25:04.356: INFO: Found 1 / 1
    Mar  2 01:25:04.356: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Mar  2 01:25:04.372: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar  2 01:25:04.372: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Mar  2 01:25:04.372: INFO: wait on agnhost-primary startup in kubectl-8911 
    Mar  2 01:25:04.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-8911 logs agnhost-primary-knl88 agnhost-primary'
    Mar  2 01:25:04.575: INFO: stderr: ""
    Mar  2 01:25:04.575: INFO: stdout: "Paused\n"
    STEP: exposing RC 03/02/23 01:25:04.575
    Mar  2 01:25:04.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-8911 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Mar  2 01:25:04.828: INFO: stderr: ""
    Mar  2 01:25:04.828: INFO: stdout: "service/rm2 exposed\n"
    Mar  2 01:25:04.846: INFO: Service rm2 in namespace kubectl-8911 found.
    STEP: exposing service 03/02/23 01:25:06.872
    Mar  2 01:25:06.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-8911 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Mar  2 01:25:07.053: INFO: stderr: ""
    Mar  2 01:25:07.053: INFO: stdout: "service/rm3 exposed\n"
    Mar  2 01:25:07.074: INFO: Service rm3 in namespace kubectl-8911 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  2 01:25:09.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8911" for this suite. 03/02/23 01:25:09.151
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:25:09.169
Mar  2 01:25:09.170: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename resourcequota 03/02/23 01:25:09.171
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:25:09.243
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:25:09.257
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
STEP: Creating a ResourceQuota 03/02/23 01:25:09.271
STEP: Getting a ResourceQuota 03/02/23 01:25:09.286
STEP: Listing all ResourceQuotas with LabelSelector 03/02/23 01:25:09.296
STEP: Patching the ResourceQuota 03/02/23 01:25:09.309
STEP: Deleting a Collection of ResourceQuotas 03/02/23 01:25:09.34
STEP: Verifying the deleted ResourceQuota 03/02/23 01:25:09.39
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar  2 01:25:09.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5817" for this suite. 03/02/23 01:25:09.446
{"msg":"PASSED [sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]","completed":60,"skipped":1118,"failed":0}
------------------------------
• [0.293 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:25:09.169
    Mar  2 01:25:09.170: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename resourcequota 03/02/23 01:25:09.171
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:25:09.243
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:25:09.257
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:933
    STEP: Creating a ResourceQuota 03/02/23 01:25:09.271
    STEP: Getting a ResourceQuota 03/02/23 01:25:09.286
    STEP: Listing all ResourceQuotas with LabelSelector 03/02/23 01:25:09.296
    STEP: Patching the ResourceQuota 03/02/23 01:25:09.309
    STEP: Deleting a Collection of ResourceQuotas 03/02/23 01:25:09.34
    STEP: Verifying the deleted ResourceQuota 03/02/23 01:25:09.39
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar  2 01:25:09.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-5817" for this suite. 03/02/23 01:25:09.446
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:25:09.463
Mar  2 01:25:09.463: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename deployment 03/02/23 01:25:09.464
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:25:09.533
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:25:09.542
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Mar  2 01:25:09.553: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
W0302 01:25:09.579025      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Mar  2 01:25:09.589: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar  2 01:25:14.608: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/02/23 01:25:14.608
Mar  2 01:25:14.608: INFO: Creating deployment "test-rolling-update-deployment"
Mar  2 01:25:14.626: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Mar  2 01:25:14.651: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Mar  2 01:25:16.681: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Mar  2 01:25:16.695: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 25, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 25, 14, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 25, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 25, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-78f575d8ff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 01:25:18.708: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar  2 01:25:18.767: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-2243  28b6cbe0-a825-4a4d-872a-9e7219edb783 79708 1 2023-03-02 01:25:14 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-03-02 01:25:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 01:25:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00bd81758 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-02 01:25:14 +0000 UTC,LastTransitionTime:2023-03-02 01:25:14 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-03-02 01:25:17 +0000 UTC,LastTransitionTime:2023-03-02 01:25:14 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar  2 01:25:18.789: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-2243  288eab6b-2dc2-4047-a08d-a698942521db 79697 1 2023-03-02 01:25:14 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 28b6cbe0-a825-4a4d-872a-9e7219edb783 0xc00bd81c77 0xc00bd81c78}] [] [{kube-controller-manager Update apps/v1 2023-03-02 01:25:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"28b6cbe0-a825-4a4d-872a-9e7219edb783\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 01:25:17 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00bd81d28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar  2 01:25:18.789: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Mar  2 01:25:18.789: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-2243  5c1cee7d-7a8c-41c1-8cce-656e6c01d71f 79707 2 2023-03-02 01:25:09 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 28b6cbe0-a825-4a4d-872a-9e7219edb783 0xc00bd81b47 0xc00bd81b48}] [] [{e2e.test Update apps/v1 2023-03-02 01:25:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 01:25:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"28b6cbe0-a825-4a4d-872a-9e7219edb783\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-03-02 01:25:17 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00bd81c08 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  2 01:25:18.839: INFO: Pod "test-rolling-update-deployment-78f575d8ff-5p6zc" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-5p6zc test-rolling-update-deployment-78f575d8ff- deployment-2243  5600b20d-281d-42fa-9a2f-20c264a30a37 79695 0 2023-03-02 01:25:14 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cni.projectcalico.org/containerID:789999a8a435db8a91fc555634633e2664ec9d313b36c9fe0d497cc9619d45b8 cni.projectcalico.org/podIP:172.30.156.99/32 cni.projectcalico.org/podIPs:172.30.156.99/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.156.99"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.156.99"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff 288eab6b-2dc2-4047-a08d-a698942521db 0xc00c296077 0xc00c296078}] [] [{kube-controller-manager Update v1 2023-03-02 01:25:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"288eab6b-2dc2-4047-a08d-a698942521db\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 01:25:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 01:25:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-02 01:25:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.156.99\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5wl9t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5wl9t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.132.92.143,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c38,c37,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-d745b,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:25:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:25:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:25:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:25:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.132.92.143,PodIP:172.30.156.99,StartTime:2023-03-02 01:25:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 01:25:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787,ContainerID:cri-o://1f3b704127f3b95125abc267a4123c973c750237a8ace8c5d42f660b44eb778f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.156.99,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar  2 01:25:18.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2243" for this suite. 03/02/23 01:25:18.862
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","completed":61,"skipped":1121,"failed":0}
------------------------------
• [SLOW TEST] [9.420 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:25:09.463
    Mar  2 01:25:09.463: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename deployment 03/02/23 01:25:09.464
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:25:09.533
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:25:09.542
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Mar  2 01:25:09.553: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    W0302 01:25:09.579025      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Mar  2 01:25:09.589: INFO: Pod name sample-pod: Found 0 pods out of 1
    Mar  2 01:25:14.608: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/02/23 01:25:14.608
    Mar  2 01:25:14.608: INFO: Creating deployment "test-rolling-update-deployment"
    Mar  2 01:25:14.626: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Mar  2 01:25:14.651: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Mar  2 01:25:16.681: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Mar  2 01:25:16.695: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 25, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 25, 14, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 25, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 25, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-78f575d8ff\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  2 01:25:18.708: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar  2 01:25:18.767: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-2243  28b6cbe0-a825-4a4d-872a-9e7219edb783 79708 1 2023-03-02 01:25:14 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-03-02 01:25:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 01:25:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00bd81758 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-02 01:25:14 +0000 UTC,LastTransitionTime:2023-03-02 01:25:14 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-03-02 01:25:17 +0000 UTC,LastTransitionTime:2023-03-02 01:25:14 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Mar  2 01:25:18.789: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-2243  288eab6b-2dc2-4047-a08d-a698942521db 79697 1 2023-03-02 01:25:14 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 28b6cbe0-a825-4a4d-872a-9e7219edb783 0xc00bd81c77 0xc00bd81c78}] [] [{kube-controller-manager Update apps/v1 2023-03-02 01:25:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"28b6cbe0-a825-4a4d-872a-9e7219edb783\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 01:25:17 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00bd81d28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar  2 01:25:18.789: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Mar  2 01:25:18.789: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-2243  5c1cee7d-7a8c-41c1-8cce-656e6c01d71f 79707 2 2023-03-02 01:25:09 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 28b6cbe0-a825-4a4d-872a-9e7219edb783 0xc00bd81b47 0xc00bd81b48}] [] [{e2e.test Update apps/v1 2023-03-02 01:25:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 01:25:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"28b6cbe0-a825-4a4d-872a-9e7219edb783\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-03-02 01:25:17 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00bd81c08 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar  2 01:25:18.839: INFO: Pod "test-rolling-update-deployment-78f575d8ff-5p6zc" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-5p6zc test-rolling-update-deployment-78f575d8ff- deployment-2243  5600b20d-281d-42fa-9a2f-20c264a30a37 79695 0 2023-03-02 01:25:14 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cni.projectcalico.org/containerID:789999a8a435db8a91fc555634633e2664ec9d313b36c9fe0d497cc9619d45b8 cni.projectcalico.org/podIP:172.30.156.99/32 cni.projectcalico.org/podIPs:172.30.156.99/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.156.99"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.156.99"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff 288eab6b-2dc2-4047-a08d-a698942521db 0xc00c296077 0xc00c296078}] [] [{kube-controller-manager Update v1 2023-03-02 01:25:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"288eab6b-2dc2-4047-a08d-a698942521db\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 01:25:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 01:25:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-02 01:25:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.156.99\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5wl9t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5wl9t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.132.92.143,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c38,c37,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-d745b,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:25:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:25:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:25:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:25:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.132.92.143,PodIP:172.30.156.99,StartTime:2023-03-02 01:25:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 01:25:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787,ContainerID:cri-o://1f3b704127f3b95125abc267a4123c973c750237a8ace8c5d42f660b44eb778f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.156.99,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar  2 01:25:18.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-2243" for this suite. 03/02/23 01:25:18.862
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:25:18.883
Mar  2 01:25:18.884: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename secrets 03/02/23 01:25:18.886
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:25:18.938
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:25:18.948
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
STEP: Creating secret with name secret-test-e90b18ae-7839-4514-9406-a5884e9da214 03/02/23 01:25:18.989
STEP: Creating a pod to test consume secrets 03/02/23 01:25:19.039
Mar  2 01:25:19.109: INFO: Waiting up to 5m0s for pod "pod-secrets-54079f31-97d5-4548-a713-7d49839c6ef3" in namespace "secrets-3095" to be "Succeeded or Failed"
Mar  2 01:25:19.139: INFO: Pod "pod-secrets-54079f31-97d5-4548-a713-7d49839c6ef3": Phase="Pending", Reason="", readiness=false. Elapsed: 29.433832ms
Mar  2 01:25:21.155: INFO: Pod "pod-secrets-54079f31-97d5-4548-a713-7d49839c6ef3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046087058s
Mar  2 01:25:23.156: INFO: Pod "pod-secrets-54079f31-97d5-4548-a713-7d49839c6ef3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046385292s
Mar  2 01:25:25.159: INFO: Pod "pod-secrets-54079f31-97d5-4548-a713-7d49839c6ef3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.049661122s
STEP: Saw pod success 03/02/23 01:25:25.159
Mar  2 01:25:25.159: INFO: Pod "pod-secrets-54079f31-97d5-4548-a713-7d49839c6ef3" satisfied condition "Succeeded or Failed"
Mar  2 01:25:25.181: INFO: Trying to get logs from node 10.132.92.143 pod pod-secrets-54079f31-97d5-4548-a713-7d49839c6ef3 container secret-volume-test: <nil>
STEP: delete the pod 03/02/23 01:25:25.214
Mar  2 01:25:25.275: INFO: Waiting for pod pod-secrets-54079f31-97d5-4548-a713-7d49839c6ef3 to disappear
Mar  2 01:25:25.305: INFO: Pod pod-secrets-54079f31-97d5-4548-a713-7d49839c6ef3 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar  2 01:25:25.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3095" for this suite. 03/02/23 01:25:25.341
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","completed":62,"skipped":1121,"failed":0}
------------------------------
• [SLOW TEST] [6.492 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:25:18.883
    Mar  2 01:25:18.884: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename secrets 03/02/23 01:25:18.886
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:25:18.938
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:25:18.948
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:46
    STEP: Creating secret with name secret-test-e90b18ae-7839-4514-9406-a5884e9da214 03/02/23 01:25:18.989
    STEP: Creating a pod to test consume secrets 03/02/23 01:25:19.039
    Mar  2 01:25:19.109: INFO: Waiting up to 5m0s for pod "pod-secrets-54079f31-97d5-4548-a713-7d49839c6ef3" in namespace "secrets-3095" to be "Succeeded or Failed"
    Mar  2 01:25:19.139: INFO: Pod "pod-secrets-54079f31-97d5-4548-a713-7d49839c6ef3": Phase="Pending", Reason="", readiness=false. Elapsed: 29.433832ms
    Mar  2 01:25:21.155: INFO: Pod "pod-secrets-54079f31-97d5-4548-a713-7d49839c6ef3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046087058s
    Mar  2 01:25:23.156: INFO: Pod "pod-secrets-54079f31-97d5-4548-a713-7d49839c6ef3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046385292s
    Mar  2 01:25:25.159: INFO: Pod "pod-secrets-54079f31-97d5-4548-a713-7d49839c6ef3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.049661122s
    STEP: Saw pod success 03/02/23 01:25:25.159
    Mar  2 01:25:25.159: INFO: Pod "pod-secrets-54079f31-97d5-4548-a713-7d49839c6ef3" satisfied condition "Succeeded or Failed"
    Mar  2 01:25:25.181: INFO: Trying to get logs from node 10.132.92.143 pod pod-secrets-54079f31-97d5-4548-a713-7d49839c6ef3 container secret-volume-test: <nil>
    STEP: delete the pod 03/02/23 01:25:25.214
    Mar  2 01:25:25.275: INFO: Waiting for pod pod-secrets-54079f31-97d5-4548-a713-7d49839c6ef3 to disappear
    Mar  2 01:25:25.305: INFO: Pod pod-secrets-54079f31-97d5-4548-a713-7d49839c6ef3 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar  2 01:25:25.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-3095" for this suite. 03/02/23 01:25:25.341
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:25:25.383
Mar  2 01:25:25.383: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename replication-controller 03/02/23 01:25:25.385
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:25:25.449
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:25:25.478
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
Mar  2 01:25:25.491: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 03/02/23 01:25:26.567
STEP: Checking rc "condition-test" has the desired failure condition set 03/02/23 01:25:26.586
STEP: Scaling down rc "condition-test" to satisfy pod quota 03/02/23 01:25:27.615
Mar  2 01:25:27.657: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 03/02/23 01:25:27.657
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Mar  2 01:25:28.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-34" for this suite. 03/02/23 01:25:28.701
{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","completed":63,"skipped":1151,"failed":0}
------------------------------
• [3.339 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:25:25.383
    Mar  2 01:25:25.383: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename replication-controller 03/02/23 01:25:25.385
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:25:25.449
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:25:25.478
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:82
    Mar  2 01:25:25.491: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 03/02/23 01:25:26.567
    STEP: Checking rc "condition-test" has the desired failure condition set 03/02/23 01:25:26.586
    STEP: Scaling down rc "condition-test" to satisfy pod quota 03/02/23 01:25:27.615
    Mar  2 01:25:27.657: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 03/02/23 01:25:27.657
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Mar  2 01:25:28.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-34" for this suite. 03/02/23 01:25:28.701
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:25:28.724
Mar  2 01:25:28.724: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename configmap 03/02/23 01:25:28.726
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:25:28.771
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:25:28.783
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
Mar  2 01:25:28.814: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-c7968291-5b4e-48a4-ab2e-8877653a74b2 03/02/23 01:25:28.814
STEP: Creating configMap with name cm-test-opt-upd-f4c4b75e-a716-48c7-8ecf-eae98a7e5a69 03/02/23 01:25:28.835
STEP: Creating the pod 03/02/23 01:25:28.87
Mar  2 01:25:28.950: INFO: Waiting up to 5m0s for pod "pod-configmaps-cae92fa0-6114-4f82-9cec-08848f187a7e" in namespace "configmap-1700" to be "running and ready"
Mar  2 01:25:28.986: INFO: Pod "pod-configmaps-cae92fa0-6114-4f82-9cec-08848f187a7e": Phase="Pending", Reason="", readiness=false. Elapsed: 35.82636ms
Mar  2 01:25:28.986: INFO: The phase of Pod pod-configmaps-cae92fa0-6114-4f82-9cec-08848f187a7e is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:25:31.034: INFO: Pod "pod-configmaps-cae92fa0-6114-4f82-9cec-08848f187a7e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.083498779s
Mar  2 01:25:31.034: INFO: The phase of Pod pod-configmaps-cae92fa0-6114-4f82-9cec-08848f187a7e is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:25:33.005: INFO: Pod "pod-configmaps-cae92fa0-6114-4f82-9cec-08848f187a7e": Phase="Running", Reason="", readiness=true. Elapsed: 4.05470996s
Mar  2 01:25:33.005: INFO: The phase of Pod pod-configmaps-cae92fa0-6114-4f82-9cec-08848f187a7e is Running (Ready = true)
Mar  2 01:25:33.005: INFO: Pod "pod-configmaps-cae92fa0-6114-4f82-9cec-08848f187a7e" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-c7968291-5b4e-48a4-ab2e-8877653a74b2 03/02/23 01:25:33.133
STEP: Updating configmap cm-test-opt-upd-f4c4b75e-a716-48c7-8ecf-eae98a7e5a69 03/02/23 01:25:33.163
STEP: Creating configMap with name cm-test-opt-create-1269049e-72fe-4d43-afc3-f8e009476150 03/02/23 01:25:33.184
STEP: waiting to observe update in volume 03/02/23 01:25:33.209
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar  2 01:26:53.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1700" for this suite. 03/02/23 01:26:53.134
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":64,"skipped":1170,"failed":0}
------------------------------
• [SLOW TEST] [84.431 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:25:28.724
    Mar  2 01:25:28.724: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename configmap 03/02/23 01:25:28.726
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:25:28.771
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:25:28.783
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:239
    Mar  2 01:25:28.814: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating configMap with name cm-test-opt-del-c7968291-5b4e-48a4-ab2e-8877653a74b2 03/02/23 01:25:28.814
    STEP: Creating configMap with name cm-test-opt-upd-f4c4b75e-a716-48c7-8ecf-eae98a7e5a69 03/02/23 01:25:28.835
    STEP: Creating the pod 03/02/23 01:25:28.87
    Mar  2 01:25:28.950: INFO: Waiting up to 5m0s for pod "pod-configmaps-cae92fa0-6114-4f82-9cec-08848f187a7e" in namespace "configmap-1700" to be "running and ready"
    Mar  2 01:25:28.986: INFO: Pod "pod-configmaps-cae92fa0-6114-4f82-9cec-08848f187a7e": Phase="Pending", Reason="", readiness=false. Elapsed: 35.82636ms
    Mar  2 01:25:28.986: INFO: The phase of Pod pod-configmaps-cae92fa0-6114-4f82-9cec-08848f187a7e is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 01:25:31.034: INFO: Pod "pod-configmaps-cae92fa0-6114-4f82-9cec-08848f187a7e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.083498779s
    Mar  2 01:25:31.034: INFO: The phase of Pod pod-configmaps-cae92fa0-6114-4f82-9cec-08848f187a7e is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 01:25:33.005: INFO: Pod "pod-configmaps-cae92fa0-6114-4f82-9cec-08848f187a7e": Phase="Running", Reason="", readiness=true. Elapsed: 4.05470996s
    Mar  2 01:25:33.005: INFO: The phase of Pod pod-configmaps-cae92fa0-6114-4f82-9cec-08848f187a7e is Running (Ready = true)
    Mar  2 01:25:33.005: INFO: Pod "pod-configmaps-cae92fa0-6114-4f82-9cec-08848f187a7e" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-c7968291-5b4e-48a4-ab2e-8877653a74b2 03/02/23 01:25:33.133
    STEP: Updating configmap cm-test-opt-upd-f4c4b75e-a716-48c7-8ecf-eae98a7e5a69 03/02/23 01:25:33.163
    STEP: Creating configMap with name cm-test-opt-create-1269049e-72fe-4d43-afc3-f8e009476150 03/02/23 01:25:33.184
    STEP: waiting to observe update in volume 03/02/23 01:25:33.209
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  2 01:26:53.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1700" for this suite. 03/02/23 01:26:53.134
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:26:53.172
Mar  2 01:26:53.172: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename daemonsets 03/02/23 01:26:53.173
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:26:53.23
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:26:53.242
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
Mar  2 01:26:53.364: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 03/02/23 01:26:53.389
Mar  2 01:26:53.408: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 01:26:53.408: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 03/02/23 01:26:53.408
Mar  2 01:26:53.562: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 01:26:53.562: INFO: Node 10.132.92.188 is running 0 daemon pod, expected 1
Mar  2 01:26:54.579: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 01:26:54.579: INFO: Node 10.132.92.188 is running 0 daemon pod, expected 1
Mar  2 01:26:55.582: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 01:26:55.582: INFO: Node 10.132.92.188 is running 0 daemon pod, expected 1
Mar  2 01:26:56.590: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar  2 01:26:56.591: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 03/02/23 01:26:56.607
Mar  2 01:26:56.694: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar  2 01:26:56.694: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Mar  2 01:26:57.718: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 01:26:57.718: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 03/02/23 01:26:57.718
Mar  2 01:26:57.751: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 01:26:57.751: INFO: Node 10.132.92.188 is running 0 daemon pod, expected 1
Mar  2 01:26:58.788: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 01:26:58.788: INFO: Node 10.132.92.188 is running 0 daemon pod, expected 1
Mar  2 01:26:59.773: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 01:26:59.773: INFO: Node 10.132.92.188 is running 0 daemon pod, expected 1
Mar  2 01:27:00.773: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 01:27:00.773: INFO: Node 10.132.92.188 is running 0 daemon pod, expected 1
Mar  2 01:27:01.770: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 01:27:01.770: INFO: Node 10.132.92.188 is running 0 daemon pod, expected 1
Mar  2 01:27:02.811: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar  2 01:27:02.811: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 03/02/23 01:27:02.85
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4441, will wait for the garbage collector to delete the pods 03/02/23 01:27:02.85
Mar  2 01:27:02.985: INFO: Deleting DaemonSet.extensions daemon-set took: 54.056993ms
Mar  2 01:27:03.186: INFO: Terminating DaemonSet.extensions daemon-set pods took: 201.171359ms
Mar  2 01:27:07.301: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 01:27:07.301: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar  2 01:27:07.315: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"80762"},"items":null}

Mar  2 01:27:07.329: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"80762"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Mar  2 01:27:07.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4441" for this suite. 03/02/23 01:27:07.462
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","completed":65,"skipped":1246,"failed":0}
------------------------------
• [SLOW TEST] [14.316 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:26:53.172
    Mar  2 01:26:53.172: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename daemonsets 03/02/23 01:26:53.173
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:26:53.23
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:26:53.242
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:193
    Mar  2 01:26:53.364: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 03/02/23 01:26:53.389
    Mar  2 01:26:53.408: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 01:26:53.408: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 03/02/23 01:26:53.408
    Mar  2 01:26:53.562: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 01:26:53.562: INFO: Node 10.132.92.188 is running 0 daemon pod, expected 1
    Mar  2 01:26:54.579: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 01:26:54.579: INFO: Node 10.132.92.188 is running 0 daemon pod, expected 1
    Mar  2 01:26:55.582: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 01:26:55.582: INFO: Node 10.132.92.188 is running 0 daemon pod, expected 1
    Mar  2 01:26:56.590: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar  2 01:26:56.591: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 03/02/23 01:26:56.607
    Mar  2 01:26:56.694: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar  2 01:26:56.694: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Mar  2 01:26:57.718: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 01:26:57.718: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 03/02/23 01:26:57.718
    Mar  2 01:26:57.751: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 01:26:57.751: INFO: Node 10.132.92.188 is running 0 daemon pod, expected 1
    Mar  2 01:26:58.788: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 01:26:58.788: INFO: Node 10.132.92.188 is running 0 daemon pod, expected 1
    Mar  2 01:26:59.773: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 01:26:59.773: INFO: Node 10.132.92.188 is running 0 daemon pod, expected 1
    Mar  2 01:27:00.773: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 01:27:00.773: INFO: Node 10.132.92.188 is running 0 daemon pod, expected 1
    Mar  2 01:27:01.770: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 01:27:01.770: INFO: Node 10.132.92.188 is running 0 daemon pod, expected 1
    Mar  2 01:27:02.811: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar  2 01:27:02.811: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 03/02/23 01:27:02.85
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4441, will wait for the garbage collector to delete the pods 03/02/23 01:27:02.85
    Mar  2 01:27:02.985: INFO: Deleting DaemonSet.extensions daemon-set took: 54.056993ms
    Mar  2 01:27:03.186: INFO: Terminating DaemonSet.extensions daemon-set pods took: 201.171359ms
    Mar  2 01:27:07.301: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 01:27:07.301: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar  2 01:27:07.315: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"80762"},"items":null}

    Mar  2 01:27:07.329: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"80762"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Mar  2 01:27:07.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-4441" for this suite. 03/02/23 01:27:07.462
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:27:07.49
Mar  2 01:27:07.491: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename dns 03/02/23 01:27:07.492
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:27:07.539
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:27:07.558
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 03/02/23 01:27:07.586
Mar  2 01:27:07.666: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-9379  5ee8e959-813f-42d1-8091-54700d71c5b9 80784 0 2023-03-02 01:27:07 +0000 UTC <nil> <nil> map[] map[openshift.io/scc:anyuid] [] [] [{e2e.test Update v1 2023-03-02 01:27:07 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-96hfc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-96hfc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c39,c24,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 01:27:07.666: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-9379" to be "running and ready"
Mar  2 01:27:07.710: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 44.307194ms
Mar  2 01:27:07.711: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:27:09.726: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 2.060414804s
Mar  2 01:27:09.726: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:27:11.726: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 4.060271204s
Mar  2 01:27:11.726: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Mar  2 01:27:11.726: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 03/02/23 01:27:11.726
Mar  2 01:27:11.727: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-9379 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 01:27:11.727: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
Mar  2 01:27:11.728: INFO: ExecWithOptions: Clientset creation
Mar  2 01:27:11.728: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/dns-9379/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 03/02/23 01:27:12.021
Mar  2 01:27:12.021: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-9379 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 01:27:12.021: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
Mar  2 01:27:12.022: INFO: ExecWithOptions: Clientset creation
Mar  2 01:27:12.022: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/dns-9379/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar  2 01:27:12.332: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar  2 01:27:12.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9379" for this suite. 03/02/23 01:27:12.449
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","completed":66,"skipped":1253,"failed":0}
------------------------------
• [4.975 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:27:07.49
    Mar  2 01:27:07.491: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename dns 03/02/23 01:27:07.492
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:27:07.539
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:27:07.558
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 03/02/23 01:27:07.586
    Mar  2 01:27:07.666: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-9379  5ee8e959-813f-42d1-8091-54700d71c5b9 80784 0 2023-03-02 01:27:07 +0000 UTC <nil> <nil> map[] map[openshift.io/scc:anyuid] [] [] [{e2e.test Update v1 2023-03-02 01:27:07 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-96hfc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-96hfc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c39,c24,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 01:27:07.666: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-9379" to be "running and ready"
    Mar  2 01:27:07.710: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 44.307194ms
    Mar  2 01:27:07.711: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 01:27:09.726: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 2.060414804s
    Mar  2 01:27:09.726: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 01:27:11.726: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 4.060271204s
    Mar  2 01:27:11.726: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Mar  2 01:27:11.726: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 03/02/23 01:27:11.726
    Mar  2 01:27:11.727: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-9379 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 01:27:11.727: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    Mar  2 01:27:11.728: INFO: ExecWithOptions: Clientset creation
    Mar  2 01:27:11.728: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/dns-9379/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 03/02/23 01:27:12.021
    Mar  2 01:27:12.021: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-9379 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 01:27:12.021: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    Mar  2 01:27:12.022: INFO: ExecWithOptions: Clientset creation
    Mar  2 01:27:12.022: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/dns-9379/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar  2 01:27:12.332: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar  2 01:27:12.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-9379" for this suite. 03/02/23 01:27:12.449
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:27:12.469
Mar  2 01:27:12.469: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename kubectl 03/02/23 01:27:12.47
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:27:12.523
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:27:12.534
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1492
STEP: creating the pod 03/02/23 01:27:12.542
Mar  2 01:27:12.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-6374 create -f -'
Mar  2 01:27:13.366: INFO: stderr: ""
Mar  2 01:27:13.366: INFO: stdout: "pod/pause created\n"
Mar  2 01:27:13.366: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Mar  2 01:27:13.366: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-6374" to be "running and ready"
Mar  2 01:27:13.386: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 19.202807ms
Mar  2 01:27:13.386: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '' to be 'Running' but was 'Pending'
Mar  2 01:27:15.425: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058989301s
Mar  2 01:27:15.425: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '10.132.92.143' to be 'Running' but was 'Pending'
Mar  2 01:27:17.420: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.053459146s
Mar  2 01:27:17.420: INFO: Pod "pause" satisfied condition "running and ready"
Mar  2 01:27:17.420: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
STEP: adding the label testing-label with value testing-label-value to a pod 03/02/23 01:27:17.42
Mar  2 01:27:17.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-6374 label pods pause testing-label=testing-label-value'
Mar  2 01:27:17.625: INFO: stderr: ""
Mar  2 01:27:17.625: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 03/02/23 01:27:17.625
Mar  2 01:27:17.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-6374 get pod pause -L testing-label'
Mar  2 01:27:17.756: INFO: stderr: ""
Mar  2 01:27:17.756: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod 03/02/23 01:27:17.756
Mar  2 01:27:17.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-6374 label pods pause testing-label-'
Mar  2 01:27:17.931: INFO: stderr: ""
Mar  2 01:27:17.931: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 03/02/23 01:27:17.931
Mar  2 01:27:17.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-6374 get pod pause -L testing-label'
Mar  2 01:27:18.054: INFO: stderr: ""
Mar  2 01:27:18.054: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1498
STEP: using delete to clean up resources 03/02/23 01:27:18.054
Mar  2 01:27:18.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-6374 delete --grace-period=0 --force -f -'
Mar  2 01:27:18.211: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 01:27:18.211: INFO: stdout: "pod \"pause\" force deleted\n"
Mar  2 01:27:18.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-6374 get rc,svc -l name=pause --no-headers'
Mar  2 01:27:18.391: INFO: stderr: "No resources found in kubectl-6374 namespace.\n"
Mar  2 01:27:18.391: INFO: stdout: ""
Mar  2 01:27:18.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-6374 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  2 01:27:18.543: INFO: stderr: ""
Mar  2 01:27:18.543: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  2 01:27:18.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6374" for this suite. 03/02/23 01:27:18.572
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","completed":67,"skipped":1305,"failed":0}
------------------------------
• [SLOW TEST] [6.123 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1490
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:27:12.469
    Mar  2 01:27:12.469: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename kubectl 03/02/23 01:27:12.47
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:27:12.523
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:27:12.534
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1492
    STEP: creating the pod 03/02/23 01:27:12.542
    Mar  2 01:27:12.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-6374 create -f -'
    Mar  2 01:27:13.366: INFO: stderr: ""
    Mar  2 01:27:13.366: INFO: stdout: "pod/pause created\n"
    Mar  2 01:27:13.366: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Mar  2 01:27:13.366: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-6374" to be "running and ready"
    Mar  2 01:27:13.386: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 19.202807ms
    Mar  2 01:27:13.386: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '' to be 'Running' but was 'Pending'
    Mar  2 01:27:15.425: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058989301s
    Mar  2 01:27:15.425: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '10.132.92.143' to be 'Running' but was 'Pending'
    Mar  2 01:27:17.420: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.053459146s
    Mar  2 01:27:17.420: INFO: Pod "pause" satisfied condition "running and ready"
    Mar  2 01:27:17.420: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1507
    STEP: adding the label testing-label with value testing-label-value to a pod 03/02/23 01:27:17.42
    Mar  2 01:27:17.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-6374 label pods pause testing-label=testing-label-value'
    Mar  2 01:27:17.625: INFO: stderr: ""
    Mar  2 01:27:17.625: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 03/02/23 01:27:17.625
    Mar  2 01:27:17.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-6374 get pod pause -L testing-label'
    Mar  2 01:27:17.756: INFO: stderr: ""
    Mar  2 01:27:17.756: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 03/02/23 01:27:17.756
    Mar  2 01:27:17.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-6374 label pods pause testing-label-'
    Mar  2 01:27:17.931: INFO: stderr: ""
    Mar  2 01:27:17.931: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 03/02/23 01:27:17.931
    Mar  2 01:27:17.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-6374 get pod pause -L testing-label'
    Mar  2 01:27:18.054: INFO: stderr: ""
    Mar  2 01:27:18.054: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1498
    STEP: using delete to clean up resources 03/02/23 01:27:18.054
    Mar  2 01:27:18.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-6374 delete --grace-period=0 --force -f -'
    Mar  2 01:27:18.211: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar  2 01:27:18.211: INFO: stdout: "pod \"pause\" force deleted\n"
    Mar  2 01:27:18.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-6374 get rc,svc -l name=pause --no-headers'
    Mar  2 01:27:18.391: INFO: stderr: "No resources found in kubectl-6374 namespace.\n"
    Mar  2 01:27:18.391: INFO: stdout: ""
    Mar  2 01:27:18.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-6374 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Mar  2 01:27:18.543: INFO: stderr: ""
    Mar  2 01:27:18.543: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  2 01:27:18.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6374" for this suite. 03/02/23 01:27:18.572
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:27:18.594
Mar  2 01:27:18.594: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename events 03/02/23 01:27:18.595
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:27:18.646
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:27:18.659
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 03/02/23 01:27:18.707
Mar  2 01:27:18.730: INFO: created test-event-1
Mar  2 01:27:18.805: INFO: created test-event-2
Mar  2 01:27:18.827: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 03/02/23 01:27:18.827
STEP: delete collection of events 03/02/23 01:27:18.844
Mar  2 01:27:18.845: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 03/02/23 01:27:18.963
Mar  2 01:27:18.963: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Mar  2 01:27:18.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7748" for this suite. 03/02/23 01:27:18.995
{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","completed":68,"skipped":1326,"failed":0}
------------------------------
• [0.420 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:27:18.594
    Mar  2 01:27:18.594: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename events 03/02/23 01:27:18.595
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:27:18.646
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:27:18.659
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 03/02/23 01:27:18.707
    Mar  2 01:27:18.730: INFO: created test-event-1
    Mar  2 01:27:18.805: INFO: created test-event-2
    Mar  2 01:27:18.827: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 03/02/23 01:27:18.827
    STEP: delete collection of events 03/02/23 01:27:18.844
    Mar  2 01:27:18.845: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 03/02/23 01:27:18.963
    Mar  2 01:27:18.963: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Mar  2 01:27:18.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-7748" for this suite. 03/02/23 01:27:18.995
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:27:19.018
Mar  2 01:27:19.018: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename containers 03/02/23 01:27:19.019
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:27:19.151
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:27:19.161
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
STEP: Creating a pod to test override arguments 03/02/23 01:27:19.175
W0302 01:27:19.392932      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Mar  2 01:27:19.393: INFO: Waiting up to 5m0s for pod "client-containers-f0385faa-bfa0-4204-973b-04983179f9b5" in namespace "containers-8305" to be "Succeeded or Failed"
Mar  2 01:27:19.443: INFO: Pod "client-containers-f0385faa-bfa0-4204-973b-04983179f9b5": Phase="Pending", Reason="", readiness=false. Elapsed: 49.697336ms
Mar  2 01:27:21.468: INFO: Pod "client-containers-f0385faa-bfa0-4204-973b-04983179f9b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.074526294s
Mar  2 01:27:23.461: INFO: Pod "client-containers-f0385faa-bfa0-4204-973b-04983179f9b5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.067574611s
Mar  2 01:27:25.458: INFO: Pod "client-containers-f0385faa-bfa0-4204-973b-04983179f9b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.06435425s
STEP: Saw pod success 03/02/23 01:27:25.458
Mar  2 01:27:25.458: INFO: Pod "client-containers-f0385faa-bfa0-4204-973b-04983179f9b5" satisfied condition "Succeeded or Failed"
Mar  2 01:27:25.474: INFO: Trying to get logs from node 10.132.92.143 pod client-containers-f0385faa-bfa0-4204-973b-04983179f9b5 container agnhost-container: <nil>
STEP: delete the pod 03/02/23 01:27:25.509
Mar  2 01:27:25.565: INFO: Waiting for pod client-containers-f0385faa-bfa0-4204-973b-04983179f9b5 to disappear
Mar  2 01:27:25.582: INFO: Pod client-containers-f0385faa-bfa0-4204-973b-04983179f9b5 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Mar  2 01:27:25.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8305" for this suite. 03/02/23 01:27:25.602
{"msg":"PASSED [sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]","completed":69,"skipped":1342,"failed":0}
------------------------------
• [SLOW TEST] [6.603 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:27:19.018
    Mar  2 01:27:19.018: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename containers 03/02/23 01:27:19.019
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:27:19.151
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:27:19.161
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:58
    STEP: Creating a pod to test override arguments 03/02/23 01:27:19.175
    W0302 01:27:19.392932      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Mar  2 01:27:19.393: INFO: Waiting up to 5m0s for pod "client-containers-f0385faa-bfa0-4204-973b-04983179f9b5" in namespace "containers-8305" to be "Succeeded or Failed"
    Mar  2 01:27:19.443: INFO: Pod "client-containers-f0385faa-bfa0-4204-973b-04983179f9b5": Phase="Pending", Reason="", readiness=false. Elapsed: 49.697336ms
    Mar  2 01:27:21.468: INFO: Pod "client-containers-f0385faa-bfa0-4204-973b-04983179f9b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.074526294s
    Mar  2 01:27:23.461: INFO: Pod "client-containers-f0385faa-bfa0-4204-973b-04983179f9b5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.067574611s
    Mar  2 01:27:25.458: INFO: Pod "client-containers-f0385faa-bfa0-4204-973b-04983179f9b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.06435425s
    STEP: Saw pod success 03/02/23 01:27:25.458
    Mar  2 01:27:25.458: INFO: Pod "client-containers-f0385faa-bfa0-4204-973b-04983179f9b5" satisfied condition "Succeeded or Failed"
    Mar  2 01:27:25.474: INFO: Trying to get logs from node 10.132.92.143 pod client-containers-f0385faa-bfa0-4204-973b-04983179f9b5 container agnhost-container: <nil>
    STEP: delete the pod 03/02/23 01:27:25.509
    Mar  2 01:27:25.565: INFO: Waiting for pod client-containers-f0385faa-bfa0-4204-973b-04983179f9b5 to disappear
    Mar  2 01:27:25.582: INFO: Pod client-containers-f0385faa-bfa0-4204-973b-04983179f9b5 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Mar  2 01:27:25.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-8305" for this suite. 03/02/23 01:27:25.602
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:27:25.625
Mar  2 01:27:25.625: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename podtemplate 03/02/23 01:27:25.626
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:27:25.723
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:27:25.775
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Mar  2 01:27:25.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-2809" for this suite. 03/02/23 01:27:25.996
{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","completed":70,"skipped":1372,"failed":0}
------------------------------
• [0.419 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:27:25.625
    Mar  2 01:27:25.625: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename podtemplate 03/02/23 01:27:25.626
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:27:25.723
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:27:25.775
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Mar  2 01:27:25.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-2809" for this suite. 03/02/23 01:27:25.996
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:27:26.047
Mar  2 01:27:26.048: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename configmap 03/02/23 01:27:26.049
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:27:26.164
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:27:26.183
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
STEP: Creating configMap with name configmap-test-volume-108a917a-4c6c-4023-831a-720d36fc5057 03/02/23 01:27:26.223
STEP: Creating a pod to test consume configMaps 03/02/23 01:27:26.246
Mar  2 01:27:26.346: INFO: Waiting up to 5m0s for pod "pod-configmaps-d666e067-ea57-41ab-9767-59b382cdd70b" in namespace "configmap-4280" to be "Succeeded or Failed"
Mar  2 01:27:26.375: INFO: Pod "pod-configmaps-d666e067-ea57-41ab-9767-59b382cdd70b": Phase="Pending", Reason="", readiness=false. Elapsed: 29.726432ms
Mar  2 01:27:28.390: INFO: Pod "pod-configmaps-d666e067-ea57-41ab-9767-59b382cdd70b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044453039s
Mar  2 01:27:30.421: INFO: Pod "pod-configmaps-d666e067-ea57-41ab-9767-59b382cdd70b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.075440136s
STEP: Saw pod success 03/02/23 01:27:30.421
Mar  2 01:27:30.422: INFO: Pod "pod-configmaps-d666e067-ea57-41ab-9767-59b382cdd70b" satisfied condition "Succeeded or Failed"
Mar  2 01:27:30.474: INFO: Trying to get logs from node 10.132.92.143 pod pod-configmaps-d666e067-ea57-41ab-9767-59b382cdd70b container agnhost-container: <nil>
STEP: delete the pod 03/02/23 01:27:30.565
Mar  2 01:27:30.632: INFO: Waiting for pod pod-configmaps-d666e067-ea57-41ab-9767-59b382cdd70b to disappear
Mar  2 01:27:30.645: INFO: Pod pod-configmaps-d666e067-ea57-41ab-9767-59b382cdd70b no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar  2 01:27:30.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4280" for this suite. 03/02/23 01:27:30.679
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":71,"skipped":1392,"failed":0}
------------------------------
• [4.649 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:27:26.047
    Mar  2 01:27:26.048: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename configmap 03/02/23 01:27:26.049
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:27:26.164
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:27:26.183
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:56
    STEP: Creating configMap with name configmap-test-volume-108a917a-4c6c-4023-831a-720d36fc5057 03/02/23 01:27:26.223
    STEP: Creating a pod to test consume configMaps 03/02/23 01:27:26.246
    Mar  2 01:27:26.346: INFO: Waiting up to 5m0s for pod "pod-configmaps-d666e067-ea57-41ab-9767-59b382cdd70b" in namespace "configmap-4280" to be "Succeeded or Failed"
    Mar  2 01:27:26.375: INFO: Pod "pod-configmaps-d666e067-ea57-41ab-9767-59b382cdd70b": Phase="Pending", Reason="", readiness=false. Elapsed: 29.726432ms
    Mar  2 01:27:28.390: INFO: Pod "pod-configmaps-d666e067-ea57-41ab-9767-59b382cdd70b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044453039s
    Mar  2 01:27:30.421: INFO: Pod "pod-configmaps-d666e067-ea57-41ab-9767-59b382cdd70b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.075440136s
    STEP: Saw pod success 03/02/23 01:27:30.421
    Mar  2 01:27:30.422: INFO: Pod "pod-configmaps-d666e067-ea57-41ab-9767-59b382cdd70b" satisfied condition "Succeeded or Failed"
    Mar  2 01:27:30.474: INFO: Trying to get logs from node 10.132.92.143 pod pod-configmaps-d666e067-ea57-41ab-9767-59b382cdd70b container agnhost-container: <nil>
    STEP: delete the pod 03/02/23 01:27:30.565
    Mar  2 01:27:30.632: INFO: Waiting for pod pod-configmaps-d666e067-ea57-41ab-9767-59b382cdd70b to disappear
    Mar  2 01:27:30.645: INFO: Pod pod-configmaps-d666e067-ea57-41ab-9767-59b382cdd70b no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  2 01:27:30.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4280" for this suite. 03/02/23 01:27:30.679
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:27:30.697
Mar  2 01:27:30.706: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename projected 03/02/23 01:27:30.708
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:27:30.784
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:27:30.796
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
STEP: Creating the pod 03/02/23 01:27:30.822
Mar  2 01:27:31.001: INFO: Waiting up to 5m0s for pod "annotationupdate8a9f2ebe-f0a4-4712-9fa9-007fbf1d5a46" in namespace "projected-8740" to be "running and ready"
Mar  2 01:27:31.075: INFO: Pod "annotationupdate8a9f2ebe-f0a4-4712-9fa9-007fbf1d5a46": Phase="Pending", Reason="", readiness=false. Elapsed: 74.796428ms
Mar  2 01:27:31.075: INFO: The phase of Pod annotationupdate8a9f2ebe-f0a4-4712-9fa9-007fbf1d5a46 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:27:33.097: INFO: Pod "annotationupdate8a9f2ebe-f0a4-4712-9fa9-007fbf1d5a46": Phase="Pending", Reason="", readiness=false. Elapsed: 2.096096932s
Mar  2 01:27:33.097: INFO: The phase of Pod annotationupdate8a9f2ebe-f0a4-4712-9fa9-007fbf1d5a46 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:27:35.095: INFO: Pod "annotationupdate8a9f2ebe-f0a4-4712-9fa9-007fbf1d5a46": Phase="Running", Reason="", readiness=true. Elapsed: 4.09398569s
Mar  2 01:27:35.095: INFO: The phase of Pod annotationupdate8a9f2ebe-f0a4-4712-9fa9-007fbf1d5a46 is Running (Ready = true)
Mar  2 01:27:35.095: INFO: Pod "annotationupdate8a9f2ebe-f0a4-4712-9fa9-007fbf1d5a46" satisfied condition "running and ready"
Mar  2 01:27:35.868: INFO: Successfully updated pod "annotationupdate8a9f2ebe-f0a4-4712-9fa9-007fbf1d5a46"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar  2 01:27:37.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8740" for this suite. 03/02/23 01:27:37.99
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","completed":72,"skipped":1393,"failed":0}
------------------------------
• [SLOW TEST] [7.317 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:27:30.697
    Mar  2 01:27:30.706: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename projected 03/02/23 01:27:30.708
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:27:30.784
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:27:30.796
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:161
    STEP: Creating the pod 03/02/23 01:27:30.822
    Mar  2 01:27:31.001: INFO: Waiting up to 5m0s for pod "annotationupdate8a9f2ebe-f0a4-4712-9fa9-007fbf1d5a46" in namespace "projected-8740" to be "running and ready"
    Mar  2 01:27:31.075: INFO: Pod "annotationupdate8a9f2ebe-f0a4-4712-9fa9-007fbf1d5a46": Phase="Pending", Reason="", readiness=false. Elapsed: 74.796428ms
    Mar  2 01:27:31.075: INFO: The phase of Pod annotationupdate8a9f2ebe-f0a4-4712-9fa9-007fbf1d5a46 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 01:27:33.097: INFO: Pod "annotationupdate8a9f2ebe-f0a4-4712-9fa9-007fbf1d5a46": Phase="Pending", Reason="", readiness=false. Elapsed: 2.096096932s
    Mar  2 01:27:33.097: INFO: The phase of Pod annotationupdate8a9f2ebe-f0a4-4712-9fa9-007fbf1d5a46 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 01:27:35.095: INFO: Pod "annotationupdate8a9f2ebe-f0a4-4712-9fa9-007fbf1d5a46": Phase="Running", Reason="", readiness=true. Elapsed: 4.09398569s
    Mar  2 01:27:35.095: INFO: The phase of Pod annotationupdate8a9f2ebe-f0a4-4712-9fa9-007fbf1d5a46 is Running (Ready = true)
    Mar  2 01:27:35.095: INFO: Pod "annotationupdate8a9f2ebe-f0a4-4712-9fa9-007fbf1d5a46" satisfied condition "running and ready"
    Mar  2 01:27:35.868: INFO: Successfully updated pod "annotationupdate8a9f2ebe-f0a4-4712-9fa9-007fbf1d5a46"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar  2 01:27:37.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8740" for this suite. 03/02/23 01:27:37.99
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:27:38.018
Mar  2 01:27:38.018: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename projected 03/02/23 01:27:38.019
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:27:38.077
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:27:38.087
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
STEP: Creating projection with secret that has name projected-secret-test-map-eda731c4-a2e6-49c4-8426-b186d38c5221 03/02/23 01:27:38.099
STEP: Creating a pod to test consume secrets 03/02/23 01:27:38.123
Mar  2 01:27:38.180: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-be244de1-9d0f-428a-83c0-3137e99cc829" in namespace "projected-799" to be "Succeeded or Failed"
Mar  2 01:27:38.201: INFO: Pod "pod-projected-secrets-be244de1-9d0f-428a-83c0-3137e99cc829": Phase="Pending", Reason="", readiness=false. Elapsed: 20.929407ms
Mar  2 01:27:40.230: INFO: Pod "pod-projected-secrets-be244de1-9d0f-428a-83c0-3137e99cc829": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049694081s
Mar  2 01:27:42.224: INFO: Pod "pod-projected-secrets-be244de1-9d0f-428a-83c0-3137e99cc829": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044441933s
Mar  2 01:27:44.247: INFO: Pod "pod-projected-secrets-be244de1-9d0f-428a-83c0-3137e99cc829": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.067380085s
STEP: Saw pod success 03/02/23 01:27:44.247
Mar  2 01:27:44.248: INFO: Pod "pod-projected-secrets-be244de1-9d0f-428a-83c0-3137e99cc829" satisfied condition "Succeeded or Failed"
Mar  2 01:27:44.267: INFO: Trying to get logs from node 10.132.92.143 pod pod-projected-secrets-be244de1-9d0f-428a-83c0-3137e99cc829 container projected-secret-volume-test: <nil>
STEP: delete the pod 03/02/23 01:27:44.384
Mar  2 01:27:44.493: INFO: Waiting for pod pod-projected-secrets-be244de1-9d0f-428a-83c0-3137e99cc829 to disappear
Mar  2 01:27:44.513: INFO: Pod pod-projected-secrets-be244de1-9d0f-428a-83c0-3137e99cc829 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Mar  2 01:27:44.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-799" for this suite. 03/02/23 01:27:44.54
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":73,"skipped":1446,"failed":0}
------------------------------
• [SLOW TEST] [6.542 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:27:38.018
    Mar  2 01:27:38.018: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename projected 03/02/23 01:27:38.019
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:27:38.077
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:27:38.087
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:77
    STEP: Creating projection with secret that has name projected-secret-test-map-eda731c4-a2e6-49c4-8426-b186d38c5221 03/02/23 01:27:38.099
    STEP: Creating a pod to test consume secrets 03/02/23 01:27:38.123
    Mar  2 01:27:38.180: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-be244de1-9d0f-428a-83c0-3137e99cc829" in namespace "projected-799" to be "Succeeded or Failed"
    Mar  2 01:27:38.201: INFO: Pod "pod-projected-secrets-be244de1-9d0f-428a-83c0-3137e99cc829": Phase="Pending", Reason="", readiness=false. Elapsed: 20.929407ms
    Mar  2 01:27:40.230: INFO: Pod "pod-projected-secrets-be244de1-9d0f-428a-83c0-3137e99cc829": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049694081s
    Mar  2 01:27:42.224: INFO: Pod "pod-projected-secrets-be244de1-9d0f-428a-83c0-3137e99cc829": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044441933s
    Mar  2 01:27:44.247: INFO: Pod "pod-projected-secrets-be244de1-9d0f-428a-83c0-3137e99cc829": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.067380085s
    STEP: Saw pod success 03/02/23 01:27:44.247
    Mar  2 01:27:44.248: INFO: Pod "pod-projected-secrets-be244de1-9d0f-428a-83c0-3137e99cc829" satisfied condition "Succeeded or Failed"
    Mar  2 01:27:44.267: INFO: Trying to get logs from node 10.132.92.143 pod pod-projected-secrets-be244de1-9d0f-428a-83c0-3137e99cc829 container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/02/23 01:27:44.384
    Mar  2 01:27:44.493: INFO: Waiting for pod pod-projected-secrets-be244de1-9d0f-428a-83c0-3137e99cc829 to disappear
    Mar  2 01:27:44.513: INFO: Pod pod-projected-secrets-be244de1-9d0f-428a-83c0-3137e99cc829 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Mar  2 01:27:44.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-799" for this suite. 03/02/23 01:27:44.54
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:27:44.569
Mar  2 01:27:44.570: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename webhook 03/02/23 01:27:44.571
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:27:44.626
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:27:44.649
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/02/23 01:27:44.767
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 01:27:45.963
STEP: Deploying the webhook pod 03/02/23 01:27:46.008
STEP: Wait for the deployment to be ready 03/02/23 01:27:46.04
Mar  2 01:27:46.072: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 01:27:48.133: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 1, 27, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 27, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 27, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 27, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/02/23 01:27:50.146
STEP: Verifying the service has paired with the endpoint 03/02/23 01:27:50.212
Mar  2 01:27:51.213: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 03/02/23 01:27:51.244
STEP: create a configmap that should be updated by the webhook 03/02/23 01:27:51.353
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 01:27:51.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1806" for this suite. 03/02/23 01:27:51.544
STEP: Destroying namespace "webhook-1806-markers" for this suite. 03/02/23 01:27:51.563
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","completed":74,"skipped":1525,"failed":0}
------------------------------
• [SLOW TEST] [7.177 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:27:44.569
    Mar  2 01:27:44.570: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename webhook 03/02/23 01:27:44.571
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:27:44.626
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:27:44.649
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/02/23 01:27:44.767
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 01:27:45.963
    STEP: Deploying the webhook pod 03/02/23 01:27:46.008
    STEP: Wait for the deployment to be ready 03/02/23 01:27:46.04
    Mar  2 01:27:46.072: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Mar  2 01:27:48.133: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 1, 27, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 27, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 27, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 27, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/02/23 01:27:50.146
    STEP: Verifying the service has paired with the endpoint 03/02/23 01:27:50.212
    Mar  2 01:27:51.213: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:251
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 03/02/23 01:27:51.244
    STEP: create a configmap that should be updated by the webhook 03/02/23 01:27:51.353
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 01:27:51.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1806" for this suite. 03/02/23 01:27:51.544
    STEP: Destroying namespace "webhook-1806-markers" for this suite. 03/02/23 01:27:51.563
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:27:51.749
Mar  2 01:27:51.749: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename projected 03/02/23 01:27:51.75
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:27:51.798
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:27:51.808
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
STEP: Creating configMap with name projected-configmap-test-volume-027b170e-fcb9-4465-982d-6d4ffe888526 03/02/23 01:27:51.819
STEP: Creating a pod to test consume configMaps 03/02/23 01:27:51.856
Mar  2 01:27:51.918: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1f8f2439-e016-4889-a959-791923d420e9" in namespace "projected-6455" to be "Succeeded or Failed"
Mar  2 01:27:51.949: INFO: Pod "pod-projected-configmaps-1f8f2439-e016-4889-a959-791923d420e9": Phase="Pending", Reason="", readiness=false. Elapsed: 31.053381ms
Mar  2 01:27:53.970: INFO: Pod "pod-projected-configmaps-1f8f2439-e016-4889-a959-791923d420e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052257989s
Mar  2 01:27:55.964: INFO: Pod "pod-projected-configmaps-1f8f2439-e016-4889-a959-791923d420e9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046313547s
Mar  2 01:27:57.970: INFO: Pod "pod-projected-configmaps-1f8f2439-e016-4889-a959-791923d420e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.052573682s
STEP: Saw pod success 03/02/23 01:27:57.97
Mar  2 01:27:57.971: INFO: Pod "pod-projected-configmaps-1f8f2439-e016-4889-a959-791923d420e9" satisfied condition "Succeeded or Failed"
Mar  2 01:27:58.000: INFO: Trying to get logs from node 10.132.92.143 pod pod-projected-configmaps-1f8f2439-e016-4889-a959-791923d420e9 container projected-configmap-volume-test: <nil>
STEP: delete the pod 03/02/23 01:27:58.036
Mar  2 01:27:58.102: INFO: Waiting for pod pod-projected-configmaps-1f8f2439-e016-4889-a959-791923d420e9 to disappear
Mar  2 01:27:58.116: INFO: Pod pod-projected-configmaps-1f8f2439-e016-4889-a959-791923d420e9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar  2 01:27:58.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6455" for this suite. 03/02/23 01:27:58.141
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":75,"skipped":1538,"failed":0}
------------------------------
• [SLOW TEST] [6.422 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:27:51.749
    Mar  2 01:27:51.749: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename projected 03/02/23 01:27:51.75
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:27:51.798
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:27:51.808
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:374
    STEP: Creating configMap with name projected-configmap-test-volume-027b170e-fcb9-4465-982d-6d4ffe888526 03/02/23 01:27:51.819
    STEP: Creating a pod to test consume configMaps 03/02/23 01:27:51.856
    Mar  2 01:27:51.918: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1f8f2439-e016-4889-a959-791923d420e9" in namespace "projected-6455" to be "Succeeded or Failed"
    Mar  2 01:27:51.949: INFO: Pod "pod-projected-configmaps-1f8f2439-e016-4889-a959-791923d420e9": Phase="Pending", Reason="", readiness=false. Elapsed: 31.053381ms
    Mar  2 01:27:53.970: INFO: Pod "pod-projected-configmaps-1f8f2439-e016-4889-a959-791923d420e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052257989s
    Mar  2 01:27:55.964: INFO: Pod "pod-projected-configmaps-1f8f2439-e016-4889-a959-791923d420e9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046313547s
    Mar  2 01:27:57.970: INFO: Pod "pod-projected-configmaps-1f8f2439-e016-4889-a959-791923d420e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.052573682s
    STEP: Saw pod success 03/02/23 01:27:57.97
    Mar  2 01:27:57.971: INFO: Pod "pod-projected-configmaps-1f8f2439-e016-4889-a959-791923d420e9" satisfied condition "Succeeded or Failed"
    Mar  2 01:27:58.000: INFO: Trying to get logs from node 10.132.92.143 pod pod-projected-configmaps-1f8f2439-e016-4889-a959-791923d420e9 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 03/02/23 01:27:58.036
    Mar  2 01:27:58.102: INFO: Waiting for pod pod-projected-configmaps-1f8f2439-e016-4889-a959-791923d420e9 to disappear
    Mar  2 01:27:58.116: INFO: Pod pod-projected-configmaps-1f8f2439-e016-4889-a959-791923d420e9 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar  2 01:27:58.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6455" for this suite. 03/02/23 01:27:58.141
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:27:58.172
Mar  2 01:27:58.172: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename namespaces 03/02/23 01:27:58.173
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:27:58.227
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:27:58.244
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
STEP: Creating a test namespace 03/02/23 01:27:58.258
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:27:58.313
STEP: Creating a pod in the namespace 03/02/23 01:27:58.322
STEP: Waiting for the pod to have running status 03/02/23 01:27:58.395
Mar  2 01:27:58.396: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-7589" to be "running"
Mar  2 01:27:58.419: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 23.3788ms
Mar  2 01:28:00.434: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038755346s
Mar  2 01:28:02.441: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.045124253s
Mar  2 01:28:02.441: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 03/02/23 01:28:02.441
STEP: Waiting for the namespace to be removed. 03/02/23 01:28:02.46
STEP: Recreating the namespace 03/02/23 01:28:18.471
STEP: Verifying there are no pods in the namespace 03/02/23 01:28:18.518
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Mar  2 01:28:18.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6991" for this suite. 03/02/23 01:28:18.567
STEP: Destroying namespace "nsdeletetest-7589" for this suite. 03/02/23 01:28:18.587
Mar  2 01:28:18.598: INFO: Namespace nsdeletetest-7589 was already deleted
STEP: Destroying namespace "nsdeletetest-1990" for this suite. 03/02/23 01:28:18.598
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","completed":76,"skipped":1544,"failed":0}
------------------------------
• [SLOW TEST] [20.461 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:27:58.172
    Mar  2 01:27:58.172: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename namespaces 03/02/23 01:27:58.173
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:27:58.227
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:27:58.244
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:242
    STEP: Creating a test namespace 03/02/23 01:27:58.258
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:27:58.313
    STEP: Creating a pod in the namespace 03/02/23 01:27:58.322
    STEP: Waiting for the pod to have running status 03/02/23 01:27:58.395
    Mar  2 01:27:58.396: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-7589" to be "running"
    Mar  2 01:27:58.419: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 23.3788ms
    Mar  2 01:28:00.434: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038755346s
    Mar  2 01:28:02.441: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.045124253s
    Mar  2 01:28:02.441: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 03/02/23 01:28:02.441
    STEP: Waiting for the namespace to be removed. 03/02/23 01:28:02.46
    STEP: Recreating the namespace 03/02/23 01:28:18.471
    STEP: Verifying there are no pods in the namespace 03/02/23 01:28:18.518
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Mar  2 01:28:18.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-6991" for this suite. 03/02/23 01:28:18.567
    STEP: Destroying namespace "nsdeletetest-7589" for this suite. 03/02/23 01:28:18.587
    Mar  2 01:28:18.598: INFO: Namespace nsdeletetest-7589 was already deleted
    STEP: Destroying namespace "nsdeletetest-1990" for this suite. 03/02/23 01:28:18.598
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:28:18.641
Mar  2 01:28:18.641: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename watch 03/02/23 01:28:18.643
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:28:18.704
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:28:18.712
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 03/02/23 01:28:18.723
STEP: modifying the configmap once 03/02/23 01:28:18.744
STEP: modifying the configmap a second time 03/02/23 01:28:18.793
STEP: deleting the configmap 03/02/23 01:28:18.854
STEP: creating a watch on configmaps from the resource version returned by the first update 03/02/23 01:28:18.89
STEP: Expecting to observe notifications for all changes to the configmap after the first update 03/02/23 01:28:18.904
Mar  2 01:28:18.905: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4281  18d34a44-7da6-48fa-bfd5-42e5ecefc2be 82118 0 2023-03-02 01:28:18 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-03-02 01:28:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 01:28:18.905: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4281  18d34a44-7da6-48fa-bfd5-42e5ecefc2be 82122 0 2023-03-02 01:28:18 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-03-02 01:28:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Mar  2 01:28:18.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4281" for this suite. 03/02/23 01:28:18.924
{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","completed":77,"skipped":1568,"failed":0}
------------------------------
• [0.302 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:28:18.641
    Mar  2 01:28:18.641: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename watch 03/02/23 01:28:18.643
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:28:18.704
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:28:18.712
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 03/02/23 01:28:18.723
    STEP: modifying the configmap once 03/02/23 01:28:18.744
    STEP: modifying the configmap a second time 03/02/23 01:28:18.793
    STEP: deleting the configmap 03/02/23 01:28:18.854
    STEP: creating a watch on configmaps from the resource version returned by the first update 03/02/23 01:28:18.89
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 03/02/23 01:28:18.904
    Mar  2 01:28:18.905: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4281  18d34a44-7da6-48fa-bfd5-42e5ecefc2be 82118 0 2023-03-02 01:28:18 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-03-02 01:28:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar  2 01:28:18.905: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4281  18d34a44-7da6-48fa-bfd5-42e5ecefc2be 82122 0 2023-03-02 01:28:18 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-03-02 01:28:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Mar  2 01:28:18.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-4281" for this suite. 03/02/23 01:28:18.924
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:28:18.946
Mar  2 01:28:18.946: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename container-runtime 03/02/23 01:28:18.947
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:28:18.992
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:28:19.003
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 03/02/23 01:28:19.071
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 03/02/23 01:28:37.569
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 03/02/23 01:28:37.585
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 03/02/23 01:28:37.623
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 03/02/23 01:28:37.623
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 03/02/23 01:28:37.738
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 03/02/23 01:28:40.809
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 03/02/23 01:28:42.865
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 03/02/23 01:28:42.9
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 03/02/23 01:28:42.9
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 03/02/23 01:28:43
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 03/02/23 01:28:44.029
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 03/02/23 01:28:47.094
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 03/02/23 01:28:47.123
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 03/02/23 01:28:47.123
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Mar  2 01:28:47.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-214" for this suite. 03/02/23 01:28:47.279
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","completed":78,"skipped":1572,"failed":0}
------------------------------
• [SLOW TEST] [28.354 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    when starting a container that exits
    test/e2e/common/node/runtime.go:44
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:28:18.946
    Mar  2 01:28:18.946: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename container-runtime 03/02/23 01:28:18.947
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:28:18.992
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:28:19.003
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 03/02/23 01:28:19.071
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 03/02/23 01:28:37.569
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 03/02/23 01:28:37.585
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 03/02/23 01:28:37.623
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 03/02/23 01:28:37.623
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 03/02/23 01:28:37.738
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 03/02/23 01:28:40.809
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 03/02/23 01:28:42.865
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 03/02/23 01:28:42.9
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 03/02/23 01:28:42.9
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 03/02/23 01:28:43
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 03/02/23 01:28:44.029
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 03/02/23 01:28:47.094
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 03/02/23 01:28:47.123
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 03/02/23 01:28:47.123
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Mar  2 01:28:47.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-214" for this suite. 03/02/23 01:28:47.279
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:28:47.302
Mar  2 01:28:47.302: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename secrets 03/02/23 01:28:47.303
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:28:47.346
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:28:47.36
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar  2 01:28:47.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3975" for this suite. 03/02/23 01:28:47.564
{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","completed":79,"skipped":1604,"failed":0}
------------------------------
• [0.301 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:28:47.302
    Mar  2 01:28:47.302: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename secrets 03/02/23 01:28:47.303
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:28:47.346
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:28:47.36
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:385
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar  2 01:28:47.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-3975" for this suite. 03/02/23 01:28:47.564
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:28:47.603
Mar  2 01:28:47.604: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename pods 03/02/23 01:28:47.605
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:28:47.647
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:28:47.658
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
STEP: Create set of pods 03/02/23 01:28:47.667
Mar  2 01:28:47.717: INFO: created test-pod-1
Mar  2 01:28:47.787: INFO: created test-pod-2
Mar  2 01:28:47.833: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 03/02/23 01:28:47.833
Mar  2 01:28:47.833: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-8951' to be running and ready
Mar  2 01:28:47.888: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar  2 01:28:47.888: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar  2 01:28:47.888: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar  2 01:28:47.888: INFO: 0 / 3 pods in namespace 'pods-8951' are running and ready (0 seconds elapsed)
Mar  2 01:28:47.888: INFO: expected 0 pod replicas in namespace 'pods-8951', 0 are Running and Ready.
Mar  2 01:28:47.888: INFO: POD         NODE           PHASE    GRACE  CONDITIONS
Mar  2 01:28:47.888: INFO: test-pod-1  10.132.92.143  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:28:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:28:47 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:28:47 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:28:47 +0000 UTC  }]
Mar  2 01:28:47.888: INFO: test-pod-2  10.132.92.143  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:28:47 +0000 UTC  }]
Mar  2 01:28:47.888: INFO: test-pod-3  10.132.92.143  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:28:47 +0000 UTC  }]
Mar  2 01:28:47.888: INFO: 
Mar  2 01:28:49.942: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar  2 01:28:49.942: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar  2 01:28:49.942: INFO: 1 / 3 pods in namespace 'pods-8951' are running and ready (2 seconds elapsed)
Mar  2 01:28:49.942: INFO: expected 0 pod replicas in namespace 'pods-8951', 0 are Running and Ready.
Mar  2 01:28:49.942: INFO: POD         NODE           PHASE    GRACE  CONDITIONS
Mar  2 01:28:49.942: INFO: test-pod-2  10.132.92.143  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:28:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:28:47 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:28:47 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:28:47 +0000 UTC  }]
Mar  2 01:28:49.942: INFO: test-pod-3  10.132.92.143  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:28:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:28:47 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:28:47 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:28:47 +0000 UTC  }]
Mar  2 01:28:49.942: INFO: 
Mar  2 01:28:51.939: INFO: 3 / 3 pods in namespace 'pods-8951' are running and ready (4 seconds elapsed)
Mar  2 01:28:51.939: INFO: expected 0 pod replicas in namespace 'pods-8951', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 03/02/23 01:28:52.119
Mar  2 01:28:52.144: INFO: Pod quantity 3 is different from expected quantity 0
Mar  2 01:28:53.169: INFO: Pod quantity 3 is different from expected quantity 0
Mar  2 01:28:54.162: INFO: Pod quantity 2 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar  2 01:28:55.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8951" for this suite. 03/02/23 01:28:55.186
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","completed":80,"skipped":1605,"failed":0}
------------------------------
• [SLOW TEST] [7.600 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:28:47.603
    Mar  2 01:28:47.604: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename pods 03/02/23 01:28:47.605
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:28:47.647
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:28:47.658
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:844
    STEP: Create set of pods 03/02/23 01:28:47.667
    Mar  2 01:28:47.717: INFO: created test-pod-1
    Mar  2 01:28:47.787: INFO: created test-pod-2
    Mar  2 01:28:47.833: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 03/02/23 01:28:47.833
    Mar  2 01:28:47.833: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-8951' to be running and ready
    Mar  2 01:28:47.888: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar  2 01:28:47.888: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar  2 01:28:47.888: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar  2 01:28:47.888: INFO: 0 / 3 pods in namespace 'pods-8951' are running and ready (0 seconds elapsed)
    Mar  2 01:28:47.888: INFO: expected 0 pod replicas in namespace 'pods-8951', 0 are Running and Ready.
    Mar  2 01:28:47.888: INFO: POD         NODE           PHASE    GRACE  CONDITIONS
    Mar  2 01:28:47.888: INFO: test-pod-1  10.132.92.143  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:28:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:28:47 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:28:47 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:28:47 +0000 UTC  }]
    Mar  2 01:28:47.888: INFO: test-pod-2  10.132.92.143  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:28:47 +0000 UTC  }]
    Mar  2 01:28:47.888: INFO: test-pod-3  10.132.92.143  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:28:47 +0000 UTC  }]
    Mar  2 01:28:47.888: INFO: 
    Mar  2 01:28:49.942: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar  2 01:28:49.942: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar  2 01:28:49.942: INFO: 1 / 3 pods in namespace 'pods-8951' are running and ready (2 seconds elapsed)
    Mar  2 01:28:49.942: INFO: expected 0 pod replicas in namespace 'pods-8951', 0 are Running and Ready.
    Mar  2 01:28:49.942: INFO: POD         NODE           PHASE    GRACE  CONDITIONS
    Mar  2 01:28:49.942: INFO: test-pod-2  10.132.92.143  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:28:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:28:47 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:28:47 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:28:47 +0000 UTC  }]
    Mar  2 01:28:49.942: INFO: test-pod-3  10.132.92.143  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:28:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:28:47 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:28:47 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:28:47 +0000 UTC  }]
    Mar  2 01:28:49.942: INFO: 
    Mar  2 01:28:51.939: INFO: 3 / 3 pods in namespace 'pods-8951' are running and ready (4 seconds elapsed)
    Mar  2 01:28:51.939: INFO: expected 0 pod replicas in namespace 'pods-8951', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 03/02/23 01:28:52.119
    Mar  2 01:28:52.144: INFO: Pod quantity 3 is different from expected quantity 0
    Mar  2 01:28:53.169: INFO: Pod quantity 3 is different from expected quantity 0
    Mar  2 01:28:54.162: INFO: Pod quantity 2 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar  2 01:28:55.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-8951" for this suite. 03/02/23 01:28:55.186
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:28:55.204
Mar  2 01:28:55.204: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename services 03/02/23 01:28:55.207
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:28:55.288
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:28:55.304
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
STEP: creating an Endpoint 03/02/23 01:28:55.331
STEP: waiting for available Endpoint 03/02/23 01:28:55.351
STEP: listing all Endpoints 03/02/23 01:28:55.356
STEP: updating the Endpoint 03/02/23 01:28:55.373
STEP: fetching the Endpoint 03/02/23 01:28:55.393
STEP: patching the Endpoint 03/02/23 01:28:55.403
STEP: fetching the Endpoint 03/02/23 01:28:55.441
STEP: deleting the Endpoint by Collection 03/02/23 01:28:55.455
STEP: waiting for Endpoint deletion 03/02/23 01:28:55.477
STEP: fetching the Endpoint 03/02/23 01:28:55.483
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  2 01:28:55.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4915" for this suite. 03/02/23 01:28:55.514
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","completed":81,"skipped":1608,"failed":0}
------------------------------
• [0.329 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:28:55.204
    Mar  2 01:28:55.204: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename services 03/02/23 01:28:55.207
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:28:55.288
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:28:55.304
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3231
    STEP: creating an Endpoint 03/02/23 01:28:55.331
    STEP: waiting for available Endpoint 03/02/23 01:28:55.351
    STEP: listing all Endpoints 03/02/23 01:28:55.356
    STEP: updating the Endpoint 03/02/23 01:28:55.373
    STEP: fetching the Endpoint 03/02/23 01:28:55.393
    STEP: patching the Endpoint 03/02/23 01:28:55.403
    STEP: fetching the Endpoint 03/02/23 01:28:55.441
    STEP: deleting the Endpoint by Collection 03/02/23 01:28:55.455
    STEP: waiting for Endpoint deletion 03/02/23 01:28:55.477
    STEP: fetching the Endpoint 03/02/23 01:28:55.483
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  2 01:28:55.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4915" for this suite. 03/02/23 01:28:55.514
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:28:55.537
Mar  2 01:28:55.537: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename crd-publish-openapi 03/02/23 01:28:55.539
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:28:55.59
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:28:55.611
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
Mar  2 01:28:55.626: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/02/23 01:29:11.534
Mar  2 01:29:11.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-8065 --namespace=crd-publish-openapi-8065 create -f -'
Mar  2 01:29:13.652: INFO: stderr: ""
Mar  2 01:29:13.652: INFO: stdout: "e2e-test-crd-publish-openapi-2564-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar  2 01:29:13.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-8065 --namespace=crd-publish-openapi-8065 delete e2e-test-crd-publish-openapi-2564-crds test-cr'
Mar  2 01:29:13.797: INFO: stderr: ""
Mar  2 01:29:13.797: INFO: stdout: "e2e-test-crd-publish-openapi-2564-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Mar  2 01:29:13.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-8065 --namespace=crd-publish-openapi-8065 apply -f -'
Mar  2 01:29:17.465: INFO: stderr: ""
Mar  2 01:29:17.465: INFO: stdout: "e2e-test-crd-publish-openapi-2564-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar  2 01:29:17.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-8065 --namespace=crd-publish-openapi-8065 delete e2e-test-crd-publish-openapi-2564-crds test-cr'
Mar  2 01:29:17.619: INFO: stderr: ""
Mar  2 01:29:17.620: INFO: stdout: "e2e-test-crd-publish-openapi-2564-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 03/02/23 01:29:17.62
Mar  2 01:29:17.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-8065 explain e2e-test-crd-publish-openapi-2564-crds'
Mar  2 01:29:19.192: INFO: stderr: ""
Mar  2 01:29:19.193: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2564-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 01:29:33.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8065" for this suite. 03/02/23 01:29:33.794
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","completed":82,"skipped":1629,"failed":0}
------------------------------
• [SLOW TEST] [38.283 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:28:55.537
    Mar  2 01:28:55.537: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename crd-publish-openapi 03/02/23 01:28:55.539
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:28:55.59
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:28:55.611
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:235
    Mar  2 01:28:55.626: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/02/23 01:29:11.534
    Mar  2 01:29:11.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-8065 --namespace=crd-publish-openapi-8065 create -f -'
    Mar  2 01:29:13.652: INFO: stderr: ""
    Mar  2 01:29:13.652: INFO: stdout: "e2e-test-crd-publish-openapi-2564-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Mar  2 01:29:13.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-8065 --namespace=crd-publish-openapi-8065 delete e2e-test-crd-publish-openapi-2564-crds test-cr'
    Mar  2 01:29:13.797: INFO: stderr: ""
    Mar  2 01:29:13.797: INFO: stdout: "e2e-test-crd-publish-openapi-2564-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Mar  2 01:29:13.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-8065 --namespace=crd-publish-openapi-8065 apply -f -'
    Mar  2 01:29:17.465: INFO: stderr: ""
    Mar  2 01:29:17.465: INFO: stdout: "e2e-test-crd-publish-openapi-2564-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Mar  2 01:29:17.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-8065 --namespace=crd-publish-openapi-8065 delete e2e-test-crd-publish-openapi-2564-crds test-cr'
    Mar  2 01:29:17.619: INFO: stderr: ""
    Mar  2 01:29:17.620: INFO: stdout: "e2e-test-crd-publish-openapi-2564-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 03/02/23 01:29:17.62
    Mar  2 01:29:17.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-8065 explain e2e-test-crd-publish-openapi-2564-crds'
    Mar  2 01:29:19.192: INFO: stderr: ""
    Mar  2 01:29:19.193: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2564-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 01:29:33.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-8065" for this suite. 03/02/23 01:29:33.794
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:29:33.83
Mar  2 01:29:33.830: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename crd-watch 03/02/23 01:29:33.831
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:29:33.893
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:29:33.904
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Mar  2 01:29:33.915: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Creating first CR  03/02/23 01:29:36.579
Mar  2 01:29:36.598: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-02T01:29:36Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-02T01:29:36Z]] name:name1 resourceVersion:83004 uid:6e20d6d1-9177-4615-9c3b-0d5f24307c68] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 03/02/23 01:29:46.6
Mar  2 01:29:46.625: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-02T01:29:46Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-02T01:29:46Z]] name:name2 resourceVersion:83065 uid:9ab41804-322f-419e-bd9b-aa09fae0e331] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 03/02/23 01:29:56.643
Mar  2 01:29:56.667: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-02T01:29:36Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-02T01:29:56Z]] name:name1 resourceVersion:83129 uid:6e20d6d1-9177-4615-9c3b-0d5f24307c68] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 03/02/23 01:30:06.668
Mar  2 01:30:06.696: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-02T01:29:46Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-02T01:30:06Z]] name:name2 resourceVersion:83231 uid:9ab41804-322f-419e-bd9b-aa09fae0e331] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 03/02/23 01:30:16.699
Mar  2 01:30:16.725: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-02T01:29:36Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-02T01:29:56Z]] name:name1 resourceVersion:83322 uid:6e20d6d1-9177-4615-9c3b-0d5f24307c68] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 03/02/23 01:30:26.726
Mar  2 01:30:26.754: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-02T01:29:46Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-02T01:30:06Z]] name:name2 resourceVersion:83375 uid:9ab41804-322f-419e-bd9b-aa09fae0e331] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 01:30:37.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-9344" for this suite. 03/02/23 01:30:37.324
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","completed":83,"skipped":1690,"failed":0}
------------------------------
• [SLOW TEST] [63.516 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:29:33.83
    Mar  2 01:29:33.830: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename crd-watch 03/02/23 01:29:33.831
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:29:33.893
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:29:33.904
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Mar  2 01:29:33.915: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Creating first CR  03/02/23 01:29:36.579
    Mar  2 01:29:36.598: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-02T01:29:36Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-02T01:29:36Z]] name:name1 resourceVersion:83004 uid:6e20d6d1-9177-4615-9c3b-0d5f24307c68] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 03/02/23 01:29:46.6
    Mar  2 01:29:46.625: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-02T01:29:46Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-02T01:29:46Z]] name:name2 resourceVersion:83065 uid:9ab41804-322f-419e-bd9b-aa09fae0e331] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 03/02/23 01:29:56.643
    Mar  2 01:29:56.667: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-02T01:29:36Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-02T01:29:56Z]] name:name1 resourceVersion:83129 uid:6e20d6d1-9177-4615-9c3b-0d5f24307c68] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 03/02/23 01:30:06.668
    Mar  2 01:30:06.696: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-02T01:29:46Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-02T01:30:06Z]] name:name2 resourceVersion:83231 uid:9ab41804-322f-419e-bd9b-aa09fae0e331] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 03/02/23 01:30:16.699
    Mar  2 01:30:16.725: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-02T01:29:36Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-02T01:29:56Z]] name:name1 resourceVersion:83322 uid:6e20d6d1-9177-4615-9c3b-0d5f24307c68] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 03/02/23 01:30:26.726
    Mar  2 01:30:26.754: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-02T01:29:46Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-02T01:30:06Z]] name:name2 resourceVersion:83375 uid:9ab41804-322f-419e-bd9b-aa09fae0e331] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 01:30:37.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-watch-9344" for this suite. 03/02/23 01:30:37.324
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:30:37.35
Mar  2 01:30:37.350: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename security-context-test 03/02/23 01:30:37.351
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:30:37.446
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:30:37.459
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
Mar  2 01:30:37.542: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-ca637b85-1450-4f1a-b4ae-d12fc8378709" in namespace "security-context-test-1531" to be "Succeeded or Failed"
Mar  2 01:30:37.581: INFO: Pod "busybox-readonly-false-ca637b85-1450-4f1a-b4ae-d12fc8378709": Phase="Pending", Reason="", readiness=false. Elapsed: 38.249919ms
Mar  2 01:30:39.594: INFO: Pod "busybox-readonly-false-ca637b85-1450-4f1a-b4ae-d12fc8378709": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051741174s
Mar  2 01:30:41.595: INFO: Pod "busybox-readonly-false-ca637b85-1450-4f1a-b4ae-d12fc8378709": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052174101s
Mar  2 01:30:41.595: INFO: Pod "busybox-readonly-false-ca637b85-1450-4f1a-b4ae-d12fc8378709" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Mar  2 01:30:41.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1531" for this suite. 03/02/23 01:30:41.618
{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","completed":84,"skipped":1716,"failed":0}
------------------------------
• [4.303 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:429
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:485

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:30:37.35
    Mar  2 01:30:37.350: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename security-context-test 03/02/23 01:30:37.351
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:30:37.446
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:30:37.459
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:485
    Mar  2 01:30:37.542: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-ca637b85-1450-4f1a-b4ae-d12fc8378709" in namespace "security-context-test-1531" to be "Succeeded or Failed"
    Mar  2 01:30:37.581: INFO: Pod "busybox-readonly-false-ca637b85-1450-4f1a-b4ae-d12fc8378709": Phase="Pending", Reason="", readiness=false. Elapsed: 38.249919ms
    Mar  2 01:30:39.594: INFO: Pod "busybox-readonly-false-ca637b85-1450-4f1a-b4ae-d12fc8378709": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051741174s
    Mar  2 01:30:41.595: INFO: Pod "busybox-readonly-false-ca637b85-1450-4f1a-b4ae-d12fc8378709": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052174101s
    Mar  2 01:30:41.595: INFO: Pod "busybox-readonly-false-ca637b85-1450-4f1a-b4ae-d12fc8378709" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Mar  2 01:30:41.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-1531" for this suite. 03/02/23 01:30:41.618
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:30:41.653
Mar  2 01:30:41.653: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename init-container 03/02/23 01:30:41.655
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:30:41.721
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:30:41.731
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
STEP: creating the pod 03/02/23 01:30:41.761
Mar  2 01:30:41.761: INFO: PodSpec: initContainers in spec.initContainers
Mar  2 01:31:28.733: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-64dc79ab-4312-44fb-8b89-0982043c90e6", GenerateName:"", Namespace:"init-container-2836", SelfLink:"", UID:"063792b0-a093-4867-b859-aaa72a4bedcd", ResourceVersion:"83889", Generation:0, CreationTimestamp:time.Date(2023, time.March, 2, 1, 30, 41, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"761416006"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"45f1a21524397bac3d4dc7e20d5d4acc1d79f8625a897aa2ba16a7bcab80c637", "cni.projectcalico.org/podIP":"172.30.156.108/32", "cni.projectcalico.org/podIPs":"172.30.156.108/32", "k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.156.108\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.156.108\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 2, 1, 30, 41, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00b1f48b8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 2, 1, 30, 42, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00b1f48e8), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"multus", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 2, 1, 30, 42, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00b1f4918), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 2, 1, 31, 28, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00b1f4948), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-kszj2", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc00c42e380), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-kszj2", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc00b2584e0), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-kszj2", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc00b258540), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-kszj2", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc00b258480), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0072d5150), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.132.92.143", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0034d39d0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0072d5250)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0072d5280)}, v1.Toleration{Key:"node.kubernetes.io/memory-pressure", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0072d529c), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0072d52a0), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc005e6bed0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 2, 1, 30, 41, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 2, 1, 30, 41, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 2, 1, 30, 41, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 2, 1, 30, 41, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.132.92.143", PodIP:"172.30.156.108", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.30.156.108"}}, StartTime:time.Date(2023, time.March, 2, 1, 30, 41, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0034d3ab0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0034d3b20)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"cri-o://457d373dc905f526ae0f69b8fd277e160ae42609a130d3c80688334e2e48ecdc", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00c42e420), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00c42e3e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc0072d5334)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Mar  2 01:31:28.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2836" for this suite. 03/02/23 01:31:28.767
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","completed":85,"skipped":1716,"failed":0}
------------------------------
• [SLOW TEST] [47.200 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:30:41.653
    Mar  2 01:30:41.653: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename init-container 03/02/23 01:30:41.655
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:30:41.721
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:30:41.731
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:333
    STEP: creating the pod 03/02/23 01:30:41.761
    Mar  2 01:30:41.761: INFO: PodSpec: initContainers in spec.initContainers
    Mar  2 01:31:28.733: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-64dc79ab-4312-44fb-8b89-0982043c90e6", GenerateName:"", Namespace:"init-container-2836", SelfLink:"", UID:"063792b0-a093-4867-b859-aaa72a4bedcd", ResourceVersion:"83889", Generation:0, CreationTimestamp:time.Date(2023, time.March, 2, 1, 30, 41, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"761416006"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"45f1a21524397bac3d4dc7e20d5d4acc1d79f8625a897aa2ba16a7bcab80c637", "cni.projectcalico.org/podIP":"172.30.156.108/32", "cni.projectcalico.org/podIPs":"172.30.156.108/32", "k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.156.108\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.156.108\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 2, 1, 30, 41, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00b1f48b8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 2, 1, 30, 42, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00b1f48e8), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"multus", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 2, 1, 30, 42, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00b1f4918), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 2, 1, 31, 28, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00b1f4948), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-kszj2", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc00c42e380), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-kszj2", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc00b2584e0), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-kszj2", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc00b258540), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-kszj2", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc00b258480), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0072d5150), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.132.92.143", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0034d39d0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0072d5250)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0072d5280)}, v1.Toleration{Key:"node.kubernetes.io/memory-pressure", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0072d529c), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0072d52a0), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc005e6bed0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 2, 1, 30, 41, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 2, 1, 30, 41, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 2, 1, 30, 41, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 2, 1, 30, 41, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.132.92.143", PodIP:"172.30.156.108", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.30.156.108"}}, StartTime:time.Date(2023, time.March, 2, 1, 30, 41, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0034d3ab0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0034d3b20)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"cri-o://457d373dc905f526ae0f69b8fd277e160ae42609a130d3c80688334e2e48ecdc", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00c42e420), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00c42e3e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc0072d5334)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Mar  2 01:31:28.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-2836" for this suite. 03/02/23 01:31:28.767
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:31:28.867
Mar  2 01:31:28.868: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename pods 03/02/23 01:31:28.869
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:31:28.948
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:31:28.955
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 03/02/23 01:31:28.965
STEP: submitting the pod to kubernetes 03/02/23 01:31:28.965
STEP: verifying QOS class is set on the pod 03/02/23 01:31:29.055
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:187
Mar  2 01:31:29.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3536" for this suite. 03/02/23 01:31:29.097
{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","completed":86,"skipped":1758,"failed":0}
------------------------------
• [0.290 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:31:28.867
    Mar  2 01:31:28.868: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename pods 03/02/23 01:31:28.869
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:31:28.948
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:31:28.955
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 03/02/23 01:31:28.965
    STEP: submitting the pod to kubernetes 03/02/23 01:31:28.965
    STEP: verifying QOS class is set on the pod 03/02/23 01:31:29.055
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:187
    Mar  2 01:31:29.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-3536" for this suite. 03/02/23 01:31:29.097
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:31:29.158
Mar  2 01:31:29.158: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename cronjob 03/02/23 01:31:29.159
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:31:29.238
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:31:29.25
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 03/02/23 01:31:29.262
STEP: Ensuring more than one job is running at a time 03/02/23 01:31:29.295
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 03/02/23 01:33:01.308
STEP: Removing cronjob 03/02/23 01:33:01.33
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Mar  2 01:33:01.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-9706" for this suite. 03/02/23 01:33:01.44
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","completed":87,"skipped":1774,"failed":0}
------------------------------
• [SLOW TEST] [92.309 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:31:29.158
    Mar  2 01:31:29.158: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename cronjob 03/02/23 01:31:29.159
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:31:29.238
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:31:29.25
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 03/02/23 01:31:29.262
    STEP: Ensuring more than one job is running at a time 03/02/23 01:31:29.295
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 03/02/23 01:33:01.308
    STEP: Removing cronjob 03/02/23 01:33:01.33
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Mar  2 01:33:01.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-9706" for this suite. 03/02/23 01:33:01.44
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:33:01.47
Mar  2 01:33:01.470: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename statefulset 03/02/23 01:33:01.471
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:33:01.56
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:33:01.593
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-4996 03/02/23 01:33:01.63
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
STEP: Creating statefulset ss in namespace statefulset-4996 03/02/23 01:33:01.677
Mar  2 01:33:01.742: INFO: Found 0 stateful pods, waiting for 1
Mar  2 01:33:11.753: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 03/02/23 01:33:11.774
STEP: Getting /status 03/02/23 01:33:11.823
Mar  2 01:33:11.837: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 03/02/23 01:33:11.837
Mar  2 01:33:11.874: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 03/02/23 01:33:11.874
Mar  2 01:33:11.879: INFO: Observed &StatefulSet event: ADDED
Mar  2 01:33:11.879: INFO: Found Statefulset ss in namespace statefulset-4996 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar  2 01:33:11.879: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 03/02/23 01:33:11.879
Mar  2 01:33:11.879: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Mar  2 01:33:11.900: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 03/02/23 01:33:11.9
Mar  2 01:33:11.909: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar  2 01:33:11.909: INFO: Deleting all statefulset in ns statefulset-4996
Mar  2 01:33:11.922: INFO: Scaling statefulset ss to 0
Mar  2 01:33:21.991: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 01:33:22.004: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar  2 01:33:22.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4996" for this suite. 03/02/23 01:33:22.081
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","completed":88,"skipped":1804,"failed":0}
------------------------------
• [SLOW TEST] [20.668 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:975

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:33:01.47
    Mar  2 01:33:01.470: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename statefulset 03/02/23 01:33:01.471
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:33:01.56
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:33:01.593
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-4996 03/02/23 01:33:01.63
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:975
    STEP: Creating statefulset ss in namespace statefulset-4996 03/02/23 01:33:01.677
    Mar  2 01:33:01.742: INFO: Found 0 stateful pods, waiting for 1
    Mar  2 01:33:11.753: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 03/02/23 01:33:11.774
    STEP: Getting /status 03/02/23 01:33:11.823
    Mar  2 01:33:11.837: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 03/02/23 01:33:11.837
    Mar  2 01:33:11.874: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 03/02/23 01:33:11.874
    Mar  2 01:33:11.879: INFO: Observed &StatefulSet event: ADDED
    Mar  2 01:33:11.879: INFO: Found Statefulset ss in namespace statefulset-4996 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar  2 01:33:11.879: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 03/02/23 01:33:11.879
    Mar  2 01:33:11.879: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Mar  2 01:33:11.900: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 03/02/23 01:33:11.9
    Mar  2 01:33:11.909: INFO: Observed &StatefulSet event: ADDED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar  2 01:33:11.909: INFO: Deleting all statefulset in ns statefulset-4996
    Mar  2 01:33:11.922: INFO: Scaling statefulset ss to 0
    Mar  2 01:33:21.991: INFO: Waiting for statefulset status.replicas updated to 0
    Mar  2 01:33:22.004: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar  2 01:33:22.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-4996" for this suite. 03/02/23 01:33:22.081
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:33:22.139
Mar  2 01:33:22.140: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename container-runtime 03/02/23 01:33:22.141
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:33:22.2
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:33:22.21
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
STEP: create the container 03/02/23 01:33:22.221
STEP: wait for the container to reach Succeeded 03/02/23 01:33:22.3
STEP: get the container status 03/02/23 01:33:26.376
STEP: the container should be terminated 03/02/23 01:33:26.396
STEP: the termination message should be set 03/02/23 01:33:26.397
Mar  2 01:33:26.397: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 03/02/23 01:33:26.397
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Mar  2 01:33:26.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8111" for this suite. 03/02/23 01:33:26.506
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":89,"skipped":1822,"failed":0}
------------------------------
• [4.412 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:33:22.139
    Mar  2 01:33:22.140: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename container-runtime 03/02/23 01:33:22.141
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:33:22.2
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:33:22.21
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247
    STEP: create the container 03/02/23 01:33:22.221
    STEP: wait for the container to reach Succeeded 03/02/23 01:33:22.3
    STEP: get the container status 03/02/23 01:33:26.376
    STEP: the container should be terminated 03/02/23 01:33:26.396
    STEP: the termination message should be set 03/02/23 01:33:26.397
    Mar  2 01:33:26.397: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 03/02/23 01:33:26.397
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Mar  2 01:33:26.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-8111" for this suite. 03/02/23 01:33:26.506
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:33:26.554
Mar  2 01:33:26.554: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename kubectl 03/02/23 01:33:26.555
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:33:26.62
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:33:26.632
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/02/23 01:33:26.643
Mar  2 01:33:26.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-6644 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Mar  2 01:33:26.923: INFO: stderr: ""
Mar  2 01:33:26.923: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 03/02/23 01:33:26.923
Mar  2 01:33:26.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-6644 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Mar  2 01:33:33.200: INFO: stderr: ""
Mar  2 01:33:33.200: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/02/23 01:33:33.2
Mar  2 01:33:33.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-6644 delete pods e2e-test-httpd-pod'
Mar  2 01:33:35.601: INFO: stderr: ""
Mar  2 01:33:35.601: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  2 01:33:35.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6644" for this suite. 03/02/23 01:33:35.653
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","completed":90,"skipped":1827,"failed":0}
------------------------------
• [SLOW TEST] [9.170 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:954
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:960

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:33:26.554
    Mar  2 01:33:26.554: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename kubectl 03/02/23 01:33:26.555
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:33:26.62
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:33:26.632
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:960
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/02/23 01:33:26.643
    Mar  2 01:33:26.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-6644 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Mar  2 01:33:26.923: INFO: stderr: ""
    Mar  2 01:33:26.923: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 03/02/23 01:33:26.923
    Mar  2 01:33:26.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-6644 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
    Mar  2 01:33:33.200: INFO: stderr: ""
    Mar  2 01:33:33.200: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/02/23 01:33:33.2
    Mar  2 01:33:33.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-6644 delete pods e2e-test-httpd-pod'
    Mar  2 01:33:35.601: INFO: stderr: ""
    Mar  2 01:33:35.601: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  2 01:33:35.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6644" for this suite. 03/02/23 01:33:35.653
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:33:35.724
Mar  2 01:33:35.724: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename namespaces 03/02/23 01:33:35.74
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:33:35.824
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:33:35.836
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
STEP: Read namespace status 03/02/23 01:33:35.845
Mar  2 01:33:35.903: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 03/02/23 01:33:35.903
Mar  2 01:33:35.960: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 03/02/23 01:33:35.961
Mar  2 01:33:36.103: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Mar  2 01:33:36.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5953" for this suite. 03/02/23 01:33:36.191
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]","completed":91,"skipped":1830,"failed":0}
------------------------------
• [0.498 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:33:35.724
    Mar  2 01:33:35.724: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename namespaces 03/02/23 01:33:35.74
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:33:35.824
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:33:35.836
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:298
    STEP: Read namespace status 03/02/23 01:33:35.845
    Mar  2 01:33:35.903: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 03/02/23 01:33:35.903
    Mar  2 01:33:35.960: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 03/02/23 01:33:35.961
    Mar  2 01:33:36.103: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Mar  2 01:33:36.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-5953" for this suite. 03/02/23 01:33:36.191
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:33:36.225
Mar  2 01:33:36.225: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename ingressclass 03/02/23 01:33:36.226
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:33:36.329
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:33:36.351
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 03/02/23 01:33:36.375
STEP: getting /apis/networking.k8s.io 03/02/23 01:33:36.385
STEP: getting /apis/networking.k8s.iov1 03/02/23 01:33:36.39
STEP: creating 03/02/23 01:33:36.395
STEP: getting 03/02/23 01:33:36.565
STEP: listing 03/02/23 01:33:36.582
STEP: watching 03/02/23 01:33:36.594
Mar  2 01:33:36.594: INFO: starting watch
STEP: patching 03/02/23 01:33:36.616
STEP: updating 03/02/23 01:33:36.636
Mar  2 01:33:36.675: INFO: waiting for watch events with expected annotations
Mar  2 01:33:36.675: INFO: saw patched and updated annotations
STEP: deleting 03/02/23 01:33:36.676
STEP: deleting a collection 03/02/23 01:33:36.719
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:187
Mar  2 01:33:36.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-6726" for this suite. 03/02/23 01:33:36.79
{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","completed":92,"skipped":1833,"failed":0}
------------------------------
• [0.590 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:33:36.225
    Mar  2 01:33:36.225: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename ingressclass 03/02/23 01:33:36.226
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:33:36.329
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:33:36.351
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 03/02/23 01:33:36.375
    STEP: getting /apis/networking.k8s.io 03/02/23 01:33:36.385
    STEP: getting /apis/networking.k8s.iov1 03/02/23 01:33:36.39
    STEP: creating 03/02/23 01:33:36.395
    STEP: getting 03/02/23 01:33:36.565
    STEP: listing 03/02/23 01:33:36.582
    STEP: watching 03/02/23 01:33:36.594
    Mar  2 01:33:36.594: INFO: starting watch
    STEP: patching 03/02/23 01:33:36.616
    STEP: updating 03/02/23 01:33:36.636
    Mar  2 01:33:36.675: INFO: waiting for watch events with expected annotations
    Mar  2 01:33:36.675: INFO: saw patched and updated annotations
    STEP: deleting 03/02/23 01:33:36.676
    STEP: deleting a collection 03/02/23 01:33:36.719
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:187
    Mar  2 01:33:36.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingressclass-6726" for this suite. 03/02/23 01:33:36.79
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:33:36.815
Mar  2 01:33:36.816: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename crd-publish-openapi 03/02/23 01:33:36.818
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:33:36.902
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:33:36.912
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 03/02/23 01:33:36.941
Mar  2 01:33:36.942: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
Mar  2 01:33:52.562: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 01:34:46.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9167" for this suite. 03/02/23 01:34:46.755
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","completed":93,"skipped":1834,"failed":0}
------------------------------
• [SLOW TEST] [69.958 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:33:36.815
    Mar  2 01:33:36.816: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename crd-publish-openapi 03/02/23 01:33:36.818
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:33:36.902
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:33:36.912
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:275
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 03/02/23 01:33:36.941
    Mar  2 01:33:36.942: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    Mar  2 01:33:52.562: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 01:34:46.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-9167" for this suite. 03/02/23 01:34:46.755
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:34:46.776
Mar  2 01:34:46.777: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename deployment 03/02/23 01:34:46.777
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:34:46.851
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:34:46.89
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Mar  2 01:34:46.960: INFO: Pod name rollover-pod: Found 0 pods out of 1
Mar  2 01:34:51.978: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/02/23 01:34:51.978
Mar  2 01:34:51.978: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Mar  2 01:34:53.994: INFO: Creating deployment "test-rollover-deployment"
Mar  2 01:34:54.028: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Mar  2 01:34:56.051: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Mar  2 01:34:56.078: INFO: Ensure that both replica sets have 1 created replica
Mar  2 01:34:56.108: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Mar  2 01:34:56.138: INFO: Updating deployment test-rollover-deployment
Mar  2 01:34:56.138: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Mar  2 01:34:58.159: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Mar  2 01:34:58.188: INFO: Make sure deployment "test-rollover-deployment" is complete
Mar  2 01:34:58.221: INFO: all replica sets need to contain the pod-template-hash label
Mar  2 01:34:58.221: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 34, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 34, 54, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 34, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 34, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 01:35:00.249: INFO: all replica sets need to contain the pod-template-hash label
Mar  2 01:35:00.249: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 34, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 34, 54, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 34, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 34, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 01:35:02.258: INFO: all replica sets need to contain the pod-template-hash label
Mar  2 01:35:02.259: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 34, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 34, 54, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 34, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 34, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 01:35:04.254: INFO: all replica sets need to contain the pod-template-hash label
Mar  2 01:35:04.254: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 34, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 34, 54, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 34, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 34, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 01:35:06.252: INFO: all replica sets need to contain the pod-template-hash label
Mar  2 01:35:06.252: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 34, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 34, 54, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 34, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 34, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 01:35:08.245: INFO: all replica sets need to contain the pod-template-hash label
Mar  2 01:35:08.246: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 34, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 34, 54, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 34, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 34, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 01:35:10.260: INFO: 
Mar  2 01:35:10.260: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar  2 01:35:10.320: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-3206  10be9168-0622-4be9-9443-cb535a39d8e1 85768 2 2023-03-02 01:34:54 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-02 01:34:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 01:35:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0086aaaa8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-02 01:34:54 +0000 UTC,LastTransitionTime:2023-03-02 01:34:54 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-03-02 01:35:09 +0000 UTC,LastTransitionTime:2023-03-02 01:34:54 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar  2 01:35:10.337: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-3206  506cfb8b-28f9-4f19-87a7-bfd4ee169a6c 85757 2 2023-03-02 01:34:56 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 10be9168-0622-4be9-9443-cb535a39d8e1 0xc0086ab537 0xc0086ab538}] [] [{kube-controller-manager Update apps/v1 2023-03-02 01:34:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"10be9168-0622-4be9-9443-cb535a39d8e1\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 01:35:08 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0086ab5f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar  2 01:35:10.337: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Mar  2 01:35:10.337: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-3206  4cc29cde-98c0-4cff-a8f1-bb6aed028c9d 85766 2 2023-03-02 01:34:46 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 10be9168-0622-4be9-9443-cb535a39d8e1 0xc0086ab047 0xc0086ab048}] [] [{e2e.test Update apps/v1 2023-03-02 01:34:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 01:35:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"10be9168-0622-4be9-9443-cb535a39d8e1\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-03-02 01:35:09 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0086ab258 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  2 01:35:10.337: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-3206  56a40d09-d0b7-40f9-becf-9fd7041572a7 85678 2 2023-03-02 01:34:54 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 10be9168-0622-4be9-9443-cb535a39d8e1 0xc0086ab397 0xc0086ab398}] [] [{kube-controller-manager Update apps/v1 2023-03-02 01:34:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"10be9168-0622-4be9-9443-cb535a39d8e1\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 01:34:56 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0086ab4b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  2 01:35:10.359: INFO: Pod "test-rollover-deployment-6d45fd857b-wnprp" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-wnprp test-rollover-deployment-6d45fd857b- deployment-3206  fb722a65-a3b8-4cdd-aad9-2117360ac648 85709 0 2023-03-02 01:34:56 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cni.projectcalico.org/containerID:b76d5109ee05dfa552dc6060e939207d8a3025c16a57d50d8c9b3643692dec6d cni.projectcalico.org/podIP:172.30.156.118/32 cni.projectcalico.org/podIPs:172.30.156.118/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.156.118"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.156.118"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b 506cfb8b-28f9-4f19-87a7-bfd4ee169a6c 0xc0086e80d7 0xc0086e80d8}] [] [{kube-controller-manager Update v1 2023-03-02 01:34:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"506cfb8b-28f9-4f19-87a7-bfd4ee169a6c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 01:34:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 01:34:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-02 01:34:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.156.118\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s447h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s447h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.132.92.143,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c43,c17,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-dskbf,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:34:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:34:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:34:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:34:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.132.92.143,PodIP:172.30.156.118,StartTime:2023-03-02 01:34:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 01:34:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787,ContainerID:cri-o://fa815fe710041af44f50c5a9ae212bd0e2174eda348e59f9c813339302330b9e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.156.118,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar  2 01:35:10.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3206" for this suite. 03/02/23 01:35:10.392
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","completed":94,"skipped":1859,"failed":0}
------------------------------
• [SLOW TEST] [23.629 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:34:46.776
    Mar  2 01:34:46.777: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename deployment 03/02/23 01:34:46.777
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:34:46.851
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:34:46.89
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Mar  2 01:34:46.960: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Mar  2 01:34:51.978: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/02/23 01:34:51.978
    Mar  2 01:34:51.978: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Mar  2 01:34:53.994: INFO: Creating deployment "test-rollover-deployment"
    Mar  2 01:34:54.028: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Mar  2 01:34:56.051: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Mar  2 01:34:56.078: INFO: Ensure that both replica sets have 1 created replica
    Mar  2 01:34:56.108: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Mar  2 01:34:56.138: INFO: Updating deployment test-rollover-deployment
    Mar  2 01:34:56.138: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Mar  2 01:34:58.159: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Mar  2 01:34:58.188: INFO: Make sure deployment "test-rollover-deployment" is complete
    Mar  2 01:34:58.221: INFO: all replica sets need to contain the pod-template-hash label
    Mar  2 01:34:58.221: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 34, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 34, 54, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 34, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 34, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  2 01:35:00.249: INFO: all replica sets need to contain the pod-template-hash label
    Mar  2 01:35:00.249: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 34, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 34, 54, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 34, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 34, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  2 01:35:02.258: INFO: all replica sets need to contain the pod-template-hash label
    Mar  2 01:35:02.259: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 34, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 34, 54, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 34, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 34, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  2 01:35:04.254: INFO: all replica sets need to contain the pod-template-hash label
    Mar  2 01:35:04.254: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 34, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 34, 54, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 34, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 34, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  2 01:35:06.252: INFO: all replica sets need to contain the pod-template-hash label
    Mar  2 01:35:06.252: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 34, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 34, 54, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 34, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 34, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  2 01:35:08.245: INFO: all replica sets need to contain the pod-template-hash label
    Mar  2 01:35:08.246: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 34, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 34, 54, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 34, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 34, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar  2 01:35:10.260: INFO: 
    Mar  2 01:35:10.260: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar  2 01:35:10.320: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-3206  10be9168-0622-4be9-9443-cb535a39d8e1 85768 2 2023-03-02 01:34:54 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-02 01:34:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 01:35:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0086aaaa8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-02 01:34:54 +0000 UTC,LastTransitionTime:2023-03-02 01:34:54 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-03-02 01:35:09 +0000 UTC,LastTransitionTime:2023-03-02 01:34:54 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Mar  2 01:35:10.337: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-3206  506cfb8b-28f9-4f19-87a7-bfd4ee169a6c 85757 2 2023-03-02 01:34:56 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 10be9168-0622-4be9-9443-cb535a39d8e1 0xc0086ab537 0xc0086ab538}] [] [{kube-controller-manager Update apps/v1 2023-03-02 01:34:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"10be9168-0622-4be9-9443-cb535a39d8e1\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 01:35:08 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0086ab5f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar  2 01:35:10.337: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Mar  2 01:35:10.337: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-3206  4cc29cde-98c0-4cff-a8f1-bb6aed028c9d 85766 2 2023-03-02 01:34:46 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 10be9168-0622-4be9-9443-cb535a39d8e1 0xc0086ab047 0xc0086ab048}] [] [{e2e.test Update apps/v1 2023-03-02 01:34:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 01:35:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"10be9168-0622-4be9-9443-cb535a39d8e1\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-03-02 01:35:09 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0086ab258 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar  2 01:35:10.337: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-3206  56a40d09-d0b7-40f9-becf-9fd7041572a7 85678 2 2023-03-02 01:34:54 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 10be9168-0622-4be9-9443-cb535a39d8e1 0xc0086ab397 0xc0086ab398}] [] [{kube-controller-manager Update apps/v1 2023-03-02 01:34:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"10be9168-0622-4be9-9443-cb535a39d8e1\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 01:34:56 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0086ab4b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar  2 01:35:10.359: INFO: Pod "test-rollover-deployment-6d45fd857b-wnprp" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-wnprp test-rollover-deployment-6d45fd857b- deployment-3206  fb722a65-a3b8-4cdd-aad9-2117360ac648 85709 0 2023-03-02 01:34:56 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cni.projectcalico.org/containerID:b76d5109ee05dfa552dc6060e939207d8a3025c16a57d50d8c9b3643692dec6d cni.projectcalico.org/podIP:172.30.156.118/32 cni.projectcalico.org/podIPs:172.30.156.118/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.156.118"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.156.118"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b 506cfb8b-28f9-4f19-87a7-bfd4ee169a6c 0xc0086e80d7 0xc0086e80d8}] [] [{kube-controller-manager Update v1 2023-03-02 01:34:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"506cfb8b-28f9-4f19-87a7-bfd4ee169a6c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 01:34:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 01:34:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-02 01:34:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.156.118\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s447h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s447h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.132.92.143,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c43,c17,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-dskbf,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:34:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:34:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:34:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:34:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.132.92.143,PodIP:172.30.156.118,StartTime:2023-03-02 01:34:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 01:34:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787,ContainerID:cri-o://fa815fe710041af44f50c5a9ae212bd0e2174eda348e59f9c813339302330b9e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.156.118,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar  2 01:35:10.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-3206" for this suite. 03/02/23 01:35:10.392
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:35:10.413
Mar  2 01:35:10.413: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename gc 03/02/23 01:35:10.414
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:35:10.533
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:35:10.544
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 03/02/23 01:35:10.628
STEP: create the rc2 03/02/23 01:35:10.647
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 03/02/23 01:35:15.673
STEP: delete the rc simpletest-rc-to-be-deleted 03/02/23 01:35:18.514
STEP: wait for the rc to be deleted 03/02/23 01:35:18.533
STEP: Gathering metrics 03/02/23 01:35:23.58
W0302 01:35:23.665905      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Mar  2 01:35:23.666: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Mar  2 01:35:23.666: INFO: Deleting pod "simpletest-rc-to-be-deleted-247cz" in namespace "gc-7801"
Mar  2 01:35:23.799: INFO: Deleting pod "simpletest-rc-to-be-deleted-265pt" in namespace "gc-7801"
Mar  2 01:35:23.964: INFO: Deleting pod "simpletest-rc-to-be-deleted-2jfdp" in namespace "gc-7801"
Mar  2 01:35:24.096: INFO: Deleting pod "simpletest-rc-to-be-deleted-2v64t" in namespace "gc-7801"
Mar  2 01:35:24.218: INFO: Deleting pod "simpletest-rc-to-be-deleted-48ngr" in namespace "gc-7801"
Mar  2 01:35:24.404: INFO: Deleting pod "simpletest-rc-to-be-deleted-4gkbr" in namespace "gc-7801"
Mar  2 01:35:24.524: INFO: Deleting pod "simpletest-rc-to-be-deleted-4ldqj" in namespace "gc-7801"
Mar  2 01:35:24.673: INFO: Deleting pod "simpletest-rc-to-be-deleted-4qhbj" in namespace "gc-7801"
Mar  2 01:35:24.826: INFO: Deleting pod "simpletest-rc-to-be-deleted-58kkj" in namespace "gc-7801"
Mar  2 01:35:24.888: INFO: Deleting pod "simpletest-rc-to-be-deleted-58vz2" in namespace "gc-7801"
Mar  2 01:35:25.017: INFO: Deleting pod "simpletest-rc-to-be-deleted-58z8l" in namespace "gc-7801"
Mar  2 01:35:25.239: INFO: Deleting pod "simpletest-rc-to-be-deleted-5fpt9" in namespace "gc-7801"
Mar  2 01:35:25.378: INFO: Deleting pod "simpletest-rc-to-be-deleted-5l9f4" in namespace "gc-7801"
Mar  2 01:35:25.502: INFO: Deleting pod "simpletest-rc-to-be-deleted-5plqs" in namespace "gc-7801"
Mar  2 01:35:25.657: INFO: Deleting pod "simpletest-rc-to-be-deleted-6trhz" in namespace "gc-7801"
Mar  2 01:35:25.736: INFO: Deleting pod "simpletest-rc-to-be-deleted-6v9cb" in namespace "gc-7801"
Mar  2 01:35:25.852: INFO: Deleting pod "simpletest-rc-to-be-deleted-6wcc4" in namespace "gc-7801"
Mar  2 01:35:25.908: INFO: Deleting pod "simpletest-rc-to-be-deleted-6wpvz" in namespace "gc-7801"
Mar  2 01:35:25.960: INFO: Deleting pod "simpletest-rc-to-be-deleted-74tt5" in namespace "gc-7801"
Mar  2 01:35:26.023: INFO: Deleting pod "simpletest-rc-to-be-deleted-7kr8j" in namespace "gc-7801"
Mar  2 01:35:26.156: INFO: Deleting pod "simpletest-rc-to-be-deleted-7nf4x" in namespace "gc-7801"
Mar  2 01:35:26.251: INFO: Deleting pod "simpletest-rc-to-be-deleted-7q72c" in namespace "gc-7801"
Mar  2 01:35:26.352: INFO: Deleting pod "simpletest-rc-to-be-deleted-7vz6g" in namespace "gc-7801"
Mar  2 01:35:26.492: INFO: Deleting pod "simpletest-rc-to-be-deleted-8f4xg" in namespace "gc-7801"
Mar  2 01:35:26.567: INFO: Deleting pod "simpletest-rc-to-be-deleted-9dgpb" in namespace "gc-7801"
Mar  2 01:35:26.722: INFO: Deleting pod "simpletest-rc-to-be-deleted-9qpkw" in namespace "gc-7801"
Mar  2 01:35:26.796: INFO: Deleting pod "simpletest-rc-to-be-deleted-b5b58" in namespace "gc-7801"
Mar  2 01:35:26.874: INFO: Deleting pod "simpletest-rc-to-be-deleted-bmfrg" in namespace "gc-7801"
Mar  2 01:35:26.964: INFO: Deleting pod "simpletest-rc-to-be-deleted-br8fk" in namespace "gc-7801"
Mar  2 01:35:27.029: INFO: Deleting pod "simpletest-rc-to-be-deleted-bz8wz" in namespace "gc-7801"
Mar  2 01:35:27.101: INFO: Deleting pod "simpletest-rc-to-be-deleted-cxdxq" in namespace "gc-7801"
Mar  2 01:35:27.201: INFO: Deleting pod "simpletest-rc-to-be-deleted-d45qt" in namespace "gc-7801"
Mar  2 01:35:27.293: INFO: Deleting pod "simpletest-rc-to-be-deleted-d8cvb" in namespace "gc-7801"
Mar  2 01:35:27.428: INFO: Deleting pod "simpletest-rc-to-be-deleted-dlgrb" in namespace "gc-7801"
Mar  2 01:35:27.486: INFO: Deleting pod "simpletest-rc-to-be-deleted-dslmk" in namespace "gc-7801"
Mar  2 01:35:27.552: INFO: Deleting pod "simpletest-rc-to-be-deleted-f25n9" in namespace "gc-7801"
Mar  2 01:35:27.662: INFO: Deleting pod "simpletest-rc-to-be-deleted-f68gj" in namespace "gc-7801"
Mar  2 01:35:27.800: INFO: Deleting pod "simpletest-rc-to-be-deleted-f85l7" in namespace "gc-7801"
Mar  2 01:35:27.916: INFO: Deleting pod "simpletest-rc-to-be-deleted-f86vb" in namespace "gc-7801"
Mar  2 01:35:27.976: INFO: Deleting pod "simpletest-rc-to-be-deleted-fbxhq" in namespace "gc-7801"
Mar  2 01:35:28.082: INFO: Deleting pod "simpletest-rc-to-be-deleted-ffqdt" in namespace "gc-7801"
Mar  2 01:35:28.124: INFO: Deleting pod "simpletest-rc-to-be-deleted-fghcm" in namespace "gc-7801"
Mar  2 01:35:28.223: INFO: Deleting pod "simpletest-rc-to-be-deleted-fszh4" in namespace "gc-7801"
Mar  2 01:35:28.289: INFO: Deleting pod "simpletest-rc-to-be-deleted-fwp7b" in namespace "gc-7801"
Mar  2 01:35:28.350: INFO: Deleting pod "simpletest-rc-to-be-deleted-g64rs" in namespace "gc-7801"
Mar  2 01:35:28.412: INFO: Deleting pod "simpletest-rc-to-be-deleted-gmrjr" in namespace "gc-7801"
Mar  2 01:35:28.475: INFO: Deleting pod "simpletest-rc-to-be-deleted-gppkx" in namespace "gc-7801"
Mar  2 01:35:28.526: INFO: Deleting pod "simpletest-rc-to-be-deleted-hhd2z" in namespace "gc-7801"
Mar  2 01:35:28.567: INFO: Deleting pod "simpletest-rc-to-be-deleted-jb4jp" in namespace "gc-7801"
Mar  2 01:35:28.656: INFO: Deleting pod "simpletest-rc-to-be-deleted-jqdnr" in namespace "gc-7801"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Mar  2 01:35:28.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7801" for this suite. 03/02/23 01:35:28.787
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","completed":95,"skipped":1903,"failed":0}
------------------------------
• [SLOW TEST] [18.421 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:35:10.413
    Mar  2 01:35:10.413: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename gc 03/02/23 01:35:10.414
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:35:10.533
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:35:10.544
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 03/02/23 01:35:10.628
    STEP: create the rc2 03/02/23 01:35:10.647
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 03/02/23 01:35:15.673
    STEP: delete the rc simpletest-rc-to-be-deleted 03/02/23 01:35:18.514
    STEP: wait for the rc to be deleted 03/02/23 01:35:18.533
    STEP: Gathering metrics 03/02/23 01:35:23.58
    W0302 01:35:23.665905      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Mar  2 01:35:23.666: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Mar  2 01:35:23.666: INFO: Deleting pod "simpletest-rc-to-be-deleted-247cz" in namespace "gc-7801"
    Mar  2 01:35:23.799: INFO: Deleting pod "simpletest-rc-to-be-deleted-265pt" in namespace "gc-7801"
    Mar  2 01:35:23.964: INFO: Deleting pod "simpletest-rc-to-be-deleted-2jfdp" in namespace "gc-7801"
    Mar  2 01:35:24.096: INFO: Deleting pod "simpletest-rc-to-be-deleted-2v64t" in namespace "gc-7801"
    Mar  2 01:35:24.218: INFO: Deleting pod "simpletest-rc-to-be-deleted-48ngr" in namespace "gc-7801"
    Mar  2 01:35:24.404: INFO: Deleting pod "simpletest-rc-to-be-deleted-4gkbr" in namespace "gc-7801"
    Mar  2 01:35:24.524: INFO: Deleting pod "simpletest-rc-to-be-deleted-4ldqj" in namespace "gc-7801"
    Mar  2 01:35:24.673: INFO: Deleting pod "simpletest-rc-to-be-deleted-4qhbj" in namespace "gc-7801"
    Mar  2 01:35:24.826: INFO: Deleting pod "simpletest-rc-to-be-deleted-58kkj" in namespace "gc-7801"
    Mar  2 01:35:24.888: INFO: Deleting pod "simpletest-rc-to-be-deleted-58vz2" in namespace "gc-7801"
    Mar  2 01:35:25.017: INFO: Deleting pod "simpletest-rc-to-be-deleted-58z8l" in namespace "gc-7801"
    Mar  2 01:35:25.239: INFO: Deleting pod "simpletest-rc-to-be-deleted-5fpt9" in namespace "gc-7801"
    Mar  2 01:35:25.378: INFO: Deleting pod "simpletest-rc-to-be-deleted-5l9f4" in namespace "gc-7801"
    Mar  2 01:35:25.502: INFO: Deleting pod "simpletest-rc-to-be-deleted-5plqs" in namespace "gc-7801"
    Mar  2 01:35:25.657: INFO: Deleting pod "simpletest-rc-to-be-deleted-6trhz" in namespace "gc-7801"
    Mar  2 01:35:25.736: INFO: Deleting pod "simpletest-rc-to-be-deleted-6v9cb" in namespace "gc-7801"
    Mar  2 01:35:25.852: INFO: Deleting pod "simpletest-rc-to-be-deleted-6wcc4" in namespace "gc-7801"
    Mar  2 01:35:25.908: INFO: Deleting pod "simpletest-rc-to-be-deleted-6wpvz" in namespace "gc-7801"
    Mar  2 01:35:25.960: INFO: Deleting pod "simpletest-rc-to-be-deleted-74tt5" in namespace "gc-7801"
    Mar  2 01:35:26.023: INFO: Deleting pod "simpletest-rc-to-be-deleted-7kr8j" in namespace "gc-7801"
    Mar  2 01:35:26.156: INFO: Deleting pod "simpletest-rc-to-be-deleted-7nf4x" in namespace "gc-7801"
    Mar  2 01:35:26.251: INFO: Deleting pod "simpletest-rc-to-be-deleted-7q72c" in namespace "gc-7801"
    Mar  2 01:35:26.352: INFO: Deleting pod "simpletest-rc-to-be-deleted-7vz6g" in namespace "gc-7801"
    Mar  2 01:35:26.492: INFO: Deleting pod "simpletest-rc-to-be-deleted-8f4xg" in namespace "gc-7801"
    Mar  2 01:35:26.567: INFO: Deleting pod "simpletest-rc-to-be-deleted-9dgpb" in namespace "gc-7801"
    Mar  2 01:35:26.722: INFO: Deleting pod "simpletest-rc-to-be-deleted-9qpkw" in namespace "gc-7801"
    Mar  2 01:35:26.796: INFO: Deleting pod "simpletest-rc-to-be-deleted-b5b58" in namespace "gc-7801"
    Mar  2 01:35:26.874: INFO: Deleting pod "simpletest-rc-to-be-deleted-bmfrg" in namespace "gc-7801"
    Mar  2 01:35:26.964: INFO: Deleting pod "simpletest-rc-to-be-deleted-br8fk" in namespace "gc-7801"
    Mar  2 01:35:27.029: INFO: Deleting pod "simpletest-rc-to-be-deleted-bz8wz" in namespace "gc-7801"
    Mar  2 01:35:27.101: INFO: Deleting pod "simpletest-rc-to-be-deleted-cxdxq" in namespace "gc-7801"
    Mar  2 01:35:27.201: INFO: Deleting pod "simpletest-rc-to-be-deleted-d45qt" in namespace "gc-7801"
    Mar  2 01:35:27.293: INFO: Deleting pod "simpletest-rc-to-be-deleted-d8cvb" in namespace "gc-7801"
    Mar  2 01:35:27.428: INFO: Deleting pod "simpletest-rc-to-be-deleted-dlgrb" in namespace "gc-7801"
    Mar  2 01:35:27.486: INFO: Deleting pod "simpletest-rc-to-be-deleted-dslmk" in namespace "gc-7801"
    Mar  2 01:35:27.552: INFO: Deleting pod "simpletest-rc-to-be-deleted-f25n9" in namespace "gc-7801"
    Mar  2 01:35:27.662: INFO: Deleting pod "simpletest-rc-to-be-deleted-f68gj" in namespace "gc-7801"
    Mar  2 01:35:27.800: INFO: Deleting pod "simpletest-rc-to-be-deleted-f85l7" in namespace "gc-7801"
    Mar  2 01:35:27.916: INFO: Deleting pod "simpletest-rc-to-be-deleted-f86vb" in namespace "gc-7801"
    Mar  2 01:35:27.976: INFO: Deleting pod "simpletest-rc-to-be-deleted-fbxhq" in namespace "gc-7801"
    Mar  2 01:35:28.082: INFO: Deleting pod "simpletest-rc-to-be-deleted-ffqdt" in namespace "gc-7801"
    Mar  2 01:35:28.124: INFO: Deleting pod "simpletest-rc-to-be-deleted-fghcm" in namespace "gc-7801"
    Mar  2 01:35:28.223: INFO: Deleting pod "simpletest-rc-to-be-deleted-fszh4" in namespace "gc-7801"
    Mar  2 01:35:28.289: INFO: Deleting pod "simpletest-rc-to-be-deleted-fwp7b" in namespace "gc-7801"
    Mar  2 01:35:28.350: INFO: Deleting pod "simpletest-rc-to-be-deleted-g64rs" in namespace "gc-7801"
    Mar  2 01:35:28.412: INFO: Deleting pod "simpletest-rc-to-be-deleted-gmrjr" in namespace "gc-7801"
    Mar  2 01:35:28.475: INFO: Deleting pod "simpletest-rc-to-be-deleted-gppkx" in namespace "gc-7801"
    Mar  2 01:35:28.526: INFO: Deleting pod "simpletest-rc-to-be-deleted-hhd2z" in namespace "gc-7801"
    Mar  2 01:35:28.567: INFO: Deleting pod "simpletest-rc-to-be-deleted-jb4jp" in namespace "gc-7801"
    Mar  2 01:35:28.656: INFO: Deleting pod "simpletest-rc-to-be-deleted-jqdnr" in namespace "gc-7801"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Mar  2 01:35:28.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-7801" for this suite. 03/02/23 01:35:28.787
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:35:28.836
Mar  2 01:35:28.836: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename kubelet-test 03/02/23 01:35:28.847
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:35:28.957
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:35:28.978
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Mar  2 01:35:29.094: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsb3039001-dde1-4a14-8181-e5f12e0f5a69" in namespace "kubelet-test-7158" to be "running and ready"
Mar  2 01:35:29.120: INFO: Pod "busybox-readonly-fsb3039001-dde1-4a14-8181-e5f12e0f5a69": Phase="Pending", Reason="", readiness=false. Elapsed: 26.210473ms
Mar  2 01:35:29.120: INFO: The phase of Pod busybox-readonly-fsb3039001-dde1-4a14-8181-e5f12e0f5a69 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:35:31.141: INFO: Pod "busybox-readonly-fsb3039001-dde1-4a14-8181-e5f12e0f5a69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047131468s
Mar  2 01:35:31.141: INFO: The phase of Pod busybox-readonly-fsb3039001-dde1-4a14-8181-e5f12e0f5a69 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:35:33.164: INFO: Pod "busybox-readonly-fsb3039001-dde1-4a14-8181-e5f12e0f5a69": Phase="Pending", Reason="", readiness=false. Elapsed: 4.070275574s
Mar  2 01:35:33.164: INFO: The phase of Pod busybox-readonly-fsb3039001-dde1-4a14-8181-e5f12e0f5a69 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:35:35.178: INFO: Pod "busybox-readonly-fsb3039001-dde1-4a14-8181-e5f12e0f5a69": Phase="Pending", Reason="", readiness=false. Elapsed: 6.083570698s
Mar  2 01:35:35.178: INFO: The phase of Pod busybox-readonly-fsb3039001-dde1-4a14-8181-e5f12e0f5a69 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:35:37.180: INFO: Pod "busybox-readonly-fsb3039001-dde1-4a14-8181-e5f12e0f5a69": Phase="Pending", Reason="", readiness=false. Elapsed: 8.086239907s
Mar  2 01:35:37.180: INFO: The phase of Pod busybox-readonly-fsb3039001-dde1-4a14-8181-e5f12e0f5a69 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:35:39.138: INFO: Pod "busybox-readonly-fsb3039001-dde1-4a14-8181-e5f12e0f5a69": Phase="Pending", Reason="", readiness=false. Elapsed: 10.044247087s
Mar  2 01:35:39.138: INFO: The phase of Pod busybox-readonly-fsb3039001-dde1-4a14-8181-e5f12e0f5a69 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:35:41.137: INFO: Pod "busybox-readonly-fsb3039001-dde1-4a14-8181-e5f12e0f5a69": Phase="Running", Reason="", readiness=true. Elapsed: 12.042812074s
Mar  2 01:35:41.137: INFO: The phase of Pod busybox-readonly-fsb3039001-dde1-4a14-8181-e5f12e0f5a69 is Running (Ready = true)
Mar  2 01:35:41.137: INFO: Pod "busybox-readonly-fsb3039001-dde1-4a14-8181-e5f12e0f5a69" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Mar  2 01:35:41.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7158" for this suite. 03/02/23 01:35:41.278
{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","completed":96,"skipped":1903,"failed":0}
------------------------------
• [SLOW TEST] [12.464 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:35:28.836
    Mar  2 01:35:28.836: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename kubelet-test 03/02/23 01:35:28.847
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:35:28.957
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:35:28.978
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Mar  2 01:35:29.094: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsb3039001-dde1-4a14-8181-e5f12e0f5a69" in namespace "kubelet-test-7158" to be "running and ready"
    Mar  2 01:35:29.120: INFO: Pod "busybox-readonly-fsb3039001-dde1-4a14-8181-e5f12e0f5a69": Phase="Pending", Reason="", readiness=false. Elapsed: 26.210473ms
    Mar  2 01:35:29.120: INFO: The phase of Pod busybox-readonly-fsb3039001-dde1-4a14-8181-e5f12e0f5a69 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 01:35:31.141: INFO: Pod "busybox-readonly-fsb3039001-dde1-4a14-8181-e5f12e0f5a69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047131468s
    Mar  2 01:35:31.141: INFO: The phase of Pod busybox-readonly-fsb3039001-dde1-4a14-8181-e5f12e0f5a69 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 01:35:33.164: INFO: Pod "busybox-readonly-fsb3039001-dde1-4a14-8181-e5f12e0f5a69": Phase="Pending", Reason="", readiness=false. Elapsed: 4.070275574s
    Mar  2 01:35:33.164: INFO: The phase of Pod busybox-readonly-fsb3039001-dde1-4a14-8181-e5f12e0f5a69 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 01:35:35.178: INFO: Pod "busybox-readonly-fsb3039001-dde1-4a14-8181-e5f12e0f5a69": Phase="Pending", Reason="", readiness=false. Elapsed: 6.083570698s
    Mar  2 01:35:35.178: INFO: The phase of Pod busybox-readonly-fsb3039001-dde1-4a14-8181-e5f12e0f5a69 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 01:35:37.180: INFO: Pod "busybox-readonly-fsb3039001-dde1-4a14-8181-e5f12e0f5a69": Phase="Pending", Reason="", readiness=false. Elapsed: 8.086239907s
    Mar  2 01:35:37.180: INFO: The phase of Pod busybox-readonly-fsb3039001-dde1-4a14-8181-e5f12e0f5a69 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 01:35:39.138: INFO: Pod "busybox-readonly-fsb3039001-dde1-4a14-8181-e5f12e0f5a69": Phase="Pending", Reason="", readiness=false. Elapsed: 10.044247087s
    Mar  2 01:35:39.138: INFO: The phase of Pod busybox-readonly-fsb3039001-dde1-4a14-8181-e5f12e0f5a69 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 01:35:41.137: INFO: Pod "busybox-readonly-fsb3039001-dde1-4a14-8181-e5f12e0f5a69": Phase="Running", Reason="", readiness=true. Elapsed: 12.042812074s
    Mar  2 01:35:41.137: INFO: The phase of Pod busybox-readonly-fsb3039001-dde1-4a14-8181-e5f12e0f5a69 is Running (Ready = true)
    Mar  2 01:35:41.137: INFO: Pod "busybox-readonly-fsb3039001-dde1-4a14-8181-e5f12e0f5a69" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Mar  2 01:35:41.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-7158" for this suite. 03/02/23 01:35:41.278
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:35:41.3
Mar  2 01:35:41.300: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename secrets 03/02/23 01:35:41.302
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:35:41.377
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:35:41.389
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
STEP: Creating secret with name secret-test-map-a2293bed-2b1a-42db-9fee-e93cfd2316f6 03/02/23 01:35:41.399
STEP: Creating a pod to test consume secrets 03/02/23 01:35:41.413
Mar  2 01:35:41.474: INFO: Waiting up to 5m0s for pod "pod-secrets-87fc9c3a-8490-42cf-9724-f054656b1d56" in namespace "secrets-7025" to be "Succeeded or Failed"
Mar  2 01:35:41.507: INFO: Pod "pod-secrets-87fc9c3a-8490-42cf-9724-f054656b1d56": Phase="Pending", Reason="", readiness=false. Elapsed: 32.120389ms
Mar  2 01:35:43.523: INFO: Pod "pod-secrets-87fc9c3a-8490-42cf-9724-f054656b1d56": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04822605s
Mar  2 01:35:45.527: INFO: Pod "pod-secrets-87fc9c3a-8490-42cf-9724-f054656b1d56": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052366876s
Mar  2 01:35:47.538: INFO: Pod "pod-secrets-87fc9c3a-8490-42cf-9724-f054656b1d56": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.063146994s
STEP: Saw pod success 03/02/23 01:35:47.538
Mar  2 01:35:47.538: INFO: Pod "pod-secrets-87fc9c3a-8490-42cf-9724-f054656b1d56" satisfied condition "Succeeded or Failed"
Mar  2 01:35:47.564: INFO: Trying to get logs from node 10.132.92.143 pod pod-secrets-87fc9c3a-8490-42cf-9724-f054656b1d56 container secret-volume-test: <nil>
STEP: delete the pod 03/02/23 01:35:47.62
Mar  2 01:35:47.661: INFO: Waiting for pod pod-secrets-87fc9c3a-8490-42cf-9724-f054656b1d56 to disappear
Mar  2 01:35:47.675: INFO: Pod pod-secrets-87fc9c3a-8490-42cf-9724-f054656b1d56 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar  2 01:35:47.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7025" for this suite. 03/02/23 01:35:47.702
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":97,"skipped":1909,"failed":0}
------------------------------
• [SLOW TEST] [6.423 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:35:41.3
    Mar  2 01:35:41.300: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename secrets 03/02/23 01:35:41.302
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:35:41.377
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:35:41.389
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:88
    STEP: Creating secret with name secret-test-map-a2293bed-2b1a-42db-9fee-e93cfd2316f6 03/02/23 01:35:41.399
    STEP: Creating a pod to test consume secrets 03/02/23 01:35:41.413
    Mar  2 01:35:41.474: INFO: Waiting up to 5m0s for pod "pod-secrets-87fc9c3a-8490-42cf-9724-f054656b1d56" in namespace "secrets-7025" to be "Succeeded or Failed"
    Mar  2 01:35:41.507: INFO: Pod "pod-secrets-87fc9c3a-8490-42cf-9724-f054656b1d56": Phase="Pending", Reason="", readiness=false. Elapsed: 32.120389ms
    Mar  2 01:35:43.523: INFO: Pod "pod-secrets-87fc9c3a-8490-42cf-9724-f054656b1d56": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04822605s
    Mar  2 01:35:45.527: INFO: Pod "pod-secrets-87fc9c3a-8490-42cf-9724-f054656b1d56": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052366876s
    Mar  2 01:35:47.538: INFO: Pod "pod-secrets-87fc9c3a-8490-42cf-9724-f054656b1d56": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.063146994s
    STEP: Saw pod success 03/02/23 01:35:47.538
    Mar  2 01:35:47.538: INFO: Pod "pod-secrets-87fc9c3a-8490-42cf-9724-f054656b1d56" satisfied condition "Succeeded or Failed"
    Mar  2 01:35:47.564: INFO: Trying to get logs from node 10.132.92.143 pod pod-secrets-87fc9c3a-8490-42cf-9724-f054656b1d56 container secret-volume-test: <nil>
    STEP: delete the pod 03/02/23 01:35:47.62
    Mar  2 01:35:47.661: INFO: Waiting for pod pod-secrets-87fc9c3a-8490-42cf-9724-f054656b1d56 to disappear
    Mar  2 01:35:47.675: INFO: Pod pod-secrets-87fc9c3a-8490-42cf-9724-f054656b1d56 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar  2 01:35:47.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-7025" for this suite. 03/02/23 01:35:47.702
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:35:47.725
Mar  2 01:35:47.725: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename projected 03/02/23 01:35:47.726
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:35:47.781
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:35:47.792
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
Mar  2 01:35:47.833: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating projection with configMap that has name projected-configmap-test-upd-dd6500b6-3f43-4f0f-a280-923997a3ef92 03/02/23 01:35:47.833
STEP: Creating the pod 03/02/23 01:35:47.87
Mar  2 01:35:47.940: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8646c652-c304-48e7-8803-ea47b05d51bb" in namespace "projected-2512" to be "running and ready"
Mar  2 01:35:47.958: INFO: Pod "pod-projected-configmaps-8646c652-c304-48e7-8803-ea47b05d51bb": Phase="Pending", Reason="", readiness=false. Elapsed: 18.154657ms
Mar  2 01:35:47.958: INFO: The phase of Pod pod-projected-configmaps-8646c652-c304-48e7-8803-ea47b05d51bb is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:35:49.975: INFO: Pod "pod-projected-configmaps-8646c652-c304-48e7-8803-ea47b05d51bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035754804s
Mar  2 01:35:49.975: INFO: The phase of Pod pod-projected-configmaps-8646c652-c304-48e7-8803-ea47b05d51bb is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:35:51.986: INFO: Pod "pod-projected-configmaps-8646c652-c304-48e7-8803-ea47b05d51bb": Phase="Running", Reason="", readiness=true. Elapsed: 4.046944852s
Mar  2 01:35:51.987: INFO: The phase of Pod pod-projected-configmaps-8646c652-c304-48e7-8803-ea47b05d51bb is Running (Ready = true)
Mar  2 01:35:51.987: INFO: Pod "pod-projected-configmaps-8646c652-c304-48e7-8803-ea47b05d51bb" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-dd6500b6-3f43-4f0f-a280-923997a3ef92 03/02/23 01:35:52.038
STEP: waiting to observe update in volume 03/02/23 01:35:52.059
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar  2 01:37:05.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2512" for this suite. 03/02/23 01:37:05.802
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":98,"skipped":1926,"failed":0}
------------------------------
• [SLOW TEST] [78.098 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:35:47.725
    Mar  2 01:35:47.725: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename projected 03/02/23 01:35:47.726
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:35:47.781
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:35:47.792
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:123
    Mar  2 01:35:47.833: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-dd6500b6-3f43-4f0f-a280-923997a3ef92 03/02/23 01:35:47.833
    STEP: Creating the pod 03/02/23 01:35:47.87
    Mar  2 01:35:47.940: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8646c652-c304-48e7-8803-ea47b05d51bb" in namespace "projected-2512" to be "running and ready"
    Mar  2 01:35:47.958: INFO: Pod "pod-projected-configmaps-8646c652-c304-48e7-8803-ea47b05d51bb": Phase="Pending", Reason="", readiness=false. Elapsed: 18.154657ms
    Mar  2 01:35:47.958: INFO: The phase of Pod pod-projected-configmaps-8646c652-c304-48e7-8803-ea47b05d51bb is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 01:35:49.975: INFO: Pod "pod-projected-configmaps-8646c652-c304-48e7-8803-ea47b05d51bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035754804s
    Mar  2 01:35:49.975: INFO: The phase of Pod pod-projected-configmaps-8646c652-c304-48e7-8803-ea47b05d51bb is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 01:35:51.986: INFO: Pod "pod-projected-configmaps-8646c652-c304-48e7-8803-ea47b05d51bb": Phase="Running", Reason="", readiness=true. Elapsed: 4.046944852s
    Mar  2 01:35:51.987: INFO: The phase of Pod pod-projected-configmaps-8646c652-c304-48e7-8803-ea47b05d51bb is Running (Ready = true)
    Mar  2 01:35:51.987: INFO: Pod "pod-projected-configmaps-8646c652-c304-48e7-8803-ea47b05d51bb" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-dd6500b6-3f43-4f0f-a280-923997a3ef92 03/02/23 01:35:52.038
    STEP: waiting to observe update in volume 03/02/23 01:35:52.059
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar  2 01:37:05.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2512" for this suite. 03/02/23 01:37:05.802
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:37:05.825
Mar  2 01:37:05.827: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename emptydir 03/02/23 01:37:05.828
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:37:05.899
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:37:05.917
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
STEP: Creating a pod to test emptydir 0666 on node default medium 03/02/23 01:37:05.936
Mar  2 01:37:06.027: INFO: Waiting up to 5m0s for pod "pod-48b72b14-ff72-4745-bb4e-0928e43aab0d" in namespace "emptydir-6293" to be "Succeeded or Failed"
Mar  2 01:37:06.061: INFO: Pod "pod-48b72b14-ff72-4745-bb4e-0928e43aab0d": Phase="Pending", Reason="", readiness=false. Elapsed: 33.263762ms
Mar  2 01:37:08.078: INFO: Pod "pod-48b72b14-ff72-4745-bb4e-0928e43aab0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050318776s
Mar  2 01:37:10.078: INFO: Pod "pod-48b72b14-ff72-4745-bb4e-0928e43aab0d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050631633s
Mar  2 01:37:12.105: INFO: Pod "pod-48b72b14-ff72-4745-bb4e-0928e43aab0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.077694596s
STEP: Saw pod success 03/02/23 01:37:12.105
Mar  2 01:37:12.105: INFO: Pod "pod-48b72b14-ff72-4745-bb4e-0928e43aab0d" satisfied condition "Succeeded or Failed"
Mar  2 01:37:12.118: INFO: Trying to get logs from node 10.132.92.143 pod pod-48b72b14-ff72-4745-bb4e-0928e43aab0d container test-container: <nil>
STEP: delete the pod 03/02/23 01:37:12.203
Mar  2 01:37:12.257: INFO: Waiting for pod pod-48b72b14-ff72-4745-bb4e-0928e43aab0d to disappear
Mar  2 01:37:12.273: INFO: Pod pod-48b72b14-ff72-4745-bb4e-0928e43aab0d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  2 01:37:12.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6293" for this suite. 03/02/23 01:37:12.298
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":99,"skipped":1937,"failed":0}
------------------------------
• [SLOW TEST] [6.498 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:37:05.825
    Mar  2 01:37:05.827: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename emptydir 03/02/23 01:37:05.828
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:37:05.899
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:37:05.917
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:206
    STEP: Creating a pod to test emptydir 0666 on node default medium 03/02/23 01:37:05.936
    Mar  2 01:37:06.027: INFO: Waiting up to 5m0s for pod "pod-48b72b14-ff72-4745-bb4e-0928e43aab0d" in namespace "emptydir-6293" to be "Succeeded or Failed"
    Mar  2 01:37:06.061: INFO: Pod "pod-48b72b14-ff72-4745-bb4e-0928e43aab0d": Phase="Pending", Reason="", readiness=false. Elapsed: 33.263762ms
    Mar  2 01:37:08.078: INFO: Pod "pod-48b72b14-ff72-4745-bb4e-0928e43aab0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050318776s
    Mar  2 01:37:10.078: INFO: Pod "pod-48b72b14-ff72-4745-bb4e-0928e43aab0d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050631633s
    Mar  2 01:37:12.105: INFO: Pod "pod-48b72b14-ff72-4745-bb4e-0928e43aab0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.077694596s
    STEP: Saw pod success 03/02/23 01:37:12.105
    Mar  2 01:37:12.105: INFO: Pod "pod-48b72b14-ff72-4745-bb4e-0928e43aab0d" satisfied condition "Succeeded or Failed"
    Mar  2 01:37:12.118: INFO: Trying to get logs from node 10.132.92.143 pod pod-48b72b14-ff72-4745-bb4e-0928e43aab0d container test-container: <nil>
    STEP: delete the pod 03/02/23 01:37:12.203
    Mar  2 01:37:12.257: INFO: Waiting for pod pod-48b72b14-ff72-4745-bb4e-0928e43aab0d to disappear
    Mar  2 01:37:12.273: INFO: Pod pod-48b72b14-ff72-4745-bb4e-0928e43aab0d no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  2 01:37:12.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-6293" for this suite. 03/02/23 01:37:12.298
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:37:12.33
Mar  2 01:37:12.330: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename secrets 03/02/23 01:37:12.331
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:37:12.417
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:37:12.465
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
STEP: Creating secret with name secret-test-0d898647-edc3-4dbe-95be-98905c53d63c 03/02/23 01:37:12.478
STEP: Creating a pod to test consume secrets 03/02/23 01:37:12.492
Mar  2 01:37:12.569: INFO: Waiting up to 5m0s for pod "pod-secrets-2fce0e28-0b6a-43f1-961c-fd8a7bd82bcb" in namespace "secrets-3822" to be "Succeeded or Failed"
Mar  2 01:37:12.593: INFO: Pod "pod-secrets-2fce0e28-0b6a-43f1-961c-fd8a7bd82bcb": Phase="Pending", Reason="", readiness=false. Elapsed: 23.939219ms
Mar  2 01:37:14.623: INFO: Pod "pod-secrets-2fce0e28-0b6a-43f1-961c-fd8a7bd82bcb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053231007s
Mar  2 01:37:16.609: INFO: Pod "pod-secrets-2fce0e28-0b6a-43f1-961c-fd8a7bd82bcb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040045941s
Mar  2 01:37:18.613: INFO: Pod "pod-secrets-2fce0e28-0b6a-43f1-961c-fd8a7bd82bcb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043287567s
STEP: Saw pod success 03/02/23 01:37:18.613
Mar  2 01:37:18.613: INFO: Pod "pod-secrets-2fce0e28-0b6a-43f1-961c-fd8a7bd82bcb" satisfied condition "Succeeded or Failed"
Mar  2 01:37:18.645: INFO: Trying to get logs from node 10.132.92.143 pod pod-secrets-2fce0e28-0b6a-43f1-961c-fd8a7bd82bcb container secret-volume-test: <nil>
STEP: delete the pod 03/02/23 01:37:18.683
Mar  2 01:37:18.739: INFO: Waiting for pod pod-secrets-2fce0e28-0b6a-43f1-961c-fd8a7bd82bcb to disappear
Mar  2 01:37:18.756: INFO: Pod pod-secrets-2fce0e28-0b6a-43f1-961c-fd8a7bd82bcb no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar  2 01:37:18.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3822" for this suite. 03/02/23 01:37:18.777
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":100,"skipped":1973,"failed":0}
------------------------------
• [SLOW TEST] [6.467 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:37:12.33
    Mar  2 01:37:12.330: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename secrets 03/02/23 01:37:12.331
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:37:12.417
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:37:12.465
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:56
    STEP: Creating secret with name secret-test-0d898647-edc3-4dbe-95be-98905c53d63c 03/02/23 01:37:12.478
    STEP: Creating a pod to test consume secrets 03/02/23 01:37:12.492
    Mar  2 01:37:12.569: INFO: Waiting up to 5m0s for pod "pod-secrets-2fce0e28-0b6a-43f1-961c-fd8a7bd82bcb" in namespace "secrets-3822" to be "Succeeded or Failed"
    Mar  2 01:37:12.593: INFO: Pod "pod-secrets-2fce0e28-0b6a-43f1-961c-fd8a7bd82bcb": Phase="Pending", Reason="", readiness=false. Elapsed: 23.939219ms
    Mar  2 01:37:14.623: INFO: Pod "pod-secrets-2fce0e28-0b6a-43f1-961c-fd8a7bd82bcb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053231007s
    Mar  2 01:37:16.609: INFO: Pod "pod-secrets-2fce0e28-0b6a-43f1-961c-fd8a7bd82bcb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040045941s
    Mar  2 01:37:18.613: INFO: Pod "pod-secrets-2fce0e28-0b6a-43f1-961c-fd8a7bd82bcb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043287567s
    STEP: Saw pod success 03/02/23 01:37:18.613
    Mar  2 01:37:18.613: INFO: Pod "pod-secrets-2fce0e28-0b6a-43f1-961c-fd8a7bd82bcb" satisfied condition "Succeeded or Failed"
    Mar  2 01:37:18.645: INFO: Trying to get logs from node 10.132.92.143 pod pod-secrets-2fce0e28-0b6a-43f1-961c-fd8a7bd82bcb container secret-volume-test: <nil>
    STEP: delete the pod 03/02/23 01:37:18.683
    Mar  2 01:37:18.739: INFO: Waiting for pod pod-secrets-2fce0e28-0b6a-43f1-961c-fd8a7bd82bcb to disappear
    Mar  2 01:37:18.756: INFO: Pod pod-secrets-2fce0e28-0b6a-43f1-961c-fd8a7bd82bcb no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar  2 01:37:18.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-3822" for this suite. 03/02/23 01:37:18.777
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:37:18.798
Mar  2 01:37:18.799: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename emptydir 03/02/23 01:37:18.802
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:37:18.856
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:37:18.869
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
STEP: Creating a pod to test emptydir 0777 on tmpfs 03/02/23 01:37:18.881
Mar  2 01:37:18.946: INFO: Waiting up to 5m0s for pod "pod-c8a0a974-64da-4625-aa50-e37c789c51b9" in namespace "emptydir-2743" to be "Succeeded or Failed"
Mar  2 01:37:19.011: INFO: Pod "pod-c8a0a974-64da-4625-aa50-e37c789c51b9": Phase="Pending", Reason="", readiness=false. Elapsed: 64.965177ms
Mar  2 01:37:21.036: INFO: Pod "pod-c8a0a974-64da-4625-aa50-e37c789c51b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.090018007s
Mar  2 01:37:23.027: INFO: Pod "pod-c8a0a974-64da-4625-aa50-e37c789c51b9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.081599283s
Mar  2 01:37:25.044: INFO: Pod "pod-c8a0a974-64da-4625-aa50-e37c789c51b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.098074035s
STEP: Saw pod success 03/02/23 01:37:25.044
Mar  2 01:37:25.044: INFO: Pod "pod-c8a0a974-64da-4625-aa50-e37c789c51b9" satisfied condition "Succeeded or Failed"
Mar  2 01:37:25.066: INFO: Trying to get logs from node 10.132.92.143 pod pod-c8a0a974-64da-4625-aa50-e37c789c51b9 container test-container: <nil>
STEP: delete the pod 03/02/23 01:37:25.111
Mar  2 01:37:25.176: INFO: Waiting for pod pod-c8a0a974-64da-4625-aa50-e37c789c51b9 to disappear
Mar  2 01:37:25.200: INFO: Pod pod-c8a0a974-64da-4625-aa50-e37c789c51b9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  2 01:37:25.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2743" for this suite. 03/02/23 01:37:25.231
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":101,"skipped":1979,"failed":0}
------------------------------
• [SLOW TEST] [6.457 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:37:18.798
    Mar  2 01:37:18.799: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename emptydir 03/02/23 01:37:18.802
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:37:18.856
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:37:18.869
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:116
    STEP: Creating a pod to test emptydir 0777 on tmpfs 03/02/23 01:37:18.881
    Mar  2 01:37:18.946: INFO: Waiting up to 5m0s for pod "pod-c8a0a974-64da-4625-aa50-e37c789c51b9" in namespace "emptydir-2743" to be "Succeeded or Failed"
    Mar  2 01:37:19.011: INFO: Pod "pod-c8a0a974-64da-4625-aa50-e37c789c51b9": Phase="Pending", Reason="", readiness=false. Elapsed: 64.965177ms
    Mar  2 01:37:21.036: INFO: Pod "pod-c8a0a974-64da-4625-aa50-e37c789c51b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.090018007s
    Mar  2 01:37:23.027: INFO: Pod "pod-c8a0a974-64da-4625-aa50-e37c789c51b9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.081599283s
    Mar  2 01:37:25.044: INFO: Pod "pod-c8a0a974-64da-4625-aa50-e37c789c51b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.098074035s
    STEP: Saw pod success 03/02/23 01:37:25.044
    Mar  2 01:37:25.044: INFO: Pod "pod-c8a0a974-64da-4625-aa50-e37c789c51b9" satisfied condition "Succeeded or Failed"
    Mar  2 01:37:25.066: INFO: Trying to get logs from node 10.132.92.143 pod pod-c8a0a974-64da-4625-aa50-e37c789c51b9 container test-container: <nil>
    STEP: delete the pod 03/02/23 01:37:25.111
    Mar  2 01:37:25.176: INFO: Waiting for pod pod-c8a0a974-64da-4625-aa50-e37c789c51b9 to disappear
    Mar  2 01:37:25.200: INFO: Pod pod-c8a0a974-64da-4625-aa50-e37c789c51b9 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  2 01:37:25.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2743" for this suite. 03/02/23 01:37:25.231
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:37:25.259
Mar  2 01:37:25.259: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename crd-publish-openapi 03/02/23 01:37:25.261
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:37:25.322
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:37:25.36
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 03/02/23 01:37:25.373
Mar  2 01:37:25.374: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
Mar  2 01:37:40.565: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 01:38:33.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1274" for this suite. 03/02/23 01:38:33.884
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","completed":102,"skipped":1988,"failed":0}
------------------------------
• [SLOW TEST] [68.655 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:37:25.259
    Mar  2 01:37:25.259: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename crd-publish-openapi 03/02/23 01:37:25.261
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:37:25.322
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:37:25.36
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:356
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 03/02/23 01:37:25.373
    Mar  2 01:37:25.374: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    Mar  2 01:37:40.565: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 01:38:33.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-1274" for this suite. 03/02/23 01:38:33.884
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:38:33.917
Mar  2 01:38:33.917: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename crd-webhook 03/02/23 01:38:33.918
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:38:33.981
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:38:33.993
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 03/02/23 01:38:34.008
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 03/02/23 01:38:34.657
STEP: Deploying the custom resource conversion webhook pod 03/02/23 01:38:34.697
STEP: Wait for the deployment to be ready 03/02/23 01:38:34.747
Mar  2 01:38:34.775: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
Mar  2 01:38:36.817: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 1, 38, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 38, 34, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 38, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 38, 34, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/02/23 01:38:38.834
STEP: Verifying the service has paired with the endpoint 03/02/23 01:38:38.87
Mar  2 01:38:39.871: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Mar  2 01:38:39.884: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Creating a v1 custom resource 03/02/23 01:38:42.65
STEP: Create a v2 custom resource 03/02/23 01:38:42.724
STEP: List CRs in v1 03/02/23 01:38:42.889
STEP: List CRs in v2 03/02/23 01:38:42.919
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 01:38:43.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-3366" for this suite. 03/02/23 01:38:43.532
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","completed":103,"skipped":2036,"failed":0}
------------------------------
• [SLOW TEST] [9.884 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:38:33.917
    Mar  2 01:38:33.917: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename crd-webhook 03/02/23 01:38:33.918
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:38:33.981
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:38:33.993
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 03/02/23 01:38:34.008
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 03/02/23 01:38:34.657
    STEP: Deploying the custom resource conversion webhook pod 03/02/23 01:38:34.697
    STEP: Wait for the deployment to be ready 03/02/23 01:38:34.747
    Mar  2 01:38:34.775: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
    Mar  2 01:38:36.817: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 1, 38, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 38, 34, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 38, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 38, 34, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/02/23 01:38:38.834
    STEP: Verifying the service has paired with the endpoint 03/02/23 01:38:38.87
    Mar  2 01:38:39.871: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Mar  2 01:38:39.884: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Creating a v1 custom resource 03/02/23 01:38:42.65
    STEP: Create a v2 custom resource 03/02/23 01:38:42.724
    STEP: List CRs in v1 03/02/23 01:38:42.889
    STEP: List CRs in v2 03/02/23 01:38:42.919
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 01:38:43.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-3366" for this suite. 03/02/23 01:38:43.532
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:38:43.803
Mar  2 01:38:43.804: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename subpath 03/02/23 01:38:43.805
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:38:43.89
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:38:43.907
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/02/23 01:38:43.923
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-728k 03/02/23 01:38:43.967
STEP: Creating a pod to test atomic-volume-subpath 03/02/23 01:38:43.967
Mar  2 01:38:44.144: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-728k" in namespace "subpath-8260" to be "Succeeded or Failed"
Mar  2 01:38:44.155: INFO: Pod "pod-subpath-test-secret-728k": Phase="Pending", Reason="", readiness=false. Elapsed: 11.17487ms
Mar  2 01:38:46.173: INFO: Pod "pod-subpath-test-secret-728k": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028819192s
Mar  2 01:38:48.187: INFO: Pod "pod-subpath-test-secret-728k": Phase="Running", Reason="", readiness=true. Elapsed: 4.042705023s
Mar  2 01:38:50.169: INFO: Pod "pod-subpath-test-secret-728k": Phase="Running", Reason="", readiness=true. Elapsed: 6.025203557s
Mar  2 01:38:52.172: INFO: Pod "pod-subpath-test-secret-728k": Phase="Running", Reason="", readiness=true. Elapsed: 8.027686194s
Mar  2 01:38:54.174: INFO: Pod "pod-subpath-test-secret-728k": Phase="Running", Reason="", readiness=true. Elapsed: 10.030318121s
Mar  2 01:38:56.168: INFO: Pod "pod-subpath-test-secret-728k": Phase="Running", Reason="", readiness=true. Elapsed: 12.024480061s
Mar  2 01:38:58.171: INFO: Pod "pod-subpath-test-secret-728k": Phase="Running", Reason="", readiness=true. Elapsed: 14.027424061s
Mar  2 01:39:00.168: INFO: Pod "pod-subpath-test-secret-728k": Phase="Running", Reason="", readiness=true. Elapsed: 16.023818337s
Mar  2 01:39:02.171: INFO: Pod "pod-subpath-test-secret-728k": Phase="Running", Reason="", readiness=true. Elapsed: 18.027024431s
Mar  2 01:39:04.170: INFO: Pod "pod-subpath-test-secret-728k": Phase="Running", Reason="", readiness=true. Elapsed: 20.025668112s
Mar  2 01:39:06.173: INFO: Pod "pod-subpath-test-secret-728k": Phase="Running", Reason="", readiness=true. Elapsed: 22.028884267s
Mar  2 01:39:08.175: INFO: Pod "pod-subpath-test-secret-728k": Phase="Running", Reason="", readiness=false. Elapsed: 24.030676677s
Mar  2 01:39:10.169: INFO: Pod "pod-subpath-test-secret-728k": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.025013259s
STEP: Saw pod success 03/02/23 01:39:10.169
Mar  2 01:39:10.169: INFO: Pod "pod-subpath-test-secret-728k" satisfied condition "Succeeded or Failed"
Mar  2 01:39:10.179: INFO: Trying to get logs from node 10.132.92.143 pod pod-subpath-test-secret-728k container test-container-subpath-secret-728k: <nil>
STEP: delete the pod 03/02/23 01:39:10.265
Mar  2 01:39:10.305: INFO: Waiting for pod pod-subpath-test-secret-728k to disappear
Mar  2 01:39:10.318: INFO: Pod pod-subpath-test-secret-728k no longer exists
STEP: Deleting pod pod-subpath-test-secret-728k 03/02/23 01:39:10.318
Mar  2 01:39:10.318: INFO: Deleting pod "pod-subpath-test-secret-728k" in namespace "subpath-8260"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Mar  2 01:39:10.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8260" for this suite. 03/02/23 01:39:10.367
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]","completed":104,"skipped":2045,"failed":0}
------------------------------
• [SLOW TEST] [26.589 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:38:43.803
    Mar  2 01:38:43.804: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename subpath 03/02/23 01:38:43.805
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:38:43.89
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:38:43.907
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/02/23 01:38:43.923
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-728k 03/02/23 01:38:43.967
    STEP: Creating a pod to test atomic-volume-subpath 03/02/23 01:38:43.967
    Mar  2 01:38:44.144: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-728k" in namespace "subpath-8260" to be "Succeeded or Failed"
    Mar  2 01:38:44.155: INFO: Pod "pod-subpath-test-secret-728k": Phase="Pending", Reason="", readiness=false. Elapsed: 11.17487ms
    Mar  2 01:38:46.173: INFO: Pod "pod-subpath-test-secret-728k": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028819192s
    Mar  2 01:38:48.187: INFO: Pod "pod-subpath-test-secret-728k": Phase="Running", Reason="", readiness=true. Elapsed: 4.042705023s
    Mar  2 01:38:50.169: INFO: Pod "pod-subpath-test-secret-728k": Phase="Running", Reason="", readiness=true. Elapsed: 6.025203557s
    Mar  2 01:38:52.172: INFO: Pod "pod-subpath-test-secret-728k": Phase="Running", Reason="", readiness=true. Elapsed: 8.027686194s
    Mar  2 01:38:54.174: INFO: Pod "pod-subpath-test-secret-728k": Phase="Running", Reason="", readiness=true. Elapsed: 10.030318121s
    Mar  2 01:38:56.168: INFO: Pod "pod-subpath-test-secret-728k": Phase="Running", Reason="", readiness=true. Elapsed: 12.024480061s
    Mar  2 01:38:58.171: INFO: Pod "pod-subpath-test-secret-728k": Phase="Running", Reason="", readiness=true. Elapsed: 14.027424061s
    Mar  2 01:39:00.168: INFO: Pod "pod-subpath-test-secret-728k": Phase="Running", Reason="", readiness=true. Elapsed: 16.023818337s
    Mar  2 01:39:02.171: INFO: Pod "pod-subpath-test-secret-728k": Phase="Running", Reason="", readiness=true. Elapsed: 18.027024431s
    Mar  2 01:39:04.170: INFO: Pod "pod-subpath-test-secret-728k": Phase="Running", Reason="", readiness=true. Elapsed: 20.025668112s
    Mar  2 01:39:06.173: INFO: Pod "pod-subpath-test-secret-728k": Phase="Running", Reason="", readiness=true. Elapsed: 22.028884267s
    Mar  2 01:39:08.175: INFO: Pod "pod-subpath-test-secret-728k": Phase="Running", Reason="", readiness=false. Elapsed: 24.030676677s
    Mar  2 01:39:10.169: INFO: Pod "pod-subpath-test-secret-728k": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.025013259s
    STEP: Saw pod success 03/02/23 01:39:10.169
    Mar  2 01:39:10.169: INFO: Pod "pod-subpath-test-secret-728k" satisfied condition "Succeeded or Failed"
    Mar  2 01:39:10.179: INFO: Trying to get logs from node 10.132.92.143 pod pod-subpath-test-secret-728k container test-container-subpath-secret-728k: <nil>
    STEP: delete the pod 03/02/23 01:39:10.265
    Mar  2 01:39:10.305: INFO: Waiting for pod pod-subpath-test-secret-728k to disappear
    Mar  2 01:39:10.318: INFO: Pod pod-subpath-test-secret-728k no longer exists
    STEP: Deleting pod pod-subpath-test-secret-728k 03/02/23 01:39:10.318
    Mar  2 01:39:10.318: INFO: Deleting pod "pod-subpath-test-secret-728k" in namespace "subpath-8260"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Mar  2 01:39:10.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-8260" for this suite. 03/02/23 01:39:10.367
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:39:10.394
Mar  2 01:39:10.394: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename sched-pred 03/02/23 01:39:10.396
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:39:10.479
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:39:10.489
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Mar  2 01:39:10.501: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  2 01:39:10.618: INFO: Waiting for terminating namespaces to be deleted...
Mar  2 01:39:10.668: INFO: 
Logging pods the apiserver thinks is on node 10.132.92.143 before test
Mar  2 01:39:10.728: INFO: calico-node-99tft from calico-system started at 2023-03-01 22:49:50 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.729: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 01:39:10.729: INFO: calico-typha-b7f8b755-xlnll from calico-system started at 2023-03-01 22:49:57 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.729: INFO: 	Container calico-typha ready: true, restart count 0
Mar  2 01:39:10.729: INFO: ibm-cloud-provider-ip-169-56-11-242-558f6ff69-jtkgc from ibm-system started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.729: INFO: 	Container ibm-cloud-provider-ip-169-56-11-242 ready: true, restart count 0
Mar  2 01:39:10.729: INFO: ibm-keepalived-watcher-dp97c from kube-system started at 2023-03-01 22:48:16 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.729: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar  2 01:39:10.730: INFO: ibm-master-proxy-static-10.132.92.143 from kube-system started at 2023-03-01 22:48:15 +0000 UTC (2 container statuses recorded)
Mar  2 01:39:10.730: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar  2 01:39:10.730: INFO: 	Container pause ready: true, restart count 0
Mar  2 01:39:10.730: INFO: ibmcloud-block-storage-driver-zfc8r from kube-system started at 2023-03-01 22:48:23 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.730: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar  2 01:39:10.730: INFO: vpn-f6c799ddd-kvwzk from kube-system started at 2023-03-01 22:59:08 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.730: INFO: 	Container vpn ready: true, restart count 0
Mar  2 01:39:10.730: INFO: tuned-w95xr from openshift-cluster-node-tuning-operator started at 2023-03-01 22:55:42 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.730: INFO: 	Container tuned ready: true, restart count 0
Mar  2 01:39:10.730: INFO: console-6c8dcd4bdd-wsgwn from openshift-console started at 2023-03-01 22:59:22 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.731: INFO: 	Container console ready: true, restart count 0
Mar  2 01:39:10.731: INFO: dns-default-zr27n from openshift-dns started at 2023-03-01 22:55:41 +0000 UTC (2 container statuses recorded)
Mar  2 01:39:10.731: INFO: 	Container dns ready: true, restart count 0
Mar  2 01:39:10.731: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:39:10.731: INFO: node-resolver-7wklz from openshift-dns started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.731: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 01:39:10.731: INFO: node-ca-q4vcl from openshift-image-registry started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.731: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 01:39:10.731: INFO: registry-pvc-permissions-6wblc from openshift-image-registry started at 2023-03-01 22:58:39 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.731: INFO: 	Container pvc-permissions ready: false, restart count 0
Mar  2 01:39:10.732: INFO: ingress-canary-rwxvb from openshift-ingress-canary started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.732: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Mar  2 01:39:10.732: INFO: router-default-68bc8785b7-zkcbh from openshift-ingress started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.732: INFO: 	Container router ready: true, restart count 0
Mar  2 01:39:10.732: INFO: openshift-kube-proxy-zdts7 from openshift-kube-proxy started at 2023-03-01 22:49:00 +0000 UTC (2 container statuses recorded)
Mar  2 01:39:10.732: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  2 01:39:10.732: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:39:10.732: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-03-01 22:58:06 +0000 UTC (6 container statuses recorded)
Mar  2 01:39:10.732: INFO: 	Container alertmanager ready: true, restart count 1
Mar  2 01:39:10.732: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 01:39:10.732: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 01:39:10.732: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:39:10.732: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Mar  2 01:39:10.732: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 01:39:10.732: INFO: kube-state-metrics-554994774b-mttch from openshift-monitoring started at 2023-03-01 22:56:39 +0000 UTC (3 container statuses recorded)
Mar  2 01:39:10.732: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Mar  2 01:39:10.732: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Mar  2 01:39:10.732: INFO: 	Container kube-state-metrics ready: true, restart count 0
Mar  2 01:39:10.733: INFO: node-exporter-z52nh from openshift-monitoring started at 2023-03-01 22:56:40 +0000 UTC (2 container statuses recorded)
Mar  2 01:39:10.733: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:39:10.733: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 01:39:10.733: INFO: prometheus-adapter-95d69f68c-2rgvr from openshift-monitoring started at 2023-03-01 22:58:02 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.733: INFO: 	Container prometheus-adapter ready: true, restart count 0
Mar  2 01:39:10.733: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-03-01 22:58:12 +0000 UTC (6 container statuses recorded)
Mar  2 01:39:10.733: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 01:39:10.733: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:39:10.733: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Mar  2 01:39:10.733: INFO: 	Container prometheus ready: true, restart count 0
Mar  2 01:39:10.733: INFO: 	Container prometheus-proxy ready: true, restart count 0
Mar  2 01:39:10.733: INFO: 	Container thanos-sidecar ready: true, restart count 0
Mar  2 01:39:10.733: INFO: prometheus-operator-admission-webhook-6d5fbffb86-7xdt5 from openshift-monitoring started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.733: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Mar  2 01:39:10.733: INFO: telemeter-client-769c487d5b-fv8s4 from openshift-monitoring started at 2023-03-01 22:58:07 +0000 UTC (3 container statuses recorded)
Mar  2 01:39:10.733: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:39:10.733: INFO: 	Container reload ready: true, restart count 0
Mar  2 01:39:10.733: INFO: 	Container telemeter-client ready: true, restart count 0
Mar  2 01:39:10.733: INFO: thanos-querier-6cd8656bbb-6ffxj from openshift-monitoring started at 2023-03-01 22:56:48 +0000 UTC (6 container statuses recorded)
Mar  2 01:39:10.733: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:39:10.733: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Mar  2 01:39:10.733: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Mar  2 01:39:10.733: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 01:39:10.734: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 01:39:10.734: INFO: 	Container thanos-query ready: true, restart count 0
Mar  2 01:39:10.734: INFO: multus-48w86 from openshift-multus started at 2023-03-01 22:48:55 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.734: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 01:39:10.734: INFO: multus-additional-cni-plugins-d2kv5 from openshift-multus started at 2023-03-01 22:48:55 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.734: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Mar  2 01:39:10.734: INFO: multus-admission-controller-6f984f76c7-mtmdg from openshift-multus started at 2023-03-01 22:56:08 +0000 UTC (2 container statuses recorded)
Mar  2 01:39:10.734: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:39:10.734: INFO: 	Container multus-admission-controller ready: true, restart count 0
Mar  2 01:39:10.734: INFO: network-metrics-daemon-7jfj4 from openshift-multus started at 2023-03-01 22:48:56 +0000 UTC (2 container statuses recorded)
Mar  2 01:39:10.734: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:39:10.734: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Mar  2 01:39:10.734: INFO: network-check-target-w7x6h from openshift-network-diagnostics started at 2023-03-01 22:49:03 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.734: INFO: 	Container network-check-target-container ready: true, restart count 0
Mar  2 01:39:10.734: INFO: sonobuoy from sonobuoy started at 2023-03-02 01:07:05 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.734: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  2 01:39:10.734: INFO: sonobuoy-systemd-logs-daemon-set-7266eb5e626146f7-4c6tp from sonobuoy started at 2023-03-02 01:07:14 +0000 UTC (2 container statuses recorded)
Mar  2 01:39:10.734: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 01:39:10.734: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 01:39:10.734: INFO: 
Logging pods the apiserver thinks is on node 10.132.92.186 before test
Mar  2 01:39:10.818: INFO: calico-kube-controllers-5b6b964466-vlvht from calico-system started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.818: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar  2 01:39:10.818: INFO: calico-node-5vvbw from calico-system started at 2023-03-01 22:49:50 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.818: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 01:39:10.818: INFO: calico-typha-b7f8b755-bdnm9 from calico-system started at 2023-03-01 22:49:50 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.818: INFO: 	Container calico-typha ready: true, restart count 0
Mar  2 01:39:10.818: INFO: managed-storage-validation-webhooks-757b765d54-f72mn from ibm-odf-validation-webhook started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.818: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Mar  2 01:39:10.818: INFO: managed-storage-validation-webhooks-757b765d54-gshct from ibm-odf-validation-webhook started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.818: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Mar  2 01:39:10.818: INFO: managed-storage-validation-webhooks-757b765d54-h4j5p from ibm-odf-validation-webhook started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.818: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Mar  2 01:39:10.818: INFO: ibm-file-plugin-68bfbbdc7-77m77 from kube-system started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.818: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Mar  2 01:39:10.818: INFO: ibm-keepalived-watcher-knrjl from kube-system started at 2023-03-01 22:48:27 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.818: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar  2 01:39:10.818: INFO: ibm-master-proxy-static-10.132.92.186 from kube-system started at 2023-03-01 22:48:22 +0000 UTC (2 container statuses recorded)
Mar  2 01:39:10.818: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar  2 01:39:10.818: INFO: 	Container pause ready: true, restart count 0
Mar  2 01:39:10.818: INFO: ibm-storage-metrics-agent-6c459b7855-cv8d6 from kube-system started at 2023-03-01 22:51:20 +0000 UTC (2 container statuses recorded)
Mar  2 01:39:10.818: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
Mar  2 01:39:10.818: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Mar  2 01:39:10.818: INFO: ibm-storage-watcher-67589fd788-z5w99 from kube-system started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.818: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Mar  2 01:39:10.818: INFO: ibmcloud-block-storage-driver-l498z from kube-system started at 2023-03-01 22:48:36 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.818: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar  2 01:39:10.818: INFO: ibmcloud-block-storage-plugin-86f4cbdf95-klfqp from kube-system started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.818: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Mar  2 01:39:10.818: INFO: cluster-node-tuning-operator-5df67c68d9-gd552 from openshift-cluster-node-tuning-operator started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.818: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Mar  2 01:39:10.818: INFO: tuned-h4jnq from openshift-cluster-node-tuning-operator started at 2023-03-01 22:55:42 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.818: INFO: 	Container tuned ready: true, restart count 0
Mar  2 01:39:10.818: INFO: cluster-samples-operator-5948f44764-88mzn from openshift-cluster-samples-operator started at 2023-03-01 22:51:20 +0000 UTC (2 container statuses recorded)
Mar  2 01:39:10.818: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Mar  2 01:39:10.818: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Mar  2 01:39:10.818: INFO: cluster-storage-operator-54cf568cb7-smstf from openshift-cluster-storage-operator started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.818: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Mar  2 01:39:10.818: INFO: csi-snapshot-controller-85d666c956-5hhrv from openshift-cluster-storage-operator started at 2023-03-01 22:51:57 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.818: INFO: 	Container snapshot-controller ready: true, restart count 0
Mar  2 01:39:10.818: INFO: csi-snapshot-controller-operator-6b76cb8c94-9bvdj from openshift-cluster-storage-operator started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.818: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Mar  2 01:39:10.818: INFO: csi-snapshot-webhook-6cbf55b956-z69cb from openshift-cluster-storage-operator started at 2023-03-01 22:51:57 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.818: INFO: 	Container webhook ready: true, restart count 0
Mar  2 01:39:10.818: INFO: console-operator-6975f688f8-xwx92 from openshift-console-operator started at 2023-03-01 22:51:20 +0000 UTC (2 container statuses recorded)
Mar  2 01:39:10.818: INFO: 	Container console-operator ready: true, restart count 1
Mar  2 01:39:10.818: INFO: 	Container conversion-webhook-server ready: true, restart count 3
Mar  2 01:39:10.818: INFO: console-6c8dcd4bdd-c2dbn from openshift-console started at 2023-03-01 22:59:49 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.818: INFO: 	Container console ready: true, restart count 0
Mar  2 01:39:10.818: INFO: downloads-8b4bf4d5-pjlj4 from openshift-console started at 2023-03-01 22:51:55 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.818: INFO: 	Container download-server ready: true, restart count 0
Mar  2 01:39:10.818: INFO: dns-operator-b8598b55c-87ghp from openshift-dns-operator started at 2023-03-01 22:51:20 +0000 UTC (2 container statuses recorded)
Mar  2 01:39:10.818: INFO: 	Container dns-operator ready: true, restart count 0
Mar  2 01:39:10.818: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:39:10.818: INFO: dns-default-rdd5h from openshift-dns started at 2023-03-01 22:55:41 +0000 UTC (2 container statuses recorded)
Mar  2 01:39:10.818: INFO: 	Container dns ready: true, restart count 0
Mar  2 01:39:10.818: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:39:10.818: INFO: node-resolver-9wrks from openshift-dns started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.818: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 01:39:10.818: INFO: cluster-image-registry-operator-797c5fb5f8-tvjwj from openshift-image-registry started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.818: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Mar  2 01:39:10.818: INFO: node-ca-6ffsq from openshift-image-registry started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.818: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 01:39:10.818: INFO: ingress-canary-zgjd4 from openshift-ingress-canary started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.818: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Mar  2 01:39:10.818: INFO: ingress-operator-7b8c5ddb4b-wr6sb from openshift-ingress-operator started at 2023-03-01 22:51:20 +0000 UTC (2 container statuses recorded)
Mar  2 01:39:10.818: INFO: 	Container ingress-operator ready: true, restart count 0
Mar  2 01:39:10.818: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:39:10.818: INFO: insights-operator-7b97bdcbd8-5zm5p from openshift-insights started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.818: INFO: 	Container insights-operator ready: true, restart count 1
Mar  2 01:39:10.818: INFO: openshift-kube-proxy-xgktp from openshift-kube-proxy started at 2023-03-01 22:49:00 +0000 UTC (2 container statuses recorded)
Mar  2 01:39:10.818: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  2 01:39:10.818: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:39:10.818: INFO: kube-storage-version-migrator-operator-78bc45549c-qmzfr from openshift-kube-storage-version-migrator-operator started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.818: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Mar  2 01:39:10.818: INFO: marketplace-operator-66f5c8895c-l9r8f from openshift-marketplace started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.818: INFO: 	Container marketplace-operator ready: true, restart count 0
Mar  2 01:39:10.818: INFO: cluster-monitoring-operator-7ffc6575dd-kkb8m from openshift-monitoring started at 2023-03-01 22:51:20 +0000 UTC (2 container statuses recorded)
Mar  2 01:39:10.818: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Mar  2 01:39:10.818: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:39:10.818: INFO: node-exporter-95fzh from openshift-monitoring started at 2023-03-01 22:56:40 +0000 UTC (2 container statuses recorded)
Mar  2 01:39:10.818: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:39:10.818: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 01:39:10.818: INFO: multus-89qpj from openshift-multus started at 2023-03-01 22:48:55 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.818: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 01:39:10.818: INFO: multus-additional-cni-plugins-ff7kf from openshift-multus started at 2023-03-01 22:48:55 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.818: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Mar  2 01:39:10.818: INFO: network-metrics-daemon-7m69v from openshift-multus started at 2023-03-01 22:48:56 +0000 UTC (2 container statuses recorded)
Mar  2 01:39:10.818: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:39:10.818: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Mar  2 01:39:10.818: INFO: network-check-source-848f87f9d4-bxlxt from openshift-network-diagnostics started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.818: INFO: 	Container check-endpoints ready: true, restart count 0
Mar  2 01:39:10.818: INFO: network-check-target-jqnqt from openshift-network-diagnostics started at 2023-03-01 22:49:03 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.818: INFO: 	Container network-check-target-container ready: true, restart count 0
Mar  2 01:39:10.819: INFO: catalog-operator-657ff7bb9f-c7tqq from openshift-operator-lifecycle-manager started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.819: INFO: 	Container catalog-operator ready: true, restart count 0
Mar  2 01:39:10.819: INFO: collect-profiles-27962010-nhpf6 from openshift-operator-lifecycle-manager started at 2023-03-02 01:30:00 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.819: INFO: 	Container collect-profiles ready: false, restart count 0
Mar  2 01:39:10.819: INFO: olm-operator-56ccd7794f-4ffrj from openshift-operator-lifecycle-manager started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.819: INFO: 	Container olm-operator ready: true, restart count 0
Mar  2 01:39:10.819: INFO: package-server-manager-599745f678-rmjg4 from openshift-operator-lifecycle-manager started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.819: INFO: 	Container package-server-manager ready: true, restart count 0
Mar  2 01:39:10.819: INFO: packageserver-6c7c56df95-jq89f from openshift-operator-lifecycle-manager started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.819: INFO: 	Container packageserver ready: true, restart count 0
Mar  2 01:39:10.819: INFO: metrics-784dfd67b7-rjkpg from openshift-roks-metrics started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.819: INFO: 	Container metrics ready: true, restart count 3
Mar  2 01:39:10.819: INFO: push-gateway-646cc86d79-cgtct from openshift-roks-metrics started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.819: INFO: 	Container push-gateway ready: true, restart count 0
Mar  2 01:39:10.819: INFO: service-ca-operator-6f8744cbc5-w5srs from openshift-service-ca-operator started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.819: INFO: 	Container service-ca-operator ready: true, restart count 1
Mar  2 01:39:10.819: INFO: sonobuoy-systemd-logs-daemon-set-7266eb5e626146f7-n2dsq from sonobuoy started at 2023-03-02 01:07:14 +0000 UTC (2 container statuses recorded)
Mar  2 01:39:10.819: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 01:39:10.819: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 01:39:10.819: INFO: 
Logging pods the apiserver thinks is on node 10.132.92.188 before test
Mar  2 01:39:10.866: INFO: calico-node-mkvsm from calico-system started at 2023-03-01 22:49:50 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.866: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 01:39:10.866: INFO: ibm-cloud-provider-ip-169-56-11-242-558f6ff69-xvh2v from ibm-system started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.866: INFO: 	Container ibm-cloud-provider-ip-169-56-11-242 ready: true, restart count 0
Mar  2 01:39:10.866: INFO: ibm-keepalived-watcher-zwfb8 from kube-system started at 2023-03-01 22:48:10 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.866: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar  2 01:39:10.867: INFO: ibm-master-proxy-static-10.132.92.188 from kube-system started at 2023-03-01 22:48:07 +0000 UTC (2 container statuses recorded)
Mar  2 01:39:10.867: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar  2 01:39:10.867: INFO: 	Container pause ready: true, restart count 0
Mar  2 01:39:10.867: INFO: ibmcloud-block-storage-driver-qbg9f from kube-system started at 2023-03-01 22:48:16 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.867: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar  2 01:39:10.867: INFO: tuned-5z98r from openshift-cluster-node-tuning-operator started at 2023-03-01 22:55:42 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.867: INFO: 	Container tuned ready: true, restart count 0
Mar  2 01:39:10.867: INFO: csi-snapshot-controller-85d666c956-xsh6c from openshift-cluster-storage-operator started at 2023-03-01 22:51:57 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.867: INFO: 	Container snapshot-controller ready: true, restart count 0
Mar  2 01:39:10.867: INFO: csi-snapshot-webhook-6cbf55b956-wq8bh from openshift-cluster-storage-operator started at 2023-03-01 22:51:57 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.867: INFO: 	Container webhook ready: true, restart count 0
Mar  2 01:39:10.867: INFO: downloads-8b4bf4d5-b9kfz from openshift-console started at 2023-03-01 22:51:55 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.867: INFO: 	Container download-server ready: true, restart count 0
Mar  2 01:39:10.867: INFO: dns-default-nt82k from openshift-dns started at 2023-03-01 22:55:41 +0000 UTC (2 container statuses recorded)
Mar  2 01:39:10.867: INFO: 	Container dns ready: true, restart count 0
Mar  2 01:39:10.867: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:39:10.867: INFO: node-resolver-wcng9 from openshift-dns started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.868: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 01:39:10.868: INFO: image-pruner-27961920-wd4pv from openshift-image-registry started at 2023-03-02 00:00:00 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.868: INFO: 	Container image-pruner ready: false, restart count 0
Mar  2 01:39:10.868: INFO: image-registry-7fb7bd8df6-lb2bt from openshift-image-registry started at 2023-03-01 22:58:31 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.868: INFO: 	Container registry ready: true, restart count 0
Mar  2 01:39:10.868: INFO: node-ca-qlt4w from openshift-image-registry started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.868: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 01:39:10.868: INFO: ingress-canary-qvcz9 from openshift-ingress-canary started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.868: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Mar  2 01:39:10.868: INFO: router-default-68bc8785b7-wz8ct from openshift-ingress started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.868: INFO: 	Container router ready: true, restart count 0
Mar  2 01:39:10.868: INFO: openshift-kube-proxy-mth7h from openshift-kube-proxy started at 2023-03-01 22:49:00 +0000 UTC (2 container statuses recorded)
Mar  2 01:39:10.868: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  2 01:39:10.868: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:39:10.868: INFO: migrator-5687b466dd-glln7 from openshift-kube-storage-version-migrator started at 2023-03-01 22:51:57 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.868: INFO: 	Container migrator ready: true, restart count 0
Mar  2 01:39:10.868: INFO: certified-operators-8n5vw from openshift-marketplace started at 2023-03-02 00:13:35 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.868: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 01:39:10.869: INFO: community-operators-lngtl from openshift-marketplace started at 2023-03-01 22:54:15 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.869: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 01:39:10.869: INFO: redhat-marketplace-ld8wh from openshift-marketplace started at 2023-03-02 00:27:02 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.869: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 01:39:10.869: INFO: redhat-operators-tk548 from openshift-marketplace started at 2023-03-01 22:54:14 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.869: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 01:39:10.869: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-03-01 22:58:39 +0000 UTC (6 container statuses recorded)
Mar  2 01:39:10.869: INFO: 	Container alertmanager ready: true, restart count 1
Mar  2 01:39:10.869: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 01:39:10.869: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 01:39:10.869: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:39:10.869: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Mar  2 01:39:10.869: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 01:39:10.869: INFO: node-exporter-gj7lr from openshift-monitoring started at 2023-03-01 22:56:39 +0000 UTC (2 container statuses recorded)
Mar  2 01:39:10.869: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:39:10.869: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 01:39:10.869: INFO: openshift-state-metrics-56599d9b6d-6sw48 from openshift-monitoring started at 2023-03-01 22:56:39 +0000 UTC (3 container statuses recorded)
Mar  2 01:39:10.869: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Mar  2 01:39:10.869: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Mar  2 01:39:10.869: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Mar  2 01:39:10.869: INFO: prometheus-adapter-95d69f68c-gtx2d from openshift-monitoring started at 2023-03-01 22:58:02 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.870: INFO: 	Container prometheus-adapter ready: true, restart count 0
Mar  2 01:39:10.870: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-03-01 22:58:30 +0000 UTC (6 container statuses recorded)
Mar  2 01:39:10.870: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 01:39:10.870: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:39:10.870: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Mar  2 01:39:10.870: INFO: 	Container prometheus ready: true, restart count 0
Mar  2 01:39:10.870: INFO: 	Container prometheus-proxy ready: true, restart count 0
Mar  2 01:39:10.870: INFO: 	Container thanos-sidecar ready: true, restart count 0
Mar  2 01:39:10.870: INFO: prometheus-operator-9bf584898-89sgf from openshift-monitoring started at 2023-03-01 22:56:19 +0000 UTC (2 container statuses recorded)
Mar  2 01:39:10.870: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:39:10.870: INFO: 	Container prometheus-operator ready: true, restart count 0
Mar  2 01:39:10.870: INFO: prometheus-operator-admission-webhook-6d5fbffb86-hhfbw from openshift-monitoring started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.870: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Mar  2 01:39:10.870: INFO: thanos-querier-6cd8656bbb-8hv4v from openshift-monitoring started at 2023-03-01 22:56:48 +0000 UTC (6 container statuses recorded)
Mar  2 01:39:10.870: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:39:10.870: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Mar  2 01:39:10.870: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Mar  2 01:39:10.870: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 01:39:10.871: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 01:39:10.871: INFO: 	Container thanos-query ready: true, restart count 0
Mar  2 01:39:10.871: INFO: multus-additional-cni-plugins-5pjnd from openshift-multus started at 2023-03-01 22:48:55 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.871: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Mar  2 01:39:10.871: INFO: multus-admission-controller-6f984f76c7-hclf4 from openshift-multus started at 2023-03-01 22:55:41 +0000 UTC (2 container statuses recorded)
Mar  2 01:39:10.871: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:39:10.871: INFO: 	Container multus-admission-controller ready: true, restart count 0
Mar  2 01:39:10.871: INFO: multus-g4k8f from openshift-multus started at 2023-03-01 22:48:55 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.871: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 01:39:10.871: INFO: network-metrics-daemon-95rvd from openshift-multus started at 2023-03-01 22:48:56 +0000 UTC (2 container statuses recorded)
Mar  2 01:39:10.871: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:39:10.871: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Mar  2 01:39:10.871: INFO: network-check-target-4nkgt from openshift-network-diagnostics started at 2023-03-01 22:49:03 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.871: INFO: 	Container network-check-target-container ready: true, restart count 0
Mar  2 01:39:10.871: INFO: network-operator-68bbf8cc76-mg97z from openshift-network-operator started at 2023-03-01 22:48:19 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.871: INFO: 	Container network-operator ready: true, restart count 1
Mar  2 01:39:10.871: INFO: collect-profiles-27961980-jmjzh from openshift-operator-lifecycle-manager started at 2023-03-02 01:00:00 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.871: INFO: 	Container collect-profiles ready: false, restart count 0
Mar  2 01:39:10.871: INFO: collect-profiles-27961995-9hjgn from openshift-operator-lifecycle-manager started at 2023-03-02 01:15:00 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.871: INFO: 	Container collect-profiles ready: false, restart count 0
Mar  2 01:39:10.872: INFO: packageserver-6c7c56df95-7wr7f from openshift-operator-lifecycle-manager started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.872: INFO: 	Container packageserver ready: true, restart count 0
Mar  2 01:39:10.872: INFO: service-ca-578d67f6f8-89gtp from openshift-service-ca started at 2023-03-01 22:52:15 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.872: INFO: 	Container service-ca-controller ready: true, restart count 0
Mar  2 01:39:10.872: INFO: sonobuoy-e2e-job-c6c00a6e72aa4160 from sonobuoy started at 2023-03-02 01:07:14 +0000 UTC (2 container statuses recorded)
Mar  2 01:39:10.872: INFO: 	Container e2e ready: true, restart count 0
Mar  2 01:39:10.872: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 01:39:10.872: INFO: sonobuoy-systemd-logs-daemon-set-7266eb5e626146f7-fgqrz from sonobuoy started at 2023-03-02 01:07:14 +0000 UTC (2 container statuses recorded)
Mar  2 01:39:10.872: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 01:39:10.872: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 01:39:10.872: INFO: tigera-operator-84f4f4565b-5w7s7 from tigera-operator started at 2023-03-01 22:48:19 +0000 UTC (1 container statuses recorded)
Mar  2 01:39:10.872: INFO: 	Container tigera-operator ready: true, restart count 2
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
STEP: verifying the node has the label node 10.132.92.143 03/02/23 01:39:11.008
STEP: verifying the node has the label node 10.132.92.186 03/02/23 01:39:11.054
STEP: verifying the node has the label node 10.132.92.188 03/02/23 01:39:11.116
Mar  2 01:39:11.221: INFO: Pod calico-kube-controllers-5b6b964466-vlvht requesting resource cpu=10m on Node 10.132.92.186
Mar  2 01:39:11.221: INFO: Pod calico-node-5vvbw requesting resource cpu=250m on Node 10.132.92.186
Mar  2 01:39:11.221: INFO: Pod calico-node-99tft requesting resource cpu=250m on Node 10.132.92.143
Mar  2 01:39:11.221: INFO: Pod calico-node-mkvsm requesting resource cpu=250m on Node 10.132.92.188
Mar  2 01:39:11.221: INFO: Pod calico-typha-b7f8b755-bdnm9 requesting resource cpu=250m on Node 10.132.92.186
Mar  2 01:39:11.221: INFO: Pod calico-typha-b7f8b755-xlnll requesting resource cpu=250m on Node 10.132.92.143
Mar  2 01:39:11.221: INFO: Pod managed-storage-validation-webhooks-757b765d54-f72mn requesting resource cpu=10m on Node 10.132.92.186
Mar  2 01:39:11.221: INFO: Pod managed-storage-validation-webhooks-757b765d54-gshct requesting resource cpu=10m on Node 10.132.92.186
Mar  2 01:39:11.221: INFO: Pod managed-storage-validation-webhooks-757b765d54-h4j5p requesting resource cpu=10m on Node 10.132.92.186
Mar  2 01:39:11.221: INFO: Pod ibm-cloud-provider-ip-169-56-11-242-558f6ff69-jtkgc requesting resource cpu=5m on Node 10.132.92.143
Mar  2 01:39:11.221: INFO: Pod ibm-cloud-provider-ip-169-56-11-242-558f6ff69-xvh2v requesting resource cpu=5m on Node 10.132.92.188
Mar  2 01:39:11.221: INFO: Pod ibm-file-plugin-68bfbbdc7-77m77 requesting resource cpu=50m on Node 10.132.92.186
Mar  2 01:39:11.221: INFO: Pod ibm-keepalived-watcher-dp97c requesting resource cpu=5m on Node 10.132.92.143
Mar  2 01:39:11.221: INFO: Pod ibm-keepalived-watcher-knrjl requesting resource cpu=5m on Node 10.132.92.186
Mar  2 01:39:11.221: INFO: Pod ibm-keepalived-watcher-zwfb8 requesting resource cpu=5m on Node 10.132.92.188
Mar  2 01:39:11.221: INFO: Pod ibm-master-proxy-static-10.132.92.143 requesting resource cpu=26m on Node 10.132.92.143
Mar  2 01:39:11.221: INFO: Pod ibm-master-proxy-static-10.132.92.186 requesting resource cpu=26m on Node 10.132.92.186
Mar  2 01:39:11.221: INFO: Pod ibm-master-proxy-static-10.132.92.188 requesting resource cpu=26m on Node 10.132.92.188
Mar  2 01:39:11.221: INFO: Pod ibm-storage-metrics-agent-6c459b7855-cv8d6 requesting resource cpu=60m on Node 10.132.92.186
Mar  2 01:39:11.221: INFO: Pod ibm-storage-watcher-67589fd788-z5w99 requesting resource cpu=50m on Node 10.132.92.186
Mar  2 01:39:11.221: INFO: Pod ibmcloud-block-storage-driver-l498z requesting resource cpu=50m on Node 10.132.92.186
Mar  2 01:39:11.221: INFO: Pod ibmcloud-block-storage-driver-qbg9f requesting resource cpu=50m on Node 10.132.92.188
Mar  2 01:39:11.221: INFO: Pod ibmcloud-block-storage-driver-zfc8r requesting resource cpu=50m on Node 10.132.92.143
Mar  2 01:39:11.221: INFO: Pod ibmcloud-block-storage-plugin-86f4cbdf95-klfqp requesting resource cpu=50m on Node 10.132.92.186
Mar  2 01:39:11.221: INFO: Pod vpn-f6c799ddd-kvwzk requesting resource cpu=5m on Node 10.132.92.143
Mar  2 01:39:11.221: INFO: Pod cluster-node-tuning-operator-5df67c68d9-gd552 requesting resource cpu=10m on Node 10.132.92.186
Mar  2 01:39:11.221: INFO: Pod tuned-5z98r requesting resource cpu=10m on Node 10.132.92.188
Mar  2 01:39:11.221: INFO: Pod tuned-h4jnq requesting resource cpu=10m on Node 10.132.92.186
Mar  2 01:39:11.221: INFO: Pod tuned-w95xr requesting resource cpu=10m on Node 10.132.92.143
Mar  2 01:39:11.221: INFO: Pod cluster-samples-operator-5948f44764-88mzn requesting resource cpu=20m on Node 10.132.92.186
Mar  2 01:39:11.221: INFO: Pod cluster-storage-operator-54cf568cb7-smstf requesting resource cpu=10m on Node 10.132.92.186
Mar  2 01:39:11.221: INFO: Pod csi-snapshot-controller-85d666c956-5hhrv requesting resource cpu=10m on Node 10.132.92.186
Mar  2 01:39:11.221: INFO: Pod csi-snapshot-controller-85d666c956-xsh6c requesting resource cpu=10m on Node 10.132.92.188
Mar  2 01:39:11.221: INFO: Pod csi-snapshot-controller-operator-6b76cb8c94-9bvdj requesting resource cpu=10m on Node 10.132.92.186
Mar  2 01:39:11.221: INFO: Pod csi-snapshot-webhook-6cbf55b956-wq8bh requesting resource cpu=10m on Node 10.132.92.188
Mar  2 01:39:11.221: INFO: Pod csi-snapshot-webhook-6cbf55b956-z69cb requesting resource cpu=10m on Node 10.132.92.186
Mar  2 01:39:11.221: INFO: Pod console-operator-6975f688f8-xwx92 requesting resource cpu=20m on Node 10.132.92.186
Mar  2 01:39:11.221: INFO: Pod console-6c8dcd4bdd-c2dbn requesting resource cpu=10m on Node 10.132.92.186
Mar  2 01:39:11.221: INFO: Pod console-6c8dcd4bdd-wsgwn requesting resource cpu=10m on Node 10.132.92.143
Mar  2 01:39:11.221: INFO: Pod downloads-8b4bf4d5-b9kfz requesting resource cpu=10m on Node 10.132.92.188
Mar  2 01:39:11.221: INFO: Pod downloads-8b4bf4d5-pjlj4 requesting resource cpu=10m on Node 10.132.92.186
Mar  2 01:39:11.221: INFO: Pod dns-operator-b8598b55c-87ghp requesting resource cpu=20m on Node 10.132.92.186
Mar  2 01:39:11.221: INFO: Pod dns-default-nt82k requesting resource cpu=60m on Node 10.132.92.188
Mar  2 01:39:11.221: INFO: Pod dns-default-rdd5h requesting resource cpu=60m on Node 10.132.92.186
Mar  2 01:39:11.221: INFO: Pod dns-default-zr27n requesting resource cpu=60m on Node 10.132.92.143
Mar  2 01:39:11.221: INFO: Pod node-resolver-7wklz requesting resource cpu=5m on Node 10.132.92.143
Mar  2 01:39:11.221: INFO: Pod node-resolver-9wrks requesting resource cpu=5m on Node 10.132.92.186
Mar  2 01:39:11.221: INFO: Pod node-resolver-wcng9 requesting resource cpu=5m on Node 10.132.92.188
Mar  2 01:39:11.221: INFO: Pod cluster-image-registry-operator-797c5fb5f8-tvjwj requesting resource cpu=10m on Node 10.132.92.186
Mar  2 01:39:11.221: INFO: Pod image-registry-7fb7bd8df6-lb2bt requesting resource cpu=100m on Node 10.132.92.188
Mar  2 01:39:11.221: INFO: Pod node-ca-6ffsq requesting resource cpu=10m on Node 10.132.92.186
Mar  2 01:39:11.221: INFO: Pod node-ca-q4vcl requesting resource cpu=10m on Node 10.132.92.143
Mar  2 01:39:11.221: INFO: Pod node-ca-qlt4w requesting resource cpu=10m on Node 10.132.92.188
Mar  2 01:39:11.221: INFO: Pod ingress-canary-qvcz9 requesting resource cpu=10m on Node 10.132.92.188
Mar  2 01:39:11.221: INFO: Pod ingress-canary-rwxvb requesting resource cpu=10m on Node 10.132.92.143
Mar  2 01:39:11.221: INFO: Pod ingress-canary-zgjd4 requesting resource cpu=10m on Node 10.132.92.186
Mar  2 01:39:11.221: INFO: Pod ingress-operator-7b8c5ddb4b-wr6sb requesting resource cpu=20m on Node 10.132.92.186
Mar  2 01:39:11.221: INFO: Pod router-default-68bc8785b7-wz8ct requesting resource cpu=100m on Node 10.132.92.188
Mar  2 01:39:11.221: INFO: Pod router-default-68bc8785b7-zkcbh requesting resource cpu=100m on Node 10.132.92.143
Mar  2 01:39:11.221: INFO: Pod insights-operator-7b97bdcbd8-5zm5p requesting resource cpu=10m on Node 10.132.92.186
Mar  2 01:39:11.221: INFO: Pod openshift-kube-proxy-mth7h requesting resource cpu=110m on Node 10.132.92.188
Mar  2 01:39:11.221: INFO: Pod openshift-kube-proxy-xgktp requesting resource cpu=110m on Node 10.132.92.186
Mar  2 01:39:11.221: INFO: Pod openshift-kube-proxy-zdts7 requesting resource cpu=110m on Node 10.132.92.143
Mar  2 01:39:11.221: INFO: Pod kube-storage-version-migrator-operator-78bc45549c-qmzfr requesting resource cpu=10m on Node 10.132.92.186
Mar  2 01:39:11.221: INFO: Pod migrator-5687b466dd-glln7 requesting resource cpu=10m on Node 10.132.92.188
Mar  2 01:39:11.221: INFO: Pod certified-operators-8n5vw requesting resource cpu=10m on Node 10.132.92.188
Mar  2 01:39:11.221: INFO: Pod community-operators-lngtl requesting resource cpu=10m on Node 10.132.92.188
Mar  2 01:39:11.221: INFO: Pod marketplace-operator-66f5c8895c-l9r8f requesting resource cpu=10m on Node 10.132.92.186
Mar  2 01:39:11.221: INFO: Pod redhat-marketplace-ld8wh requesting resource cpu=10m on Node 10.132.92.188
Mar  2 01:39:11.221: INFO: Pod redhat-operators-tk548 requesting resource cpu=10m on Node 10.132.92.188
Mar  2 01:39:11.221: INFO: Pod alertmanager-main-0 requesting resource cpu=9m on Node 10.132.92.188
Mar  2 01:39:11.221: INFO: Pod alertmanager-main-1 requesting resource cpu=9m on Node 10.132.92.143
Mar  2 01:39:11.221: INFO: Pod cluster-monitoring-operator-7ffc6575dd-kkb8m requesting resource cpu=11m on Node 10.132.92.186
Mar  2 01:39:11.221: INFO: Pod kube-state-metrics-554994774b-mttch requesting resource cpu=4m on Node 10.132.92.143
Mar  2 01:39:11.221: INFO: Pod node-exporter-95fzh requesting resource cpu=9m on Node 10.132.92.186
Mar  2 01:39:11.221: INFO: Pod node-exporter-gj7lr requesting resource cpu=9m on Node 10.132.92.188
Mar  2 01:39:11.221: INFO: Pod node-exporter-z52nh requesting resource cpu=9m on Node 10.132.92.143
Mar  2 01:39:11.221: INFO: Pod openshift-state-metrics-56599d9b6d-6sw48 requesting resource cpu=3m on Node 10.132.92.188
Mar  2 01:39:11.221: INFO: Pod prometheus-adapter-95d69f68c-2rgvr requesting resource cpu=1m on Node 10.132.92.143
Mar  2 01:39:11.221: INFO: Pod prometheus-adapter-95d69f68c-gtx2d requesting resource cpu=1m on Node 10.132.92.188
Mar  2 01:39:11.221: INFO: Pod prometheus-k8s-0 requesting resource cpu=75m on Node 10.132.92.188
Mar  2 01:39:11.221: INFO: Pod prometheus-k8s-1 requesting resource cpu=75m on Node 10.132.92.143
Mar  2 01:39:11.221: INFO: Pod prometheus-operator-9bf584898-89sgf requesting resource cpu=6m on Node 10.132.92.188
Mar  2 01:39:11.221: INFO: Pod prometheus-operator-admission-webhook-6d5fbffb86-7xdt5 requesting resource cpu=5m on Node 10.132.92.143
Mar  2 01:39:11.221: INFO: Pod prometheus-operator-admission-webhook-6d5fbffb86-hhfbw requesting resource cpu=5m on Node 10.132.92.188
Mar  2 01:39:11.221: INFO: Pod telemeter-client-769c487d5b-fv8s4 requesting resource cpu=3m on Node 10.132.92.143
Mar  2 01:39:11.221: INFO: Pod thanos-querier-6cd8656bbb-6ffxj requesting resource cpu=15m on Node 10.132.92.143
Mar  2 01:39:11.221: INFO: Pod thanos-querier-6cd8656bbb-8hv4v requesting resource cpu=15m on Node 10.132.92.188
Mar  2 01:39:11.221: INFO: Pod multus-48w86 requesting resource cpu=10m on Node 10.132.92.143
Mar  2 01:39:11.221: INFO: Pod multus-89qpj requesting resource cpu=10m on Node 10.132.92.186
Mar  2 01:39:11.221: INFO: Pod multus-additional-cni-plugins-5pjnd requesting resource cpu=10m on Node 10.132.92.188
Mar  2 01:39:11.221: INFO: Pod multus-additional-cni-plugins-d2kv5 requesting resource cpu=10m on Node 10.132.92.143
Mar  2 01:39:11.221: INFO: Pod multus-additional-cni-plugins-ff7kf requesting resource cpu=10m on Node 10.132.92.186
Mar  2 01:39:11.221: INFO: Pod multus-admission-controller-6f984f76c7-hclf4 requesting resource cpu=20m on Node 10.132.92.188
Mar  2 01:39:11.221: INFO: Pod multus-admission-controller-6f984f76c7-mtmdg requesting resource cpu=20m on Node 10.132.92.143
Mar  2 01:39:11.221: INFO: Pod multus-g4k8f requesting resource cpu=10m on Node 10.132.92.188
Mar  2 01:39:11.221: INFO: Pod network-metrics-daemon-7jfj4 requesting resource cpu=20m on Node 10.132.92.143
Mar  2 01:39:11.221: INFO: Pod network-metrics-daemon-7m69v requesting resource cpu=20m on Node 10.132.92.186
Mar  2 01:39:11.221: INFO: Pod network-metrics-daemon-95rvd requesting resource cpu=20m on Node 10.132.92.188
Mar  2 01:39:11.221: INFO: Pod network-check-source-848f87f9d4-bxlxt requesting resource cpu=10m on Node 10.132.92.186
Mar  2 01:39:11.221: INFO: Pod network-check-target-4nkgt requesting resource cpu=10m on Node 10.132.92.188
Mar  2 01:39:11.222: INFO: Pod network-check-target-jqnqt requesting resource cpu=10m on Node 10.132.92.186
Mar  2 01:39:11.222: INFO: Pod network-check-target-w7x6h requesting resource cpu=10m on Node 10.132.92.143
Mar  2 01:39:11.222: INFO: Pod network-operator-68bbf8cc76-mg97z requesting resource cpu=10m on Node 10.132.92.188
Mar  2 01:39:11.222: INFO: Pod catalog-operator-657ff7bb9f-c7tqq requesting resource cpu=10m on Node 10.132.92.186
Mar  2 01:39:11.222: INFO: Pod olm-operator-56ccd7794f-4ffrj requesting resource cpu=10m on Node 10.132.92.186
Mar  2 01:39:11.222: INFO: Pod package-server-manager-599745f678-rmjg4 requesting resource cpu=10m on Node 10.132.92.186
Mar  2 01:39:11.222: INFO: Pod packageserver-6c7c56df95-7wr7f requesting resource cpu=10m on Node 10.132.92.188
Mar  2 01:39:11.222: INFO: Pod packageserver-6c7c56df95-jq89f requesting resource cpu=10m on Node 10.132.92.186
Mar  2 01:39:11.222: INFO: Pod metrics-784dfd67b7-rjkpg requesting resource cpu=10m on Node 10.132.92.186
Mar  2 01:39:11.222: INFO: Pod push-gateway-646cc86d79-cgtct requesting resource cpu=10m on Node 10.132.92.186
Mar  2 01:39:11.222: INFO: Pod service-ca-operator-6f8744cbc5-w5srs requesting resource cpu=10m on Node 10.132.92.186
Mar  2 01:39:11.222: INFO: Pod service-ca-578d67f6f8-89gtp requesting resource cpu=10m on Node 10.132.92.188
Mar  2 01:39:11.222: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.132.92.143
Mar  2 01:39:11.222: INFO: Pod sonobuoy-e2e-job-c6c00a6e72aa4160 requesting resource cpu=0m on Node 10.132.92.188
Mar  2 01:39:11.222: INFO: Pod sonobuoy-systemd-logs-daemon-set-7266eb5e626146f7-4c6tp requesting resource cpu=0m on Node 10.132.92.143
Mar  2 01:39:11.222: INFO: Pod sonobuoy-systemd-logs-daemon-set-7266eb5e626146f7-fgqrz requesting resource cpu=0m on Node 10.132.92.188
Mar  2 01:39:11.222: INFO: Pod sonobuoy-systemd-logs-daemon-set-7266eb5e626146f7-n2dsq requesting resource cpu=0m on Node 10.132.92.186
Mar  2 01:39:11.222: INFO: Pod tigera-operator-84f4f4565b-5w7s7 requesting resource cpu=100m on Node 10.132.92.188
STEP: Starting Pods to consume most of the cluster CPU. 03/02/23 01:39:11.222
Mar  2 01:39:11.222: INFO: Creating a pod which consumes cpu=1969m on Node 10.132.92.143
Mar  2 01:39:11.282: INFO: Creating a pod which consumes cpu=1773m on Node 10.132.92.186
Mar  2 01:39:11.354: INFO: Creating a pod which consumes cpu=1936m on Node 10.132.92.188
Mar  2 01:39:11.406: INFO: Waiting up to 5m0s for pod "filler-pod-6bbfebf3-a713-4bdb-aafa-c587f2542bd6" in namespace "sched-pred-6258" to be "running"
Mar  2 01:39:11.422: INFO: Pod "filler-pod-6bbfebf3-a713-4bdb-aafa-c587f2542bd6": Phase="Pending", Reason="", readiness=false. Elapsed: 15.574981ms
Mar  2 01:39:13.437: INFO: Pod "filler-pod-6bbfebf3-a713-4bdb-aafa-c587f2542bd6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03085507s
Mar  2 01:39:15.444: INFO: Pod "filler-pod-6bbfebf3-a713-4bdb-aafa-c587f2542bd6": Phase="Running", Reason="", readiness=true. Elapsed: 4.037776959s
Mar  2 01:39:15.444: INFO: Pod "filler-pod-6bbfebf3-a713-4bdb-aafa-c587f2542bd6" satisfied condition "running"
Mar  2 01:39:15.444: INFO: Waiting up to 5m0s for pod "filler-pod-6e74985e-9caa-465d-9f7f-b8ee11e82df8" in namespace "sched-pred-6258" to be "running"
Mar  2 01:39:15.458: INFO: Pod "filler-pod-6e74985e-9caa-465d-9f7f-b8ee11e82df8": Phase="Running", Reason="", readiness=true. Elapsed: 13.246473ms
Mar  2 01:39:15.458: INFO: Pod "filler-pod-6e74985e-9caa-465d-9f7f-b8ee11e82df8" satisfied condition "running"
Mar  2 01:39:15.458: INFO: Waiting up to 5m0s for pod "filler-pod-12ce6fe3-05cf-47f3-83ae-1d7810244951" in namespace "sched-pred-6258" to be "running"
Mar  2 01:39:15.472: INFO: Pod "filler-pod-12ce6fe3-05cf-47f3-83ae-1d7810244951": Phase="Running", Reason="", readiness=true. Elapsed: 14.77329ms
Mar  2 01:39:15.472: INFO: Pod "filler-pod-12ce6fe3-05cf-47f3-83ae-1d7810244951" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 03/02/23 01:39:15.472
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-12ce6fe3-05cf-47f3-83ae-1d7810244951.1748767f8ff2d0ab], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6258/filler-pod-12ce6fe3-05cf-47f3-83ae-1d7810244951 to 10.132.92.188] 03/02/23 01:39:15.516
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-12ce6fe3-05cf-47f3-83ae-1d7810244951.1748767fcf1e90d9], Reason = [AddedInterface], Message = [Add eth0 [172.30.201.232/32] from k8s-pod-network] 03/02/23 01:39:15.517
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-12ce6fe3-05cf-47f3-83ae-1d7810244951.1748767fe51a8da1], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 03/02/23 01:39:15.517
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-12ce6fe3-05cf-47f3-83ae-1d7810244951.1748767ff6d6865c], Reason = [Created], Message = [Created container filler-pod-12ce6fe3-05cf-47f3-83ae-1d7810244951] 03/02/23 01:39:15.517
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-12ce6fe3-05cf-47f3-83ae-1d7810244951.1748767ff95ff202], Reason = [Started], Message = [Started container filler-pod-12ce6fe3-05cf-47f3-83ae-1d7810244951] 03/02/23 01:39:15.517
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6bbfebf3-a713-4bdb-aafa-c587f2542bd6.1748767f8937520d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6258/filler-pod-6bbfebf3-a713-4bdb-aafa-c587f2542bd6 to 10.132.92.143] 03/02/23 01:39:15.517
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6bbfebf3-a713-4bdb-aafa-c587f2542bd6.1748767fc96c2c4b], Reason = [AddedInterface], Message = [Add eth0 [172.30.156.108/32] from k8s-pod-network] 03/02/23 01:39:15.518
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6bbfebf3-a713-4bdb-aafa-c587f2542bd6.1748767fdd900de2], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 03/02/23 01:39:15.518
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6bbfebf3-a713-4bdb-aafa-c587f2542bd6.1748767fec2237b0], Reason = [Created], Message = [Created container filler-pod-6bbfebf3-a713-4bdb-aafa-c587f2542bd6] 03/02/23 01:39:15.518
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6bbfebf3-a713-4bdb-aafa-c587f2542bd6.1748767fee221278], Reason = [Started], Message = [Started container filler-pod-6bbfebf3-a713-4bdb-aafa-c587f2542bd6] 03/02/23 01:39:15.519
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6e74985e-9caa-465d-9f7f-b8ee11e82df8.1748767f8ad8bd41], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6258/filler-pod-6e74985e-9caa-465d-9f7f-b8ee11e82df8 to 10.132.92.186] 03/02/23 01:39:15.526
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6e74985e-9caa-465d-9f7f-b8ee11e82df8.1748767fcb1d9127], Reason = [AddedInterface], Message = [Add eth0 [172.30.62.253/32] from k8s-pod-network] 03/02/23 01:39:15.526
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6e74985e-9caa-465d-9f7f-b8ee11e82df8.1748767fde86eea1], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 03/02/23 01:39:15.526
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6e74985e-9caa-465d-9f7f-b8ee11e82df8.1748767fefe7f0ae], Reason = [Created], Message = [Created container filler-pod-6e74985e-9caa-465d-9f7f-b8ee11e82df8] 03/02/23 01:39:15.526
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6e74985e-9caa-465d-9f7f-b8ee11e82df8.1748767ff22ff187], Reason = [Started], Message = [Started container filler-pod-6e74985e-9caa-465d-9f7f-b8ee11e82df8] 03/02/23 01:39:15.526
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.17487680866bbcce], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod.] 03/02/23 01:39:15.629
STEP: removing the label node off the node 10.132.92.143 03/02/23 01:39:16.61
STEP: verifying the node doesn't have the label node 03/02/23 01:39:16.663
STEP: removing the label node off the node 10.132.92.186 03/02/23 01:39:16.683
STEP: verifying the node doesn't have the label node 03/02/23 01:39:16.739
STEP: removing the label node off the node 10.132.92.188 03/02/23 01:39:16.757
STEP: verifying the node doesn't have the label node 03/02/23 01:39:16.848
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Mar  2 01:39:16.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6258" for this suite. 03/02/23 01:39:16.905
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","completed":105,"skipped":2052,"failed":0}
------------------------------
• [SLOW TEST] [6.540 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:39:10.394
    Mar  2 01:39:10.394: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename sched-pred 03/02/23 01:39:10.396
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:39:10.479
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:39:10.489
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Mar  2 01:39:10.501: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Mar  2 01:39:10.618: INFO: Waiting for terminating namespaces to be deleted...
    Mar  2 01:39:10.668: INFO: 
    Logging pods the apiserver thinks is on node 10.132.92.143 before test
    Mar  2 01:39:10.728: INFO: calico-node-99tft from calico-system started at 2023-03-01 22:49:50 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.729: INFO: 	Container calico-node ready: true, restart count 0
    Mar  2 01:39:10.729: INFO: calico-typha-b7f8b755-xlnll from calico-system started at 2023-03-01 22:49:57 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.729: INFO: 	Container calico-typha ready: true, restart count 0
    Mar  2 01:39:10.729: INFO: ibm-cloud-provider-ip-169-56-11-242-558f6ff69-jtkgc from ibm-system started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.729: INFO: 	Container ibm-cloud-provider-ip-169-56-11-242 ready: true, restart count 0
    Mar  2 01:39:10.729: INFO: ibm-keepalived-watcher-dp97c from kube-system started at 2023-03-01 22:48:16 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.729: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Mar  2 01:39:10.730: INFO: ibm-master-proxy-static-10.132.92.143 from kube-system started at 2023-03-01 22:48:15 +0000 UTC (2 container statuses recorded)
    Mar  2 01:39:10.730: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Mar  2 01:39:10.730: INFO: 	Container pause ready: true, restart count 0
    Mar  2 01:39:10.730: INFO: ibmcloud-block-storage-driver-zfc8r from kube-system started at 2023-03-01 22:48:23 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.730: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Mar  2 01:39:10.730: INFO: vpn-f6c799ddd-kvwzk from kube-system started at 2023-03-01 22:59:08 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.730: INFO: 	Container vpn ready: true, restart count 0
    Mar  2 01:39:10.730: INFO: tuned-w95xr from openshift-cluster-node-tuning-operator started at 2023-03-01 22:55:42 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.730: INFO: 	Container tuned ready: true, restart count 0
    Mar  2 01:39:10.730: INFO: console-6c8dcd4bdd-wsgwn from openshift-console started at 2023-03-01 22:59:22 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.731: INFO: 	Container console ready: true, restart count 0
    Mar  2 01:39:10.731: INFO: dns-default-zr27n from openshift-dns started at 2023-03-01 22:55:41 +0000 UTC (2 container statuses recorded)
    Mar  2 01:39:10.731: INFO: 	Container dns ready: true, restart count 0
    Mar  2 01:39:10.731: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 01:39:10.731: INFO: node-resolver-7wklz from openshift-dns started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.731: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Mar  2 01:39:10.731: INFO: node-ca-q4vcl from openshift-image-registry started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.731: INFO: 	Container node-ca ready: true, restart count 0
    Mar  2 01:39:10.731: INFO: registry-pvc-permissions-6wblc from openshift-image-registry started at 2023-03-01 22:58:39 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.731: INFO: 	Container pvc-permissions ready: false, restart count 0
    Mar  2 01:39:10.732: INFO: ingress-canary-rwxvb from openshift-ingress-canary started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.732: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Mar  2 01:39:10.732: INFO: router-default-68bc8785b7-zkcbh from openshift-ingress started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.732: INFO: 	Container router ready: true, restart count 0
    Mar  2 01:39:10.732: INFO: openshift-kube-proxy-zdts7 from openshift-kube-proxy started at 2023-03-01 22:49:00 +0000 UTC (2 container statuses recorded)
    Mar  2 01:39:10.732: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar  2 01:39:10.732: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 01:39:10.732: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-03-01 22:58:06 +0000 UTC (6 container statuses recorded)
    Mar  2 01:39:10.732: INFO: 	Container alertmanager ready: true, restart count 1
    Mar  2 01:39:10.732: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Mar  2 01:39:10.732: INFO: 	Container config-reloader ready: true, restart count 0
    Mar  2 01:39:10.732: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 01:39:10.732: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Mar  2 01:39:10.732: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Mar  2 01:39:10.732: INFO: kube-state-metrics-554994774b-mttch from openshift-monitoring started at 2023-03-01 22:56:39 +0000 UTC (3 container statuses recorded)
    Mar  2 01:39:10.732: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Mar  2 01:39:10.732: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Mar  2 01:39:10.732: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Mar  2 01:39:10.733: INFO: node-exporter-z52nh from openshift-monitoring started at 2023-03-01 22:56:40 +0000 UTC (2 container statuses recorded)
    Mar  2 01:39:10.733: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 01:39:10.733: INFO: 	Container node-exporter ready: true, restart count 0
    Mar  2 01:39:10.733: INFO: prometheus-adapter-95d69f68c-2rgvr from openshift-monitoring started at 2023-03-01 22:58:02 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.733: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Mar  2 01:39:10.733: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-03-01 22:58:12 +0000 UTC (6 container statuses recorded)
    Mar  2 01:39:10.733: INFO: 	Container config-reloader ready: true, restart count 0
    Mar  2 01:39:10.733: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 01:39:10.733: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Mar  2 01:39:10.733: INFO: 	Container prometheus ready: true, restart count 0
    Mar  2 01:39:10.733: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Mar  2 01:39:10.733: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Mar  2 01:39:10.733: INFO: prometheus-operator-admission-webhook-6d5fbffb86-7xdt5 from openshift-monitoring started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.733: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Mar  2 01:39:10.733: INFO: telemeter-client-769c487d5b-fv8s4 from openshift-monitoring started at 2023-03-01 22:58:07 +0000 UTC (3 container statuses recorded)
    Mar  2 01:39:10.733: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 01:39:10.733: INFO: 	Container reload ready: true, restart count 0
    Mar  2 01:39:10.733: INFO: 	Container telemeter-client ready: true, restart count 0
    Mar  2 01:39:10.733: INFO: thanos-querier-6cd8656bbb-6ffxj from openshift-monitoring started at 2023-03-01 22:56:48 +0000 UTC (6 container statuses recorded)
    Mar  2 01:39:10.733: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 01:39:10.733: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Mar  2 01:39:10.733: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Mar  2 01:39:10.733: INFO: 	Container oauth-proxy ready: true, restart count 0
    Mar  2 01:39:10.734: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Mar  2 01:39:10.734: INFO: 	Container thanos-query ready: true, restart count 0
    Mar  2 01:39:10.734: INFO: multus-48w86 from openshift-multus started at 2023-03-01 22:48:55 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.734: INFO: 	Container kube-multus ready: true, restart count 0
    Mar  2 01:39:10.734: INFO: multus-additional-cni-plugins-d2kv5 from openshift-multus started at 2023-03-01 22:48:55 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.734: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Mar  2 01:39:10.734: INFO: multus-admission-controller-6f984f76c7-mtmdg from openshift-multus started at 2023-03-01 22:56:08 +0000 UTC (2 container statuses recorded)
    Mar  2 01:39:10.734: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 01:39:10.734: INFO: 	Container multus-admission-controller ready: true, restart count 0
    Mar  2 01:39:10.734: INFO: network-metrics-daemon-7jfj4 from openshift-multus started at 2023-03-01 22:48:56 +0000 UTC (2 container statuses recorded)
    Mar  2 01:39:10.734: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 01:39:10.734: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Mar  2 01:39:10.734: INFO: network-check-target-w7x6h from openshift-network-diagnostics started at 2023-03-01 22:49:03 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.734: INFO: 	Container network-check-target-container ready: true, restart count 0
    Mar  2 01:39:10.734: INFO: sonobuoy from sonobuoy started at 2023-03-02 01:07:05 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.734: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Mar  2 01:39:10.734: INFO: sonobuoy-systemd-logs-daemon-set-7266eb5e626146f7-4c6tp from sonobuoy started at 2023-03-02 01:07:14 +0000 UTC (2 container statuses recorded)
    Mar  2 01:39:10.734: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  2 01:39:10.734: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar  2 01:39:10.734: INFO: 
    Logging pods the apiserver thinks is on node 10.132.92.186 before test
    Mar  2 01:39:10.818: INFO: calico-kube-controllers-5b6b964466-vlvht from calico-system started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.818: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Mar  2 01:39:10.818: INFO: calico-node-5vvbw from calico-system started at 2023-03-01 22:49:50 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.818: INFO: 	Container calico-node ready: true, restart count 0
    Mar  2 01:39:10.818: INFO: calico-typha-b7f8b755-bdnm9 from calico-system started at 2023-03-01 22:49:50 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.818: INFO: 	Container calico-typha ready: true, restart count 0
    Mar  2 01:39:10.818: INFO: managed-storage-validation-webhooks-757b765d54-f72mn from ibm-odf-validation-webhook started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.818: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
    Mar  2 01:39:10.818: INFO: managed-storage-validation-webhooks-757b765d54-gshct from ibm-odf-validation-webhook started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.818: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
    Mar  2 01:39:10.818: INFO: managed-storage-validation-webhooks-757b765d54-h4j5p from ibm-odf-validation-webhook started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.818: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
    Mar  2 01:39:10.818: INFO: ibm-file-plugin-68bfbbdc7-77m77 from kube-system started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.818: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
    Mar  2 01:39:10.818: INFO: ibm-keepalived-watcher-knrjl from kube-system started at 2023-03-01 22:48:27 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.818: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Mar  2 01:39:10.818: INFO: ibm-master-proxy-static-10.132.92.186 from kube-system started at 2023-03-01 22:48:22 +0000 UTC (2 container statuses recorded)
    Mar  2 01:39:10.818: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Mar  2 01:39:10.818: INFO: 	Container pause ready: true, restart count 0
    Mar  2 01:39:10.818: INFO: ibm-storage-metrics-agent-6c459b7855-cv8d6 from kube-system started at 2023-03-01 22:51:20 +0000 UTC (2 container statuses recorded)
    Mar  2 01:39:10.818: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
    Mar  2 01:39:10.818: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
    Mar  2 01:39:10.818: INFO: ibm-storage-watcher-67589fd788-z5w99 from kube-system started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.818: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
    Mar  2 01:39:10.818: INFO: ibmcloud-block-storage-driver-l498z from kube-system started at 2023-03-01 22:48:36 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.818: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Mar  2 01:39:10.818: INFO: ibmcloud-block-storage-plugin-86f4cbdf95-klfqp from kube-system started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.818: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
    Mar  2 01:39:10.818: INFO: cluster-node-tuning-operator-5df67c68d9-gd552 from openshift-cluster-node-tuning-operator started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.818: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
    Mar  2 01:39:10.818: INFO: tuned-h4jnq from openshift-cluster-node-tuning-operator started at 2023-03-01 22:55:42 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.818: INFO: 	Container tuned ready: true, restart count 0
    Mar  2 01:39:10.818: INFO: cluster-samples-operator-5948f44764-88mzn from openshift-cluster-samples-operator started at 2023-03-01 22:51:20 +0000 UTC (2 container statuses recorded)
    Mar  2 01:39:10.818: INFO: 	Container cluster-samples-operator ready: true, restart count 0
    Mar  2 01:39:10.818: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
    Mar  2 01:39:10.818: INFO: cluster-storage-operator-54cf568cb7-smstf from openshift-cluster-storage-operator started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.818: INFO: 	Container cluster-storage-operator ready: true, restart count 1
    Mar  2 01:39:10.818: INFO: csi-snapshot-controller-85d666c956-5hhrv from openshift-cluster-storage-operator started at 2023-03-01 22:51:57 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.818: INFO: 	Container snapshot-controller ready: true, restart count 0
    Mar  2 01:39:10.818: INFO: csi-snapshot-controller-operator-6b76cb8c94-9bvdj from openshift-cluster-storage-operator started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.818: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
    Mar  2 01:39:10.818: INFO: csi-snapshot-webhook-6cbf55b956-z69cb from openshift-cluster-storage-operator started at 2023-03-01 22:51:57 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.818: INFO: 	Container webhook ready: true, restart count 0
    Mar  2 01:39:10.818: INFO: console-operator-6975f688f8-xwx92 from openshift-console-operator started at 2023-03-01 22:51:20 +0000 UTC (2 container statuses recorded)
    Mar  2 01:39:10.818: INFO: 	Container console-operator ready: true, restart count 1
    Mar  2 01:39:10.818: INFO: 	Container conversion-webhook-server ready: true, restart count 3
    Mar  2 01:39:10.818: INFO: console-6c8dcd4bdd-c2dbn from openshift-console started at 2023-03-01 22:59:49 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.818: INFO: 	Container console ready: true, restart count 0
    Mar  2 01:39:10.818: INFO: downloads-8b4bf4d5-pjlj4 from openshift-console started at 2023-03-01 22:51:55 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.818: INFO: 	Container download-server ready: true, restart count 0
    Mar  2 01:39:10.818: INFO: dns-operator-b8598b55c-87ghp from openshift-dns-operator started at 2023-03-01 22:51:20 +0000 UTC (2 container statuses recorded)
    Mar  2 01:39:10.818: INFO: 	Container dns-operator ready: true, restart count 0
    Mar  2 01:39:10.818: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 01:39:10.818: INFO: dns-default-rdd5h from openshift-dns started at 2023-03-01 22:55:41 +0000 UTC (2 container statuses recorded)
    Mar  2 01:39:10.818: INFO: 	Container dns ready: true, restart count 0
    Mar  2 01:39:10.818: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 01:39:10.818: INFO: node-resolver-9wrks from openshift-dns started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.818: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Mar  2 01:39:10.818: INFO: cluster-image-registry-operator-797c5fb5f8-tvjwj from openshift-image-registry started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.818: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
    Mar  2 01:39:10.818: INFO: node-ca-6ffsq from openshift-image-registry started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.818: INFO: 	Container node-ca ready: true, restart count 0
    Mar  2 01:39:10.818: INFO: ingress-canary-zgjd4 from openshift-ingress-canary started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.818: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Mar  2 01:39:10.818: INFO: ingress-operator-7b8c5ddb4b-wr6sb from openshift-ingress-operator started at 2023-03-01 22:51:20 +0000 UTC (2 container statuses recorded)
    Mar  2 01:39:10.818: INFO: 	Container ingress-operator ready: true, restart count 0
    Mar  2 01:39:10.818: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 01:39:10.818: INFO: insights-operator-7b97bdcbd8-5zm5p from openshift-insights started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.818: INFO: 	Container insights-operator ready: true, restart count 1
    Mar  2 01:39:10.818: INFO: openshift-kube-proxy-xgktp from openshift-kube-proxy started at 2023-03-01 22:49:00 +0000 UTC (2 container statuses recorded)
    Mar  2 01:39:10.818: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar  2 01:39:10.818: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 01:39:10.818: INFO: kube-storage-version-migrator-operator-78bc45549c-qmzfr from openshift-kube-storage-version-migrator-operator started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.818: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
    Mar  2 01:39:10.818: INFO: marketplace-operator-66f5c8895c-l9r8f from openshift-marketplace started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.818: INFO: 	Container marketplace-operator ready: true, restart count 0
    Mar  2 01:39:10.818: INFO: cluster-monitoring-operator-7ffc6575dd-kkb8m from openshift-monitoring started at 2023-03-01 22:51:20 +0000 UTC (2 container statuses recorded)
    Mar  2 01:39:10.818: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
    Mar  2 01:39:10.818: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 01:39:10.818: INFO: node-exporter-95fzh from openshift-monitoring started at 2023-03-01 22:56:40 +0000 UTC (2 container statuses recorded)
    Mar  2 01:39:10.818: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 01:39:10.818: INFO: 	Container node-exporter ready: true, restart count 0
    Mar  2 01:39:10.818: INFO: multus-89qpj from openshift-multus started at 2023-03-01 22:48:55 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.818: INFO: 	Container kube-multus ready: true, restart count 0
    Mar  2 01:39:10.818: INFO: multus-additional-cni-plugins-ff7kf from openshift-multus started at 2023-03-01 22:48:55 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.818: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Mar  2 01:39:10.818: INFO: network-metrics-daemon-7m69v from openshift-multus started at 2023-03-01 22:48:56 +0000 UTC (2 container statuses recorded)
    Mar  2 01:39:10.818: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 01:39:10.818: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Mar  2 01:39:10.818: INFO: network-check-source-848f87f9d4-bxlxt from openshift-network-diagnostics started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.818: INFO: 	Container check-endpoints ready: true, restart count 0
    Mar  2 01:39:10.818: INFO: network-check-target-jqnqt from openshift-network-diagnostics started at 2023-03-01 22:49:03 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.818: INFO: 	Container network-check-target-container ready: true, restart count 0
    Mar  2 01:39:10.819: INFO: catalog-operator-657ff7bb9f-c7tqq from openshift-operator-lifecycle-manager started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.819: INFO: 	Container catalog-operator ready: true, restart count 0
    Mar  2 01:39:10.819: INFO: collect-profiles-27962010-nhpf6 from openshift-operator-lifecycle-manager started at 2023-03-02 01:30:00 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.819: INFO: 	Container collect-profiles ready: false, restart count 0
    Mar  2 01:39:10.819: INFO: olm-operator-56ccd7794f-4ffrj from openshift-operator-lifecycle-manager started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.819: INFO: 	Container olm-operator ready: true, restart count 0
    Mar  2 01:39:10.819: INFO: package-server-manager-599745f678-rmjg4 from openshift-operator-lifecycle-manager started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.819: INFO: 	Container package-server-manager ready: true, restart count 0
    Mar  2 01:39:10.819: INFO: packageserver-6c7c56df95-jq89f from openshift-operator-lifecycle-manager started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.819: INFO: 	Container packageserver ready: true, restart count 0
    Mar  2 01:39:10.819: INFO: metrics-784dfd67b7-rjkpg from openshift-roks-metrics started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.819: INFO: 	Container metrics ready: true, restart count 3
    Mar  2 01:39:10.819: INFO: push-gateway-646cc86d79-cgtct from openshift-roks-metrics started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.819: INFO: 	Container push-gateway ready: true, restart count 0
    Mar  2 01:39:10.819: INFO: service-ca-operator-6f8744cbc5-w5srs from openshift-service-ca-operator started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.819: INFO: 	Container service-ca-operator ready: true, restart count 1
    Mar  2 01:39:10.819: INFO: sonobuoy-systemd-logs-daemon-set-7266eb5e626146f7-n2dsq from sonobuoy started at 2023-03-02 01:07:14 +0000 UTC (2 container statuses recorded)
    Mar  2 01:39:10.819: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  2 01:39:10.819: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar  2 01:39:10.819: INFO: 
    Logging pods the apiserver thinks is on node 10.132.92.188 before test
    Mar  2 01:39:10.866: INFO: calico-node-mkvsm from calico-system started at 2023-03-01 22:49:50 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.866: INFO: 	Container calico-node ready: true, restart count 0
    Mar  2 01:39:10.866: INFO: ibm-cloud-provider-ip-169-56-11-242-558f6ff69-xvh2v from ibm-system started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.866: INFO: 	Container ibm-cloud-provider-ip-169-56-11-242 ready: true, restart count 0
    Mar  2 01:39:10.866: INFO: ibm-keepalived-watcher-zwfb8 from kube-system started at 2023-03-01 22:48:10 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.866: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Mar  2 01:39:10.867: INFO: ibm-master-proxy-static-10.132.92.188 from kube-system started at 2023-03-01 22:48:07 +0000 UTC (2 container statuses recorded)
    Mar  2 01:39:10.867: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Mar  2 01:39:10.867: INFO: 	Container pause ready: true, restart count 0
    Mar  2 01:39:10.867: INFO: ibmcloud-block-storage-driver-qbg9f from kube-system started at 2023-03-01 22:48:16 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.867: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Mar  2 01:39:10.867: INFO: tuned-5z98r from openshift-cluster-node-tuning-operator started at 2023-03-01 22:55:42 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.867: INFO: 	Container tuned ready: true, restart count 0
    Mar  2 01:39:10.867: INFO: csi-snapshot-controller-85d666c956-xsh6c from openshift-cluster-storage-operator started at 2023-03-01 22:51:57 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.867: INFO: 	Container snapshot-controller ready: true, restart count 0
    Mar  2 01:39:10.867: INFO: csi-snapshot-webhook-6cbf55b956-wq8bh from openshift-cluster-storage-operator started at 2023-03-01 22:51:57 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.867: INFO: 	Container webhook ready: true, restart count 0
    Mar  2 01:39:10.867: INFO: downloads-8b4bf4d5-b9kfz from openshift-console started at 2023-03-01 22:51:55 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.867: INFO: 	Container download-server ready: true, restart count 0
    Mar  2 01:39:10.867: INFO: dns-default-nt82k from openshift-dns started at 2023-03-01 22:55:41 +0000 UTC (2 container statuses recorded)
    Mar  2 01:39:10.867: INFO: 	Container dns ready: true, restart count 0
    Mar  2 01:39:10.867: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 01:39:10.867: INFO: node-resolver-wcng9 from openshift-dns started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.868: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Mar  2 01:39:10.868: INFO: image-pruner-27961920-wd4pv from openshift-image-registry started at 2023-03-02 00:00:00 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.868: INFO: 	Container image-pruner ready: false, restart count 0
    Mar  2 01:39:10.868: INFO: image-registry-7fb7bd8df6-lb2bt from openshift-image-registry started at 2023-03-01 22:58:31 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.868: INFO: 	Container registry ready: true, restart count 0
    Mar  2 01:39:10.868: INFO: node-ca-qlt4w from openshift-image-registry started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.868: INFO: 	Container node-ca ready: true, restart count 0
    Mar  2 01:39:10.868: INFO: ingress-canary-qvcz9 from openshift-ingress-canary started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.868: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Mar  2 01:39:10.868: INFO: router-default-68bc8785b7-wz8ct from openshift-ingress started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.868: INFO: 	Container router ready: true, restart count 0
    Mar  2 01:39:10.868: INFO: openshift-kube-proxy-mth7h from openshift-kube-proxy started at 2023-03-01 22:49:00 +0000 UTC (2 container statuses recorded)
    Mar  2 01:39:10.868: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar  2 01:39:10.868: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 01:39:10.868: INFO: migrator-5687b466dd-glln7 from openshift-kube-storage-version-migrator started at 2023-03-01 22:51:57 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.868: INFO: 	Container migrator ready: true, restart count 0
    Mar  2 01:39:10.868: INFO: certified-operators-8n5vw from openshift-marketplace started at 2023-03-02 00:13:35 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.868: INFO: 	Container registry-server ready: true, restart count 0
    Mar  2 01:39:10.869: INFO: community-operators-lngtl from openshift-marketplace started at 2023-03-01 22:54:15 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.869: INFO: 	Container registry-server ready: true, restart count 0
    Mar  2 01:39:10.869: INFO: redhat-marketplace-ld8wh from openshift-marketplace started at 2023-03-02 00:27:02 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.869: INFO: 	Container registry-server ready: true, restart count 0
    Mar  2 01:39:10.869: INFO: redhat-operators-tk548 from openshift-marketplace started at 2023-03-01 22:54:14 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.869: INFO: 	Container registry-server ready: true, restart count 0
    Mar  2 01:39:10.869: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-03-01 22:58:39 +0000 UTC (6 container statuses recorded)
    Mar  2 01:39:10.869: INFO: 	Container alertmanager ready: true, restart count 1
    Mar  2 01:39:10.869: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Mar  2 01:39:10.869: INFO: 	Container config-reloader ready: true, restart count 0
    Mar  2 01:39:10.869: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 01:39:10.869: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Mar  2 01:39:10.869: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Mar  2 01:39:10.869: INFO: node-exporter-gj7lr from openshift-monitoring started at 2023-03-01 22:56:39 +0000 UTC (2 container statuses recorded)
    Mar  2 01:39:10.869: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 01:39:10.869: INFO: 	Container node-exporter ready: true, restart count 0
    Mar  2 01:39:10.869: INFO: openshift-state-metrics-56599d9b6d-6sw48 from openshift-monitoring started at 2023-03-01 22:56:39 +0000 UTC (3 container statuses recorded)
    Mar  2 01:39:10.869: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Mar  2 01:39:10.869: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Mar  2 01:39:10.869: INFO: 	Container openshift-state-metrics ready: true, restart count 0
    Mar  2 01:39:10.869: INFO: prometheus-adapter-95d69f68c-gtx2d from openshift-monitoring started at 2023-03-01 22:58:02 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.870: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Mar  2 01:39:10.870: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-03-01 22:58:30 +0000 UTC (6 container statuses recorded)
    Mar  2 01:39:10.870: INFO: 	Container config-reloader ready: true, restart count 0
    Mar  2 01:39:10.870: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 01:39:10.870: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Mar  2 01:39:10.870: INFO: 	Container prometheus ready: true, restart count 0
    Mar  2 01:39:10.870: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Mar  2 01:39:10.870: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Mar  2 01:39:10.870: INFO: prometheus-operator-9bf584898-89sgf from openshift-monitoring started at 2023-03-01 22:56:19 +0000 UTC (2 container statuses recorded)
    Mar  2 01:39:10.870: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 01:39:10.870: INFO: 	Container prometheus-operator ready: true, restart count 0
    Mar  2 01:39:10.870: INFO: prometheus-operator-admission-webhook-6d5fbffb86-hhfbw from openshift-monitoring started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.870: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Mar  2 01:39:10.870: INFO: thanos-querier-6cd8656bbb-8hv4v from openshift-monitoring started at 2023-03-01 22:56:48 +0000 UTC (6 container statuses recorded)
    Mar  2 01:39:10.870: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 01:39:10.870: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Mar  2 01:39:10.870: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Mar  2 01:39:10.870: INFO: 	Container oauth-proxy ready: true, restart count 0
    Mar  2 01:39:10.871: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Mar  2 01:39:10.871: INFO: 	Container thanos-query ready: true, restart count 0
    Mar  2 01:39:10.871: INFO: multus-additional-cni-plugins-5pjnd from openshift-multus started at 2023-03-01 22:48:55 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.871: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Mar  2 01:39:10.871: INFO: multus-admission-controller-6f984f76c7-hclf4 from openshift-multus started at 2023-03-01 22:55:41 +0000 UTC (2 container statuses recorded)
    Mar  2 01:39:10.871: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 01:39:10.871: INFO: 	Container multus-admission-controller ready: true, restart count 0
    Mar  2 01:39:10.871: INFO: multus-g4k8f from openshift-multus started at 2023-03-01 22:48:55 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.871: INFO: 	Container kube-multus ready: true, restart count 0
    Mar  2 01:39:10.871: INFO: network-metrics-daemon-95rvd from openshift-multus started at 2023-03-01 22:48:56 +0000 UTC (2 container statuses recorded)
    Mar  2 01:39:10.871: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 01:39:10.871: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Mar  2 01:39:10.871: INFO: network-check-target-4nkgt from openshift-network-diagnostics started at 2023-03-01 22:49:03 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.871: INFO: 	Container network-check-target-container ready: true, restart count 0
    Mar  2 01:39:10.871: INFO: network-operator-68bbf8cc76-mg97z from openshift-network-operator started at 2023-03-01 22:48:19 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.871: INFO: 	Container network-operator ready: true, restart count 1
    Mar  2 01:39:10.871: INFO: collect-profiles-27961980-jmjzh from openshift-operator-lifecycle-manager started at 2023-03-02 01:00:00 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.871: INFO: 	Container collect-profiles ready: false, restart count 0
    Mar  2 01:39:10.871: INFO: collect-profiles-27961995-9hjgn from openshift-operator-lifecycle-manager started at 2023-03-02 01:15:00 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.871: INFO: 	Container collect-profiles ready: false, restart count 0
    Mar  2 01:39:10.872: INFO: packageserver-6c7c56df95-7wr7f from openshift-operator-lifecycle-manager started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.872: INFO: 	Container packageserver ready: true, restart count 0
    Mar  2 01:39:10.872: INFO: service-ca-578d67f6f8-89gtp from openshift-service-ca started at 2023-03-01 22:52:15 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.872: INFO: 	Container service-ca-controller ready: true, restart count 0
    Mar  2 01:39:10.872: INFO: sonobuoy-e2e-job-c6c00a6e72aa4160 from sonobuoy started at 2023-03-02 01:07:14 +0000 UTC (2 container statuses recorded)
    Mar  2 01:39:10.872: INFO: 	Container e2e ready: true, restart count 0
    Mar  2 01:39:10.872: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  2 01:39:10.872: INFO: sonobuoy-systemd-logs-daemon-set-7266eb5e626146f7-fgqrz from sonobuoy started at 2023-03-02 01:07:14 +0000 UTC (2 container statuses recorded)
    Mar  2 01:39:10.872: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  2 01:39:10.872: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar  2 01:39:10.872: INFO: tigera-operator-84f4f4565b-5w7s7 from tigera-operator started at 2023-03-01 22:48:19 +0000 UTC (1 container statuses recorded)
    Mar  2 01:39:10.872: INFO: 	Container tigera-operator ready: true, restart count 2
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:326
    STEP: verifying the node has the label node 10.132.92.143 03/02/23 01:39:11.008
    STEP: verifying the node has the label node 10.132.92.186 03/02/23 01:39:11.054
    STEP: verifying the node has the label node 10.132.92.188 03/02/23 01:39:11.116
    Mar  2 01:39:11.221: INFO: Pod calico-kube-controllers-5b6b964466-vlvht requesting resource cpu=10m on Node 10.132.92.186
    Mar  2 01:39:11.221: INFO: Pod calico-node-5vvbw requesting resource cpu=250m on Node 10.132.92.186
    Mar  2 01:39:11.221: INFO: Pod calico-node-99tft requesting resource cpu=250m on Node 10.132.92.143
    Mar  2 01:39:11.221: INFO: Pod calico-node-mkvsm requesting resource cpu=250m on Node 10.132.92.188
    Mar  2 01:39:11.221: INFO: Pod calico-typha-b7f8b755-bdnm9 requesting resource cpu=250m on Node 10.132.92.186
    Mar  2 01:39:11.221: INFO: Pod calico-typha-b7f8b755-xlnll requesting resource cpu=250m on Node 10.132.92.143
    Mar  2 01:39:11.221: INFO: Pod managed-storage-validation-webhooks-757b765d54-f72mn requesting resource cpu=10m on Node 10.132.92.186
    Mar  2 01:39:11.221: INFO: Pod managed-storage-validation-webhooks-757b765d54-gshct requesting resource cpu=10m on Node 10.132.92.186
    Mar  2 01:39:11.221: INFO: Pod managed-storage-validation-webhooks-757b765d54-h4j5p requesting resource cpu=10m on Node 10.132.92.186
    Mar  2 01:39:11.221: INFO: Pod ibm-cloud-provider-ip-169-56-11-242-558f6ff69-jtkgc requesting resource cpu=5m on Node 10.132.92.143
    Mar  2 01:39:11.221: INFO: Pod ibm-cloud-provider-ip-169-56-11-242-558f6ff69-xvh2v requesting resource cpu=5m on Node 10.132.92.188
    Mar  2 01:39:11.221: INFO: Pod ibm-file-plugin-68bfbbdc7-77m77 requesting resource cpu=50m on Node 10.132.92.186
    Mar  2 01:39:11.221: INFO: Pod ibm-keepalived-watcher-dp97c requesting resource cpu=5m on Node 10.132.92.143
    Mar  2 01:39:11.221: INFO: Pod ibm-keepalived-watcher-knrjl requesting resource cpu=5m on Node 10.132.92.186
    Mar  2 01:39:11.221: INFO: Pod ibm-keepalived-watcher-zwfb8 requesting resource cpu=5m on Node 10.132.92.188
    Mar  2 01:39:11.221: INFO: Pod ibm-master-proxy-static-10.132.92.143 requesting resource cpu=26m on Node 10.132.92.143
    Mar  2 01:39:11.221: INFO: Pod ibm-master-proxy-static-10.132.92.186 requesting resource cpu=26m on Node 10.132.92.186
    Mar  2 01:39:11.221: INFO: Pod ibm-master-proxy-static-10.132.92.188 requesting resource cpu=26m on Node 10.132.92.188
    Mar  2 01:39:11.221: INFO: Pod ibm-storage-metrics-agent-6c459b7855-cv8d6 requesting resource cpu=60m on Node 10.132.92.186
    Mar  2 01:39:11.221: INFO: Pod ibm-storage-watcher-67589fd788-z5w99 requesting resource cpu=50m on Node 10.132.92.186
    Mar  2 01:39:11.221: INFO: Pod ibmcloud-block-storage-driver-l498z requesting resource cpu=50m on Node 10.132.92.186
    Mar  2 01:39:11.221: INFO: Pod ibmcloud-block-storage-driver-qbg9f requesting resource cpu=50m on Node 10.132.92.188
    Mar  2 01:39:11.221: INFO: Pod ibmcloud-block-storage-driver-zfc8r requesting resource cpu=50m on Node 10.132.92.143
    Mar  2 01:39:11.221: INFO: Pod ibmcloud-block-storage-plugin-86f4cbdf95-klfqp requesting resource cpu=50m on Node 10.132.92.186
    Mar  2 01:39:11.221: INFO: Pod vpn-f6c799ddd-kvwzk requesting resource cpu=5m on Node 10.132.92.143
    Mar  2 01:39:11.221: INFO: Pod cluster-node-tuning-operator-5df67c68d9-gd552 requesting resource cpu=10m on Node 10.132.92.186
    Mar  2 01:39:11.221: INFO: Pod tuned-5z98r requesting resource cpu=10m on Node 10.132.92.188
    Mar  2 01:39:11.221: INFO: Pod tuned-h4jnq requesting resource cpu=10m on Node 10.132.92.186
    Mar  2 01:39:11.221: INFO: Pod tuned-w95xr requesting resource cpu=10m on Node 10.132.92.143
    Mar  2 01:39:11.221: INFO: Pod cluster-samples-operator-5948f44764-88mzn requesting resource cpu=20m on Node 10.132.92.186
    Mar  2 01:39:11.221: INFO: Pod cluster-storage-operator-54cf568cb7-smstf requesting resource cpu=10m on Node 10.132.92.186
    Mar  2 01:39:11.221: INFO: Pod csi-snapshot-controller-85d666c956-5hhrv requesting resource cpu=10m on Node 10.132.92.186
    Mar  2 01:39:11.221: INFO: Pod csi-snapshot-controller-85d666c956-xsh6c requesting resource cpu=10m on Node 10.132.92.188
    Mar  2 01:39:11.221: INFO: Pod csi-snapshot-controller-operator-6b76cb8c94-9bvdj requesting resource cpu=10m on Node 10.132.92.186
    Mar  2 01:39:11.221: INFO: Pod csi-snapshot-webhook-6cbf55b956-wq8bh requesting resource cpu=10m on Node 10.132.92.188
    Mar  2 01:39:11.221: INFO: Pod csi-snapshot-webhook-6cbf55b956-z69cb requesting resource cpu=10m on Node 10.132.92.186
    Mar  2 01:39:11.221: INFO: Pod console-operator-6975f688f8-xwx92 requesting resource cpu=20m on Node 10.132.92.186
    Mar  2 01:39:11.221: INFO: Pod console-6c8dcd4bdd-c2dbn requesting resource cpu=10m on Node 10.132.92.186
    Mar  2 01:39:11.221: INFO: Pod console-6c8dcd4bdd-wsgwn requesting resource cpu=10m on Node 10.132.92.143
    Mar  2 01:39:11.221: INFO: Pod downloads-8b4bf4d5-b9kfz requesting resource cpu=10m on Node 10.132.92.188
    Mar  2 01:39:11.221: INFO: Pod downloads-8b4bf4d5-pjlj4 requesting resource cpu=10m on Node 10.132.92.186
    Mar  2 01:39:11.221: INFO: Pod dns-operator-b8598b55c-87ghp requesting resource cpu=20m on Node 10.132.92.186
    Mar  2 01:39:11.221: INFO: Pod dns-default-nt82k requesting resource cpu=60m on Node 10.132.92.188
    Mar  2 01:39:11.221: INFO: Pod dns-default-rdd5h requesting resource cpu=60m on Node 10.132.92.186
    Mar  2 01:39:11.221: INFO: Pod dns-default-zr27n requesting resource cpu=60m on Node 10.132.92.143
    Mar  2 01:39:11.221: INFO: Pod node-resolver-7wklz requesting resource cpu=5m on Node 10.132.92.143
    Mar  2 01:39:11.221: INFO: Pod node-resolver-9wrks requesting resource cpu=5m on Node 10.132.92.186
    Mar  2 01:39:11.221: INFO: Pod node-resolver-wcng9 requesting resource cpu=5m on Node 10.132.92.188
    Mar  2 01:39:11.221: INFO: Pod cluster-image-registry-operator-797c5fb5f8-tvjwj requesting resource cpu=10m on Node 10.132.92.186
    Mar  2 01:39:11.221: INFO: Pod image-registry-7fb7bd8df6-lb2bt requesting resource cpu=100m on Node 10.132.92.188
    Mar  2 01:39:11.221: INFO: Pod node-ca-6ffsq requesting resource cpu=10m on Node 10.132.92.186
    Mar  2 01:39:11.221: INFO: Pod node-ca-q4vcl requesting resource cpu=10m on Node 10.132.92.143
    Mar  2 01:39:11.221: INFO: Pod node-ca-qlt4w requesting resource cpu=10m on Node 10.132.92.188
    Mar  2 01:39:11.221: INFO: Pod ingress-canary-qvcz9 requesting resource cpu=10m on Node 10.132.92.188
    Mar  2 01:39:11.221: INFO: Pod ingress-canary-rwxvb requesting resource cpu=10m on Node 10.132.92.143
    Mar  2 01:39:11.221: INFO: Pod ingress-canary-zgjd4 requesting resource cpu=10m on Node 10.132.92.186
    Mar  2 01:39:11.221: INFO: Pod ingress-operator-7b8c5ddb4b-wr6sb requesting resource cpu=20m on Node 10.132.92.186
    Mar  2 01:39:11.221: INFO: Pod router-default-68bc8785b7-wz8ct requesting resource cpu=100m on Node 10.132.92.188
    Mar  2 01:39:11.221: INFO: Pod router-default-68bc8785b7-zkcbh requesting resource cpu=100m on Node 10.132.92.143
    Mar  2 01:39:11.221: INFO: Pod insights-operator-7b97bdcbd8-5zm5p requesting resource cpu=10m on Node 10.132.92.186
    Mar  2 01:39:11.221: INFO: Pod openshift-kube-proxy-mth7h requesting resource cpu=110m on Node 10.132.92.188
    Mar  2 01:39:11.221: INFO: Pod openshift-kube-proxy-xgktp requesting resource cpu=110m on Node 10.132.92.186
    Mar  2 01:39:11.221: INFO: Pod openshift-kube-proxy-zdts7 requesting resource cpu=110m on Node 10.132.92.143
    Mar  2 01:39:11.221: INFO: Pod kube-storage-version-migrator-operator-78bc45549c-qmzfr requesting resource cpu=10m on Node 10.132.92.186
    Mar  2 01:39:11.221: INFO: Pod migrator-5687b466dd-glln7 requesting resource cpu=10m on Node 10.132.92.188
    Mar  2 01:39:11.221: INFO: Pod certified-operators-8n5vw requesting resource cpu=10m on Node 10.132.92.188
    Mar  2 01:39:11.221: INFO: Pod community-operators-lngtl requesting resource cpu=10m on Node 10.132.92.188
    Mar  2 01:39:11.221: INFO: Pod marketplace-operator-66f5c8895c-l9r8f requesting resource cpu=10m on Node 10.132.92.186
    Mar  2 01:39:11.221: INFO: Pod redhat-marketplace-ld8wh requesting resource cpu=10m on Node 10.132.92.188
    Mar  2 01:39:11.221: INFO: Pod redhat-operators-tk548 requesting resource cpu=10m on Node 10.132.92.188
    Mar  2 01:39:11.221: INFO: Pod alertmanager-main-0 requesting resource cpu=9m on Node 10.132.92.188
    Mar  2 01:39:11.221: INFO: Pod alertmanager-main-1 requesting resource cpu=9m on Node 10.132.92.143
    Mar  2 01:39:11.221: INFO: Pod cluster-monitoring-operator-7ffc6575dd-kkb8m requesting resource cpu=11m on Node 10.132.92.186
    Mar  2 01:39:11.221: INFO: Pod kube-state-metrics-554994774b-mttch requesting resource cpu=4m on Node 10.132.92.143
    Mar  2 01:39:11.221: INFO: Pod node-exporter-95fzh requesting resource cpu=9m on Node 10.132.92.186
    Mar  2 01:39:11.221: INFO: Pod node-exporter-gj7lr requesting resource cpu=9m on Node 10.132.92.188
    Mar  2 01:39:11.221: INFO: Pod node-exporter-z52nh requesting resource cpu=9m on Node 10.132.92.143
    Mar  2 01:39:11.221: INFO: Pod openshift-state-metrics-56599d9b6d-6sw48 requesting resource cpu=3m on Node 10.132.92.188
    Mar  2 01:39:11.221: INFO: Pod prometheus-adapter-95d69f68c-2rgvr requesting resource cpu=1m on Node 10.132.92.143
    Mar  2 01:39:11.221: INFO: Pod prometheus-adapter-95d69f68c-gtx2d requesting resource cpu=1m on Node 10.132.92.188
    Mar  2 01:39:11.221: INFO: Pod prometheus-k8s-0 requesting resource cpu=75m on Node 10.132.92.188
    Mar  2 01:39:11.221: INFO: Pod prometheus-k8s-1 requesting resource cpu=75m on Node 10.132.92.143
    Mar  2 01:39:11.221: INFO: Pod prometheus-operator-9bf584898-89sgf requesting resource cpu=6m on Node 10.132.92.188
    Mar  2 01:39:11.221: INFO: Pod prometheus-operator-admission-webhook-6d5fbffb86-7xdt5 requesting resource cpu=5m on Node 10.132.92.143
    Mar  2 01:39:11.221: INFO: Pod prometheus-operator-admission-webhook-6d5fbffb86-hhfbw requesting resource cpu=5m on Node 10.132.92.188
    Mar  2 01:39:11.221: INFO: Pod telemeter-client-769c487d5b-fv8s4 requesting resource cpu=3m on Node 10.132.92.143
    Mar  2 01:39:11.221: INFO: Pod thanos-querier-6cd8656bbb-6ffxj requesting resource cpu=15m on Node 10.132.92.143
    Mar  2 01:39:11.221: INFO: Pod thanos-querier-6cd8656bbb-8hv4v requesting resource cpu=15m on Node 10.132.92.188
    Mar  2 01:39:11.221: INFO: Pod multus-48w86 requesting resource cpu=10m on Node 10.132.92.143
    Mar  2 01:39:11.221: INFO: Pod multus-89qpj requesting resource cpu=10m on Node 10.132.92.186
    Mar  2 01:39:11.221: INFO: Pod multus-additional-cni-plugins-5pjnd requesting resource cpu=10m on Node 10.132.92.188
    Mar  2 01:39:11.221: INFO: Pod multus-additional-cni-plugins-d2kv5 requesting resource cpu=10m on Node 10.132.92.143
    Mar  2 01:39:11.221: INFO: Pod multus-additional-cni-plugins-ff7kf requesting resource cpu=10m on Node 10.132.92.186
    Mar  2 01:39:11.221: INFO: Pod multus-admission-controller-6f984f76c7-hclf4 requesting resource cpu=20m on Node 10.132.92.188
    Mar  2 01:39:11.221: INFO: Pod multus-admission-controller-6f984f76c7-mtmdg requesting resource cpu=20m on Node 10.132.92.143
    Mar  2 01:39:11.221: INFO: Pod multus-g4k8f requesting resource cpu=10m on Node 10.132.92.188
    Mar  2 01:39:11.221: INFO: Pod network-metrics-daemon-7jfj4 requesting resource cpu=20m on Node 10.132.92.143
    Mar  2 01:39:11.221: INFO: Pod network-metrics-daemon-7m69v requesting resource cpu=20m on Node 10.132.92.186
    Mar  2 01:39:11.221: INFO: Pod network-metrics-daemon-95rvd requesting resource cpu=20m on Node 10.132.92.188
    Mar  2 01:39:11.221: INFO: Pod network-check-source-848f87f9d4-bxlxt requesting resource cpu=10m on Node 10.132.92.186
    Mar  2 01:39:11.221: INFO: Pod network-check-target-4nkgt requesting resource cpu=10m on Node 10.132.92.188
    Mar  2 01:39:11.222: INFO: Pod network-check-target-jqnqt requesting resource cpu=10m on Node 10.132.92.186
    Mar  2 01:39:11.222: INFO: Pod network-check-target-w7x6h requesting resource cpu=10m on Node 10.132.92.143
    Mar  2 01:39:11.222: INFO: Pod network-operator-68bbf8cc76-mg97z requesting resource cpu=10m on Node 10.132.92.188
    Mar  2 01:39:11.222: INFO: Pod catalog-operator-657ff7bb9f-c7tqq requesting resource cpu=10m on Node 10.132.92.186
    Mar  2 01:39:11.222: INFO: Pod olm-operator-56ccd7794f-4ffrj requesting resource cpu=10m on Node 10.132.92.186
    Mar  2 01:39:11.222: INFO: Pod package-server-manager-599745f678-rmjg4 requesting resource cpu=10m on Node 10.132.92.186
    Mar  2 01:39:11.222: INFO: Pod packageserver-6c7c56df95-7wr7f requesting resource cpu=10m on Node 10.132.92.188
    Mar  2 01:39:11.222: INFO: Pod packageserver-6c7c56df95-jq89f requesting resource cpu=10m on Node 10.132.92.186
    Mar  2 01:39:11.222: INFO: Pod metrics-784dfd67b7-rjkpg requesting resource cpu=10m on Node 10.132.92.186
    Mar  2 01:39:11.222: INFO: Pod push-gateway-646cc86d79-cgtct requesting resource cpu=10m on Node 10.132.92.186
    Mar  2 01:39:11.222: INFO: Pod service-ca-operator-6f8744cbc5-w5srs requesting resource cpu=10m on Node 10.132.92.186
    Mar  2 01:39:11.222: INFO: Pod service-ca-578d67f6f8-89gtp requesting resource cpu=10m on Node 10.132.92.188
    Mar  2 01:39:11.222: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.132.92.143
    Mar  2 01:39:11.222: INFO: Pod sonobuoy-e2e-job-c6c00a6e72aa4160 requesting resource cpu=0m on Node 10.132.92.188
    Mar  2 01:39:11.222: INFO: Pod sonobuoy-systemd-logs-daemon-set-7266eb5e626146f7-4c6tp requesting resource cpu=0m on Node 10.132.92.143
    Mar  2 01:39:11.222: INFO: Pod sonobuoy-systemd-logs-daemon-set-7266eb5e626146f7-fgqrz requesting resource cpu=0m on Node 10.132.92.188
    Mar  2 01:39:11.222: INFO: Pod sonobuoy-systemd-logs-daemon-set-7266eb5e626146f7-n2dsq requesting resource cpu=0m on Node 10.132.92.186
    Mar  2 01:39:11.222: INFO: Pod tigera-operator-84f4f4565b-5w7s7 requesting resource cpu=100m on Node 10.132.92.188
    STEP: Starting Pods to consume most of the cluster CPU. 03/02/23 01:39:11.222
    Mar  2 01:39:11.222: INFO: Creating a pod which consumes cpu=1969m on Node 10.132.92.143
    Mar  2 01:39:11.282: INFO: Creating a pod which consumes cpu=1773m on Node 10.132.92.186
    Mar  2 01:39:11.354: INFO: Creating a pod which consumes cpu=1936m on Node 10.132.92.188
    Mar  2 01:39:11.406: INFO: Waiting up to 5m0s for pod "filler-pod-6bbfebf3-a713-4bdb-aafa-c587f2542bd6" in namespace "sched-pred-6258" to be "running"
    Mar  2 01:39:11.422: INFO: Pod "filler-pod-6bbfebf3-a713-4bdb-aafa-c587f2542bd6": Phase="Pending", Reason="", readiness=false. Elapsed: 15.574981ms
    Mar  2 01:39:13.437: INFO: Pod "filler-pod-6bbfebf3-a713-4bdb-aafa-c587f2542bd6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03085507s
    Mar  2 01:39:15.444: INFO: Pod "filler-pod-6bbfebf3-a713-4bdb-aafa-c587f2542bd6": Phase="Running", Reason="", readiness=true. Elapsed: 4.037776959s
    Mar  2 01:39:15.444: INFO: Pod "filler-pod-6bbfebf3-a713-4bdb-aafa-c587f2542bd6" satisfied condition "running"
    Mar  2 01:39:15.444: INFO: Waiting up to 5m0s for pod "filler-pod-6e74985e-9caa-465d-9f7f-b8ee11e82df8" in namespace "sched-pred-6258" to be "running"
    Mar  2 01:39:15.458: INFO: Pod "filler-pod-6e74985e-9caa-465d-9f7f-b8ee11e82df8": Phase="Running", Reason="", readiness=true. Elapsed: 13.246473ms
    Mar  2 01:39:15.458: INFO: Pod "filler-pod-6e74985e-9caa-465d-9f7f-b8ee11e82df8" satisfied condition "running"
    Mar  2 01:39:15.458: INFO: Waiting up to 5m0s for pod "filler-pod-12ce6fe3-05cf-47f3-83ae-1d7810244951" in namespace "sched-pred-6258" to be "running"
    Mar  2 01:39:15.472: INFO: Pod "filler-pod-12ce6fe3-05cf-47f3-83ae-1d7810244951": Phase="Running", Reason="", readiness=true. Elapsed: 14.77329ms
    Mar  2 01:39:15.472: INFO: Pod "filler-pod-12ce6fe3-05cf-47f3-83ae-1d7810244951" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 03/02/23 01:39:15.472
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-12ce6fe3-05cf-47f3-83ae-1d7810244951.1748767f8ff2d0ab], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6258/filler-pod-12ce6fe3-05cf-47f3-83ae-1d7810244951 to 10.132.92.188] 03/02/23 01:39:15.516
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-12ce6fe3-05cf-47f3-83ae-1d7810244951.1748767fcf1e90d9], Reason = [AddedInterface], Message = [Add eth0 [172.30.201.232/32] from k8s-pod-network] 03/02/23 01:39:15.517
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-12ce6fe3-05cf-47f3-83ae-1d7810244951.1748767fe51a8da1], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 03/02/23 01:39:15.517
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-12ce6fe3-05cf-47f3-83ae-1d7810244951.1748767ff6d6865c], Reason = [Created], Message = [Created container filler-pod-12ce6fe3-05cf-47f3-83ae-1d7810244951] 03/02/23 01:39:15.517
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-12ce6fe3-05cf-47f3-83ae-1d7810244951.1748767ff95ff202], Reason = [Started], Message = [Started container filler-pod-12ce6fe3-05cf-47f3-83ae-1d7810244951] 03/02/23 01:39:15.517
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-6bbfebf3-a713-4bdb-aafa-c587f2542bd6.1748767f8937520d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6258/filler-pod-6bbfebf3-a713-4bdb-aafa-c587f2542bd6 to 10.132.92.143] 03/02/23 01:39:15.517
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-6bbfebf3-a713-4bdb-aafa-c587f2542bd6.1748767fc96c2c4b], Reason = [AddedInterface], Message = [Add eth0 [172.30.156.108/32] from k8s-pod-network] 03/02/23 01:39:15.518
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-6bbfebf3-a713-4bdb-aafa-c587f2542bd6.1748767fdd900de2], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 03/02/23 01:39:15.518
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-6bbfebf3-a713-4bdb-aafa-c587f2542bd6.1748767fec2237b0], Reason = [Created], Message = [Created container filler-pod-6bbfebf3-a713-4bdb-aafa-c587f2542bd6] 03/02/23 01:39:15.518
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-6bbfebf3-a713-4bdb-aafa-c587f2542bd6.1748767fee221278], Reason = [Started], Message = [Started container filler-pod-6bbfebf3-a713-4bdb-aafa-c587f2542bd6] 03/02/23 01:39:15.519
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-6e74985e-9caa-465d-9f7f-b8ee11e82df8.1748767f8ad8bd41], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6258/filler-pod-6e74985e-9caa-465d-9f7f-b8ee11e82df8 to 10.132.92.186] 03/02/23 01:39:15.526
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-6e74985e-9caa-465d-9f7f-b8ee11e82df8.1748767fcb1d9127], Reason = [AddedInterface], Message = [Add eth0 [172.30.62.253/32] from k8s-pod-network] 03/02/23 01:39:15.526
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-6e74985e-9caa-465d-9f7f-b8ee11e82df8.1748767fde86eea1], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 03/02/23 01:39:15.526
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-6e74985e-9caa-465d-9f7f-b8ee11e82df8.1748767fefe7f0ae], Reason = [Created], Message = [Created container filler-pod-6e74985e-9caa-465d-9f7f-b8ee11e82df8] 03/02/23 01:39:15.526
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-6e74985e-9caa-465d-9f7f-b8ee11e82df8.1748767ff22ff187], Reason = [Started], Message = [Started container filler-pod-6e74985e-9caa-465d-9f7f-b8ee11e82df8] 03/02/23 01:39:15.526
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.17487680866bbcce], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod.] 03/02/23 01:39:15.629
    STEP: removing the label node off the node 10.132.92.143 03/02/23 01:39:16.61
    STEP: verifying the node doesn't have the label node 03/02/23 01:39:16.663
    STEP: removing the label node off the node 10.132.92.186 03/02/23 01:39:16.683
    STEP: verifying the node doesn't have the label node 03/02/23 01:39:16.739
    STEP: removing the label node off the node 10.132.92.188 03/02/23 01:39:16.757
    STEP: verifying the node doesn't have the label node 03/02/23 01:39:16.848
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Mar  2 01:39:16.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-6258" for this suite. 03/02/23 01:39:16.905
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:39:16.94
Mar  2 01:39:16.941: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename containers 03/02/23 01:39:16.942
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:39:17.014
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:39:17.035
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
Mar  2 01:39:17.194: INFO: Waiting up to 5m0s for pod "client-containers-e91f2066-ef59-4931-af83-2d80b85a2cb1" in namespace "containers-1669" to be "running"
Mar  2 01:39:17.210: INFO: Pod "client-containers-e91f2066-ef59-4931-af83-2d80b85a2cb1": Phase="Pending", Reason="", readiness=false. Elapsed: 15.323612ms
Mar  2 01:39:19.226: INFO: Pod "client-containers-e91f2066-ef59-4931-af83-2d80b85a2cb1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031084055s
Mar  2 01:39:21.222: INFO: Pod "client-containers-e91f2066-ef59-4931-af83-2d80b85a2cb1": Phase="Running", Reason="", readiness=true. Elapsed: 4.027436752s
Mar  2 01:39:21.222: INFO: Pod "client-containers-e91f2066-ef59-4931-af83-2d80b85a2cb1" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Mar  2 01:39:21.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1669" for this suite. 03/02/23 01:39:21.306
{"msg":"PASSED [sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","completed":106,"skipped":2073,"failed":0}
------------------------------
• [4.393 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:39:16.94
    Mar  2 01:39:16.941: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename containers 03/02/23 01:39:16.942
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:39:17.014
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:39:17.035
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:38
    Mar  2 01:39:17.194: INFO: Waiting up to 5m0s for pod "client-containers-e91f2066-ef59-4931-af83-2d80b85a2cb1" in namespace "containers-1669" to be "running"
    Mar  2 01:39:17.210: INFO: Pod "client-containers-e91f2066-ef59-4931-af83-2d80b85a2cb1": Phase="Pending", Reason="", readiness=false. Elapsed: 15.323612ms
    Mar  2 01:39:19.226: INFO: Pod "client-containers-e91f2066-ef59-4931-af83-2d80b85a2cb1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031084055s
    Mar  2 01:39:21.222: INFO: Pod "client-containers-e91f2066-ef59-4931-af83-2d80b85a2cb1": Phase="Running", Reason="", readiness=true. Elapsed: 4.027436752s
    Mar  2 01:39:21.222: INFO: Pod "client-containers-e91f2066-ef59-4931-af83-2d80b85a2cb1" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Mar  2 01:39:21.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-1669" for this suite. 03/02/23 01:39:21.306
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:39:21.335
Mar  2 01:39:21.338: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename dns 03/02/23 01:39:21.339
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:39:21.402
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:39:21.413
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 03/02/23 01:39:21.424
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7339.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7339.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7339.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7339.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7339.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7339.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7339.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7339.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7339.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7339.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7339.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7339.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 77.101.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.101.77_udp@PTR;check="$$(dig +tcp +noall +answer +search 77.101.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.101.77_tcp@PTR;sleep 1; done
 03/02/23 01:39:21.493
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7339.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7339.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7339.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7339.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7339.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7339.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7339.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7339.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7339.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7339.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7339.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7339.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 77.101.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.101.77_udp@PTR;check="$$(dig +tcp +noall +answer +search 77.101.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.101.77_tcp@PTR;sleep 1; done
 03/02/23 01:39:21.493
STEP: creating a pod to probe DNS 03/02/23 01:39:21.493
STEP: submitting the pod to kubernetes 03/02/23 01:39:21.494
Mar  2 01:39:21.562: INFO: Waiting up to 15m0s for pod "dns-test-6bd8bd08-35c9-4abf-85a1-35e3cce72f40" in namespace "dns-7339" to be "running"
Mar  2 01:39:21.576: INFO: Pod "dns-test-6bd8bd08-35c9-4abf-85a1-35e3cce72f40": Phase="Pending", Reason="", readiness=false. Elapsed: 13.676951ms
Mar  2 01:39:23.589: INFO: Pod "dns-test-6bd8bd08-35c9-4abf-85a1-35e3cce72f40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026554691s
Mar  2 01:39:25.589: INFO: Pod "dns-test-6bd8bd08-35c9-4abf-85a1-35e3cce72f40": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027029604s
Mar  2 01:39:27.589: INFO: Pod "dns-test-6bd8bd08-35c9-4abf-85a1-35e3cce72f40": Phase="Pending", Reason="", readiness=false. Elapsed: 6.027077008s
Mar  2 01:39:29.590: INFO: Pod "dns-test-6bd8bd08-35c9-4abf-85a1-35e3cce72f40": Phase="Pending", Reason="", readiness=false. Elapsed: 8.027620586s
Mar  2 01:39:31.589: INFO: Pod "dns-test-6bd8bd08-35c9-4abf-85a1-35e3cce72f40": Phase="Pending", Reason="", readiness=false. Elapsed: 10.026523725s
Mar  2 01:39:33.599: INFO: Pod "dns-test-6bd8bd08-35c9-4abf-85a1-35e3cce72f40": Phase="Pending", Reason="", readiness=false. Elapsed: 12.037091106s
Mar  2 01:39:35.602: INFO: Pod "dns-test-6bd8bd08-35c9-4abf-85a1-35e3cce72f40": Phase="Pending", Reason="", readiness=false. Elapsed: 14.039566322s
Mar  2 01:39:37.590: INFO: Pod "dns-test-6bd8bd08-35c9-4abf-85a1-35e3cce72f40": Phase="Running", Reason="", readiness=true. Elapsed: 16.027784752s
Mar  2 01:39:37.590: INFO: Pod "dns-test-6bd8bd08-35c9-4abf-85a1-35e3cce72f40" satisfied condition "running"
STEP: retrieving the pod 03/02/23 01:39:37.59
STEP: looking for the results for each expected name from probers 03/02/23 01:39:37.606
Mar  2 01:39:37.634: INFO: Unable to read wheezy_udp@dns-test-service.dns-7339.svc.cluster.local from pod dns-7339/dns-test-6bd8bd08-35c9-4abf-85a1-35e3cce72f40: the server could not find the requested resource (get pods dns-test-6bd8bd08-35c9-4abf-85a1-35e3cce72f40)
Mar  2 01:39:37.670: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7339.svc.cluster.local from pod dns-7339/dns-test-6bd8bd08-35c9-4abf-85a1-35e3cce72f40: the server could not find the requested resource (get pods dns-test-6bd8bd08-35c9-4abf-85a1-35e3cce72f40)
Mar  2 01:39:37.780: INFO: Unable to read jessie_udp@dns-test-service.dns-7339.svc.cluster.local from pod dns-7339/dns-test-6bd8bd08-35c9-4abf-85a1-35e3cce72f40: the server could not find the requested resource (get pods dns-test-6bd8bd08-35c9-4abf-85a1-35e3cce72f40)
Mar  2 01:39:37.799: INFO: Unable to read jessie_tcp@dns-test-service.dns-7339.svc.cluster.local from pod dns-7339/dns-test-6bd8bd08-35c9-4abf-85a1-35e3cce72f40: the server could not find the requested resource (get pods dns-test-6bd8bd08-35c9-4abf-85a1-35e3cce72f40)
Mar  2 01:39:37.815: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7339.svc.cluster.local from pod dns-7339/dns-test-6bd8bd08-35c9-4abf-85a1-35e3cce72f40: the server could not find the requested resource (get pods dns-test-6bd8bd08-35c9-4abf-85a1-35e3cce72f40)
Mar  2 01:39:37.909: INFO: Lookups using dns-7339/dns-test-6bd8bd08-35c9-4abf-85a1-35e3cce72f40 failed for: [wheezy_udp@dns-test-service.dns-7339.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7339.svc.cluster.local jessie_udp@dns-test-service.dns-7339.svc.cluster.local jessie_tcp@dns-test-service.dns-7339.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7339.svc.cluster.local]

Mar  2 01:39:43.235: INFO: DNS probes using dns-7339/dns-test-6bd8bd08-35c9-4abf-85a1-35e3cce72f40 succeeded

STEP: deleting the pod 03/02/23 01:39:43.235
STEP: deleting the test service 03/02/23 01:39:43.271
STEP: deleting the test headless service 03/02/23 01:39:43.324
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar  2 01:39:43.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7339" for this suite. 03/02/23 01:39:43.389
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","completed":107,"skipped":2077,"failed":0}
------------------------------
• [SLOW TEST] [22.075 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:39:21.335
    Mar  2 01:39:21.338: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename dns 03/02/23 01:39:21.339
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:39:21.402
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:39:21.413
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 03/02/23 01:39:21.424
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7339.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7339.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7339.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7339.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7339.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7339.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7339.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7339.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7339.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7339.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7339.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7339.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 77.101.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.101.77_udp@PTR;check="$$(dig +tcp +noall +answer +search 77.101.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.101.77_tcp@PTR;sleep 1; done
     03/02/23 01:39:21.493
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7339.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7339.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7339.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7339.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7339.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7339.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7339.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7339.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7339.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7339.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7339.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7339.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 77.101.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.101.77_udp@PTR;check="$$(dig +tcp +noall +answer +search 77.101.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.101.77_tcp@PTR;sleep 1; done
     03/02/23 01:39:21.493
    STEP: creating a pod to probe DNS 03/02/23 01:39:21.493
    STEP: submitting the pod to kubernetes 03/02/23 01:39:21.494
    Mar  2 01:39:21.562: INFO: Waiting up to 15m0s for pod "dns-test-6bd8bd08-35c9-4abf-85a1-35e3cce72f40" in namespace "dns-7339" to be "running"
    Mar  2 01:39:21.576: INFO: Pod "dns-test-6bd8bd08-35c9-4abf-85a1-35e3cce72f40": Phase="Pending", Reason="", readiness=false. Elapsed: 13.676951ms
    Mar  2 01:39:23.589: INFO: Pod "dns-test-6bd8bd08-35c9-4abf-85a1-35e3cce72f40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026554691s
    Mar  2 01:39:25.589: INFO: Pod "dns-test-6bd8bd08-35c9-4abf-85a1-35e3cce72f40": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027029604s
    Mar  2 01:39:27.589: INFO: Pod "dns-test-6bd8bd08-35c9-4abf-85a1-35e3cce72f40": Phase="Pending", Reason="", readiness=false. Elapsed: 6.027077008s
    Mar  2 01:39:29.590: INFO: Pod "dns-test-6bd8bd08-35c9-4abf-85a1-35e3cce72f40": Phase="Pending", Reason="", readiness=false. Elapsed: 8.027620586s
    Mar  2 01:39:31.589: INFO: Pod "dns-test-6bd8bd08-35c9-4abf-85a1-35e3cce72f40": Phase="Pending", Reason="", readiness=false. Elapsed: 10.026523725s
    Mar  2 01:39:33.599: INFO: Pod "dns-test-6bd8bd08-35c9-4abf-85a1-35e3cce72f40": Phase="Pending", Reason="", readiness=false. Elapsed: 12.037091106s
    Mar  2 01:39:35.602: INFO: Pod "dns-test-6bd8bd08-35c9-4abf-85a1-35e3cce72f40": Phase="Pending", Reason="", readiness=false. Elapsed: 14.039566322s
    Mar  2 01:39:37.590: INFO: Pod "dns-test-6bd8bd08-35c9-4abf-85a1-35e3cce72f40": Phase="Running", Reason="", readiness=true. Elapsed: 16.027784752s
    Mar  2 01:39:37.590: INFO: Pod "dns-test-6bd8bd08-35c9-4abf-85a1-35e3cce72f40" satisfied condition "running"
    STEP: retrieving the pod 03/02/23 01:39:37.59
    STEP: looking for the results for each expected name from probers 03/02/23 01:39:37.606
    Mar  2 01:39:37.634: INFO: Unable to read wheezy_udp@dns-test-service.dns-7339.svc.cluster.local from pod dns-7339/dns-test-6bd8bd08-35c9-4abf-85a1-35e3cce72f40: the server could not find the requested resource (get pods dns-test-6bd8bd08-35c9-4abf-85a1-35e3cce72f40)
    Mar  2 01:39:37.670: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7339.svc.cluster.local from pod dns-7339/dns-test-6bd8bd08-35c9-4abf-85a1-35e3cce72f40: the server could not find the requested resource (get pods dns-test-6bd8bd08-35c9-4abf-85a1-35e3cce72f40)
    Mar  2 01:39:37.780: INFO: Unable to read jessie_udp@dns-test-service.dns-7339.svc.cluster.local from pod dns-7339/dns-test-6bd8bd08-35c9-4abf-85a1-35e3cce72f40: the server could not find the requested resource (get pods dns-test-6bd8bd08-35c9-4abf-85a1-35e3cce72f40)
    Mar  2 01:39:37.799: INFO: Unable to read jessie_tcp@dns-test-service.dns-7339.svc.cluster.local from pod dns-7339/dns-test-6bd8bd08-35c9-4abf-85a1-35e3cce72f40: the server could not find the requested resource (get pods dns-test-6bd8bd08-35c9-4abf-85a1-35e3cce72f40)
    Mar  2 01:39:37.815: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7339.svc.cluster.local from pod dns-7339/dns-test-6bd8bd08-35c9-4abf-85a1-35e3cce72f40: the server could not find the requested resource (get pods dns-test-6bd8bd08-35c9-4abf-85a1-35e3cce72f40)
    Mar  2 01:39:37.909: INFO: Lookups using dns-7339/dns-test-6bd8bd08-35c9-4abf-85a1-35e3cce72f40 failed for: [wheezy_udp@dns-test-service.dns-7339.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7339.svc.cluster.local jessie_udp@dns-test-service.dns-7339.svc.cluster.local jessie_tcp@dns-test-service.dns-7339.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7339.svc.cluster.local]

    Mar  2 01:39:43.235: INFO: DNS probes using dns-7339/dns-test-6bd8bd08-35c9-4abf-85a1-35e3cce72f40 succeeded

    STEP: deleting the pod 03/02/23 01:39:43.235
    STEP: deleting the test service 03/02/23 01:39:43.271
    STEP: deleting the test headless service 03/02/23 01:39:43.324
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar  2 01:39:43.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-7339" for this suite. 03/02/23 01:39:43.389
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:39:43.41
Mar  2 01:39:43.410: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename services 03/02/23 01:39:43.412
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:39:43.47
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:39:43.486
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
STEP: creating a service externalname-service with the type=ExternalName in namespace services-1876 03/02/23 01:39:43.498
STEP: changing the ExternalName service to type=ClusterIP 03/02/23 01:39:43.524
STEP: creating replication controller externalname-service in namespace services-1876 03/02/23 01:39:43.584
I0302 01:39:43.609983      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-1876, replica count: 2
I0302 01:39:46.661210      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 01:39:46.661: INFO: Creating new exec pod
Mar  2 01:39:46.718: INFO: Waiting up to 5m0s for pod "execpodfvpfs" in namespace "services-1876" to be "running"
Mar  2 01:39:46.730: INFO: Pod "execpodfvpfs": Phase="Pending", Reason="", readiness=false. Elapsed: 12.306264ms
Mar  2 01:39:48.750: INFO: Pod "execpodfvpfs": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032202025s
Mar  2 01:39:50.751: INFO: Pod "execpodfvpfs": Phase="Running", Reason="", readiness=true. Elapsed: 4.033715395s
Mar  2 01:39:50.752: INFO: Pod "execpodfvpfs" satisfied condition "running"
Mar  2 01:39:51.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-1876 exec execpodfvpfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Mar  2 01:39:52.186: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar  2 01:39:52.186: INFO: stdout: ""
Mar  2 01:39:53.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-1876 exec execpodfvpfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Mar  2 01:39:53.493: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar  2 01:39:53.493: INFO: stdout: ""
Mar  2 01:39:54.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-1876 exec execpodfvpfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Mar  2 01:39:54.619: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar  2 01:39:54.619: INFO: stdout: "externalname-service-m5z2w"
Mar  2 01:39:54.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-1876 exec execpodfvpfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.253.226 80'
Mar  2 01:39:54.965: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.253.226 80\nConnection to 172.21.253.226 80 port [tcp/http] succeeded!\n"
Mar  2 01:39:54.965: INFO: stdout: ""
Mar  2 01:39:55.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-1876 exec execpodfvpfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.253.226 80'
Mar  2 01:39:56.334: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.253.226 80\nConnection to 172.21.253.226 80 port [tcp/http] succeeded!\n"
Mar  2 01:39:56.334: INFO: stdout: "externalname-service-kl4z6"
Mar  2 01:39:56.334: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  2 01:39:56.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1876" for this suite. 03/02/23 01:39:56.433
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","completed":108,"skipped":2079,"failed":0}
------------------------------
• [SLOW TEST] [13.044 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:39:43.41
    Mar  2 01:39:43.410: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename services 03/02/23 01:39:43.412
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:39:43.47
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:39:43.486
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1404
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-1876 03/02/23 01:39:43.498
    STEP: changing the ExternalName service to type=ClusterIP 03/02/23 01:39:43.524
    STEP: creating replication controller externalname-service in namespace services-1876 03/02/23 01:39:43.584
    I0302 01:39:43.609983      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-1876, replica count: 2
    I0302 01:39:46.661210      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar  2 01:39:46.661: INFO: Creating new exec pod
    Mar  2 01:39:46.718: INFO: Waiting up to 5m0s for pod "execpodfvpfs" in namespace "services-1876" to be "running"
    Mar  2 01:39:46.730: INFO: Pod "execpodfvpfs": Phase="Pending", Reason="", readiness=false. Elapsed: 12.306264ms
    Mar  2 01:39:48.750: INFO: Pod "execpodfvpfs": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032202025s
    Mar  2 01:39:50.751: INFO: Pod "execpodfvpfs": Phase="Running", Reason="", readiness=true. Elapsed: 4.033715395s
    Mar  2 01:39:50.752: INFO: Pod "execpodfvpfs" satisfied condition "running"
    Mar  2 01:39:51.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-1876 exec execpodfvpfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Mar  2 01:39:52.186: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Mar  2 01:39:52.186: INFO: stdout: ""
    Mar  2 01:39:53.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-1876 exec execpodfvpfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Mar  2 01:39:53.493: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Mar  2 01:39:53.493: INFO: stdout: ""
    Mar  2 01:39:54.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-1876 exec execpodfvpfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Mar  2 01:39:54.619: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Mar  2 01:39:54.619: INFO: stdout: "externalname-service-m5z2w"
    Mar  2 01:39:54.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-1876 exec execpodfvpfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.253.226 80'
    Mar  2 01:39:54.965: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.253.226 80\nConnection to 172.21.253.226 80 port [tcp/http] succeeded!\n"
    Mar  2 01:39:54.965: INFO: stdout: ""
    Mar  2 01:39:55.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-1876 exec execpodfvpfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.253.226 80'
    Mar  2 01:39:56.334: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.253.226 80\nConnection to 172.21.253.226 80 port [tcp/http] succeeded!\n"
    Mar  2 01:39:56.334: INFO: stdout: "externalname-service-kl4z6"
    Mar  2 01:39:56.334: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  2 01:39:56.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1876" for this suite. 03/02/23 01:39:56.433
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:39:56.461
Mar  2 01:39:56.461: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename webhook 03/02/23 01:39:56.482
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:39:56.544
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:39:56.562
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/02/23 01:39:56.668
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 01:39:57.891
STEP: Deploying the webhook pod 03/02/23 01:39:57.968
STEP: Wait for the deployment to be ready 03/02/23 01:39:58.093
Mar  2 01:39:58.121: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/02/23 01:40:00.191
STEP: Verifying the service has paired with the endpoint 03/02/23 01:40:00.224
Mar  2 01:40:01.225: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
STEP: Setting timeout (1s) shorter than webhook latency (5s) 03/02/23 01:40:01.243
STEP: Registering slow webhook via the AdmissionRegistration API 03/02/23 01:40:01.243
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 03/02/23 01:40:01.406
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 03/02/23 01:40:02.505
STEP: Registering slow webhook via the AdmissionRegistration API 03/02/23 01:40:02.505
STEP: Having no error when timeout is longer than webhook latency 03/02/23 01:40:03.78
STEP: Registering slow webhook via the AdmissionRegistration API 03/02/23 01:40:03.781
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 03/02/23 01:40:08.978
STEP: Registering slow webhook via the AdmissionRegistration API 03/02/23 01:40:08.978
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 01:40:14.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5239" for this suite. 03/02/23 01:40:14.192
STEP: Destroying namespace "webhook-5239-markers" for this suite. 03/02/23 01:40:14.216
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","completed":109,"skipped":2110,"failed":0}
------------------------------
• [SLOW TEST] [17.952 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:39:56.461
    Mar  2 01:39:56.461: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename webhook 03/02/23 01:39:56.482
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:39:56.544
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:39:56.562
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/02/23 01:39:56.668
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 01:39:57.891
    STEP: Deploying the webhook pod 03/02/23 01:39:57.968
    STEP: Wait for the deployment to be ready 03/02/23 01:39:58.093
    Mar  2 01:39:58.121: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/02/23 01:40:00.191
    STEP: Verifying the service has paired with the endpoint 03/02/23 01:40:00.224
    Mar  2 01:40:01.225: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:380
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 03/02/23 01:40:01.243
    STEP: Registering slow webhook via the AdmissionRegistration API 03/02/23 01:40:01.243
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 03/02/23 01:40:01.406
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 03/02/23 01:40:02.505
    STEP: Registering slow webhook via the AdmissionRegistration API 03/02/23 01:40:02.505
    STEP: Having no error when timeout is longer than webhook latency 03/02/23 01:40:03.78
    STEP: Registering slow webhook via the AdmissionRegistration API 03/02/23 01:40:03.781
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 03/02/23 01:40:08.978
    STEP: Registering slow webhook via the AdmissionRegistration API 03/02/23 01:40:08.978
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 01:40:14.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5239" for this suite. 03/02/23 01:40:14.192
    STEP: Destroying namespace "webhook-5239-markers" for this suite. 03/02/23 01:40:14.216
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:40:14.414
Mar  2 01:40:14.414: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename pods 03/02/23 01:40:14.416
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:40:14.494
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:40:14.54
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
STEP: Create a pod 03/02/23 01:40:14.568
Mar  2 01:40:14.705: INFO: Waiting up to 5m0s for pod "pod-chbcn" in namespace "pods-2624" to be "running"
Mar  2 01:40:14.777: INFO: Pod "pod-chbcn": Phase="Pending", Reason="", readiness=false. Elapsed: 71.883245ms
Mar  2 01:40:16.801: INFO: Pod "pod-chbcn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.095906551s
Mar  2 01:40:18.801: INFO: Pod "pod-chbcn": Phase="Running", Reason="", readiness=true. Elapsed: 4.096634803s
Mar  2 01:40:18.801: INFO: Pod "pod-chbcn" satisfied condition "running"
STEP: patching /status 03/02/23 01:40:18.801
Mar  2 01:40:18.829: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar  2 01:40:18.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2624" for this suite. 03/02/23 01:40:18.876
{"msg":"PASSED [sig-node] Pods should patch a pod status [Conformance]","completed":110,"skipped":2140,"failed":0}
------------------------------
• [4.489 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:40:14.414
    Mar  2 01:40:14.414: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename pods 03/02/23 01:40:14.416
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:40:14.494
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:40:14.54
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1082
    STEP: Create a pod 03/02/23 01:40:14.568
    Mar  2 01:40:14.705: INFO: Waiting up to 5m0s for pod "pod-chbcn" in namespace "pods-2624" to be "running"
    Mar  2 01:40:14.777: INFO: Pod "pod-chbcn": Phase="Pending", Reason="", readiness=false. Elapsed: 71.883245ms
    Mar  2 01:40:16.801: INFO: Pod "pod-chbcn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.095906551s
    Mar  2 01:40:18.801: INFO: Pod "pod-chbcn": Phase="Running", Reason="", readiness=true. Elapsed: 4.096634803s
    Mar  2 01:40:18.801: INFO: Pod "pod-chbcn" satisfied condition "running"
    STEP: patching /status 03/02/23 01:40:18.801
    Mar  2 01:40:18.829: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar  2 01:40:18.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-2624" for this suite. 03/02/23 01:40:18.876
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:40:18.904
Mar  2 01:40:18.904: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename downward-api 03/02/23 01:40:18.905
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:40:18.959
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:40:18.972
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
STEP: Creating a pod to test downward API volume plugin 03/02/23 01:40:18.988
Mar  2 01:40:19.143: INFO: Waiting up to 5m0s for pod "downwardapi-volume-129ded14-9ce2-4c91-b7a8-8a42fae8afbd" in namespace "downward-api-1536" to be "Succeeded or Failed"
Mar  2 01:40:19.155: INFO: Pod "downwardapi-volume-129ded14-9ce2-4c91-b7a8-8a42fae8afbd": Phase="Pending", Reason="", readiness=false. Elapsed: 12.240461ms
Mar  2 01:40:21.168: INFO: Pod "downwardapi-volume-129ded14-9ce2-4c91-b7a8-8a42fae8afbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025277256s
Mar  2 01:40:23.168: INFO: Pod "downwardapi-volume-129ded14-9ce2-4c91-b7a8-8a42fae8afbd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025667007s
Mar  2 01:40:25.174: INFO: Pod "downwardapi-volume-129ded14-9ce2-4c91-b7a8-8a42fae8afbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031389878s
STEP: Saw pod success 03/02/23 01:40:25.174
Mar  2 01:40:25.174: INFO: Pod "downwardapi-volume-129ded14-9ce2-4c91-b7a8-8a42fae8afbd" satisfied condition "Succeeded or Failed"
Mar  2 01:40:25.193: INFO: Trying to get logs from node 10.132.92.143 pod downwardapi-volume-129ded14-9ce2-4c91-b7a8-8a42fae8afbd container client-container: <nil>
STEP: delete the pod 03/02/23 01:40:25.27
Mar  2 01:40:25.329: INFO: Waiting for pod downwardapi-volume-129ded14-9ce2-4c91-b7a8-8a42fae8afbd to disappear
Mar  2 01:40:25.346: INFO: Pod downwardapi-volume-129ded14-9ce2-4c91-b7a8-8a42fae8afbd no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar  2 01:40:25.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1536" for this suite. 03/02/23 01:40:25.408
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":111,"skipped":2145,"failed":0}
------------------------------
• [SLOW TEST] [6.545 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:40:18.904
    Mar  2 01:40:18.904: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename downward-api 03/02/23 01:40:18.905
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:40:18.959
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:40:18.972
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:67
    STEP: Creating a pod to test downward API volume plugin 03/02/23 01:40:18.988
    Mar  2 01:40:19.143: INFO: Waiting up to 5m0s for pod "downwardapi-volume-129ded14-9ce2-4c91-b7a8-8a42fae8afbd" in namespace "downward-api-1536" to be "Succeeded or Failed"
    Mar  2 01:40:19.155: INFO: Pod "downwardapi-volume-129ded14-9ce2-4c91-b7a8-8a42fae8afbd": Phase="Pending", Reason="", readiness=false. Elapsed: 12.240461ms
    Mar  2 01:40:21.168: INFO: Pod "downwardapi-volume-129ded14-9ce2-4c91-b7a8-8a42fae8afbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025277256s
    Mar  2 01:40:23.168: INFO: Pod "downwardapi-volume-129ded14-9ce2-4c91-b7a8-8a42fae8afbd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025667007s
    Mar  2 01:40:25.174: INFO: Pod "downwardapi-volume-129ded14-9ce2-4c91-b7a8-8a42fae8afbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031389878s
    STEP: Saw pod success 03/02/23 01:40:25.174
    Mar  2 01:40:25.174: INFO: Pod "downwardapi-volume-129ded14-9ce2-4c91-b7a8-8a42fae8afbd" satisfied condition "Succeeded or Failed"
    Mar  2 01:40:25.193: INFO: Trying to get logs from node 10.132.92.143 pod downwardapi-volume-129ded14-9ce2-4c91-b7a8-8a42fae8afbd container client-container: <nil>
    STEP: delete the pod 03/02/23 01:40:25.27
    Mar  2 01:40:25.329: INFO: Waiting for pod downwardapi-volume-129ded14-9ce2-4c91-b7a8-8a42fae8afbd to disappear
    Mar  2 01:40:25.346: INFO: Pod downwardapi-volume-129ded14-9ce2-4c91-b7a8-8a42fae8afbd no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar  2 01:40:25.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1536" for this suite. 03/02/23 01:40:25.408
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:40:25.451
Mar  2 01:40:25.451: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename downward-api 03/02/23 01:40:25.453
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:40:25.567
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:40:25.578
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
STEP: Creating a pod to test downward api env vars 03/02/23 01:40:25.605
Mar  2 01:40:25.716: INFO: Waiting up to 5m0s for pod "downward-api-617af42e-70e6-48fd-8b3f-7edf3eada449" in namespace "downward-api-4743" to be "Succeeded or Failed"
Mar  2 01:40:25.748: INFO: Pod "downward-api-617af42e-70e6-48fd-8b3f-7edf3eada449": Phase="Pending", Reason="", readiness=false. Elapsed: 13.913383ms
Mar  2 01:40:27.763: INFO: Pod "downward-api-617af42e-70e6-48fd-8b3f-7edf3eada449": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02838564s
Mar  2 01:40:29.762: INFO: Pod "downward-api-617af42e-70e6-48fd-8b3f-7edf3eada449": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027924683s
Mar  2 01:40:31.767: INFO: Pod "downward-api-617af42e-70e6-48fd-8b3f-7edf3eada449": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032578861s
STEP: Saw pod success 03/02/23 01:40:31.767
Mar  2 01:40:31.767: INFO: Pod "downward-api-617af42e-70e6-48fd-8b3f-7edf3eada449" satisfied condition "Succeeded or Failed"
Mar  2 01:40:31.781: INFO: Trying to get logs from node 10.132.92.143 pod downward-api-617af42e-70e6-48fd-8b3f-7edf3eada449 container dapi-container: <nil>
STEP: delete the pod 03/02/23 01:40:31.828
Mar  2 01:40:31.921: INFO: Waiting for pod downward-api-617af42e-70e6-48fd-8b3f-7edf3eada449 to disappear
Mar  2 01:40:31.944: INFO: Pod downward-api-617af42e-70e6-48fd-8b3f-7edf3eada449 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Mar  2 01:40:31.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4743" for this suite. 03/02/23 01:40:31.995
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","completed":112,"skipped":2152,"failed":0}
------------------------------
• [SLOW TEST] [6.607 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:40:25.451
    Mar  2 01:40:25.451: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename downward-api 03/02/23 01:40:25.453
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:40:25.567
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:40:25.578
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:266
    STEP: Creating a pod to test downward api env vars 03/02/23 01:40:25.605
    Mar  2 01:40:25.716: INFO: Waiting up to 5m0s for pod "downward-api-617af42e-70e6-48fd-8b3f-7edf3eada449" in namespace "downward-api-4743" to be "Succeeded or Failed"
    Mar  2 01:40:25.748: INFO: Pod "downward-api-617af42e-70e6-48fd-8b3f-7edf3eada449": Phase="Pending", Reason="", readiness=false. Elapsed: 13.913383ms
    Mar  2 01:40:27.763: INFO: Pod "downward-api-617af42e-70e6-48fd-8b3f-7edf3eada449": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02838564s
    Mar  2 01:40:29.762: INFO: Pod "downward-api-617af42e-70e6-48fd-8b3f-7edf3eada449": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027924683s
    Mar  2 01:40:31.767: INFO: Pod "downward-api-617af42e-70e6-48fd-8b3f-7edf3eada449": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032578861s
    STEP: Saw pod success 03/02/23 01:40:31.767
    Mar  2 01:40:31.767: INFO: Pod "downward-api-617af42e-70e6-48fd-8b3f-7edf3eada449" satisfied condition "Succeeded or Failed"
    Mar  2 01:40:31.781: INFO: Trying to get logs from node 10.132.92.143 pod downward-api-617af42e-70e6-48fd-8b3f-7edf3eada449 container dapi-container: <nil>
    STEP: delete the pod 03/02/23 01:40:31.828
    Mar  2 01:40:31.921: INFO: Waiting for pod downward-api-617af42e-70e6-48fd-8b3f-7edf3eada449 to disappear
    Mar  2 01:40:31.944: INFO: Pod downward-api-617af42e-70e6-48fd-8b3f-7edf3eada449 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Mar  2 01:40:31.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-4743" for this suite. 03/02/23 01:40:31.995
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:40:32.066
Mar  2 01:40:32.067: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename gc 03/02/23 01:40:32.067
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:40:32.154
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:40:32.176
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 03/02/23 01:40:32.241
STEP: delete the rc 03/02/23 01:40:37.358
STEP: wait for the rc to be deleted 03/02/23 01:40:37.498
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 03/02/23 01:40:42.533
STEP: Gathering metrics 03/02/23 01:41:12.572
W0302 01:41:12.593714      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Mar  2 01:41:12.593: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Mar  2 01:41:12.593: INFO: Deleting pod "simpletest.rc-2lt4x" in namespace "gc-4999"
Mar  2 01:41:12.633: INFO: Deleting pod "simpletest.rc-2xqc2" in namespace "gc-4999"
Mar  2 01:41:12.675: INFO: Deleting pod "simpletest.rc-44l5j" in namespace "gc-4999"
Mar  2 01:41:12.715: INFO: Deleting pod "simpletest.rc-4bt8j" in namespace "gc-4999"
Mar  2 01:41:12.760: INFO: Deleting pod "simpletest.rc-4gq5c" in namespace "gc-4999"
Mar  2 01:41:12.801: INFO: Deleting pod "simpletest.rc-4hjdf" in namespace "gc-4999"
Mar  2 01:41:12.839: INFO: Deleting pod "simpletest.rc-4sjnf" in namespace "gc-4999"
Mar  2 01:41:12.907: INFO: Deleting pod "simpletest.rc-556pg" in namespace "gc-4999"
Mar  2 01:41:12.955: INFO: Deleting pod "simpletest.rc-5bt2m" in namespace "gc-4999"
Mar  2 01:41:13.032: INFO: Deleting pod "simpletest.rc-5fl6f" in namespace "gc-4999"
Mar  2 01:41:13.095: INFO: Deleting pod "simpletest.rc-5krdt" in namespace "gc-4999"
Mar  2 01:41:13.144: INFO: Deleting pod "simpletest.rc-5qlnc" in namespace "gc-4999"
Mar  2 01:41:13.213: INFO: Deleting pod "simpletest.rc-5tdzj" in namespace "gc-4999"
Mar  2 01:41:13.329: INFO: Deleting pod "simpletest.rc-6kxx5" in namespace "gc-4999"
Mar  2 01:41:13.416: INFO: Deleting pod "simpletest.rc-6n9zc" in namespace "gc-4999"
Mar  2 01:41:13.459: INFO: Deleting pod "simpletest.rc-6wnb4" in namespace "gc-4999"
Mar  2 01:41:13.493: INFO: Deleting pod "simpletest.rc-74v7h" in namespace "gc-4999"
Mar  2 01:41:13.539: INFO: Deleting pod "simpletest.rc-7gb42" in namespace "gc-4999"
Mar  2 01:41:13.587: INFO: Deleting pod "simpletest.rc-7hrb7" in namespace "gc-4999"
Mar  2 01:41:13.687: INFO: Deleting pod "simpletest.rc-7jzlp" in namespace "gc-4999"
Mar  2 01:41:13.732: INFO: Deleting pod "simpletest.rc-7ks78" in namespace "gc-4999"
Mar  2 01:41:13.766: INFO: Deleting pod "simpletest.rc-82vq2" in namespace "gc-4999"
Mar  2 01:41:13.820: INFO: Deleting pod "simpletest.rc-8sj4b" in namespace "gc-4999"
Mar  2 01:41:13.857: INFO: Deleting pod "simpletest.rc-94r6l" in namespace "gc-4999"
Mar  2 01:41:13.910: INFO: Deleting pod "simpletest.rc-96nsj" in namespace "gc-4999"
Mar  2 01:41:13.962: INFO: Deleting pod "simpletest.rc-9hgwn" in namespace "gc-4999"
Mar  2 01:41:14.031: INFO: Deleting pod "simpletest.rc-9s2l7" in namespace "gc-4999"
Mar  2 01:41:14.087: INFO: Deleting pod "simpletest.rc-b6fdl" in namespace "gc-4999"
Mar  2 01:41:14.166: INFO: Deleting pod "simpletest.rc-cdmrb" in namespace "gc-4999"
Mar  2 01:41:14.232: INFO: Deleting pod "simpletest.rc-csmpt" in namespace "gc-4999"
Mar  2 01:41:14.278: INFO: Deleting pod "simpletest.rc-cszxv" in namespace "gc-4999"
Mar  2 01:41:14.321: INFO: Deleting pod "simpletest.rc-dgvd4" in namespace "gc-4999"
Mar  2 01:41:14.388: INFO: Deleting pod "simpletest.rc-dt7h2" in namespace "gc-4999"
Mar  2 01:41:14.433: INFO: Deleting pod "simpletest.rc-dwdvr" in namespace "gc-4999"
Mar  2 01:41:14.490: INFO: Deleting pod "simpletest.rc-fm8ps" in namespace "gc-4999"
Mar  2 01:41:14.534: INFO: Deleting pod "simpletest.rc-ft6lf" in namespace "gc-4999"
Mar  2 01:41:14.567: INFO: Deleting pod "simpletest.rc-fvz4s" in namespace "gc-4999"
Mar  2 01:41:14.658: INFO: Deleting pod "simpletest.rc-fwgxj" in namespace "gc-4999"
Mar  2 01:41:14.706: INFO: Deleting pod "simpletest.rc-fzfqg" in namespace "gc-4999"
Mar  2 01:41:14.751: INFO: Deleting pod "simpletest.rc-fzm47" in namespace "gc-4999"
Mar  2 01:41:14.792: INFO: Deleting pod "simpletest.rc-gc8xn" in namespace "gc-4999"
Mar  2 01:41:14.840: INFO: Deleting pod "simpletest.rc-gpplj" in namespace "gc-4999"
Mar  2 01:41:14.882: INFO: Deleting pod "simpletest.rc-gwmzs" in namespace "gc-4999"
Mar  2 01:41:14.935: INFO: Deleting pod "simpletest.rc-h6whx" in namespace "gc-4999"
Mar  2 01:41:14.985: INFO: Deleting pod "simpletest.rc-h8qcp" in namespace "gc-4999"
Mar  2 01:41:15.082: INFO: Deleting pod "simpletest.rc-hd65q" in namespace "gc-4999"
Mar  2 01:41:15.138: INFO: Deleting pod "simpletest.rc-hk8tw" in namespace "gc-4999"
Mar  2 01:41:15.176: INFO: Deleting pod "simpletest.rc-hpp9j" in namespace "gc-4999"
Mar  2 01:41:15.234: INFO: Deleting pod "simpletest.rc-j2cn7" in namespace "gc-4999"
Mar  2 01:41:15.271: INFO: Deleting pod "simpletest.rc-jf4ql" in namespace "gc-4999"
Mar  2 01:41:15.350: INFO: Deleting pod "simpletest.rc-jp24z" in namespace "gc-4999"
Mar  2 01:41:15.414: INFO: Deleting pod "simpletest.rc-jzb68" in namespace "gc-4999"
Mar  2 01:41:15.517: INFO: Deleting pod "simpletest.rc-k97r9" in namespace "gc-4999"
Mar  2 01:41:15.552: INFO: Deleting pod "simpletest.rc-kwjlt" in namespace "gc-4999"
Mar  2 01:41:15.609: INFO: Deleting pod "simpletest.rc-l9dt5" in namespace "gc-4999"
Mar  2 01:41:15.651: INFO: Deleting pod "simpletest.rc-lcdpc" in namespace "gc-4999"
Mar  2 01:41:15.685: INFO: Deleting pod "simpletest.rc-lmgr9" in namespace "gc-4999"
Mar  2 01:41:15.741: INFO: Deleting pod "simpletest.rc-lphcd" in namespace "gc-4999"
Mar  2 01:41:15.785: INFO: Deleting pod "simpletest.rc-lx8lj" in namespace "gc-4999"
Mar  2 01:41:16.020: INFO: Deleting pod "simpletest.rc-lxqn9" in namespace "gc-4999"
Mar  2 01:41:16.244: INFO: Deleting pod "simpletest.rc-m2p5k" in namespace "gc-4999"
Mar  2 01:41:16.449: INFO: Deleting pod "simpletest.rc-mdkf6" in namespace "gc-4999"
Mar  2 01:41:16.575: INFO: Deleting pod "simpletest.rc-mpqgz" in namespace "gc-4999"
Mar  2 01:41:16.611: INFO: Deleting pod "simpletest.rc-mr8lc" in namespace "gc-4999"
Mar  2 01:41:16.656: INFO: Deleting pod "simpletest.rc-mrzf6" in namespace "gc-4999"
Mar  2 01:41:16.688: INFO: Deleting pod "simpletest.rc-mw9w8" in namespace "gc-4999"
Mar  2 01:41:16.747: INFO: Deleting pod "simpletest.rc-nqjvv" in namespace "gc-4999"
Mar  2 01:41:16.803: INFO: Deleting pod "simpletest.rc-nw6jv" in namespace "gc-4999"
Mar  2 01:41:16.873: INFO: Deleting pod "simpletest.rc-pdv7p" in namespace "gc-4999"
Mar  2 01:41:16.933: INFO: Deleting pod "simpletest.rc-pg2ch" in namespace "gc-4999"
Mar  2 01:41:16.984: INFO: Deleting pod "simpletest.rc-pnblk" in namespace "gc-4999"
Mar  2 01:41:17.055: INFO: Deleting pod "simpletest.rc-prcjl" in namespace "gc-4999"
Mar  2 01:41:17.098: INFO: Deleting pod "simpletest.rc-q8tzs" in namespace "gc-4999"
Mar  2 01:41:17.158: INFO: Deleting pod "simpletest.rc-qd8c4" in namespace "gc-4999"
Mar  2 01:41:17.194: INFO: Deleting pod "simpletest.rc-qwjfl" in namespace "gc-4999"
Mar  2 01:41:17.321: INFO: Deleting pod "simpletest.rc-qwxbf" in namespace "gc-4999"
Mar  2 01:41:17.453: INFO: Deleting pod "simpletest.rc-r75zv" in namespace "gc-4999"
Mar  2 01:41:17.489: INFO: Deleting pod "simpletest.rc-rtckz" in namespace "gc-4999"
Mar  2 01:41:17.520: INFO: Deleting pod "simpletest.rc-s4czz" in namespace "gc-4999"
Mar  2 01:41:17.559: INFO: Deleting pod "simpletest.rc-sgtmr" in namespace "gc-4999"
Mar  2 01:41:17.590: INFO: Deleting pod "simpletest.rc-slc4n" in namespace "gc-4999"
Mar  2 01:41:17.634: INFO: Deleting pod "simpletest.rc-sw92k" in namespace "gc-4999"
Mar  2 01:41:17.693: INFO: Deleting pod "simpletest.rc-sxr8s" in namespace "gc-4999"
Mar  2 01:41:17.779: INFO: Deleting pod "simpletest.rc-t7zfz" in namespace "gc-4999"
Mar  2 01:41:17.842: INFO: Deleting pod "simpletest.rc-tpt7d" in namespace "gc-4999"
Mar  2 01:41:17.901: INFO: Deleting pod "simpletest.rc-twjz8" in namespace "gc-4999"
Mar  2 01:41:17.958: INFO: Deleting pod "simpletest.rc-v7mlz" in namespace "gc-4999"
Mar  2 01:41:17.998: INFO: Deleting pod "simpletest.rc-v8x4z" in namespace "gc-4999"
Mar  2 01:41:18.071: INFO: Deleting pod "simpletest.rc-vlxbd" in namespace "gc-4999"
Mar  2 01:41:18.119: INFO: Deleting pod "simpletest.rc-vvvzx" in namespace "gc-4999"
Mar  2 01:41:18.182: INFO: Deleting pod "simpletest.rc-wzr9x" in namespace "gc-4999"
Mar  2 01:41:18.225: INFO: Deleting pod "simpletest.rc-x8nn7" in namespace "gc-4999"
Mar  2 01:41:18.300: INFO: Deleting pod "simpletest.rc-x8zwt" in namespace "gc-4999"
Mar  2 01:41:18.367: INFO: Deleting pod "simpletest.rc-xdwfp" in namespace "gc-4999"
Mar  2 01:41:18.436: INFO: Deleting pod "simpletest.rc-xkzhp" in namespace "gc-4999"
Mar  2 01:41:18.514: INFO: Deleting pod "simpletest.rc-xpqmv" in namespace "gc-4999"
Mar  2 01:41:18.563: INFO: Deleting pod "simpletest.rc-xqtp6" in namespace "gc-4999"
Mar  2 01:41:18.610: INFO: Deleting pod "simpletest.rc-z2wwh" in namespace "gc-4999"
Mar  2 01:41:18.651: INFO: Deleting pod "simpletest.rc-z6ddn" in namespace "gc-4999"
Mar  2 01:41:18.727: INFO: Deleting pod "simpletest.rc-zs4nq" in namespace "gc-4999"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Mar  2 01:41:18.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4999" for this suite. 03/02/23 01:41:18.838
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","completed":113,"skipped":2174,"failed":0}
------------------------------
• [SLOW TEST] [46.800 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:40:32.066
    Mar  2 01:40:32.067: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename gc 03/02/23 01:40:32.067
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:40:32.154
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:40:32.176
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 03/02/23 01:40:32.241
    STEP: delete the rc 03/02/23 01:40:37.358
    STEP: wait for the rc to be deleted 03/02/23 01:40:37.498
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 03/02/23 01:40:42.533
    STEP: Gathering metrics 03/02/23 01:41:12.572
    W0302 01:41:12.593714      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Mar  2 01:41:12.593: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Mar  2 01:41:12.593: INFO: Deleting pod "simpletest.rc-2lt4x" in namespace "gc-4999"
    Mar  2 01:41:12.633: INFO: Deleting pod "simpletest.rc-2xqc2" in namespace "gc-4999"
    Mar  2 01:41:12.675: INFO: Deleting pod "simpletest.rc-44l5j" in namespace "gc-4999"
    Mar  2 01:41:12.715: INFO: Deleting pod "simpletest.rc-4bt8j" in namespace "gc-4999"
    Mar  2 01:41:12.760: INFO: Deleting pod "simpletest.rc-4gq5c" in namespace "gc-4999"
    Mar  2 01:41:12.801: INFO: Deleting pod "simpletest.rc-4hjdf" in namespace "gc-4999"
    Mar  2 01:41:12.839: INFO: Deleting pod "simpletest.rc-4sjnf" in namespace "gc-4999"
    Mar  2 01:41:12.907: INFO: Deleting pod "simpletest.rc-556pg" in namespace "gc-4999"
    Mar  2 01:41:12.955: INFO: Deleting pod "simpletest.rc-5bt2m" in namespace "gc-4999"
    Mar  2 01:41:13.032: INFO: Deleting pod "simpletest.rc-5fl6f" in namespace "gc-4999"
    Mar  2 01:41:13.095: INFO: Deleting pod "simpletest.rc-5krdt" in namespace "gc-4999"
    Mar  2 01:41:13.144: INFO: Deleting pod "simpletest.rc-5qlnc" in namespace "gc-4999"
    Mar  2 01:41:13.213: INFO: Deleting pod "simpletest.rc-5tdzj" in namespace "gc-4999"
    Mar  2 01:41:13.329: INFO: Deleting pod "simpletest.rc-6kxx5" in namespace "gc-4999"
    Mar  2 01:41:13.416: INFO: Deleting pod "simpletest.rc-6n9zc" in namespace "gc-4999"
    Mar  2 01:41:13.459: INFO: Deleting pod "simpletest.rc-6wnb4" in namespace "gc-4999"
    Mar  2 01:41:13.493: INFO: Deleting pod "simpletest.rc-74v7h" in namespace "gc-4999"
    Mar  2 01:41:13.539: INFO: Deleting pod "simpletest.rc-7gb42" in namespace "gc-4999"
    Mar  2 01:41:13.587: INFO: Deleting pod "simpletest.rc-7hrb7" in namespace "gc-4999"
    Mar  2 01:41:13.687: INFO: Deleting pod "simpletest.rc-7jzlp" in namespace "gc-4999"
    Mar  2 01:41:13.732: INFO: Deleting pod "simpletest.rc-7ks78" in namespace "gc-4999"
    Mar  2 01:41:13.766: INFO: Deleting pod "simpletest.rc-82vq2" in namespace "gc-4999"
    Mar  2 01:41:13.820: INFO: Deleting pod "simpletest.rc-8sj4b" in namespace "gc-4999"
    Mar  2 01:41:13.857: INFO: Deleting pod "simpletest.rc-94r6l" in namespace "gc-4999"
    Mar  2 01:41:13.910: INFO: Deleting pod "simpletest.rc-96nsj" in namespace "gc-4999"
    Mar  2 01:41:13.962: INFO: Deleting pod "simpletest.rc-9hgwn" in namespace "gc-4999"
    Mar  2 01:41:14.031: INFO: Deleting pod "simpletest.rc-9s2l7" in namespace "gc-4999"
    Mar  2 01:41:14.087: INFO: Deleting pod "simpletest.rc-b6fdl" in namespace "gc-4999"
    Mar  2 01:41:14.166: INFO: Deleting pod "simpletest.rc-cdmrb" in namespace "gc-4999"
    Mar  2 01:41:14.232: INFO: Deleting pod "simpletest.rc-csmpt" in namespace "gc-4999"
    Mar  2 01:41:14.278: INFO: Deleting pod "simpletest.rc-cszxv" in namespace "gc-4999"
    Mar  2 01:41:14.321: INFO: Deleting pod "simpletest.rc-dgvd4" in namespace "gc-4999"
    Mar  2 01:41:14.388: INFO: Deleting pod "simpletest.rc-dt7h2" in namespace "gc-4999"
    Mar  2 01:41:14.433: INFO: Deleting pod "simpletest.rc-dwdvr" in namespace "gc-4999"
    Mar  2 01:41:14.490: INFO: Deleting pod "simpletest.rc-fm8ps" in namespace "gc-4999"
    Mar  2 01:41:14.534: INFO: Deleting pod "simpletest.rc-ft6lf" in namespace "gc-4999"
    Mar  2 01:41:14.567: INFO: Deleting pod "simpletest.rc-fvz4s" in namespace "gc-4999"
    Mar  2 01:41:14.658: INFO: Deleting pod "simpletest.rc-fwgxj" in namespace "gc-4999"
    Mar  2 01:41:14.706: INFO: Deleting pod "simpletest.rc-fzfqg" in namespace "gc-4999"
    Mar  2 01:41:14.751: INFO: Deleting pod "simpletest.rc-fzm47" in namespace "gc-4999"
    Mar  2 01:41:14.792: INFO: Deleting pod "simpletest.rc-gc8xn" in namespace "gc-4999"
    Mar  2 01:41:14.840: INFO: Deleting pod "simpletest.rc-gpplj" in namespace "gc-4999"
    Mar  2 01:41:14.882: INFO: Deleting pod "simpletest.rc-gwmzs" in namespace "gc-4999"
    Mar  2 01:41:14.935: INFO: Deleting pod "simpletest.rc-h6whx" in namespace "gc-4999"
    Mar  2 01:41:14.985: INFO: Deleting pod "simpletest.rc-h8qcp" in namespace "gc-4999"
    Mar  2 01:41:15.082: INFO: Deleting pod "simpletest.rc-hd65q" in namespace "gc-4999"
    Mar  2 01:41:15.138: INFO: Deleting pod "simpletest.rc-hk8tw" in namespace "gc-4999"
    Mar  2 01:41:15.176: INFO: Deleting pod "simpletest.rc-hpp9j" in namespace "gc-4999"
    Mar  2 01:41:15.234: INFO: Deleting pod "simpletest.rc-j2cn7" in namespace "gc-4999"
    Mar  2 01:41:15.271: INFO: Deleting pod "simpletest.rc-jf4ql" in namespace "gc-4999"
    Mar  2 01:41:15.350: INFO: Deleting pod "simpletest.rc-jp24z" in namespace "gc-4999"
    Mar  2 01:41:15.414: INFO: Deleting pod "simpletest.rc-jzb68" in namespace "gc-4999"
    Mar  2 01:41:15.517: INFO: Deleting pod "simpletest.rc-k97r9" in namespace "gc-4999"
    Mar  2 01:41:15.552: INFO: Deleting pod "simpletest.rc-kwjlt" in namespace "gc-4999"
    Mar  2 01:41:15.609: INFO: Deleting pod "simpletest.rc-l9dt5" in namespace "gc-4999"
    Mar  2 01:41:15.651: INFO: Deleting pod "simpletest.rc-lcdpc" in namespace "gc-4999"
    Mar  2 01:41:15.685: INFO: Deleting pod "simpletest.rc-lmgr9" in namespace "gc-4999"
    Mar  2 01:41:15.741: INFO: Deleting pod "simpletest.rc-lphcd" in namespace "gc-4999"
    Mar  2 01:41:15.785: INFO: Deleting pod "simpletest.rc-lx8lj" in namespace "gc-4999"
    Mar  2 01:41:16.020: INFO: Deleting pod "simpletest.rc-lxqn9" in namespace "gc-4999"
    Mar  2 01:41:16.244: INFO: Deleting pod "simpletest.rc-m2p5k" in namespace "gc-4999"
    Mar  2 01:41:16.449: INFO: Deleting pod "simpletest.rc-mdkf6" in namespace "gc-4999"
    Mar  2 01:41:16.575: INFO: Deleting pod "simpletest.rc-mpqgz" in namespace "gc-4999"
    Mar  2 01:41:16.611: INFO: Deleting pod "simpletest.rc-mr8lc" in namespace "gc-4999"
    Mar  2 01:41:16.656: INFO: Deleting pod "simpletest.rc-mrzf6" in namespace "gc-4999"
    Mar  2 01:41:16.688: INFO: Deleting pod "simpletest.rc-mw9w8" in namespace "gc-4999"
    Mar  2 01:41:16.747: INFO: Deleting pod "simpletest.rc-nqjvv" in namespace "gc-4999"
    Mar  2 01:41:16.803: INFO: Deleting pod "simpletest.rc-nw6jv" in namespace "gc-4999"
    Mar  2 01:41:16.873: INFO: Deleting pod "simpletest.rc-pdv7p" in namespace "gc-4999"
    Mar  2 01:41:16.933: INFO: Deleting pod "simpletest.rc-pg2ch" in namespace "gc-4999"
    Mar  2 01:41:16.984: INFO: Deleting pod "simpletest.rc-pnblk" in namespace "gc-4999"
    Mar  2 01:41:17.055: INFO: Deleting pod "simpletest.rc-prcjl" in namespace "gc-4999"
    Mar  2 01:41:17.098: INFO: Deleting pod "simpletest.rc-q8tzs" in namespace "gc-4999"
    Mar  2 01:41:17.158: INFO: Deleting pod "simpletest.rc-qd8c4" in namespace "gc-4999"
    Mar  2 01:41:17.194: INFO: Deleting pod "simpletest.rc-qwjfl" in namespace "gc-4999"
    Mar  2 01:41:17.321: INFO: Deleting pod "simpletest.rc-qwxbf" in namespace "gc-4999"
    Mar  2 01:41:17.453: INFO: Deleting pod "simpletest.rc-r75zv" in namespace "gc-4999"
    Mar  2 01:41:17.489: INFO: Deleting pod "simpletest.rc-rtckz" in namespace "gc-4999"
    Mar  2 01:41:17.520: INFO: Deleting pod "simpletest.rc-s4czz" in namespace "gc-4999"
    Mar  2 01:41:17.559: INFO: Deleting pod "simpletest.rc-sgtmr" in namespace "gc-4999"
    Mar  2 01:41:17.590: INFO: Deleting pod "simpletest.rc-slc4n" in namespace "gc-4999"
    Mar  2 01:41:17.634: INFO: Deleting pod "simpletest.rc-sw92k" in namespace "gc-4999"
    Mar  2 01:41:17.693: INFO: Deleting pod "simpletest.rc-sxr8s" in namespace "gc-4999"
    Mar  2 01:41:17.779: INFO: Deleting pod "simpletest.rc-t7zfz" in namespace "gc-4999"
    Mar  2 01:41:17.842: INFO: Deleting pod "simpletest.rc-tpt7d" in namespace "gc-4999"
    Mar  2 01:41:17.901: INFO: Deleting pod "simpletest.rc-twjz8" in namespace "gc-4999"
    Mar  2 01:41:17.958: INFO: Deleting pod "simpletest.rc-v7mlz" in namespace "gc-4999"
    Mar  2 01:41:17.998: INFO: Deleting pod "simpletest.rc-v8x4z" in namespace "gc-4999"
    Mar  2 01:41:18.071: INFO: Deleting pod "simpletest.rc-vlxbd" in namespace "gc-4999"
    Mar  2 01:41:18.119: INFO: Deleting pod "simpletest.rc-vvvzx" in namespace "gc-4999"
    Mar  2 01:41:18.182: INFO: Deleting pod "simpletest.rc-wzr9x" in namespace "gc-4999"
    Mar  2 01:41:18.225: INFO: Deleting pod "simpletest.rc-x8nn7" in namespace "gc-4999"
    Mar  2 01:41:18.300: INFO: Deleting pod "simpletest.rc-x8zwt" in namespace "gc-4999"
    Mar  2 01:41:18.367: INFO: Deleting pod "simpletest.rc-xdwfp" in namespace "gc-4999"
    Mar  2 01:41:18.436: INFO: Deleting pod "simpletest.rc-xkzhp" in namespace "gc-4999"
    Mar  2 01:41:18.514: INFO: Deleting pod "simpletest.rc-xpqmv" in namespace "gc-4999"
    Mar  2 01:41:18.563: INFO: Deleting pod "simpletest.rc-xqtp6" in namespace "gc-4999"
    Mar  2 01:41:18.610: INFO: Deleting pod "simpletest.rc-z2wwh" in namespace "gc-4999"
    Mar  2 01:41:18.651: INFO: Deleting pod "simpletest.rc-z6ddn" in namespace "gc-4999"
    Mar  2 01:41:18.727: INFO: Deleting pod "simpletest.rc-zs4nq" in namespace "gc-4999"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Mar  2 01:41:18.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-4999" for this suite. 03/02/23 01:41:18.838
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:41:18.868
Mar  2 01:41:18.868: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename daemonsets 03/02/23 01:41:18.87
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:41:19.053
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:41:19.063
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
STEP: Creating simple DaemonSet "daemon-set" 03/02/23 01:41:19.275
STEP: Check that daemon pods launch on every node of the cluster. 03/02/23 01:41:19.295
Mar  2 01:41:19.336: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 01:41:19.336: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
Mar  2 01:41:20.394: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 01:41:20.394: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
Mar  2 01:41:21.378: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 01:41:21.378: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
Mar  2 01:41:22.390: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 01:41:22.390: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
Mar  2 01:41:23.413: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 01:41:23.413: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
Mar  2 01:41:24.383: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 01:41:24.383: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
Mar  2 01:41:25.377: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 01:41:25.377: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
Mar  2 01:41:26.434: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 01:41:26.434: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
Mar  2 01:41:27.424: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 01:41:27.424: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
Mar  2 01:41:28.553: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar  2 01:41:28.553: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
Mar  2 01:41:29.472: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar  2 01:41:29.472: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
Mar  2 01:41:30.412: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar  2 01:41:30.412: INFO: Node 10.132.92.188 is running 0 daemon pod, expected 1
Mar  2 01:41:31.371: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar  2 01:41:31.371: INFO: Node 10.132.92.188 is running 0 daemon pod, expected 1
Mar  2 01:41:32.374: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar  2 01:41:32.374: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 03/02/23 01:41:32.392
Mar  2 01:41:32.480: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar  2 01:41:32.480: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
Mar  2 01:41:33.524: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar  2 01:41:33.525: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
Mar  2 01:41:34.522: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar  2 01:41:34.522: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
Mar  2 01:41:35.523: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar  2 01:41:35.523: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
Mar  2 01:41:36.528: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar  2 01:41:36.528: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
Mar  2 01:41:37.519: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar  2 01:41:37.519: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 03/02/23 01:41:37.533
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3055, will wait for the garbage collector to delete the pods 03/02/23 01:41:37.533
Mar  2 01:41:37.625: INFO: Deleting DaemonSet.extensions daemon-set took: 26.820167ms
Mar  2 01:41:37.826: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.121348ms
Mar  2 01:41:40.840: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 01:41:40.840: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar  2 01:41:40.853: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"94240"},"items":null}

Mar  2 01:41:40.865: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"94241"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Mar  2 01:41:40.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3055" for this suite. 03/02/23 01:41:40.956
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","completed":114,"skipped":2181,"failed":0}
------------------------------
• [SLOW TEST] [22.113 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:41:18.868
    Mar  2 01:41:18.868: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename daemonsets 03/02/23 01:41:18.87
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:41:19.053
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:41:19.063
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:165
    STEP: Creating simple DaemonSet "daemon-set" 03/02/23 01:41:19.275
    STEP: Check that daemon pods launch on every node of the cluster. 03/02/23 01:41:19.295
    Mar  2 01:41:19.336: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 01:41:19.336: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
    Mar  2 01:41:20.394: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 01:41:20.394: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
    Mar  2 01:41:21.378: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 01:41:21.378: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
    Mar  2 01:41:22.390: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 01:41:22.390: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
    Mar  2 01:41:23.413: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 01:41:23.413: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
    Mar  2 01:41:24.383: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 01:41:24.383: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
    Mar  2 01:41:25.377: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 01:41:25.377: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
    Mar  2 01:41:26.434: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 01:41:26.434: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
    Mar  2 01:41:27.424: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 01:41:27.424: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
    Mar  2 01:41:28.553: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar  2 01:41:28.553: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
    Mar  2 01:41:29.472: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar  2 01:41:29.472: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
    Mar  2 01:41:30.412: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar  2 01:41:30.412: INFO: Node 10.132.92.188 is running 0 daemon pod, expected 1
    Mar  2 01:41:31.371: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar  2 01:41:31.371: INFO: Node 10.132.92.188 is running 0 daemon pod, expected 1
    Mar  2 01:41:32.374: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar  2 01:41:32.374: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 03/02/23 01:41:32.392
    Mar  2 01:41:32.480: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar  2 01:41:32.480: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
    Mar  2 01:41:33.524: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar  2 01:41:33.525: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
    Mar  2 01:41:34.522: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar  2 01:41:34.522: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
    Mar  2 01:41:35.523: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar  2 01:41:35.523: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
    Mar  2 01:41:36.528: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar  2 01:41:36.528: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
    Mar  2 01:41:37.519: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar  2 01:41:37.519: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 03/02/23 01:41:37.533
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3055, will wait for the garbage collector to delete the pods 03/02/23 01:41:37.533
    Mar  2 01:41:37.625: INFO: Deleting DaemonSet.extensions daemon-set took: 26.820167ms
    Mar  2 01:41:37.826: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.121348ms
    Mar  2 01:41:40.840: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 01:41:40.840: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar  2 01:41:40.853: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"94240"},"items":null}

    Mar  2 01:41:40.865: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"94241"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Mar  2 01:41:40.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-3055" for this suite. 03/02/23 01:41:40.956
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:41:40.983
Mar  2 01:41:40.984: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename emptydir 03/02/23 01:41:40.985
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:41:41.109
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:41:41.119
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
STEP: Creating a pod to test emptydir 0777 on node default medium 03/02/23 01:41:41.131
Mar  2 01:41:41.237: INFO: Waiting up to 5m0s for pod "pod-dc2be246-f2df-4cf5-b867-6279f38a53b5" in namespace "emptydir-4278" to be "Succeeded or Failed"
Mar  2 01:41:41.252: INFO: Pod "pod-dc2be246-f2df-4cf5-b867-6279f38a53b5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.258617ms
Mar  2 01:41:43.265: INFO: Pod "pod-dc2be246-f2df-4cf5-b867-6279f38a53b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027736736s
Mar  2 01:41:45.273: INFO: Pod "pod-dc2be246-f2df-4cf5-b867-6279f38a53b5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035366133s
Mar  2 01:41:47.268: INFO: Pod "pod-dc2be246-f2df-4cf5-b867-6279f38a53b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030288355s
STEP: Saw pod success 03/02/23 01:41:47.268
Mar  2 01:41:47.268: INFO: Pod "pod-dc2be246-f2df-4cf5-b867-6279f38a53b5" satisfied condition "Succeeded or Failed"
Mar  2 01:41:47.280: INFO: Trying to get logs from node 10.132.92.143 pod pod-dc2be246-f2df-4cf5-b867-6279f38a53b5 container test-container: <nil>
STEP: delete the pod 03/02/23 01:41:47.343
Mar  2 01:41:47.375: INFO: Waiting for pod pod-dc2be246-f2df-4cf5-b867-6279f38a53b5 to disappear
Mar  2 01:41:47.387: INFO: Pod pod-dc2be246-f2df-4cf5-b867-6279f38a53b5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  2 01:41:47.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4278" for this suite. 03/02/23 01:41:47.413
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":115,"skipped":2188,"failed":0}
------------------------------
• [SLOW TEST] [6.456 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:41:40.983
    Mar  2 01:41:40.984: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename emptydir 03/02/23 01:41:40.985
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:41:41.109
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:41:41.119
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:186
    STEP: Creating a pod to test emptydir 0777 on node default medium 03/02/23 01:41:41.131
    Mar  2 01:41:41.237: INFO: Waiting up to 5m0s for pod "pod-dc2be246-f2df-4cf5-b867-6279f38a53b5" in namespace "emptydir-4278" to be "Succeeded or Failed"
    Mar  2 01:41:41.252: INFO: Pod "pod-dc2be246-f2df-4cf5-b867-6279f38a53b5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.258617ms
    Mar  2 01:41:43.265: INFO: Pod "pod-dc2be246-f2df-4cf5-b867-6279f38a53b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027736736s
    Mar  2 01:41:45.273: INFO: Pod "pod-dc2be246-f2df-4cf5-b867-6279f38a53b5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035366133s
    Mar  2 01:41:47.268: INFO: Pod "pod-dc2be246-f2df-4cf5-b867-6279f38a53b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030288355s
    STEP: Saw pod success 03/02/23 01:41:47.268
    Mar  2 01:41:47.268: INFO: Pod "pod-dc2be246-f2df-4cf5-b867-6279f38a53b5" satisfied condition "Succeeded or Failed"
    Mar  2 01:41:47.280: INFO: Trying to get logs from node 10.132.92.143 pod pod-dc2be246-f2df-4cf5-b867-6279f38a53b5 container test-container: <nil>
    STEP: delete the pod 03/02/23 01:41:47.343
    Mar  2 01:41:47.375: INFO: Waiting for pod pod-dc2be246-f2df-4cf5-b867-6279f38a53b5 to disappear
    Mar  2 01:41:47.387: INFO: Pod pod-dc2be246-f2df-4cf5-b867-6279f38a53b5 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  2 01:41:47.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4278" for this suite. 03/02/23 01:41:47.413
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:41:47.441
Mar  2 01:41:47.442: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename replicaset 03/02/23 01:41:47.443
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:41:47.511
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:41:47.52
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 03/02/23 01:41:47.534
W0302 01:41:47.556459      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Mar  2 01:41:47.568: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar  2 01:41:52.581: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/02/23 01:41:52.581
STEP: getting scale subresource 03/02/23 01:41:52.581
STEP: updating a scale subresource 03/02/23 01:41:52.592
STEP: verifying the replicaset Spec.Replicas was modified 03/02/23 01:41:52.611
STEP: Patch a scale subresource 03/02/23 01:41:52.626
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Mar  2 01:41:52.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8348" for this suite. 03/02/23 01:41:52.709
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","completed":116,"skipped":2198,"failed":0}
------------------------------
• [SLOW TEST] [5.290 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:41:47.441
    Mar  2 01:41:47.442: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename replicaset 03/02/23 01:41:47.443
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:41:47.511
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:41:47.52
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 03/02/23 01:41:47.534
    W0302 01:41:47.556459      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Mar  2 01:41:47.568: INFO: Pod name sample-pod: Found 0 pods out of 1
    Mar  2 01:41:52.581: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/02/23 01:41:52.581
    STEP: getting scale subresource 03/02/23 01:41:52.581
    STEP: updating a scale subresource 03/02/23 01:41:52.592
    STEP: verifying the replicaset Spec.Replicas was modified 03/02/23 01:41:52.611
    STEP: Patch a scale subresource 03/02/23 01:41:52.626
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Mar  2 01:41:52.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-8348" for this suite. 03/02/23 01:41:52.709
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:41:52.736
Mar  2 01:41:52.736: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename cronjob 03/02/23 01:41:52.737
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:41:52.816
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:41:52.839
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 03/02/23 01:41:52.872
STEP: Ensuring a job is scheduled 03/02/23 01:41:52.9
STEP: Ensuring exactly one is scheduled 03/02/23 01:42:00.923
STEP: Ensuring exactly one running job exists by listing jobs explicitly 03/02/23 01:42:00.938
STEP: Ensuring no more jobs are scheduled 03/02/23 01:42:00.953
STEP: Removing cronjob 03/02/23 01:47:00.99
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Mar  2 01:47:01.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-1599" for this suite. 03/02/23 01:47:01.032
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","completed":117,"skipped":2225,"failed":0}
------------------------------
• [SLOW TEST] [308.321 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:41:52.736
    Mar  2 01:41:52.736: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename cronjob 03/02/23 01:41:52.737
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:41:52.816
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:41:52.839
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 03/02/23 01:41:52.872
    STEP: Ensuring a job is scheduled 03/02/23 01:41:52.9
    STEP: Ensuring exactly one is scheduled 03/02/23 01:42:00.923
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 03/02/23 01:42:00.938
    STEP: Ensuring no more jobs are scheduled 03/02/23 01:42:00.953
    STEP: Removing cronjob 03/02/23 01:47:00.99
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Mar  2 01:47:01.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-1599" for this suite. 03/02/23 01:47:01.032
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:47:01.058
Mar  2 01:47:01.058: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename services 03/02/23 01:47:01.059
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:47:01.128
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:47:01.144
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
STEP: creating service in namespace services-27 03/02/23 01:47:01.163
STEP: creating service affinity-clusterip in namespace services-27 03/02/23 01:47:01.163
STEP: creating replication controller affinity-clusterip in namespace services-27 03/02/23 01:47:01.218
I0302 01:47:01.256145      20 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-27, replica count: 3
I0302 01:47:04.308632      20 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 01:47:07.311863      20 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 01:47:07.339: INFO: Creating new exec pod
Mar  2 01:47:07.390: INFO: Waiting up to 5m0s for pod "execpod-affinitybkwpv" in namespace "services-27" to be "running"
Mar  2 01:47:07.405: INFO: Pod "execpod-affinitybkwpv": Phase="Pending", Reason="", readiness=false. Elapsed: 14.963698ms
Mar  2 01:47:09.427: INFO: Pod "execpod-affinitybkwpv": Phase="Running", Reason="", readiness=true. Elapsed: 2.036440444s
Mar  2 01:47:09.427: INFO: Pod "execpod-affinitybkwpv" satisfied condition "running"
Mar  2 01:47:10.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-27 exec execpod-affinitybkwpv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Mar  2 01:47:10.768: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Mar  2 01:47:10.768: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 01:47:10.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-27 exec execpod-affinitybkwpv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.138.17 80'
Mar  2 01:47:11.153: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.138.17 80\nConnection to 172.21.138.17 80 port [tcp/http] succeeded!\n"
Mar  2 01:47:11.153: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 01:47:11.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-27 exec execpod-affinitybkwpv -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.138.17:80/ ; done'
Mar  2 01:47:11.612: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.17:80/\n"
Mar  2 01:47:11.612: INFO: stdout: "\naffinity-clusterip-6xbd2\naffinity-clusterip-6xbd2\naffinity-clusterip-6xbd2\naffinity-clusterip-6xbd2\naffinity-clusterip-6xbd2\naffinity-clusterip-6xbd2\naffinity-clusterip-6xbd2\naffinity-clusterip-6xbd2\naffinity-clusterip-6xbd2\naffinity-clusterip-6xbd2\naffinity-clusterip-6xbd2\naffinity-clusterip-6xbd2\naffinity-clusterip-6xbd2\naffinity-clusterip-6xbd2\naffinity-clusterip-6xbd2\naffinity-clusterip-6xbd2"
Mar  2 01:47:11.612: INFO: Received response from host: affinity-clusterip-6xbd2
Mar  2 01:47:11.612: INFO: Received response from host: affinity-clusterip-6xbd2
Mar  2 01:47:11.612: INFO: Received response from host: affinity-clusterip-6xbd2
Mar  2 01:47:11.612: INFO: Received response from host: affinity-clusterip-6xbd2
Mar  2 01:47:11.612: INFO: Received response from host: affinity-clusterip-6xbd2
Mar  2 01:47:11.612: INFO: Received response from host: affinity-clusterip-6xbd2
Mar  2 01:47:11.612: INFO: Received response from host: affinity-clusterip-6xbd2
Mar  2 01:47:11.612: INFO: Received response from host: affinity-clusterip-6xbd2
Mar  2 01:47:11.612: INFO: Received response from host: affinity-clusterip-6xbd2
Mar  2 01:47:11.612: INFO: Received response from host: affinity-clusterip-6xbd2
Mar  2 01:47:11.612: INFO: Received response from host: affinity-clusterip-6xbd2
Mar  2 01:47:11.612: INFO: Received response from host: affinity-clusterip-6xbd2
Mar  2 01:47:11.612: INFO: Received response from host: affinity-clusterip-6xbd2
Mar  2 01:47:11.612: INFO: Received response from host: affinity-clusterip-6xbd2
Mar  2 01:47:11.612: INFO: Received response from host: affinity-clusterip-6xbd2
Mar  2 01:47:11.612: INFO: Received response from host: affinity-clusterip-6xbd2
Mar  2 01:47:11.612: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-27, will wait for the garbage collector to delete the pods 03/02/23 01:47:11.659
Mar  2 01:47:11.747: INFO: Deleting ReplicationController affinity-clusterip took: 19.957777ms
Mar  2 01:47:11.848: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.035ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  2 01:47:15.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-27" for this suite. 03/02/23 01:47:15.024
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","completed":118,"skipped":2228,"failed":0}
------------------------------
• [SLOW TEST] [13.991 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:47:01.058
    Mar  2 01:47:01.058: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename services 03/02/23 01:47:01.059
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:47:01.128
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:47:01.144
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2157
    STEP: creating service in namespace services-27 03/02/23 01:47:01.163
    STEP: creating service affinity-clusterip in namespace services-27 03/02/23 01:47:01.163
    STEP: creating replication controller affinity-clusterip in namespace services-27 03/02/23 01:47:01.218
    I0302 01:47:01.256145      20 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-27, replica count: 3
    I0302 01:47:04.308632      20 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0302 01:47:07.311863      20 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar  2 01:47:07.339: INFO: Creating new exec pod
    Mar  2 01:47:07.390: INFO: Waiting up to 5m0s for pod "execpod-affinitybkwpv" in namespace "services-27" to be "running"
    Mar  2 01:47:07.405: INFO: Pod "execpod-affinitybkwpv": Phase="Pending", Reason="", readiness=false. Elapsed: 14.963698ms
    Mar  2 01:47:09.427: INFO: Pod "execpod-affinitybkwpv": Phase="Running", Reason="", readiness=true. Elapsed: 2.036440444s
    Mar  2 01:47:09.427: INFO: Pod "execpod-affinitybkwpv" satisfied condition "running"
    Mar  2 01:47:10.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-27 exec execpod-affinitybkwpv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
    Mar  2 01:47:10.768: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Mar  2 01:47:10.768: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  2 01:47:10.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-27 exec execpod-affinitybkwpv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.138.17 80'
    Mar  2 01:47:11.153: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.138.17 80\nConnection to 172.21.138.17 80 port [tcp/http] succeeded!\n"
    Mar  2 01:47:11.153: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  2 01:47:11.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-27 exec execpod-affinitybkwpv -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.138.17:80/ ; done'
    Mar  2 01:47:11.612: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.138.17:80/\n"
    Mar  2 01:47:11.612: INFO: stdout: "\naffinity-clusterip-6xbd2\naffinity-clusterip-6xbd2\naffinity-clusterip-6xbd2\naffinity-clusterip-6xbd2\naffinity-clusterip-6xbd2\naffinity-clusterip-6xbd2\naffinity-clusterip-6xbd2\naffinity-clusterip-6xbd2\naffinity-clusterip-6xbd2\naffinity-clusterip-6xbd2\naffinity-clusterip-6xbd2\naffinity-clusterip-6xbd2\naffinity-clusterip-6xbd2\naffinity-clusterip-6xbd2\naffinity-clusterip-6xbd2\naffinity-clusterip-6xbd2"
    Mar  2 01:47:11.612: INFO: Received response from host: affinity-clusterip-6xbd2
    Mar  2 01:47:11.612: INFO: Received response from host: affinity-clusterip-6xbd2
    Mar  2 01:47:11.612: INFO: Received response from host: affinity-clusterip-6xbd2
    Mar  2 01:47:11.612: INFO: Received response from host: affinity-clusterip-6xbd2
    Mar  2 01:47:11.612: INFO: Received response from host: affinity-clusterip-6xbd2
    Mar  2 01:47:11.612: INFO: Received response from host: affinity-clusterip-6xbd2
    Mar  2 01:47:11.612: INFO: Received response from host: affinity-clusterip-6xbd2
    Mar  2 01:47:11.612: INFO: Received response from host: affinity-clusterip-6xbd2
    Mar  2 01:47:11.612: INFO: Received response from host: affinity-clusterip-6xbd2
    Mar  2 01:47:11.612: INFO: Received response from host: affinity-clusterip-6xbd2
    Mar  2 01:47:11.612: INFO: Received response from host: affinity-clusterip-6xbd2
    Mar  2 01:47:11.612: INFO: Received response from host: affinity-clusterip-6xbd2
    Mar  2 01:47:11.612: INFO: Received response from host: affinity-clusterip-6xbd2
    Mar  2 01:47:11.612: INFO: Received response from host: affinity-clusterip-6xbd2
    Mar  2 01:47:11.612: INFO: Received response from host: affinity-clusterip-6xbd2
    Mar  2 01:47:11.612: INFO: Received response from host: affinity-clusterip-6xbd2
    Mar  2 01:47:11.612: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-27, will wait for the garbage collector to delete the pods 03/02/23 01:47:11.659
    Mar  2 01:47:11.747: INFO: Deleting ReplicationController affinity-clusterip took: 19.957777ms
    Mar  2 01:47:11.848: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.035ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  2 01:47:15.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-27" for this suite. 03/02/23 01:47:15.024
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:47:15.05
Mar  2 01:47:15.050: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename prestop 03/02/23 01:47:15.052
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:47:15.127
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:47:15.14
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-2543 03/02/23 01:47:15.153
STEP: Waiting for pods to come up. 03/02/23 01:47:15.258
Mar  2 01:47:15.258: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-2543" to be "running"
Mar  2 01:47:15.277: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 19.37531ms
Mar  2 01:47:17.291: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032727368s
Mar  2 01:47:19.291: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 4.033389354s
Mar  2 01:47:19.291: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-2543 03/02/23 01:47:19.304
Mar  2 01:47:19.346: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-2543" to be "running"
Mar  2 01:47:19.383: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 37.058994ms
Mar  2 01:47:21.397: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051287678s
Mar  2 01:47:23.399: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 4.053279036s
Mar  2 01:47:23.399: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 03/02/23 01:47:23.4
Mar  2 01:47:28.455: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 03/02/23 01:47:28.455
[AfterEach] [sig-node] PreStop
  test/e2e/framework/framework.go:187
Mar  2 01:47:28.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-2543" for this suite. 03/02/23 01:47:28.508
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","completed":119,"skipped":2235,"failed":0}
------------------------------
• [SLOW TEST] [13.484 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:47:15.05
    Mar  2 01:47:15.050: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename prestop 03/02/23 01:47:15.052
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:47:15.127
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:47:15.14
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-2543 03/02/23 01:47:15.153
    STEP: Waiting for pods to come up. 03/02/23 01:47:15.258
    Mar  2 01:47:15.258: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-2543" to be "running"
    Mar  2 01:47:15.277: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 19.37531ms
    Mar  2 01:47:17.291: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032727368s
    Mar  2 01:47:19.291: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 4.033389354s
    Mar  2 01:47:19.291: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-2543 03/02/23 01:47:19.304
    Mar  2 01:47:19.346: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-2543" to be "running"
    Mar  2 01:47:19.383: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 37.058994ms
    Mar  2 01:47:21.397: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051287678s
    Mar  2 01:47:23.399: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 4.053279036s
    Mar  2 01:47:23.399: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 03/02/23 01:47:23.4
    Mar  2 01:47:28.455: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 03/02/23 01:47:28.455
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/framework.go:187
    Mar  2 01:47:28.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "prestop-2543" for this suite. 03/02/23 01:47:28.508
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:47:28.535
Mar  2 01:47:28.535: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename statefulset 03/02/23 01:47:28.536
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:47:28.606
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:47:28.628
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-4956 03/02/23 01:47:28.641
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
STEP: Initializing watcher for selector baz=blah,foo=bar 03/02/23 01:47:28.669
STEP: Creating stateful set ss in namespace statefulset-4956 03/02/23 01:47:28.687
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4956 03/02/23 01:47:28.719
Mar  2 01:47:28.729: INFO: Found 0 stateful pods, waiting for 1
Mar  2 01:47:38.744: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 03/02/23 01:47:38.745
Mar  2 01:47:38.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=statefulset-4956 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 01:47:39.221: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 01:47:39.221: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 01:47:39.221: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 01:47:39.248: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar  2 01:47:49.270: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 01:47:49.270: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 01:47:49.382: INFO: Verifying statefulset ss doesn't scale past 1 for another 10s
Mar  2 01:47:50.397: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.971983281s
Mar  2 01:47:51.409: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.957159813s
Mar  2 01:47:52.423: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.945246377s
Mar  2 01:47:53.435: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.931312577s
Mar  2 01:47:54.448: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.919454959s
Mar  2 01:47:55.464: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.905736457s
Mar  2 01:47:56.476: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.89074287s
Mar  2 01:47:57.491: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.877392804s
Mar  2 01:47:58.505: INFO: Verifying statefulset ss doesn't scale past 1 for another 862.54115ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4956 03/02/23 01:47:59.505
Mar  2 01:47:59.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=statefulset-4956 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 01:47:59.846: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  2 01:47:59.846: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 01:47:59.846: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 01:47:59.861: INFO: Found 1 stateful pods, waiting for 3
Mar  2 01:48:09.882: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 01:48:09.882: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 01:48:09.882: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 03/02/23 01:48:09.882
STEP: Scale down will halt with unhealthy stateful pod 03/02/23 01:48:09.882
Mar  2 01:48:09.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=statefulset-4956 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 01:48:10.264: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 01:48:10.264: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 01:48:10.264: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 01:48:10.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=statefulset-4956 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 01:48:10.641: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 01:48:10.641: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 01:48:10.641: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 01:48:10.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=statefulset-4956 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 01:48:11.116: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 01:48:11.116: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 01:48:11.116: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 01:48:11.116: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 01:48:11.127: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Mar  2 01:48:21.163: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 01:48:21.163: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 01:48:21.163: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 01:48:21.213: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999998503s
Mar  2 01:48:22.239: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.985693639s
Mar  2 01:48:23.257: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.95948321s
Mar  2 01:48:24.277: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.94133773s
Mar  2 01:48:25.310: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.922166183s
Mar  2 01:48:26.329: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.888417608s
Mar  2 01:48:27.347: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.869124728s
Mar  2 01:48:28.364: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.851452031s
Mar  2 01:48:29.380: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.835778696s
Mar  2 01:48:30.429: INFO: Verifying statefulset ss doesn't scale past 3 for another 819.518373ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4956 03/02/23 01:48:31.429
Mar  2 01:48:31.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=statefulset-4956 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 01:48:31.762: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  2 01:48:31.762: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 01:48:31.762: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 01:48:31.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=statefulset-4956 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 01:48:32.152: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  2 01:48:32.152: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 01:48:32.152: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 01:48:32.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=statefulset-4956 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 01:48:32.609: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  2 01:48:32.609: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 01:48:32.609: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 01:48:32.609: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 03/02/23 01:48:42.69
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar  2 01:48:42.690: INFO: Deleting all statefulset in ns statefulset-4956
Mar  2 01:48:42.703: INFO: Scaling statefulset ss to 0
Mar  2 01:48:42.749: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 01:48:42.762: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar  2 01:48:42.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4956" for this suite. 03/02/23 01:48:42.882
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","completed":120,"skipped":2245,"failed":0}
------------------------------
• [SLOW TEST] [74.373 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:585

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:47:28.535
    Mar  2 01:47:28.535: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename statefulset 03/02/23 01:47:28.536
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:47:28.606
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:47:28.628
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-4956 03/02/23 01:47:28.641
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:585
    STEP: Initializing watcher for selector baz=blah,foo=bar 03/02/23 01:47:28.669
    STEP: Creating stateful set ss in namespace statefulset-4956 03/02/23 01:47:28.687
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4956 03/02/23 01:47:28.719
    Mar  2 01:47:28.729: INFO: Found 0 stateful pods, waiting for 1
    Mar  2 01:47:38.744: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 03/02/23 01:47:38.745
    Mar  2 01:47:38.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=statefulset-4956 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar  2 01:47:39.221: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar  2 01:47:39.221: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar  2 01:47:39.221: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar  2 01:47:39.248: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Mar  2 01:47:49.270: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Mar  2 01:47:49.270: INFO: Waiting for statefulset status.replicas updated to 0
    Mar  2 01:47:49.382: INFO: Verifying statefulset ss doesn't scale past 1 for another 10s
    Mar  2 01:47:50.397: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.971983281s
    Mar  2 01:47:51.409: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.957159813s
    Mar  2 01:47:52.423: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.945246377s
    Mar  2 01:47:53.435: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.931312577s
    Mar  2 01:47:54.448: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.919454959s
    Mar  2 01:47:55.464: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.905736457s
    Mar  2 01:47:56.476: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.89074287s
    Mar  2 01:47:57.491: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.877392804s
    Mar  2 01:47:58.505: INFO: Verifying statefulset ss doesn't scale past 1 for another 862.54115ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4956 03/02/23 01:47:59.505
    Mar  2 01:47:59.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=statefulset-4956 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar  2 01:47:59.846: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar  2 01:47:59.846: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar  2 01:47:59.846: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar  2 01:47:59.861: INFO: Found 1 stateful pods, waiting for 3
    Mar  2 01:48:09.882: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar  2 01:48:09.882: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar  2 01:48:09.882: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 03/02/23 01:48:09.882
    STEP: Scale down will halt with unhealthy stateful pod 03/02/23 01:48:09.882
    Mar  2 01:48:09.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=statefulset-4956 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar  2 01:48:10.264: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar  2 01:48:10.264: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar  2 01:48:10.264: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar  2 01:48:10.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=statefulset-4956 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar  2 01:48:10.641: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar  2 01:48:10.641: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar  2 01:48:10.641: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar  2 01:48:10.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=statefulset-4956 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar  2 01:48:11.116: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar  2 01:48:11.116: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar  2 01:48:11.116: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar  2 01:48:11.116: INFO: Waiting for statefulset status.replicas updated to 0
    Mar  2 01:48:11.127: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Mar  2 01:48:21.163: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Mar  2 01:48:21.163: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Mar  2 01:48:21.163: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Mar  2 01:48:21.213: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999998503s
    Mar  2 01:48:22.239: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.985693639s
    Mar  2 01:48:23.257: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.95948321s
    Mar  2 01:48:24.277: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.94133773s
    Mar  2 01:48:25.310: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.922166183s
    Mar  2 01:48:26.329: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.888417608s
    Mar  2 01:48:27.347: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.869124728s
    Mar  2 01:48:28.364: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.851452031s
    Mar  2 01:48:29.380: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.835778696s
    Mar  2 01:48:30.429: INFO: Verifying statefulset ss doesn't scale past 3 for another 819.518373ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4956 03/02/23 01:48:31.429
    Mar  2 01:48:31.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=statefulset-4956 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar  2 01:48:31.762: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar  2 01:48:31.762: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar  2 01:48:31.762: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar  2 01:48:31.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=statefulset-4956 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar  2 01:48:32.152: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar  2 01:48:32.152: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar  2 01:48:32.152: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar  2 01:48:32.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=statefulset-4956 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar  2 01:48:32.609: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar  2 01:48:32.609: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar  2 01:48:32.609: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar  2 01:48:32.609: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 03/02/23 01:48:42.69
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar  2 01:48:42.690: INFO: Deleting all statefulset in ns statefulset-4956
    Mar  2 01:48:42.703: INFO: Scaling statefulset ss to 0
    Mar  2 01:48:42.749: INFO: Waiting for statefulset status.replicas updated to 0
    Mar  2 01:48:42.762: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar  2 01:48:42.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-4956" for this suite. 03/02/23 01:48:42.882
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:48:42.91
Mar  2 01:48:42.910: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename downward-api 03/02/23 01:48:42.911
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:48:42.989
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:48:43.003
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
STEP: Creating a pod to test downward api env vars 03/02/23 01:48:43.013
Mar  2 01:48:43.082: INFO: Waiting up to 5m0s for pod "downward-api-3efba969-c673-4cae-994b-df197d1e2b5f" in namespace "downward-api-9130" to be "Succeeded or Failed"
Mar  2 01:48:43.097: INFO: Pod "downward-api-3efba969-c673-4cae-994b-df197d1e2b5f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.21109ms
Mar  2 01:48:45.111: INFO: Pod "downward-api-3efba969-c673-4cae-994b-df197d1e2b5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029082647s
Mar  2 01:48:47.112: INFO: Pod "downward-api-3efba969-c673-4cae-994b-df197d1e2b5f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029624425s
STEP: Saw pod success 03/02/23 01:48:47.112
Mar  2 01:48:47.112: INFO: Pod "downward-api-3efba969-c673-4cae-994b-df197d1e2b5f" satisfied condition "Succeeded or Failed"
Mar  2 01:48:47.122: INFO: Trying to get logs from node 10.132.92.143 pod downward-api-3efba969-c673-4cae-994b-df197d1e2b5f container dapi-container: <nil>
STEP: delete the pod 03/02/23 01:48:47.22
Mar  2 01:48:47.254: INFO: Waiting for pod downward-api-3efba969-c673-4cae-994b-df197d1e2b5f to disappear
Mar  2 01:48:47.265: INFO: Pod downward-api-3efba969-c673-4cae-994b-df197d1e2b5f no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Mar  2 01:48:47.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9130" for this suite. 03/02/23 01:48:47.286
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","completed":121,"skipped":2251,"failed":0}
------------------------------
• [4.400 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:48:42.91
    Mar  2 01:48:42.910: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename downward-api 03/02/23 01:48:42.911
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:48:42.989
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:48:43.003
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:216
    STEP: Creating a pod to test downward api env vars 03/02/23 01:48:43.013
    Mar  2 01:48:43.082: INFO: Waiting up to 5m0s for pod "downward-api-3efba969-c673-4cae-994b-df197d1e2b5f" in namespace "downward-api-9130" to be "Succeeded or Failed"
    Mar  2 01:48:43.097: INFO: Pod "downward-api-3efba969-c673-4cae-994b-df197d1e2b5f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.21109ms
    Mar  2 01:48:45.111: INFO: Pod "downward-api-3efba969-c673-4cae-994b-df197d1e2b5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029082647s
    Mar  2 01:48:47.112: INFO: Pod "downward-api-3efba969-c673-4cae-994b-df197d1e2b5f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029624425s
    STEP: Saw pod success 03/02/23 01:48:47.112
    Mar  2 01:48:47.112: INFO: Pod "downward-api-3efba969-c673-4cae-994b-df197d1e2b5f" satisfied condition "Succeeded or Failed"
    Mar  2 01:48:47.122: INFO: Trying to get logs from node 10.132.92.143 pod downward-api-3efba969-c673-4cae-994b-df197d1e2b5f container dapi-container: <nil>
    STEP: delete the pod 03/02/23 01:48:47.22
    Mar  2 01:48:47.254: INFO: Waiting for pod downward-api-3efba969-c673-4cae-994b-df197d1e2b5f to disappear
    Mar  2 01:48:47.265: INFO: Pod downward-api-3efba969-c673-4cae-994b-df197d1e2b5f no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Mar  2 01:48:47.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9130" for this suite. 03/02/23 01:48:47.286
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:48:47.311
Mar  2 01:48:47.311: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename resourcequota 03/02/23 01:48:47.313
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:48:47.374
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:48:47.388
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
STEP: Creating a ResourceQuota 03/02/23 01:48:47.398
STEP: Getting a ResourceQuota 03/02/23 01:48:47.416
STEP: Updating a ResourceQuota 03/02/23 01:48:47.428
STEP: Verifying a ResourceQuota was modified 03/02/23 01:48:47.452
STEP: Deleting a ResourceQuota 03/02/23 01:48:47.477
STEP: Verifying the deleted ResourceQuota 03/02/23 01:48:47.5
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar  2 01:48:47.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1459" for this suite. 03/02/23 01:48:47.564
{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","completed":122,"skipped":2281,"failed":0}
------------------------------
• [0.279 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:48:47.311
    Mar  2 01:48:47.311: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename resourcequota 03/02/23 01:48:47.313
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:48:47.374
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:48:47.388
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:874
    STEP: Creating a ResourceQuota 03/02/23 01:48:47.398
    STEP: Getting a ResourceQuota 03/02/23 01:48:47.416
    STEP: Updating a ResourceQuota 03/02/23 01:48:47.428
    STEP: Verifying a ResourceQuota was modified 03/02/23 01:48:47.452
    STEP: Deleting a ResourceQuota 03/02/23 01:48:47.477
    STEP: Verifying the deleted ResourceQuota 03/02/23 01:48:47.5
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar  2 01:48:47.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-1459" for this suite. 03/02/23 01:48:47.564
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:48:47.596
Mar  2 01:48:47.596: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename namespaces 03/02/23 01:48:47.598
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:48:47.698
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:48:47.721
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
STEP: Creating a test namespace 03/02/23 01:48:47.733
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:48:47.813
STEP: Creating a service in the namespace 03/02/23 01:48:47.825
STEP: Deleting the namespace 03/02/23 01:48:47.939
STEP: Waiting for the namespace to be removed. 03/02/23 01:48:48.016
STEP: Recreating the namespace 03/02/23 01:48:56.033
STEP: Verifying there is no service in the namespace 03/02/23 01:48:56.091
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Mar  2 01:48:56.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9576" for this suite. 03/02/23 01:48:56.151
STEP: Destroying namespace "nsdeletetest-6073" for this suite. 03/02/23 01:48:56.178
Mar  2 01:48:56.195: INFO: Namespace nsdeletetest-6073 was already deleted
STEP: Destroying namespace "nsdeletetest-821" for this suite. 03/02/23 01:48:56.195
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","completed":123,"skipped":2331,"failed":0}
------------------------------
• [SLOW TEST] [8.634 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:48:47.596
    Mar  2 01:48:47.596: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename namespaces 03/02/23 01:48:47.598
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:48:47.698
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:48:47.721
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:250
    STEP: Creating a test namespace 03/02/23 01:48:47.733
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:48:47.813
    STEP: Creating a service in the namespace 03/02/23 01:48:47.825
    STEP: Deleting the namespace 03/02/23 01:48:47.939
    STEP: Waiting for the namespace to be removed. 03/02/23 01:48:48.016
    STEP: Recreating the namespace 03/02/23 01:48:56.033
    STEP: Verifying there is no service in the namespace 03/02/23 01:48:56.091
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Mar  2 01:48:56.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-9576" for this suite. 03/02/23 01:48:56.151
    STEP: Destroying namespace "nsdeletetest-6073" for this suite. 03/02/23 01:48:56.178
    Mar  2 01:48:56.195: INFO: Namespace nsdeletetest-6073 was already deleted
    STEP: Destroying namespace "nsdeletetest-821" for this suite. 03/02/23 01:48:56.195
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:48:56.232
Mar  2 01:48:56.232: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename projected 03/02/23 01:48:56.234
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:48:56.292
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:48:56.303
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
STEP: Creating a pod to test downward API volume plugin 03/02/23 01:48:56.317
Mar  2 01:48:56.398: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e1e55148-0d29-4173-9f8d-32c987c8a7f3" in namespace "projected-6798" to be "Succeeded or Failed"
Mar  2 01:48:56.448: INFO: Pod "downwardapi-volume-e1e55148-0d29-4173-9f8d-32c987c8a7f3": Phase="Pending", Reason="", readiness=false. Elapsed: 49.860929ms
Mar  2 01:48:58.461: INFO: Pod "downwardapi-volume-e1e55148-0d29-4173-9f8d-32c987c8a7f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.063497013s
Mar  2 01:49:00.465: INFO: Pod "downwardapi-volume-e1e55148-0d29-4173-9f8d-32c987c8a7f3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.066822889s
Mar  2 01:49:02.468: INFO: Pod "downwardapi-volume-e1e55148-0d29-4173-9f8d-32c987c8a7f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.070684509s
STEP: Saw pod success 03/02/23 01:49:02.469
Mar  2 01:49:02.469: INFO: Pod "downwardapi-volume-e1e55148-0d29-4173-9f8d-32c987c8a7f3" satisfied condition "Succeeded or Failed"
Mar  2 01:49:02.489: INFO: Trying to get logs from node 10.132.92.143 pod downwardapi-volume-e1e55148-0d29-4173-9f8d-32c987c8a7f3 container client-container: <nil>
STEP: delete the pod 03/02/23 01:49:02.549
Mar  2 01:49:02.594: INFO: Waiting for pod downwardapi-volume-e1e55148-0d29-4173-9f8d-32c987c8a7f3 to disappear
Mar  2 01:49:02.621: INFO: Pod downwardapi-volume-e1e55148-0d29-4173-9f8d-32c987c8a7f3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar  2 01:49:02.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6798" for this suite. 03/02/23 01:49:02.667
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","completed":124,"skipped":2338,"failed":0}
------------------------------
• [SLOW TEST] [6.468 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:48:56.232
    Mar  2 01:48:56.232: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename projected 03/02/23 01:48:56.234
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:48:56.292
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:48:56.303
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:192
    STEP: Creating a pod to test downward API volume plugin 03/02/23 01:48:56.317
    Mar  2 01:48:56.398: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e1e55148-0d29-4173-9f8d-32c987c8a7f3" in namespace "projected-6798" to be "Succeeded or Failed"
    Mar  2 01:48:56.448: INFO: Pod "downwardapi-volume-e1e55148-0d29-4173-9f8d-32c987c8a7f3": Phase="Pending", Reason="", readiness=false. Elapsed: 49.860929ms
    Mar  2 01:48:58.461: INFO: Pod "downwardapi-volume-e1e55148-0d29-4173-9f8d-32c987c8a7f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.063497013s
    Mar  2 01:49:00.465: INFO: Pod "downwardapi-volume-e1e55148-0d29-4173-9f8d-32c987c8a7f3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.066822889s
    Mar  2 01:49:02.468: INFO: Pod "downwardapi-volume-e1e55148-0d29-4173-9f8d-32c987c8a7f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.070684509s
    STEP: Saw pod success 03/02/23 01:49:02.469
    Mar  2 01:49:02.469: INFO: Pod "downwardapi-volume-e1e55148-0d29-4173-9f8d-32c987c8a7f3" satisfied condition "Succeeded or Failed"
    Mar  2 01:49:02.489: INFO: Trying to get logs from node 10.132.92.143 pod downwardapi-volume-e1e55148-0d29-4173-9f8d-32c987c8a7f3 container client-container: <nil>
    STEP: delete the pod 03/02/23 01:49:02.549
    Mar  2 01:49:02.594: INFO: Waiting for pod downwardapi-volume-e1e55148-0d29-4173-9f8d-32c987c8a7f3 to disappear
    Mar  2 01:49:02.621: INFO: Pod downwardapi-volume-e1e55148-0d29-4173-9f8d-32c987c8a7f3 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar  2 01:49:02.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6798" for this suite. 03/02/23 01:49:02.667
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:49:02.717
Mar  2 01:49:02.717: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename kubectl 03/02/23 01:49:02.719
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:49:02.811
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:49:02.852
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
STEP: starting the proxy server 03/02/23 01:49:02.869
Mar  2 01:49:02.869: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-2488 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 03/02/23 01:49:03.012
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  2 01:49:03.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2488" for this suite. 03/02/23 01:49:03.091
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","completed":125,"skipped":2398,"failed":0}
------------------------------
• [0.421 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1785

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:49:02.717
    Mar  2 01:49:02.717: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename kubectl 03/02/23 01:49:02.719
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:49:02.811
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:49:02.852
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1785
    STEP: starting the proxy server 03/02/23 01:49:02.869
    Mar  2 01:49:02.869: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-2488 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 03/02/23 01:49:03.012
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  2 01:49:03.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2488" for this suite. 03/02/23 01:49:03.091
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:49:03.139
Mar  2 01:49:03.139: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename projected 03/02/23 01:49:03.14
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:49:03.216
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:49:03.23
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
STEP: Creating configMap with name projected-configmap-test-volume-3eda86f9-4383-4e4d-b85d-e7bdedc30a07 03/02/23 01:49:03.257
STEP: Creating a pod to test consume configMaps 03/02/23 01:49:03.28
Mar  2 01:49:03.428: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f3af1867-5ed6-4da2-9ac6-f7c000d0ec34" in namespace "projected-1630" to be "Succeeded or Failed"
Mar  2 01:49:03.443: INFO: Pod "pod-projected-configmaps-f3af1867-5ed6-4da2-9ac6-f7c000d0ec34": Phase="Pending", Reason="", readiness=false. Elapsed: 15.601251ms
Mar  2 01:49:05.468: INFO: Pod "pod-projected-configmaps-f3af1867-5ed6-4da2-9ac6-f7c000d0ec34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040472639s
Mar  2 01:49:07.461: INFO: Pod "pod-projected-configmaps-f3af1867-5ed6-4da2-9ac6-f7c000d0ec34": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033540046s
Mar  2 01:49:09.456: INFO: Pod "pod-projected-configmaps-f3af1867-5ed6-4da2-9ac6-f7c000d0ec34": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02868438s
STEP: Saw pod success 03/02/23 01:49:09.456
Mar  2 01:49:09.457: INFO: Pod "pod-projected-configmaps-f3af1867-5ed6-4da2-9ac6-f7c000d0ec34" satisfied condition "Succeeded or Failed"
Mar  2 01:49:09.468: INFO: Trying to get logs from node 10.132.92.143 pod pod-projected-configmaps-f3af1867-5ed6-4da2-9ac6-f7c000d0ec34 container agnhost-container: <nil>
STEP: delete the pod 03/02/23 01:49:09.51
Mar  2 01:49:09.576: INFO: Waiting for pod pod-projected-configmaps-f3af1867-5ed6-4da2-9ac6-f7c000d0ec34 to disappear
Mar  2 01:49:09.586: INFO: Pod pod-projected-configmaps-f3af1867-5ed6-4da2-9ac6-f7c000d0ec34 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar  2 01:49:09.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1630" for this suite. 03/02/23 01:49:09.619
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":126,"skipped":2418,"failed":0}
------------------------------
• [SLOW TEST] [6.535 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:49:03.139
    Mar  2 01:49:03.139: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename projected 03/02/23 01:49:03.14
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:49:03.216
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:49:03.23
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:46
    STEP: Creating configMap with name projected-configmap-test-volume-3eda86f9-4383-4e4d-b85d-e7bdedc30a07 03/02/23 01:49:03.257
    STEP: Creating a pod to test consume configMaps 03/02/23 01:49:03.28
    Mar  2 01:49:03.428: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f3af1867-5ed6-4da2-9ac6-f7c000d0ec34" in namespace "projected-1630" to be "Succeeded or Failed"
    Mar  2 01:49:03.443: INFO: Pod "pod-projected-configmaps-f3af1867-5ed6-4da2-9ac6-f7c000d0ec34": Phase="Pending", Reason="", readiness=false. Elapsed: 15.601251ms
    Mar  2 01:49:05.468: INFO: Pod "pod-projected-configmaps-f3af1867-5ed6-4da2-9ac6-f7c000d0ec34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040472639s
    Mar  2 01:49:07.461: INFO: Pod "pod-projected-configmaps-f3af1867-5ed6-4da2-9ac6-f7c000d0ec34": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033540046s
    Mar  2 01:49:09.456: INFO: Pod "pod-projected-configmaps-f3af1867-5ed6-4da2-9ac6-f7c000d0ec34": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02868438s
    STEP: Saw pod success 03/02/23 01:49:09.456
    Mar  2 01:49:09.457: INFO: Pod "pod-projected-configmaps-f3af1867-5ed6-4da2-9ac6-f7c000d0ec34" satisfied condition "Succeeded or Failed"
    Mar  2 01:49:09.468: INFO: Trying to get logs from node 10.132.92.143 pod pod-projected-configmaps-f3af1867-5ed6-4da2-9ac6-f7c000d0ec34 container agnhost-container: <nil>
    STEP: delete the pod 03/02/23 01:49:09.51
    Mar  2 01:49:09.576: INFO: Waiting for pod pod-projected-configmaps-f3af1867-5ed6-4da2-9ac6-f7c000d0ec34 to disappear
    Mar  2 01:49:09.586: INFO: Pod pod-projected-configmaps-f3af1867-5ed6-4da2-9ac6-f7c000d0ec34 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar  2 01:49:09.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1630" for this suite. 03/02/23 01:49:09.619
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:49:09.676
Mar  2 01:49:09.676: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename init-container 03/02/23 01:49:09.691
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:49:09.811
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:49:09.824
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
STEP: creating the pod 03/02/23 01:49:09.856
Mar  2 01:49:09.857: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Mar  2 01:49:15.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3975" for this suite. 03/02/23 01:49:15.18
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","completed":127,"skipped":2444,"failed":0}
------------------------------
• [SLOW TEST] [5.530 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:49:09.676
    Mar  2 01:49:09.676: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename init-container 03/02/23 01:49:09.691
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:49:09.811
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:49:09.824
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:457
    STEP: creating the pod 03/02/23 01:49:09.856
    Mar  2 01:49:09.857: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Mar  2 01:49:15.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-3975" for this suite. 03/02/23 01:49:15.18
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:49:15.209
Mar  2 01:49:15.209: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename crd-publish-openapi 03/02/23 01:49:15.21
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:49:15.306
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:49:15.318
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
Mar  2 01:49:15.335: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 03/02/23 01:49:30.074
Mar  2 01:49:30.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-639 --namespace=crd-publish-openapi-639 create -f -'
Mar  2 01:49:32.359: INFO: stderr: ""
Mar  2 01:49:32.359: INFO: stdout: "e2e-test-crd-publish-openapi-2361-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar  2 01:49:32.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-639 --namespace=crd-publish-openapi-639 delete e2e-test-crd-publish-openapi-2361-crds test-foo'
Mar  2 01:49:32.565: INFO: stderr: ""
Mar  2 01:49:32.565: INFO: stdout: "e2e-test-crd-publish-openapi-2361-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Mar  2 01:49:32.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-639 --namespace=crd-publish-openapi-639 apply -f -'
Mar  2 01:49:34.481: INFO: stderr: ""
Mar  2 01:49:34.481: INFO: stdout: "e2e-test-crd-publish-openapi-2361-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar  2 01:49:34.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-639 --namespace=crd-publish-openapi-639 delete e2e-test-crd-publish-openapi-2361-crds test-foo'
Mar  2 01:49:34.660: INFO: stderr: ""
Mar  2 01:49:34.660: INFO: stdout: "e2e-test-crd-publish-openapi-2361-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 03/02/23 01:49:34.66
Mar  2 01:49:34.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-639 --namespace=crd-publish-openapi-639 create -f -'
Mar  2 01:49:37.914: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 03/02/23 01:49:37.914
Mar  2 01:49:37.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-639 --namespace=crd-publish-openapi-639 create -f -'
Mar  2 01:49:38.557: INFO: rc: 1
Mar  2 01:49:38.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-639 --namespace=crd-publish-openapi-639 apply -f -'
Mar  2 01:49:39.219: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 03/02/23 01:49:39.219
Mar  2 01:49:39.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-639 --namespace=crd-publish-openapi-639 create -f -'
Mar  2 01:49:39.824: INFO: rc: 1
Mar  2 01:49:39.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-639 --namespace=crd-publish-openapi-639 apply -f -'
Mar  2 01:49:40.450: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 03/02/23 01:49:40.45
Mar  2 01:49:40.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-639 explain e2e-test-crd-publish-openapi-2361-crds'
Mar  2 01:49:41.099: INFO: stderr: ""
Mar  2 01:49:41.099: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2361-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 03/02/23 01:49:41.1
Mar  2 01:49:41.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-639 explain e2e-test-crd-publish-openapi-2361-crds.metadata'
Mar  2 01:49:41.767: INFO: stderr: ""
Mar  2 01:49:41.767: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2361-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Mar  2 01:49:41.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-639 explain e2e-test-crd-publish-openapi-2361-crds.spec'
Mar  2 01:49:42.411: INFO: stderr: ""
Mar  2 01:49:42.411: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2361-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Mar  2 01:49:42.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-639 explain e2e-test-crd-publish-openapi-2361-crds.spec.bars'
Mar  2 01:49:43.045: INFO: stderr: ""
Mar  2 01:49:43.045: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2361-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 03/02/23 01:49:43.045
Mar  2 01:49:43.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-639 explain e2e-test-crd-publish-openapi-2361-crds.spec.bars2'
Mar  2 01:49:43.598: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 01:49:58.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-639" for this suite. 03/02/23 01:49:58.222
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","completed":128,"skipped":2451,"failed":0}
------------------------------
• [SLOW TEST] [43.036 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:49:15.209
    Mar  2 01:49:15.209: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename crd-publish-openapi 03/02/23 01:49:15.21
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:49:15.306
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:49:15.318
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:68
    Mar  2 01:49:15.335: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 03/02/23 01:49:30.074
    Mar  2 01:49:30.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-639 --namespace=crd-publish-openapi-639 create -f -'
    Mar  2 01:49:32.359: INFO: stderr: ""
    Mar  2 01:49:32.359: INFO: stdout: "e2e-test-crd-publish-openapi-2361-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Mar  2 01:49:32.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-639 --namespace=crd-publish-openapi-639 delete e2e-test-crd-publish-openapi-2361-crds test-foo'
    Mar  2 01:49:32.565: INFO: stderr: ""
    Mar  2 01:49:32.565: INFO: stdout: "e2e-test-crd-publish-openapi-2361-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Mar  2 01:49:32.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-639 --namespace=crd-publish-openapi-639 apply -f -'
    Mar  2 01:49:34.481: INFO: stderr: ""
    Mar  2 01:49:34.481: INFO: stdout: "e2e-test-crd-publish-openapi-2361-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Mar  2 01:49:34.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-639 --namespace=crd-publish-openapi-639 delete e2e-test-crd-publish-openapi-2361-crds test-foo'
    Mar  2 01:49:34.660: INFO: stderr: ""
    Mar  2 01:49:34.660: INFO: stdout: "e2e-test-crd-publish-openapi-2361-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 03/02/23 01:49:34.66
    Mar  2 01:49:34.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-639 --namespace=crd-publish-openapi-639 create -f -'
    Mar  2 01:49:37.914: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 03/02/23 01:49:37.914
    Mar  2 01:49:37.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-639 --namespace=crd-publish-openapi-639 create -f -'
    Mar  2 01:49:38.557: INFO: rc: 1
    Mar  2 01:49:38.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-639 --namespace=crd-publish-openapi-639 apply -f -'
    Mar  2 01:49:39.219: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 03/02/23 01:49:39.219
    Mar  2 01:49:39.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-639 --namespace=crd-publish-openapi-639 create -f -'
    Mar  2 01:49:39.824: INFO: rc: 1
    Mar  2 01:49:39.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-639 --namespace=crd-publish-openapi-639 apply -f -'
    Mar  2 01:49:40.450: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 03/02/23 01:49:40.45
    Mar  2 01:49:40.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-639 explain e2e-test-crd-publish-openapi-2361-crds'
    Mar  2 01:49:41.099: INFO: stderr: ""
    Mar  2 01:49:41.099: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2361-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 03/02/23 01:49:41.1
    Mar  2 01:49:41.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-639 explain e2e-test-crd-publish-openapi-2361-crds.metadata'
    Mar  2 01:49:41.767: INFO: stderr: ""
    Mar  2 01:49:41.767: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2361-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Mar  2 01:49:41.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-639 explain e2e-test-crd-publish-openapi-2361-crds.spec'
    Mar  2 01:49:42.411: INFO: stderr: ""
    Mar  2 01:49:42.411: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2361-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Mar  2 01:49:42.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-639 explain e2e-test-crd-publish-openapi-2361-crds.spec.bars'
    Mar  2 01:49:43.045: INFO: stderr: ""
    Mar  2 01:49:43.045: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2361-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 03/02/23 01:49:43.045
    Mar  2 01:49:43.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=crd-publish-openapi-639 explain e2e-test-crd-publish-openapi-2361-crds.spec.bars2'
    Mar  2 01:49:43.598: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 01:49:58.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-639" for this suite. 03/02/23 01:49:58.222
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:49:58.252
Mar  2 01:49:58.253: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename subpath 03/02/23 01:49:58.254
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:49:58.31
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:49:58.323
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/02/23 01:49:58.342
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-9k9j 03/02/23 01:49:58.425
STEP: Creating a pod to test atomic-volume-subpath 03/02/23 01:49:58.425
Mar  2 01:49:58.509: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-9k9j" in namespace "subpath-5233" to be "Succeeded or Failed"
Mar  2 01:49:58.529: INFO: Pod "pod-subpath-test-configmap-9k9j": Phase="Pending", Reason="", readiness=false. Elapsed: 19.404451ms
Mar  2 01:50:00.546: INFO: Pod "pod-subpath-test-configmap-9k9j": Phase="Running", Reason="", readiness=true. Elapsed: 2.036430534s
Mar  2 01:50:02.545: INFO: Pod "pod-subpath-test-configmap-9k9j": Phase="Running", Reason="", readiness=true. Elapsed: 4.035403497s
Mar  2 01:50:04.545: INFO: Pod "pod-subpath-test-configmap-9k9j": Phase="Running", Reason="", readiness=true. Elapsed: 6.035428528s
Mar  2 01:50:06.564: INFO: Pod "pod-subpath-test-configmap-9k9j": Phase="Running", Reason="", readiness=true. Elapsed: 8.054178617s
Mar  2 01:50:08.548: INFO: Pod "pod-subpath-test-configmap-9k9j": Phase="Running", Reason="", readiness=true. Elapsed: 10.038639198s
Mar  2 01:50:10.546: INFO: Pod "pod-subpath-test-configmap-9k9j": Phase="Running", Reason="", readiness=true. Elapsed: 12.036581101s
Mar  2 01:50:12.545: INFO: Pod "pod-subpath-test-configmap-9k9j": Phase="Running", Reason="", readiness=true. Elapsed: 14.03515645s
Mar  2 01:50:14.545: INFO: Pod "pod-subpath-test-configmap-9k9j": Phase="Running", Reason="", readiness=true. Elapsed: 16.035374836s
Mar  2 01:50:16.547: INFO: Pod "pod-subpath-test-configmap-9k9j": Phase="Running", Reason="", readiness=true. Elapsed: 18.037369097s
Mar  2 01:50:18.578: INFO: Pod "pod-subpath-test-configmap-9k9j": Phase="Running", Reason="", readiness=true. Elapsed: 20.069027771s
Mar  2 01:50:20.568: INFO: Pod "pod-subpath-test-configmap-9k9j": Phase="Running", Reason="", readiness=false. Elapsed: 22.058360421s
Mar  2 01:50:22.578: INFO: Pod "pod-subpath-test-configmap-9k9j": Phase="Running", Reason="", readiness=false. Elapsed: 24.068231269s
Mar  2 01:50:24.570: INFO: Pod "pod-subpath-test-configmap-9k9j": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.060923892s
STEP: Saw pod success 03/02/23 01:50:24.57
Mar  2 01:50:24.571: INFO: Pod "pod-subpath-test-configmap-9k9j" satisfied condition "Succeeded or Failed"
Mar  2 01:50:24.584: INFO: Trying to get logs from node 10.132.92.143 pod pod-subpath-test-configmap-9k9j container test-container-subpath-configmap-9k9j: <nil>
STEP: delete the pod 03/02/23 01:50:24.649
Mar  2 01:50:24.695: INFO: Waiting for pod pod-subpath-test-configmap-9k9j to disappear
Mar  2 01:50:24.709: INFO: Pod pod-subpath-test-configmap-9k9j no longer exists
STEP: Deleting pod pod-subpath-test-configmap-9k9j 03/02/23 01:50:24.709
Mar  2 01:50:24.709: INFO: Deleting pod "pod-subpath-test-configmap-9k9j" in namespace "subpath-5233"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Mar  2 01:50:24.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5233" for this suite. 03/02/23 01:50:24.746
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]","completed":129,"skipped":2481,"failed":0}
------------------------------
• [SLOW TEST] [26.520 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:49:58.252
    Mar  2 01:49:58.253: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename subpath 03/02/23 01:49:58.254
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:49:58.31
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:49:58.323
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/02/23 01:49:58.342
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-9k9j 03/02/23 01:49:58.425
    STEP: Creating a pod to test atomic-volume-subpath 03/02/23 01:49:58.425
    Mar  2 01:49:58.509: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-9k9j" in namespace "subpath-5233" to be "Succeeded or Failed"
    Mar  2 01:49:58.529: INFO: Pod "pod-subpath-test-configmap-9k9j": Phase="Pending", Reason="", readiness=false. Elapsed: 19.404451ms
    Mar  2 01:50:00.546: INFO: Pod "pod-subpath-test-configmap-9k9j": Phase="Running", Reason="", readiness=true. Elapsed: 2.036430534s
    Mar  2 01:50:02.545: INFO: Pod "pod-subpath-test-configmap-9k9j": Phase="Running", Reason="", readiness=true. Elapsed: 4.035403497s
    Mar  2 01:50:04.545: INFO: Pod "pod-subpath-test-configmap-9k9j": Phase="Running", Reason="", readiness=true. Elapsed: 6.035428528s
    Mar  2 01:50:06.564: INFO: Pod "pod-subpath-test-configmap-9k9j": Phase="Running", Reason="", readiness=true. Elapsed: 8.054178617s
    Mar  2 01:50:08.548: INFO: Pod "pod-subpath-test-configmap-9k9j": Phase="Running", Reason="", readiness=true. Elapsed: 10.038639198s
    Mar  2 01:50:10.546: INFO: Pod "pod-subpath-test-configmap-9k9j": Phase="Running", Reason="", readiness=true. Elapsed: 12.036581101s
    Mar  2 01:50:12.545: INFO: Pod "pod-subpath-test-configmap-9k9j": Phase="Running", Reason="", readiness=true. Elapsed: 14.03515645s
    Mar  2 01:50:14.545: INFO: Pod "pod-subpath-test-configmap-9k9j": Phase="Running", Reason="", readiness=true. Elapsed: 16.035374836s
    Mar  2 01:50:16.547: INFO: Pod "pod-subpath-test-configmap-9k9j": Phase="Running", Reason="", readiness=true. Elapsed: 18.037369097s
    Mar  2 01:50:18.578: INFO: Pod "pod-subpath-test-configmap-9k9j": Phase="Running", Reason="", readiness=true. Elapsed: 20.069027771s
    Mar  2 01:50:20.568: INFO: Pod "pod-subpath-test-configmap-9k9j": Phase="Running", Reason="", readiness=false. Elapsed: 22.058360421s
    Mar  2 01:50:22.578: INFO: Pod "pod-subpath-test-configmap-9k9j": Phase="Running", Reason="", readiness=false. Elapsed: 24.068231269s
    Mar  2 01:50:24.570: INFO: Pod "pod-subpath-test-configmap-9k9j": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.060923892s
    STEP: Saw pod success 03/02/23 01:50:24.57
    Mar  2 01:50:24.571: INFO: Pod "pod-subpath-test-configmap-9k9j" satisfied condition "Succeeded or Failed"
    Mar  2 01:50:24.584: INFO: Trying to get logs from node 10.132.92.143 pod pod-subpath-test-configmap-9k9j container test-container-subpath-configmap-9k9j: <nil>
    STEP: delete the pod 03/02/23 01:50:24.649
    Mar  2 01:50:24.695: INFO: Waiting for pod pod-subpath-test-configmap-9k9j to disappear
    Mar  2 01:50:24.709: INFO: Pod pod-subpath-test-configmap-9k9j no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-9k9j 03/02/23 01:50:24.709
    Mar  2 01:50:24.709: INFO: Deleting pod "pod-subpath-test-configmap-9k9j" in namespace "subpath-5233"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Mar  2 01:50:24.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-5233" for this suite. 03/02/23 01:50:24.746
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:50:24.775
Mar  2 01:50:24.775: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename sysctl 03/02/23 01:50:24.777
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:50:24.833
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:50:24.847
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 03/02/23 01:50:24.861
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Mar  2 01:50:24.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-7639" for this suite. 03/02/23 01:50:24.946
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":130,"skipped":2515,"failed":0}
------------------------------
• [0.196 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:50:24.775
    Mar  2 01:50:24.775: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename sysctl 03/02/23 01:50:24.777
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:50:24.833
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:50:24.847
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 03/02/23 01:50:24.861
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Mar  2 01:50:24.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-7639" for this suite. 03/02/23 01:50:24.946
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:50:24.975
Mar  2 01:50:24.976: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename projected 03/02/23 01:50:24.977
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:50:25.049
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:50:25.065
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
STEP: Creating configMap with name projected-configmap-test-volume-6a9e6530-4264-4916-b6cb-d0eeddac29b1 03/02/23 01:50:25.083
STEP: Creating a pod to test consume configMaps 03/02/23 01:50:25.104
Mar  2 01:50:25.238: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f886362d-504e-446c-b536-e009f0df5a4e" in namespace "projected-882" to be "Succeeded or Failed"
Mar  2 01:50:25.282: INFO: Pod "pod-projected-configmaps-f886362d-504e-446c-b536-e009f0df5a4e": Phase="Pending", Reason="", readiness=false. Elapsed: 43.252452ms
Mar  2 01:50:27.298: INFO: Pod "pod-projected-configmaps-f886362d-504e-446c-b536-e009f0df5a4e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059433128s
Mar  2 01:50:29.297: INFO: Pod "pod-projected-configmaps-f886362d-504e-446c-b536-e009f0df5a4e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.058485972s
Mar  2 01:50:31.298: INFO: Pod "pod-projected-configmaps-f886362d-504e-446c-b536-e009f0df5a4e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.059228512s
STEP: Saw pod success 03/02/23 01:50:31.298
Mar  2 01:50:31.298: INFO: Pod "pod-projected-configmaps-f886362d-504e-446c-b536-e009f0df5a4e" satisfied condition "Succeeded or Failed"
Mar  2 01:50:31.345: INFO: Trying to get logs from node 10.132.92.143 pod pod-projected-configmaps-f886362d-504e-446c-b536-e009f0df5a4e container agnhost-container: <nil>
STEP: delete the pod 03/02/23 01:50:31.386
Mar  2 01:50:31.428: INFO: Waiting for pod pod-projected-configmaps-f886362d-504e-446c-b536-e009f0df5a4e to disappear
Mar  2 01:50:31.440: INFO: Pod pod-projected-configmaps-f886362d-504e-446c-b536-e009f0df5a4e no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar  2 01:50:31.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-882" for this suite. 03/02/23 01:50:31.467
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":131,"skipped":2566,"failed":0}
------------------------------
• [SLOW TEST] [6.514 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:50:24.975
    Mar  2 01:50:24.976: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename projected 03/02/23 01:50:24.977
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:50:25.049
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:50:25.065
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:56
    STEP: Creating configMap with name projected-configmap-test-volume-6a9e6530-4264-4916-b6cb-d0eeddac29b1 03/02/23 01:50:25.083
    STEP: Creating a pod to test consume configMaps 03/02/23 01:50:25.104
    Mar  2 01:50:25.238: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f886362d-504e-446c-b536-e009f0df5a4e" in namespace "projected-882" to be "Succeeded or Failed"
    Mar  2 01:50:25.282: INFO: Pod "pod-projected-configmaps-f886362d-504e-446c-b536-e009f0df5a4e": Phase="Pending", Reason="", readiness=false. Elapsed: 43.252452ms
    Mar  2 01:50:27.298: INFO: Pod "pod-projected-configmaps-f886362d-504e-446c-b536-e009f0df5a4e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059433128s
    Mar  2 01:50:29.297: INFO: Pod "pod-projected-configmaps-f886362d-504e-446c-b536-e009f0df5a4e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.058485972s
    Mar  2 01:50:31.298: INFO: Pod "pod-projected-configmaps-f886362d-504e-446c-b536-e009f0df5a4e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.059228512s
    STEP: Saw pod success 03/02/23 01:50:31.298
    Mar  2 01:50:31.298: INFO: Pod "pod-projected-configmaps-f886362d-504e-446c-b536-e009f0df5a4e" satisfied condition "Succeeded or Failed"
    Mar  2 01:50:31.345: INFO: Trying to get logs from node 10.132.92.143 pod pod-projected-configmaps-f886362d-504e-446c-b536-e009f0df5a4e container agnhost-container: <nil>
    STEP: delete the pod 03/02/23 01:50:31.386
    Mar  2 01:50:31.428: INFO: Waiting for pod pod-projected-configmaps-f886362d-504e-446c-b536-e009f0df5a4e to disappear
    Mar  2 01:50:31.440: INFO: Pod pod-projected-configmaps-f886362d-504e-446c-b536-e009f0df5a4e no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar  2 01:50:31.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-882" for this suite. 03/02/23 01:50:31.467
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:50:31.493
Mar  2 01:50:31.493: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename dns 03/02/23 01:50:31.494
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:50:31.587
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:50:31.599
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 03/02/23 01:50:31.611
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7586.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7586.svc.cluster.local; sleep 1; done
 03/02/23 01:50:31.646
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7586.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7586.svc.cluster.local; sleep 1; done
 03/02/23 01:50:31.646
STEP: creating a pod to probe DNS 03/02/23 01:50:31.646
STEP: submitting the pod to kubernetes 03/02/23 01:50:31.646
Mar  2 01:50:31.736: INFO: Waiting up to 15m0s for pod "dns-test-f1d0e734-e186-494d-bc8b-88439670b79f" in namespace "dns-7586" to be "running"
Mar  2 01:50:31.759: INFO: Pod "dns-test-f1d0e734-e186-494d-bc8b-88439670b79f": Phase="Pending", Reason="", readiness=false. Elapsed: 22.646801ms
Mar  2 01:50:33.775: INFO: Pod "dns-test-f1d0e734-e186-494d-bc8b-88439670b79f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039098515s
Mar  2 01:50:35.782: INFO: Pod "dns-test-f1d0e734-e186-494d-bc8b-88439670b79f": Phase="Running", Reason="", readiness=true. Elapsed: 4.046042226s
Mar  2 01:50:35.782: INFO: Pod "dns-test-f1d0e734-e186-494d-bc8b-88439670b79f" satisfied condition "running"
STEP: retrieving the pod 03/02/23 01:50:35.782
STEP: looking for the results for each expected name from probers 03/02/23 01:50:35.797
Mar  2 01:50:35.855: INFO: DNS probes using dns-test-f1d0e734-e186-494d-bc8b-88439670b79f succeeded

STEP: deleting the pod 03/02/23 01:50:35.855
STEP: changing the externalName to bar.example.com 03/02/23 01:50:35.89
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7586.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7586.svc.cluster.local; sleep 1; done
 03/02/23 01:50:35.922
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7586.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7586.svc.cluster.local; sleep 1; done
 03/02/23 01:50:35.922
STEP: creating a second pod to probe DNS 03/02/23 01:50:35.922
STEP: submitting the pod to kubernetes 03/02/23 01:50:35.923
Mar  2 01:50:35.967: INFO: Waiting up to 15m0s for pod "dns-test-2dfadd7a-5b50-405c-a4a9-455c16da98d8" in namespace "dns-7586" to be "running"
Mar  2 01:50:35.978: INFO: Pod "dns-test-2dfadd7a-5b50-405c-a4a9-455c16da98d8": Phase="Pending", Reason="", readiness=false. Elapsed: 11.673077ms
Mar  2 01:50:37.996: INFO: Pod "dns-test-2dfadd7a-5b50-405c-a4a9-455c16da98d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029033462s
Mar  2 01:50:39.997: INFO: Pod "dns-test-2dfadd7a-5b50-405c-a4a9-455c16da98d8": Phase="Running", Reason="", readiness=true. Elapsed: 4.030555372s
Mar  2 01:50:39.997: INFO: Pod "dns-test-2dfadd7a-5b50-405c-a4a9-455c16da98d8" satisfied condition "running"
STEP: retrieving the pod 03/02/23 01:50:39.997
STEP: looking for the results for each expected name from probers 03/02/23 01:50:40.01
Mar  2 01:50:40.064: INFO: DNS probes using dns-test-2dfadd7a-5b50-405c-a4a9-455c16da98d8 succeeded

STEP: deleting the pod 03/02/23 01:50:40.064
STEP: changing the service to type=ClusterIP 03/02/23 01:50:40.11
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7586.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-7586.svc.cluster.local; sleep 1; done
 03/02/23 01:50:40.162
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7586.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-7586.svc.cluster.local; sleep 1; done
 03/02/23 01:50:40.163
STEP: creating a third pod to probe DNS 03/02/23 01:50:40.163
STEP: submitting the pod to kubernetes 03/02/23 01:50:40.176
Mar  2 01:50:40.223: INFO: Waiting up to 15m0s for pod "dns-test-093ec163-2bc9-4164-a055-61b74e2f6375" in namespace "dns-7586" to be "running"
Mar  2 01:50:40.237: INFO: Pod "dns-test-093ec163-2bc9-4164-a055-61b74e2f6375": Phase="Pending", Reason="", readiness=false. Elapsed: 13.668826ms
Mar  2 01:50:42.252: INFO: Pod "dns-test-093ec163-2bc9-4164-a055-61b74e2f6375": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028820851s
Mar  2 01:50:44.262: INFO: Pod "dns-test-093ec163-2bc9-4164-a055-61b74e2f6375": Phase="Running", Reason="", readiness=true. Elapsed: 4.03848892s
Mar  2 01:50:44.262: INFO: Pod "dns-test-093ec163-2bc9-4164-a055-61b74e2f6375" satisfied condition "running"
STEP: retrieving the pod 03/02/23 01:50:44.262
STEP: looking for the results for each expected name from probers 03/02/23 01:50:44.278
Mar  2 01:50:44.328: INFO: DNS probes using dns-test-093ec163-2bc9-4164-a055-61b74e2f6375 succeeded

STEP: deleting the pod 03/02/23 01:50:44.328
STEP: deleting the test externalName service 03/02/23 01:50:44.37
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar  2 01:50:44.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7586" for this suite. 03/02/23 01:50:44.471
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","completed":132,"skipped":2593,"failed":0}
------------------------------
• [SLOW TEST] [13.029 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:50:31.493
    Mar  2 01:50:31.493: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename dns 03/02/23 01:50:31.494
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:50:31.587
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:50:31.599
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 03/02/23 01:50:31.611
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7586.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7586.svc.cluster.local; sleep 1; done
     03/02/23 01:50:31.646
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7586.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7586.svc.cluster.local; sleep 1; done
     03/02/23 01:50:31.646
    STEP: creating a pod to probe DNS 03/02/23 01:50:31.646
    STEP: submitting the pod to kubernetes 03/02/23 01:50:31.646
    Mar  2 01:50:31.736: INFO: Waiting up to 15m0s for pod "dns-test-f1d0e734-e186-494d-bc8b-88439670b79f" in namespace "dns-7586" to be "running"
    Mar  2 01:50:31.759: INFO: Pod "dns-test-f1d0e734-e186-494d-bc8b-88439670b79f": Phase="Pending", Reason="", readiness=false. Elapsed: 22.646801ms
    Mar  2 01:50:33.775: INFO: Pod "dns-test-f1d0e734-e186-494d-bc8b-88439670b79f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039098515s
    Mar  2 01:50:35.782: INFO: Pod "dns-test-f1d0e734-e186-494d-bc8b-88439670b79f": Phase="Running", Reason="", readiness=true. Elapsed: 4.046042226s
    Mar  2 01:50:35.782: INFO: Pod "dns-test-f1d0e734-e186-494d-bc8b-88439670b79f" satisfied condition "running"
    STEP: retrieving the pod 03/02/23 01:50:35.782
    STEP: looking for the results for each expected name from probers 03/02/23 01:50:35.797
    Mar  2 01:50:35.855: INFO: DNS probes using dns-test-f1d0e734-e186-494d-bc8b-88439670b79f succeeded

    STEP: deleting the pod 03/02/23 01:50:35.855
    STEP: changing the externalName to bar.example.com 03/02/23 01:50:35.89
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7586.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7586.svc.cluster.local; sleep 1; done
     03/02/23 01:50:35.922
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7586.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7586.svc.cluster.local; sleep 1; done
     03/02/23 01:50:35.922
    STEP: creating a second pod to probe DNS 03/02/23 01:50:35.922
    STEP: submitting the pod to kubernetes 03/02/23 01:50:35.923
    Mar  2 01:50:35.967: INFO: Waiting up to 15m0s for pod "dns-test-2dfadd7a-5b50-405c-a4a9-455c16da98d8" in namespace "dns-7586" to be "running"
    Mar  2 01:50:35.978: INFO: Pod "dns-test-2dfadd7a-5b50-405c-a4a9-455c16da98d8": Phase="Pending", Reason="", readiness=false. Elapsed: 11.673077ms
    Mar  2 01:50:37.996: INFO: Pod "dns-test-2dfadd7a-5b50-405c-a4a9-455c16da98d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029033462s
    Mar  2 01:50:39.997: INFO: Pod "dns-test-2dfadd7a-5b50-405c-a4a9-455c16da98d8": Phase="Running", Reason="", readiness=true. Elapsed: 4.030555372s
    Mar  2 01:50:39.997: INFO: Pod "dns-test-2dfadd7a-5b50-405c-a4a9-455c16da98d8" satisfied condition "running"
    STEP: retrieving the pod 03/02/23 01:50:39.997
    STEP: looking for the results for each expected name from probers 03/02/23 01:50:40.01
    Mar  2 01:50:40.064: INFO: DNS probes using dns-test-2dfadd7a-5b50-405c-a4a9-455c16da98d8 succeeded

    STEP: deleting the pod 03/02/23 01:50:40.064
    STEP: changing the service to type=ClusterIP 03/02/23 01:50:40.11
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7586.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-7586.svc.cluster.local; sleep 1; done
     03/02/23 01:50:40.162
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7586.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-7586.svc.cluster.local; sleep 1; done
     03/02/23 01:50:40.163
    STEP: creating a third pod to probe DNS 03/02/23 01:50:40.163
    STEP: submitting the pod to kubernetes 03/02/23 01:50:40.176
    Mar  2 01:50:40.223: INFO: Waiting up to 15m0s for pod "dns-test-093ec163-2bc9-4164-a055-61b74e2f6375" in namespace "dns-7586" to be "running"
    Mar  2 01:50:40.237: INFO: Pod "dns-test-093ec163-2bc9-4164-a055-61b74e2f6375": Phase="Pending", Reason="", readiness=false. Elapsed: 13.668826ms
    Mar  2 01:50:42.252: INFO: Pod "dns-test-093ec163-2bc9-4164-a055-61b74e2f6375": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028820851s
    Mar  2 01:50:44.262: INFO: Pod "dns-test-093ec163-2bc9-4164-a055-61b74e2f6375": Phase="Running", Reason="", readiness=true. Elapsed: 4.03848892s
    Mar  2 01:50:44.262: INFO: Pod "dns-test-093ec163-2bc9-4164-a055-61b74e2f6375" satisfied condition "running"
    STEP: retrieving the pod 03/02/23 01:50:44.262
    STEP: looking for the results for each expected name from probers 03/02/23 01:50:44.278
    Mar  2 01:50:44.328: INFO: DNS probes using dns-test-093ec163-2bc9-4164-a055-61b74e2f6375 succeeded

    STEP: deleting the pod 03/02/23 01:50:44.328
    STEP: deleting the test externalName service 03/02/23 01:50:44.37
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar  2 01:50:44.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-7586" for this suite. 03/02/23 01:50:44.471
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:50:44.526
Mar  2 01:50:44.526: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename emptydir 03/02/23 01:50:44.528
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:50:44.634
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:50:44.659
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
STEP: Creating a pod to test emptydir 0644 on tmpfs 03/02/23 01:50:44.671
Mar  2 01:50:44.729: INFO: Waiting up to 5m0s for pod "pod-ade1c148-04e2-4c81-bc8e-67aa50111369" in namespace "emptydir-8825" to be "Succeeded or Failed"
Mar  2 01:50:44.751: INFO: Pod "pod-ade1c148-04e2-4c81-bc8e-67aa50111369": Phase="Pending", Reason="", readiness=false. Elapsed: 22.578704ms
Mar  2 01:50:46.765: INFO: Pod "pod-ade1c148-04e2-4c81-bc8e-67aa50111369": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036637248s
Mar  2 01:50:48.806: INFO: Pod "pod-ade1c148-04e2-4c81-bc8e-67aa50111369": Phase="Pending", Reason="", readiness=false. Elapsed: 4.077165946s
Mar  2 01:50:50.793: INFO: Pod "pod-ade1c148-04e2-4c81-bc8e-67aa50111369": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.063929513s
STEP: Saw pod success 03/02/23 01:50:50.794
Mar  2 01:50:50.794: INFO: Pod "pod-ade1c148-04e2-4c81-bc8e-67aa50111369" satisfied condition "Succeeded or Failed"
Mar  2 01:50:50.811: INFO: Trying to get logs from node 10.132.92.143 pod pod-ade1c148-04e2-4c81-bc8e-67aa50111369 container test-container: <nil>
STEP: delete the pod 03/02/23 01:50:50.875
Mar  2 01:50:50.933: INFO: Waiting for pod pod-ade1c148-04e2-4c81-bc8e-67aa50111369 to disappear
Mar  2 01:50:50.947: INFO: Pod pod-ade1c148-04e2-4c81-bc8e-67aa50111369 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  2 01:50:50.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8825" for this suite. 03/02/23 01:50:50.969
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":133,"skipped":2605,"failed":0}
------------------------------
• [SLOW TEST] [6.474 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:50:44.526
    Mar  2 01:50:44.526: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename emptydir 03/02/23 01:50:44.528
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:50:44.634
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:50:44.659
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:96
    STEP: Creating a pod to test emptydir 0644 on tmpfs 03/02/23 01:50:44.671
    Mar  2 01:50:44.729: INFO: Waiting up to 5m0s for pod "pod-ade1c148-04e2-4c81-bc8e-67aa50111369" in namespace "emptydir-8825" to be "Succeeded or Failed"
    Mar  2 01:50:44.751: INFO: Pod "pod-ade1c148-04e2-4c81-bc8e-67aa50111369": Phase="Pending", Reason="", readiness=false. Elapsed: 22.578704ms
    Mar  2 01:50:46.765: INFO: Pod "pod-ade1c148-04e2-4c81-bc8e-67aa50111369": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036637248s
    Mar  2 01:50:48.806: INFO: Pod "pod-ade1c148-04e2-4c81-bc8e-67aa50111369": Phase="Pending", Reason="", readiness=false. Elapsed: 4.077165946s
    Mar  2 01:50:50.793: INFO: Pod "pod-ade1c148-04e2-4c81-bc8e-67aa50111369": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.063929513s
    STEP: Saw pod success 03/02/23 01:50:50.794
    Mar  2 01:50:50.794: INFO: Pod "pod-ade1c148-04e2-4c81-bc8e-67aa50111369" satisfied condition "Succeeded or Failed"
    Mar  2 01:50:50.811: INFO: Trying to get logs from node 10.132.92.143 pod pod-ade1c148-04e2-4c81-bc8e-67aa50111369 container test-container: <nil>
    STEP: delete the pod 03/02/23 01:50:50.875
    Mar  2 01:50:50.933: INFO: Waiting for pod pod-ade1c148-04e2-4c81-bc8e-67aa50111369 to disappear
    Mar  2 01:50:50.947: INFO: Pod pod-ade1c148-04e2-4c81-bc8e-67aa50111369 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  2 01:50:50.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8825" for this suite. 03/02/23 01:50:50.969
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:50:51.002
Mar  2 01:50:51.003: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename services 03/02/23 01:50:51.004
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:50:51.095
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:50:51.117
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
STEP: creating service in namespace services-5856 03/02/23 01:50:51.141
Mar  2 01:50:51.238: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-5856" to be "running and ready"
Mar  2 01:50:51.277: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 38.224568ms
Mar  2 01:50:51.277: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:50:53.294: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.055378734s
Mar  2 01:50:53.294: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Mar  2 01:50:53.294: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Mar  2 01:50:53.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-5856 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Mar  2 01:50:53.998: INFO: rc: 7
Mar  2 01:50:54.036: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Mar  2 01:50:54.050: INFO: Pod kube-proxy-mode-detector no longer exists
Mar  2 01:50:54.050: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-5856 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
Command stdout:

stderr:
+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
command terminated with exit code 7

error:
exit status 7
STEP: creating service affinity-clusterip-timeout in namespace services-5856 03/02/23 01:50:54.05
STEP: creating replication controller affinity-clusterip-timeout in namespace services-5856 03/02/23 01:50:54.087
I0302 01:50:54.110696      20 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-5856, replica count: 3
I0302 01:50:57.163212      20 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 01:50:57.189: INFO: Creating new exec pod
Mar  2 01:50:57.234: INFO: Waiting up to 5m0s for pod "execpod-affinityr7kc9" in namespace "services-5856" to be "running"
Mar  2 01:50:57.249: INFO: Pod "execpod-affinityr7kc9": Phase="Pending", Reason="", readiness=false. Elapsed: 15.451911ms
Mar  2 01:50:59.264: INFO: Pod "execpod-affinityr7kc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030487328s
Mar  2 01:51:01.264: INFO: Pod "execpod-affinityr7kc9": Phase="Running", Reason="", readiness=true. Elapsed: 4.030716529s
Mar  2 01:51:01.264: INFO: Pod "execpod-affinityr7kc9" satisfied condition "running"
Mar  2 01:51:02.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-5856 exec execpod-affinityr7kc9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Mar  2 01:51:02.949: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Mar  2 01:51:02.949: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 01:51:02.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-5856 exec execpod-affinityr7kc9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.189.91 80'
Mar  2 01:51:03.408: INFO: stderr: "+ + nc -v -t -w 2 172.21.189.91 80\necho hostName\nConnection to 172.21.189.91 80 port [tcp/http] succeeded!\n"
Mar  2 01:51:03.408: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 01:51:03.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-5856 exec execpod-affinityr7kc9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.189.91:80/ ; done'
Mar  2 01:51:03.838: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.189.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.189.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.189.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.189.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.189.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.189.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.189.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.189.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.189.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.189.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.189.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.189.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.189.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.189.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.189.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.189.91:80/\n"
Mar  2 01:51:03.838: INFO: stdout: "\naffinity-clusterip-timeout-h46vk\naffinity-clusterip-timeout-h46vk\naffinity-clusterip-timeout-h46vk\naffinity-clusterip-timeout-h46vk\naffinity-clusterip-timeout-h46vk\naffinity-clusterip-timeout-h46vk\naffinity-clusterip-timeout-h46vk\naffinity-clusterip-timeout-h46vk\naffinity-clusterip-timeout-h46vk\naffinity-clusterip-timeout-h46vk\naffinity-clusterip-timeout-h46vk\naffinity-clusterip-timeout-h46vk\naffinity-clusterip-timeout-h46vk\naffinity-clusterip-timeout-h46vk\naffinity-clusterip-timeout-h46vk\naffinity-clusterip-timeout-h46vk"
Mar  2 01:51:03.838: INFO: Received response from host: affinity-clusterip-timeout-h46vk
Mar  2 01:51:03.838: INFO: Received response from host: affinity-clusterip-timeout-h46vk
Mar  2 01:51:03.838: INFO: Received response from host: affinity-clusterip-timeout-h46vk
Mar  2 01:51:03.838: INFO: Received response from host: affinity-clusterip-timeout-h46vk
Mar  2 01:51:03.838: INFO: Received response from host: affinity-clusterip-timeout-h46vk
Mar  2 01:51:03.838: INFO: Received response from host: affinity-clusterip-timeout-h46vk
Mar  2 01:51:03.838: INFO: Received response from host: affinity-clusterip-timeout-h46vk
Mar  2 01:51:03.838: INFO: Received response from host: affinity-clusterip-timeout-h46vk
Mar  2 01:51:03.838: INFO: Received response from host: affinity-clusterip-timeout-h46vk
Mar  2 01:51:03.838: INFO: Received response from host: affinity-clusterip-timeout-h46vk
Mar  2 01:51:03.838: INFO: Received response from host: affinity-clusterip-timeout-h46vk
Mar  2 01:51:03.838: INFO: Received response from host: affinity-clusterip-timeout-h46vk
Mar  2 01:51:03.838: INFO: Received response from host: affinity-clusterip-timeout-h46vk
Mar  2 01:51:03.838: INFO: Received response from host: affinity-clusterip-timeout-h46vk
Mar  2 01:51:03.838: INFO: Received response from host: affinity-clusterip-timeout-h46vk
Mar  2 01:51:03.838: INFO: Received response from host: affinity-clusterip-timeout-h46vk
Mar  2 01:51:03.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-5856 exec execpod-affinityr7kc9 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.189.91:80/'
Mar  2 01:51:04.245: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.189.91:80/\n"
Mar  2 01:51:04.246: INFO: stdout: "affinity-clusterip-timeout-h46vk"
Mar  2 01:51:24.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-5856 exec execpod-affinityr7kc9 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.189.91:80/'
Mar  2 01:51:24.611: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.189.91:80/\n"
Mar  2 01:51:24.611: INFO: stdout: "affinity-clusterip-timeout-h46vk"
Mar  2 01:51:44.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-5856 exec execpod-affinityr7kc9 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.189.91:80/'
Mar  2 01:51:45.045: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.189.91:80/\n"
Mar  2 01:51:45.045: INFO: stdout: "affinity-clusterip-timeout-rrdhc"
Mar  2 01:51:45.045: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-5856, will wait for the garbage collector to delete the pods 03/02/23 01:51:45.152
Mar  2 01:51:45.277: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 48.815355ms
Mar  2 01:51:45.489: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 204.798199ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  2 01:51:48.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5856" for this suite. 03/02/23 01:51:48.83
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","completed":134,"skipped":2612,"failed":0}
------------------------------
• [SLOW TEST] [57.862 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:50:51.002
    Mar  2 01:50:51.003: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename services 03/02/23 01:50:51.004
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:50:51.095
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:50:51.117
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2173
    STEP: creating service in namespace services-5856 03/02/23 01:50:51.141
    Mar  2 01:50:51.238: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-5856" to be "running and ready"
    Mar  2 01:50:51.277: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 38.224568ms
    Mar  2 01:50:51.277: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 01:50:53.294: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.055378734s
    Mar  2 01:50:53.294: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Mar  2 01:50:53.294: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Mar  2 01:50:53.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-5856 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Mar  2 01:50:53.998: INFO: rc: 7
    Mar  2 01:50:54.036: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Mar  2 01:50:54.050: INFO: Pod kube-proxy-mode-detector no longer exists
    Mar  2 01:50:54.050: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-5856 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
    Command stdout:

    stderr:
    + curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
    command terminated with exit code 7

    error:
    exit status 7
    STEP: creating service affinity-clusterip-timeout in namespace services-5856 03/02/23 01:50:54.05
    STEP: creating replication controller affinity-clusterip-timeout in namespace services-5856 03/02/23 01:50:54.087
    I0302 01:50:54.110696      20 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-5856, replica count: 3
    I0302 01:50:57.163212      20 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar  2 01:50:57.189: INFO: Creating new exec pod
    Mar  2 01:50:57.234: INFO: Waiting up to 5m0s for pod "execpod-affinityr7kc9" in namespace "services-5856" to be "running"
    Mar  2 01:50:57.249: INFO: Pod "execpod-affinityr7kc9": Phase="Pending", Reason="", readiness=false. Elapsed: 15.451911ms
    Mar  2 01:50:59.264: INFO: Pod "execpod-affinityr7kc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030487328s
    Mar  2 01:51:01.264: INFO: Pod "execpod-affinityr7kc9": Phase="Running", Reason="", readiness=true. Elapsed: 4.030716529s
    Mar  2 01:51:01.264: INFO: Pod "execpod-affinityr7kc9" satisfied condition "running"
    Mar  2 01:51:02.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-5856 exec execpod-affinityr7kc9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
    Mar  2 01:51:02.949: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
    Mar  2 01:51:02.949: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  2 01:51:02.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-5856 exec execpod-affinityr7kc9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.189.91 80'
    Mar  2 01:51:03.408: INFO: stderr: "+ + nc -v -t -w 2 172.21.189.91 80\necho hostName\nConnection to 172.21.189.91 80 port [tcp/http] succeeded!\n"
    Mar  2 01:51:03.408: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  2 01:51:03.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-5856 exec execpod-affinityr7kc9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.189.91:80/ ; done'
    Mar  2 01:51:03.838: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.189.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.189.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.189.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.189.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.189.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.189.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.189.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.189.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.189.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.189.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.189.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.189.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.189.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.189.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.189.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.189.91:80/\n"
    Mar  2 01:51:03.838: INFO: stdout: "\naffinity-clusterip-timeout-h46vk\naffinity-clusterip-timeout-h46vk\naffinity-clusterip-timeout-h46vk\naffinity-clusterip-timeout-h46vk\naffinity-clusterip-timeout-h46vk\naffinity-clusterip-timeout-h46vk\naffinity-clusterip-timeout-h46vk\naffinity-clusterip-timeout-h46vk\naffinity-clusterip-timeout-h46vk\naffinity-clusterip-timeout-h46vk\naffinity-clusterip-timeout-h46vk\naffinity-clusterip-timeout-h46vk\naffinity-clusterip-timeout-h46vk\naffinity-clusterip-timeout-h46vk\naffinity-clusterip-timeout-h46vk\naffinity-clusterip-timeout-h46vk"
    Mar  2 01:51:03.838: INFO: Received response from host: affinity-clusterip-timeout-h46vk
    Mar  2 01:51:03.838: INFO: Received response from host: affinity-clusterip-timeout-h46vk
    Mar  2 01:51:03.838: INFO: Received response from host: affinity-clusterip-timeout-h46vk
    Mar  2 01:51:03.838: INFO: Received response from host: affinity-clusterip-timeout-h46vk
    Mar  2 01:51:03.838: INFO: Received response from host: affinity-clusterip-timeout-h46vk
    Mar  2 01:51:03.838: INFO: Received response from host: affinity-clusterip-timeout-h46vk
    Mar  2 01:51:03.838: INFO: Received response from host: affinity-clusterip-timeout-h46vk
    Mar  2 01:51:03.838: INFO: Received response from host: affinity-clusterip-timeout-h46vk
    Mar  2 01:51:03.838: INFO: Received response from host: affinity-clusterip-timeout-h46vk
    Mar  2 01:51:03.838: INFO: Received response from host: affinity-clusterip-timeout-h46vk
    Mar  2 01:51:03.838: INFO: Received response from host: affinity-clusterip-timeout-h46vk
    Mar  2 01:51:03.838: INFO: Received response from host: affinity-clusterip-timeout-h46vk
    Mar  2 01:51:03.838: INFO: Received response from host: affinity-clusterip-timeout-h46vk
    Mar  2 01:51:03.838: INFO: Received response from host: affinity-clusterip-timeout-h46vk
    Mar  2 01:51:03.838: INFO: Received response from host: affinity-clusterip-timeout-h46vk
    Mar  2 01:51:03.838: INFO: Received response from host: affinity-clusterip-timeout-h46vk
    Mar  2 01:51:03.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-5856 exec execpod-affinityr7kc9 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.189.91:80/'
    Mar  2 01:51:04.245: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.189.91:80/\n"
    Mar  2 01:51:04.246: INFO: stdout: "affinity-clusterip-timeout-h46vk"
    Mar  2 01:51:24.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-5856 exec execpod-affinityr7kc9 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.189.91:80/'
    Mar  2 01:51:24.611: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.189.91:80/\n"
    Mar  2 01:51:24.611: INFO: stdout: "affinity-clusterip-timeout-h46vk"
    Mar  2 01:51:44.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-5856 exec execpod-affinityr7kc9 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.189.91:80/'
    Mar  2 01:51:45.045: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.189.91:80/\n"
    Mar  2 01:51:45.045: INFO: stdout: "affinity-clusterip-timeout-rrdhc"
    Mar  2 01:51:45.045: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-5856, will wait for the garbage collector to delete the pods 03/02/23 01:51:45.152
    Mar  2 01:51:45.277: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 48.815355ms
    Mar  2 01:51:45.489: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 204.798199ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  2 01:51:48.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5856" for this suite. 03/02/23 01:51:48.83
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:51:48.876
Mar  2 01:51:48.876: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename kubelet-test 03/02/23 01:51:48.878
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:51:48.993
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:51:49.005
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
W0302 01:51:49.104507      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "bin-false055133c3-fb28-4fb2-bf49-59f82711c82d" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "bin-false055133c3-fb28-4fb2-bf49-59f82711c82d" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "bin-false055133c3-fb28-4fb2-bf49-59f82711c82d" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "bin-false055133c3-fb28-4fb2-bf49-59f82711c82d" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Mar  2 01:51:53.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6002" for this suite. 03/02/23 01:51:53.192
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","completed":135,"skipped":2634,"failed":0}
------------------------------
• [4.341 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:51:48.876
    Mar  2 01:51:48.876: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename kubelet-test 03/02/23 01:51:48.878
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:51:48.993
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:51:49.005
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    W0302 01:51:49.104507      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "bin-false055133c3-fb28-4fb2-bf49-59f82711c82d" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "bin-false055133c3-fb28-4fb2-bf49-59f82711c82d" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "bin-false055133c3-fb28-4fb2-bf49-59f82711c82d" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "bin-false055133c3-fb28-4fb2-bf49-59f82711c82d" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Mar  2 01:51:53.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-6002" for this suite. 03/02/23 01:51:53.192
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:51:53.221
Mar  2 01:51:53.221: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename projected 03/02/23 01:51:53.222
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:51:53.28
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:51:53.292
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
STEP: Creating configMap with name projected-configmap-test-volume-map-286461d2-5e17-49ee-b183-1d5af0fcf066 03/02/23 01:51:53.307
STEP: Creating a pod to test consume configMaps 03/02/23 01:51:53.333
Mar  2 01:51:53.408: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-221b5de4-0619-4dba-9498-42c4e9a1e45c" in namespace "projected-2819" to be "Succeeded or Failed"
Mar  2 01:51:53.438: INFO: Pod "pod-projected-configmaps-221b5de4-0619-4dba-9498-42c4e9a1e45c": Phase="Pending", Reason="", readiness=false. Elapsed: 30.086768ms
Mar  2 01:51:55.465: INFO: Pod "pod-projected-configmaps-221b5de4-0619-4dba-9498-42c4e9a1e45c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056769207s
Mar  2 01:51:57.455: INFO: Pod "pod-projected-configmaps-221b5de4-0619-4dba-9498-42c4e9a1e45c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046305806s
Mar  2 01:51:59.455: INFO: Pod "pod-projected-configmaps-221b5de4-0619-4dba-9498-42c4e9a1e45c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.046572957s
STEP: Saw pod success 03/02/23 01:51:59.455
Mar  2 01:51:59.455: INFO: Pod "pod-projected-configmaps-221b5de4-0619-4dba-9498-42c4e9a1e45c" satisfied condition "Succeeded or Failed"
Mar  2 01:51:59.476: INFO: Trying to get logs from node 10.132.92.143 pod pod-projected-configmaps-221b5de4-0619-4dba-9498-42c4e9a1e45c container agnhost-container: <nil>
STEP: delete the pod 03/02/23 01:51:59.527
Mar  2 01:51:59.605: INFO: Waiting for pod pod-projected-configmaps-221b5de4-0619-4dba-9498-42c4e9a1e45c to disappear
Mar  2 01:51:59.619: INFO: Pod pod-projected-configmaps-221b5de4-0619-4dba-9498-42c4e9a1e45c no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar  2 01:51:59.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2819" for this suite. 03/02/23 01:51:59.666
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":136,"skipped":2676,"failed":0}
------------------------------
• [SLOW TEST] [6.481 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:51:53.221
    Mar  2 01:51:53.221: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename projected 03/02/23 01:51:53.222
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:51:53.28
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:51:53.292
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:108
    STEP: Creating configMap with name projected-configmap-test-volume-map-286461d2-5e17-49ee-b183-1d5af0fcf066 03/02/23 01:51:53.307
    STEP: Creating a pod to test consume configMaps 03/02/23 01:51:53.333
    Mar  2 01:51:53.408: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-221b5de4-0619-4dba-9498-42c4e9a1e45c" in namespace "projected-2819" to be "Succeeded or Failed"
    Mar  2 01:51:53.438: INFO: Pod "pod-projected-configmaps-221b5de4-0619-4dba-9498-42c4e9a1e45c": Phase="Pending", Reason="", readiness=false. Elapsed: 30.086768ms
    Mar  2 01:51:55.465: INFO: Pod "pod-projected-configmaps-221b5de4-0619-4dba-9498-42c4e9a1e45c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056769207s
    Mar  2 01:51:57.455: INFO: Pod "pod-projected-configmaps-221b5de4-0619-4dba-9498-42c4e9a1e45c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046305806s
    Mar  2 01:51:59.455: INFO: Pod "pod-projected-configmaps-221b5de4-0619-4dba-9498-42c4e9a1e45c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.046572957s
    STEP: Saw pod success 03/02/23 01:51:59.455
    Mar  2 01:51:59.455: INFO: Pod "pod-projected-configmaps-221b5de4-0619-4dba-9498-42c4e9a1e45c" satisfied condition "Succeeded or Failed"
    Mar  2 01:51:59.476: INFO: Trying to get logs from node 10.132.92.143 pod pod-projected-configmaps-221b5de4-0619-4dba-9498-42c4e9a1e45c container agnhost-container: <nil>
    STEP: delete the pod 03/02/23 01:51:59.527
    Mar  2 01:51:59.605: INFO: Waiting for pod pod-projected-configmaps-221b5de4-0619-4dba-9498-42c4e9a1e45c to disappear
    Mar  2 01:51:59.619: INFO: Pod pod-projected-configmaps-221b5de4-0619-4dba-9498-42c4e9a1e45c no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar  2 01:51:59.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2819" for this suite. 03/02/23 01:51:59.666
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:51:59.703
Mar  2 01:51:59.704: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename gc 03/02/23 01:51:59.705
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:51:59.783
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:51:59.797
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 03/02/23 01:51:59.809
W0302 01:51:59.843042      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Wait for the Deployment to create new ReplicaSet 03/02/23 01:51:59.843
STEP: delete the deployment 03/02/23 01:52:00.395
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 03/02/23 01:52:00.424
STEP: Gathering metrics 03/02/23 01:52:01.141
W0302 01:52:01.196894      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Mar  2 01:52:01.196: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Mar  2 01:52:01.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9900" for this suite. 03/02/23 01:52:01.296
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","completed":137,"skipped":2686,"failed":0}
------------------------------
• [1.655 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:51:59.703
    Mar  2 01:51:59.704: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename gc 03/02/23 01:51:59.705
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:51:59.783
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:51:59.797
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 03/02/23 01:51:59.809
    W0302 01:51:59.843042      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Wait for the Deployment to create new ReplicaSet 03/02/23 01:51:59.843
    STEP: delete the deployment 03/02/23 01:52:00.395
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 03/02/23 01:52:00.424
    STEP: Gathering metrics 03/02/23 01:52:01.141
    W0302 01:52:01.196894      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Mar  2 01:52:01.196: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Mar  2 01:52:01.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-9900" for this suite. 03/02/23 01:52:01.296
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:52:01.359
Mar  2 01:52:01.359: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename job 03/02/23 01:52:01.36
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:52:01.449
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:52:01.484
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
STEP: Creating a job 03/02/23 01:52:01.513
STEP: Ensuring active pods == parallelism 03/02/23 01:52:01.554
STEP: Orphaning one of the Job's Pods 03/02/23 01:52:05.574
Mar  2 01:52:06.138: INFO: Successfully updated pod "adopt-release-dq96l"
STEP: Checking that the Job readopts the Pod 03/02/23 01:52:06.138
Mar  2 01:52:06.138: INFO: Waiting up to 15m0s for pod "adopt-release-dq96l" in namespace "job-6626" to be "adopted"
Mar  2 01:52:06.156: INFO: Pod "adopt-release-dq96l": Phase="Running", Reason="", readiness=true. Elapsed: 17.96564ms
Mar  2 01:52:08.184: INFO: Pod "adopt-release-dq96l": Phase="Running", Reason="", readiness=true. Elapsed: 2.04630328s
Mar  2 01:52:08.184: INFO: Pod "adopt-release-dq96l" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 03/02/23 01:52:08.184
Mar  2 01:52:08.772: INFO: Successfully updated pod "adopt-release-dq96l"
STEP: Checking that the Job releases the Pod 03/02/23 01:52:08.773
Mar  2 01:52:08.773: INFO: Waiting up to 15m0s for pod "adopt-release-dq96l" in namespace "job-6626" to be "released"
Mar  2 01:52:08.786: INFO: Pod "adopt-release-dq96l": Phase="Running", Reason="", readiness=true. Elapsed: 13.253985ms
Mar  2 01:52:10.802: INFO: Pod "adopt-release-dq96l": Phase="Running", Reason="", readiness=true. Elapsed: 2.028569396s
Mar  2 01:52:10.802: INFO: Pod "adopt-release-dq96l" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Mar  2 01:52:10.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6626" for this suite. 03/02/23 01:52:10.825
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","completed":138,"skipped":2688,"failed":0}
------------------------------
• [SLOW TEST] [9.496 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:52:01.359
    Mar  2 01:52:01.359: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename job 03/02/23 01:52:01.36
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:52:01.449
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:52:01.484
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:335
    STEP: Creating a job 03/02/23 01:52:01.513
    STEP: Ensuring active pods == parallelism 03/02/23 01:52:01.554
    STEP: Orphaning one of the Job's Pods 03/02/23 01:52:05.574
    Mar  2 01:52:06.138: INFO: Successfully updated pod "adopt-release-dq96l"
    STEP: Checking that the Job readopts the Pod 03/02/23 01:52:06.138
    Mar  2 01:52:06.138: INFO: Waiting up to 15m0s for pod "adopt-release-dq96l" in namespace "job-6626" to be "adopted"
    Mar  2 01:52:06.156: INFO: Pod "adopt-release-dq96l": Phase="Running", Reason="", readiness=true. Elapsed: 17.96564ms
    Mar  2 01:52:08.184: INFO: Pod "adopt-release-dq96l": Phase="Running", Reason="", readiness=true. Elapsed: 2.04630328s
    Mar  2 01:52:08.184: INFO: Pod "adopt-release-dq96l" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 03/02/23 01:52:08.184
    Mar  2 01:52:08.772: INFO: Successfully updated pod "adopt-release-dq96l"
    STEP: Checking that the Job releases the Pod 03/02/23 01:52:08.773
    Mar  2 01:52:08.773: INFO: Waiting up to 15m0s for pod "adopt-release-dq96l" in namespace "job-6626" to be "released"
    Mar  2 01:52:08.786: INFO: Pod "adopt-release-dq96l": Phase="Running", Reason="", readiness=true. Elapsed: 13.253985ms
    Mar  2 01:52:10.802: INFO: Pod "adopt-release-dq96l": Phase="Running", Reason="", readiness=true. Elapsed: 2.028569396s
    Mar  2 01:52:10.802: INFO: Pod "adopt-release-dq96l" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Mar  2 01:52:10.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-6626" for this suite. 03/02/23 01:52:10.825
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:52:10.864
Mar  2 01:52:10.864: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename custom-resource-definition 03/02/23 01:52:10.866
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:52:10.931
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:52:10.944
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 03/02/23 01:52:10.957
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 03/02/23 01:52:10.963
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 03/02/23 01:52:10.963
STEP: fetching the /apis/apiextensions.k8s.io discovery document 03/02/23 01:52:10.963
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 03/02/23 01:52:10.969
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 03/02/23 01:52:10.969
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 03/02/23 01:52:10.975
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 01:52:10.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3606" for this suite. 03/02/23 01:52:10.997
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","completed":139,"skipped":2718,"failed":0}
------------------------------
• [0.177 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:52:10.864
    Mar  2 01:52:10.864: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename custom-resource-definition 03/02/23 01:52:10.866
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:52:10.931
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:52:10.944
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 03/02/23 01:52:10.957
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 03/02/23 01:52:10.963
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 03/02/23 01:52:10.963
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 03/02/23 01:52:10.963
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 03/02/23 01:52:10.969
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 03/02/23 01:52:10.969
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 03/02/23 01:52:10.975
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 01:52:10.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-3606" for this suite. 03/02/23 01:52:10.997
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:52:11.042
Mar  2 01:52:11.042: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename crd-publish-openapi 03/02/23 01:52:11.043
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:52:11.12
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:52:11.134
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
STEP: set up a multi version CRD 03/02/23 01:52:11.147
Mar  2 01:52:11.148: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: mark a version not serverd 03/02/23 01:52:42.868
STEP: check the unserved version gets removed 03/02/23 01:52:43.009
STEP: check the other version is not changed 03/02/23 01:52:57.201
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 01:53:22.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-332" for this suite. 03/02/23 01:53:22.802
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","completed":140,"skipped":2729,"failed":0}
------------------------------
• [SLOW TEST] [71.811 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:52:11.042
    Mar  2 01:52:11.042: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename crd-publish-openapi 03/02/23 01:52:11.043
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:52:11.12
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:52:11.134
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:441
    STEP: set up a multi version CRD 03/02/23 01:52:11.147
    Mar  2 01:52:11.148: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: mark a version not serverd 03/02/23 01:52:42.868
    STEP: check the unserved version gets removed 03/02/23 01:52:43.009
    STEP: check the other version is not changed 03/02/23 01:52:57.201
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 01:53:22.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-332" for this suite. 03/02/23 01:53:22.802
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:53:22.854
Mar  2 01:53:22.858: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename deployment 03/02/23 01:53:22.86
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:53:22.942
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:53:22.958
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Mar  2 01:53:22.995: INFO: Creating deployment "webserver-deployment"
Mar  2 01:53:23.024: INFO: Waiting for observed generation 1
Mar  2 01:53:25.072: INFO: Waiting for all required pods to come up
Mar  2 01:53:25.098: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 03/02/23 01:53:25.098
Mar  2 01:53:25.098: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-xh5zb" in namespace "deployment-3600" to be "running"
Mar  2 01:53:25.099: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-646p8" in namespace "deployment-3600" to be "running"
Mar  2 01:53:25.099: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-65g2v" in namespace "deployment-3600" to be "running"
Mar  2 01:53:25.099: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-6fpxf" in namespace "deployment-3600" to be "running"
Mar  2 01:53:25.099: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-7bnjb" in namespace "deployment-3600" to be "running"
Mar  2 01:53:25.099: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-c2pgp" in namespace "deployment-3600" to be "running"
Mar  2 01:53:25.100: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-jcn9t" in namespace "deployment-3600" to be "running"
Mar  2 01:53:25.100: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-hbvrv" in namespace "deployment-3600" to be "running"
Mar  2 01:53:25.100: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-jkbb6" in namespace "deployment-3600" to be "running"
Mar  2 01:53:25.100: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-rvcch" in namespace "deployment-3600" to be "running"
Mar  2 01:53:25.128: INFO: Pod "webserver-deployment-845c8977d9-xh5zb": Phase="Pending", Reason="", readiness=false. Elapsed: 29.331964ms
Mar  2 01:53:25.138: INFO: Pod "webserver-deployment-845c8977d9-jkbb6": Phase="Pending", Reason="", readiness=false. Elapsed: 37.954501ms
Mar  2 01:53:25.139: INFO: Pod "webserver-deployment-845c8977d9-jcn9t": Phase="Pending", Reason="", readiness=false. Elapsed: 38.672924ms
Mar  2 01:53:25.139: INFO: Pod "webserver-deployment-845c8977d9-c2pgp": Phase="Pending", Reason="", readiness=false. Elapsed: 39.104723ms
Mar  2 01:53:25.139: INFO: Pod "webserver-deployment-845c8977d9-646p8": Phase="Pending", Reason="", readiness=false. Elapsed: 40.276076ms
Mar  2 01:53:25.139: INFO: Pod "webserver-deployment-845c8977d9-hbvrv": Phase="Pending", Reason="", readiness=false. Elapsed: 39.395219ms
Mar  2 01:53:25.140: INFO: Pod "webserver-deployment-845c8977d9-rvcch": Phase="Pending", Reason="", readiness=false. Elapsed: 39.272543ms
Mar  2 01:53:25.140: INFO: Pod "webserver-deployment-845c8977d9-6fpxf": Phase="Pending", Reason="", readiness=false. Elapsed: 40.965741ms
Mar  2 01:53:25.140: INFO: Pod "webserver-deployment-845c8977d9-7bnjb": Phase="Pending", Reason="", readiness=false. Elapsed: 41.096221ms
Mar  2 01:53:25.141: INFO: Pod "webserver-deployment-845c8977d9-65g2v": Phase="Pending", Reason="", readiness=false. Elapsed: 41.776395ms
Mar  2 01:53:27.143: INFO: Pod "webserver-deployment-845c8977d9-xh5zb": Phase="Running", Reason="", readiness=true. Elapsed: 2.044941677s
Mar  2 01:53:27.143: INFO: Pod "webserver-deployment-845c8977d9-xh5zb" satisfied condition "running"
Mar  2 01:53:27.154: INFO: Pod "webserver-deployment-845c8977d9-c2pgp": Phase="Running", Reason="", readiness=true. Elapsed: 2.053869335s
Mar  2 01:53:27.154: INFO: Pod "webserver-deployment-845c8977d9-c2pgp" satisfied condition "running"
Mar  2 01:53:27.159: INFO: Pod "webserver-deployment-845c8977d9-6fpxf": Phase="Running", Reason="", readiness=true. Elapsed: 2.059547179s
Mar  2 01:53:27.159: INFO: Pod "webserver-deployment-845c8977d9-6fpxf" satisfied condition "running"
Mar  2 01:53:27.159: INFO: Pod "webserver-deployment-845c8977d9-65g2v": Phase="Running", Reason="", readiness=true. Elapsed: 2.060339158s
Mar  2 01:53:27.159: INFO: Pod "webserver-deployment-845c8977d9-65g2v" satisfied condition "running"
Mar  2 01:53:27.161: INFO: Pod "webserver-deployment-845c8977d9-rvcch": Phase="Running", Reason="", readiness=true. Elapsed: 2.060963256s
Mar  2 01:53:27.161: INFO: Pod "webserver-deployment-845c8977d9-rvcch" satisfied condition "running"
Mar  2 01:53:27.162: INFO: Pod "webserver-deployment-845c8977d9-7bnjb": Phase="Running", Reason="", readiness=true. Elapsed: 2.062317809s
Mar  2 01:53:27.162: INFO: Pod "webserver-deployment-845c8977d9-7bnjb" satisfied condition "running"
Mar  2 01:53:27.162: INFO: Pod "webserver-deployment-845c8977d9-jcn9t": Phase="Running", Reason="", readiness=true. Elapsed: 2.06192217s
Mar  2 01:53:27.162: INFO: Pod "webserver-deployment-845c8977d9-jcn9t" satisfied condition "running"
Mar  2 01:53:27.162: INFO: Pod "webserver-deployment-845c8977d9-646p8": Phase="Running", Reason="", readiness=true. Elapsed: 2.062907839s
Mar  2 01:53:27.162: INFO: Pod "webserver-deployment-845c8977d9-646p8" satisfied condition "running"
Mar  2 01:53:27.162: INFO: Pod "webserver-deployment-845c8977d9-jkbb6": Phase="Running", Reason="", readiness=true. Elapsed: 2.061607889s
Mar  2 01:53:27.162: INFO: Pod "webserver-deployment-845c8977d9-jkbb6" satisfied condition "running"
Mar  2 01:53:27.162: INFO: Pod "webserver-deployment-845c8977d9-hbvrv": Phase="Running", Reason="", readiness=true. Elapsed: 2.062316466s
Mar  2 01:53:27.162: INFO: Pod "webserver-deployment-845c8977d9-hbvrv" satisfied condition "running"
Mar  2 01:53:27.162: INFO: Waiting for deployment "webserver-deployment" to complete
Mar  2 01:53:27.209: INFO: Updating deployment "webserver-deployment" with a non-existent image
Mar  2 01:53:27.304: INFO: Updating deployment webserver-deployment
Mar  2 01:53:27.304: INFO: Waiting for observed generation 2
Mar  2 01:53:29.336: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Mar  2 01:53:29.349: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Mar  2 01:53:29.368: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar  2 01:53:29.408: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Mar  2 01:53:29.408: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Mar  2 01:53:29.422: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar  2 01:53:29.449: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Mar  2 01:53:29.450: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Mar  2 01:53:29.484: INFO: Updating deployment webserver-deployment
Mar  2 01:53:29.484: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Mar  2 01:53:29.516: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Mar  2 01:53:29.530: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar  2 01:53:29.565: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-3600  81d2ef54-d97a-4667-ae0e-376763f7edbb 101087 3 2023-03-02 01:53:23 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-02 01:53:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 01:53:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005933418 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-03-02 01:53:27 +0000 UTC,LastTransitionTime:2023-03-02 01:53:23 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-02 01:53:29 +0000 UTC,LastTransitionTime:2023-03-02 01:53:29 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Mar  2 01:53:29.596: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-3600  266ba74a-2836-462c-bfa6-47629be2a766 101084 3 2023-03-02 01:53:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 81d2ef54-d97a-4667-ae0e-376763f7edbb 0xc005933a47 0xc005933a48}] [] [{kube-controller-manager Update apps/v1 2023-03-02 01:53:27 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-03-02 01:53:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"81d2ef54-d97a-4667-ae0e-376763f7edbb\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005933b08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  2 01:53:29.596: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Mar  2 01:53:29.596: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-3600  0bb5bac4-556d-4e6b-817b-3933d14af795 101081 3 2023-03-02 01:53:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 81d2ef54-d97a-4667-ae0e-376763f7edbb 0xc005933b77 0xc005933b78}] [] [{kube-controller-manager Update apps/v1 2023-03-02 01:53:27 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-03-02 01:53:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"81d2ef54-d97a-4667-ae0e-376763f7edbb\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005933c18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Mar  2 01:53:29.674: INFO: Pod "webserver-deployment-69b7448995-22h4v" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-22h4v webserver-deployment-69b7448995- deployment-3600  5fb555c7-8ec4-4f41-8a6f-d28e47aa62c7 101066 0 2023-03-02 01:53:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:575edc14c55d04d5c0fabf2b18c8ad3017cb532491ac06d96f63da2a03be4b9d cni.projectcalico.org/podIP:172.30.201.238/32 cni.projectcalico.org/podIPs:172.30.201.238/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.201.238"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.201.238"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 266ba74a-2836-462c-bfa6-47629be2a766 0xc0059903a7 0xc0059903a8}] [] [{kube-controller-manager Update v1 2023-03-02 01:53:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"266ba74a-2836-462c-bfa6-47629be2a766\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-02 01:53:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-02 01:53:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 01:53:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2glc8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2glc8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.132.92.188,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c48,c42,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6ngst,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.132.92.188,PodIP:,StartTime:2023-03-02 01:53:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 01:53:29.675: INFO: Pod "webserver-deployment-69b7448995-429s9" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-429s9 webserver-deployment-69b7448995- deployment-3600  0c7bc8d1-4696-4f80-b40e-69eb46794660 101050 0 2023-03-02 01:53:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:3add0dc917f9ef9afd017b06d24d288e2cab77e5a87b2ed8cca567fee2996d5b cni.projectcalico.org/podIP:172.30.156.89/32 cni.projectcalico.org/podIPs:172.30.156.89/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.156.89"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.156.89"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 266ba74a-2836-462c-bfa6-47629be2a766 0xc0059907b7 0xc0059907b8}] [] [{kube-controller-manager Update v1 2023-03-02 01:53:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"266ba74a-2836-462c-bfa6-47629be2a766\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-02 01:53:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-02 01:53:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 01:53:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2vstd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2vstd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.132.92.143,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c48,c42,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6ngst,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.132.92.143,PodIP:,StartTime:2023-03-02 01:53:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 01:53:29.675: INFO: Pod "webserver-deployment-69b7448995-86f65" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-86f65 webserver-deployment-69b7448995- deployment-3600  0fe2babc-bd01-4cad-af27-b7368837d973 101051 0 2023-03-02 01:53:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:f836a68a1c1f83b04f6e47d0d38df1b80d7b04ed17840f74f0287d51483f914f cni.projectcalico.org/podIP:172.30.201.222/32 cni.projectcalico.org/podIPs:172.30.201.222/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.201.222"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.201.222"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 266ba74a-2836-462c-bfa6-47629be2a766 0xc005990bb7 0xc005990bb8}] [] [{kube-controller-manager Update v1 2023-03-02 01:53:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"266ba74a-2836-462c-bfa6-47629be2a766\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-02 01:53:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-02 01:53:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 01:53:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j964c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j964c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.132.92.188,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c48,c42,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6ngst,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.132.92.188,PodIP:,StartTime:2023-03-02 01:53:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 01:53:29.676: INFO: Pod "webserver-deployment-69b7448995-cz6r5" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-cz6r5 webserver-deployment-69b7448995- deployment-3600  2483b1c7-88de-4fb5-93be-2d6261a4f5ab 101091 0 2023-03-02 01:53:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 266ba74a-2836-462c-bfa6-47629be2a766 0xc005990ff7 0xc005990ff8}] [] [{kube-controller-manager Update v1 2023-03-02 01:53:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"266ba74a-2836-462c-bfa6-47629be2a766\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z8lh7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z8lh7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c48,c42,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6ngst,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 01:53:29.676: INFO: Pod "webserver-deployment-69b7448995-f4789" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-f4789 webserver-deployment-69b7448995- deployment-3600  7e8ce471-76fa-497b-b628-c384cd63e213 101069 0 2023-03-02 01:53:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:d52e641602f156ac30a9b24be4c99ecb199cab883928239ada03b56ba6db676a cni.projectcalico.org/podIP:172.30.156.125/32 cni.projectcalico.org/podIPs:172.30.156.125/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.156.125"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.156.125"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 266ba74a-2836-462c-bfa6-47629be2a766 0xc0059911f7 0xc0059911f8}] [] [{kube-controller-manager Update v1 2023-03-02 01:53:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"266ba74a-2836-462c-bfa6-47629be2a766\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-02 01:53:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-02 01:53:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 01:53:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ntvcp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ntvcp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.132.92.143,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c48,c42,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6ngst,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.132.92.143,PodIP:,StartTime:2023-03-02 01:53:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 01:53:29.677: INFO: Pod "webserver-deployment-69b7448995-sc96t" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-sc96t webserver-deployment-69b7448995- deployment-3600  4c4179c6-51f1-4dc3-a379-662cc0ce0fe8 101070 0 2023-03-02 01:53:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:d9d93ece27b21048524d4a0e403abd849b1aff980cd8c36221bad2a6af25a006 cni.projectcalico.org/podIP:172.30.62.199/32 cni.projectcalico.org/podIPs:172.30.62.199/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.62.199"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.62.199"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 266ba74a-2836-462c-bfa6-47629be2a766 0xc005991557 0xc005991558}] [] [{kube-controller-manager Update v1 2023-03-02 01:53:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"266ba74a-2836-462c-bfa6-47629be2a766\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-02 01:53:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-02 01:53:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 01:53:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w55qw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w55qw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.132.92.186,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c48,c42,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6ngst,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.132.92.186,PodIP:,StartTime:2023-03-02 01:53:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 01:53:29.677: INFO: Pod "webserver-deployment-845c8977d9-646p8" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-646p8 webserver-deployment-845c8977d9- deployment-3600  bff0ae6c-9e4f-413e-80f1-70cfc20bfe7e 100966 0 2023-03-02 01:53:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:8828be3db0b0fd2d12185ce3505858488dfa0b0e2fdf0a532e1de7ada0c5caf9 cni.projectcalico.org/podIP:172.30.62.250/32 cni.projectcalico.org/podIPs:172.30.62.250/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.62.250"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.62.250"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 0bb5bac4-556d-4e6b-817b-3933d14af795 0xc005991847 0xc005991848}] [] [{kube-controller-manager Update v1 2023-03-02 01:53:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0bb5bac4-556d-4e6b-817b-3933d14af795\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 01:53:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 01:53:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-02 01:53:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.62.250\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-slmww,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-slmww,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.132.92.186,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c48,c42,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6ngst,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.132.92.186,PodIP:172.30.62.250,StartTime:2023-03-02 01:53:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 01:53:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://eff9ed292dbfd8d879d81d8f004fbc6a7a0fc11201057f602b52b66d477e9af1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.62.250,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 01:53:29.678: INFO: Pod "webserver-deployment-845c8977d9-65g2v" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-65g2v webserver-deployment-845c8977d9- deployment-3600  6af5be19-26cf-477c-aff3-dc3d0f929351 100956 0 2023-03-02 01:53:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:46abc12e556806360767985f845a170652d4130e9c013a45ae28515a324821f6 cni.projectcalico.org/podIP:172.30.62.255/32 cni.projectcalico.org/podIPs:172.30.62.255/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.62.255"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.62.255"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 0bb5bac4-556d-4e6b-817b-3933d14af795 0xc005991ad7 0xc005991ad8}] [] [{kube-controller-manager Update v1 2023-03-02 01:53:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0bb5bac4-556d-4e6b-817b-3933d14af795\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 01:53:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 01:53:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-02 01:53:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.62.255\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vgk94,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vgk94,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.132.92.186,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c48,c42,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6ngst,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.132.92.186,PodIP:172.30.62.255,StartTime:2023-03-02 01:53:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 01:53:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://0416b7c6181e76a9786bd3043b8bee18c82c1c71636ef5c7ef5ee5bb952cb853,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.62.255,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 01:53:29.678: INFO: Pod "webserver-deployment-845c8977d9-6vdgp" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-6vdgp webserver-deployment-845c8977d9- deployment-3600  f4090298-78b6-4a13-835b-104da592fc7a 101094 0 2023-03-02 01:53:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 0bb5bac4-556d-4e6b-817b-3933d14af795 0xc005991de7 0xc005991de8}] [] [{kube-controller-manager Update v1 2023-03-02 01:53:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0bb5bac4-556d-4e6b-817b-3933d14af795\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qxqv5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qxqv5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c48,c42,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6ngst,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 01:53:29.679: INFO: Pod "webserver-deployment-845c8977d9-7bnjb" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-7bnjb webserver-deployment-845c8977d9- deployment-3600  31734c11-3f8d-4e9f-b14f-25e02bf67b64 100936 0 2023-03-02 01:53:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:9aafc04d8ae29bfc9dc0112352f20aed104a833a6104b9224eda4ace83ce1cdf cni.projectcalico.org/podIP:172.30.156.97/32 cni.projectcalico.org/podIPs:172.30.156.97/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.156.97"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.156.97"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 0bb5bac4-556d-4e6b-817b-3933d14af795 0xc005991fd7 0xc005991fd8}] [] [{kube-controller-manager Update v1 2023-03-02 01:53:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0bb5bac4-556d-4e6b-817b-3933d14af795\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 01:53:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 01:53:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-02 01:53:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.156.97\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h8zth,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h8zth,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.132.92.143,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c48,c42,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.132.92.143,PodIP:172.30.156.97,StartTime:2023-03-02 01:53:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 01:53:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://18c457641d20ee51990a5aa430cfc6e88eb9c868ceb2a5e8997aa8e4b67ebf10,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.156.97,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 01:53:29.679: INFO: Pod "webserver-deployment-845c8977d9-hbvrv" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-hbvrv webserver-deployment-845c8977d9- deployment-3600  7b43e592-7374-4ca0-818a-2e72d490c944 100974 0 2023-03-02 01:53:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:10562bb2af33119c4b220663ffd94d9a687ff05f6fc61af1ed24259589de4959 cni.projectcalico.org/podIP:172.30.201.255/32 cni.projectcalico.org/podIPs:172.30.201.255/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.201.255"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.201.255"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 0bb5bac4-556d-4e6b-817b-3933d14af795 0xc0059ba327 0xc0059ba328}] [] [{kube-controller-manager Update v1 2023-03-02 01:53:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0bb5bac4-556d-4e6b-817b-3933d14af795\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 01:53:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 01:53:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-02 01:53:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.201.255\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d8fhj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d8fhj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.132.92.188,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c48,c42,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6ngst,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.132.92.188,PodIP:172.30.201.255,StartTime:2023-03-02 01:53:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 01:53:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://89346603bb12f2915f36bb8e05205955dc44396ce2d4de700ad19cf02782005a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.201.255,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 01:53:29.680: INFO: Pod "webserver-deployment-845c8977d9-jcn9t" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-jcn9t webserver-deployment-845c8977d9- deployment-3600  68ea6d86-833f-4735-91f3-c094af3322f8 100976 0 2023-03-02 01:53:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:98a7afd1cfe5f762b1463fa626a4f0b22aff0ec0f1adfdb89347c909647d30b5 cni.projectcalico.org/podIP:172.30.156.126/32 cni.projectcalico.org/podIPs:172.30.156.126/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.156.126"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.156.126"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 0bb5bac4-556d-4e6b-817b-3933d14af795 0xc0059ba7a7 0xc0059ba7a8}] [] [{kube-controller-manager Update v1 2023-03-02 01:53:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0bb5bac4-556d-4e6b-817b-3933d14af795\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 01:53:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 01:53:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-02 01:53:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.156.126\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rfklp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rfklp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.132.92.143,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c48,c42,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6ngst,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.132.92.143,PodIP:172.30.156.126,StartTime:2023-03-02 01:53:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 01:53:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://50176c60027a93c47b0eeb733faa9b528045893bd21e979d7e20d003f46cde09,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.156.126,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 01:53:29.680: INFO: Pod "webserver-deployment-845c8977d9-jkbb6" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-jkbb6 webserver-deployment-845c8977d9- deployment-3600  1dd255bd-3dd0-444f-a06f-971ffc51fa1e 100939 0 2023-03-02 01:53:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:47b0b0abeb117c003a15b811fa26ff964653cf324dfc80eff126342594e5a9ba cni.projectcalico.org/podIP:172.30.201.236/32 cni.projectcalico.org/podIPs:172.30.201.236/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.201.236"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.201.236"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 0bb5bac4-556d-4e6b-817b-3933d14af795 0xc0059babb7 0xc0059babb8}] [] [{kube-controller-manager Update v1 2023-03-02 01:53:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0bb5bac4-556d-4e6b-817b-3933d14af795\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 01:53:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 01:53:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-02 01:53:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.201.236\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g4sgt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g4sgt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.132.92.188,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c48,c42,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6ngst,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.132.92.188,PodIP:172.30.201.236,StartTime:2023-03-02 01:53:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 01:53:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://79ed638691ea0f4f66547b1889a7a4d68bbbdae1efae217dd74d8d4db0e2bc81,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.201.236,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 01:53:29.680: INFO: Pod "webserver-deployment-845c8977d9-jz7nf" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-jz7nf webserver-deployment-845c8977d9- deployment-3600  e99451ca-3b54-4fa5-9898-79e34ee15cbe 101090 0 2023-03-02 01:53:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 0bb5bac4-556d-4e6b-817b-3933d14af795 0xc0059baeb7 0xc0059baeb8}] [] [{kube-controller-manager Update v1 2023-03-02 01:53:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0bb5bac4-556d-4e6b-817b-3933d14af795\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bb7wf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bb7wf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.132.92.143,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c48,c42,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6ngst,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 01:53:29.681: INFO: Pod "webserver-deployment-845c8977d9-n6jhm" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-n6jhm webserver-deployment-845c8977d9- deployment-3600  c90898ce-d785-473c-b806-d7aef4e90f34 101096 0 2023-03-02 01:53:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 0bb5bac4-556d-4e6b-817b-3933d14af795 0xc0059bb117 0xc0059bb118}] [] [{kube-controller-manager Update v1 2023-03-02 01:53:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0bb5bac4-556d-4e6b-817b-3933d14af795\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mcrnk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mcrnk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c48,c42,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6ngst,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 01:53:29.681: INFO: Pod "webserver-deployment-845c8977d9-rvcch" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-rvcch webserver-deployment-845c8977d9- deployment-3600  6ad41549-a116-4bee-8cc1-c7d75f01984e 100934 0 2023-03-02 01:53:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:07fa70138941e7a0a2e18379ecd7572f203f8bdf287cc6497a18e266beea29b3 cni.projectcalico.org/podIP:172.30.201.221/32 cni.projectcalico.org/podIPs:172.30.201.221/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.201.221"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.201.221"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 0bb5bac4-556d-4e6b-817b-3933d14af795 0xc0059bb2e7 0xc0059bb2e8}] [] [{kube-controller-manager Update v1 2023-03-02 01:53:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0bb5bac4-556d-4e6b-817b-3933d14af795\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 01:53:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 01:53:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-02 01:53:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.201.221\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qbtnb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qbtnb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.132.92.188,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c48,c42,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6ngst,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.132.92.188,PodIP:172.30.201.221,StartTime:2023-03-02 01:53:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 01:53:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://1c95ca8a1aaaa639954239810fee59f9fee5e3267e141cd634d29b421f9aa66e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.201.221,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 01:53:29.682: INFO: Pod "webserver-deployment-845c8977d9-xh5zb" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-xh5zb webserver-deployment-845c8977d9- deployment-3600  587197f1-3014-447c-a845-873ec68e1255 100960 0 2023-03-02 01:53:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:beb2a85291bf324159b699e6baf5d75eb36b458e93403fe933a2c8679b21eaf1 cni.projectcalico.org/podIP:172.30.62.237/32 cni.projectcalico.org/podIPs:172.30.62.237/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.62.237"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.62.237"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 0bb5bac4-556d-4e6b-817b-3933d14af795 0xc0059bb647 0xc0059bb648}] [] [{kube-controller-manager Update v1 2023-03-02 01:53:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0bb5bac4-556d-4e6b-817b-3933d14af795\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 01:53:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 01:53:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-02 01:53:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.62.237\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gvv9z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gvv9z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.132.92.186,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c48,c42,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6ngst,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.132.92.186,PodIP:172.30.62.237,StartTime:2023-03-02 01:53:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 01:53:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://e7c23458820a99a59bf67aa5989af01b91415122ca986a11bc2ccefe939abfd4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.62.237,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar  2 01:53:29.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3600" for this suite. 03/02/23 01:53:29.726
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","completed":141,"skipped":2740,"failed":0}
------------------------------
• [SLOW TEST] [6.932 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:53:22.854
    Mar  2 01:53:22.858: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename deployment 03/02/23 01:53:22.86
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:53:22.942
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:53:22.958
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Mar  2 01:53:22.995: INFO: Creating deployment "webserver-deployment"
    Mar  2 01:53:23.024: INFO: Waiting for observed generation 1
    Mar  2 01:53:25.072: INFO: Waiting for all required pods to come up
    Mar  2 01:53:25.098: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 03/02/23 01:53:25.098
    Mar  2 01:53:25.098: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-xh5zb" in namespace "deployment-3600" to be "running"
    Mar  2 01:53:25.099: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-646p8" in namespace "deployment-3600" to be "running"
    Mar  2 01:53:25.099: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-65g2v" in namespace "deployment-3600" to be "running"
    Mar  2 01:53:25.099: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-6fpxf" in namespace "deployment-3600" to be "running"
    Mar  2 01:53:25.099: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-7bnjb" in namespace "deployment-3600" to be "running"
    Mar  2 01:53:25.099: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-c2pgp" in namespace "deployment-3600" to be "running"
    Mar  2 01:53:25.100: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-jcn9t" in namespace "deployment-3600" to be "running"
    Mar  2 01:53:25.100: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-hbvrv" in namespace "deployment-3600" to be "running"
    Mar  2 01:53:25.100: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-jkbb6" in namespace "deployment-3600" to be "running"
    Mar  2 01:53:25.100: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-rvcch" in namespace "deployment-3600" to be "running"
    Mar  2 01:53:25.128: INFO: Pod "webserver-deployment-845c8977d9-xh5zb": Phase="Pending", Reason="", readiness=false. Elapsed: 29.331964ms
    Mar  2 01:53:25.138: INFO: Pod "webserver-deployment-845c8977d9-jkbb6": Phase="Pending", Reason="", readiness=false. Elapsed: 37.954501ms
    Mar  2 01:53:25.139: INFO: Pod "webserver-deployment-845c8977d9-jcn9t": Phase="Pending", Reason="", readiness=false. Elapsed: 38.672924ms
    Mar  2 01:53:25.139: INFO: Pod "webserver-deployment-845c8977d9-c2pgp": Phase="Pending", Reason="", readiness=false. Elapsed: 39.104723ms
    Mar  2 01:53:25.139: INFO: Pod "webserver-deployment-845c8977d9-646p8": Phase="Pending", Reason="", readiness=false. Elapsed: 40.276076ms
    Mar  2 01:53:25.139: INFO: Pod "webserver-deployment-845c8977d9-hbvrv": Phase="Pending", Reason="", readiness=false. Elapsed: 39.395219ms
    Mar  2 01:53:25.140: INFO: Pod "webserver-deployment-845c8977d9-rvcch": Phase="Pending", Reason="", readiness=false. Elapsed: 39.272543ms
    Mar  2 01:53:25.140: INFO: Pod "webserver-deployment-845c8977d9-6fpxf": Phase="Pending", Reason="", readiness=false. Elapsed: 40.965741ms
    Mar  2 01:53:25.140: INFO: Pod "webserver-deployment-845c8977d9-7bnjb": Phase="Pending", Reason="", readiness=false. Elapsed: 41.096221ms
    Mar  2 01:53:25.141: INFO: Pod "webserver-deployment-845c8977d9-65g2v": Phase="Pending", Reason="", readiness=false. Elapsed: 41.776395ms
    Mar  2 01:53:27.143: INFO: Pod "webserver-deployment-845c8977d9-xh5zb": Phase="Running", Reason="", readiness=true. Elapsed: 2.044941677s
    Mar  2 01:53:27.143: INFO: Pod "webserver-deployment-845c8977d9-xh5zb" satisfied condition "running"
    Mar  2 01:53:27.154: INFO: Pod "webserver-deployment-845c8977d9-c2pgp": Phase="Running", Reason="", readiness=true. Elapsed: 2.053869335s
    Mar  2 01:53:27.154: INFO: Pod "webserver-deployment-845c8977d9-c2pgp" satisfied condition "running"
    Mar  2 01:53:27.159: INFO: Pod "webserver-deployment-845c8977d9-6fpxf": Phase="Running", Reason="", readiness=true. Elapsed: 2.059547179s
    Mar  2 01:53:27.159: INFO: Pod "webserver-deployment-845c8977d9-6fpxf" satisfied condition "running"
    Mar  2 01:53:27.159: INFO: Pod "webserver-deployment-845c8977d9-65g2v": Phase="Running", Reason="", readiness=true. Elapsed: 2.060339158s
    Mar  2 01:53:27.159: INFO: Pod "webserver-deployment-845c8977d9-65g2v" satisfied condition "running"
    Mar  2 01:53:27.161: INFO: Pod "webserver-deployment-845c8977d9-rvcch": Phase="Running", Reason="", readiness=true. Elapsed: 2.060963256s
    Mar  2 01:53:27.161: INFO: Pod "webserver-deployment-845c8977d9-rvcch" satisfied condition "running"
    Mar  2 01:53:27.162: INFO: Pod "webserver-deployment-845c8977d9-7bnjb": Phase="Running", Reason="", readiness=true. Elapsed: 2.062317809s
    Mar  2 01:53:27.162: INFO: Pod "webserver-deployment-845c8977d9-7bnjb" satisfied condition "running"
    Mar  2 01:53:27.162: INFO: Pod "webserver-deployment-845c8977d9-jcn9t": Phase="Running", Reason="", readiness=true. Elapsed: 2.06192217s
    Mar  2 01:53:27.162: INFO: Pod "webserver-deployment-845c8977d9-jcn9t" satisfied condition "running"
    Mar  2 01:53:27.162: INFO: Pod "webserver-deployment-845c8977d9-646p8": Phase="Running", Reason="", readiness=true. Elapsed: 2.062907839s
    Mar  2 01:53:27.162: INFO: Pod "webserver-deployment-845c8977d9-646p8" satisfied condition "running"
    Mar  2 01:53:27.162: INFO: Pod "webserver-deployment-845c8977d9-jkbb6": Phase="Running", Reason="", readiness=true. Elapsed: 2.061607889s
    Mar  2 01:53:27.162: INFO: Pod "webserver-deployment-845c8977d9-jkbb6" satisfied condition "running"
    Mar  2 01:53:27.162: INFO: Pod "webserver-deployment-845c8977d9-hbvrv": Phase="Running", Reason="", readiness=true. Elapsed: 2.062316466s
    Mar  2 01:53:27.162: INFO: Pod "webserver-deployment-845c8977d9-hbvrv" satisfied condition "running"
    Mar  2 01:53:27.162: INFO: Waiting for deployment "webserver-deployment" to complete
    Mar  2 01:53:27.209: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Mar  2 01:53:27.304: INFO: Updating deployment webserver-deployment
    Mar  2 01:53:27.304: INFO: Waiting for observed generation 2
    Mar  2 01:53:29.336: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Mar  2 01:53:29.349: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Mar  2 01:53:29.368: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Mar  2 01:53:29.408: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Mar  2 01:53:29.408: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Mar  2 01:53:29.422: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Mar  2 01:53:29.449: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Mar  2 01:53:29.450: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Mar  2 01:53:29.484: INFO: Updating deployment webserver-deployment
    Mar  2 01:53:29.484: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Mar  2 01:53:29.516: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Mar  2 01:53:29.530: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar  2 01:53:29.565: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-3600  81d2ef54-d97a-4667-ae0e-376763f7edbb 101087 3 2023-03-02 01:53:23 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-02 01:53:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 01:53:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005933418 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-03-02 01:53:27 +0000 UTC,LastTransitionTime:2023-03-02 01:53:23 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-02 01:53:29 +0000 UTC,LastTransitionTime:2023-03-02 01:53:29 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Mar  2 01:53:29.596: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-3600  266ba74a-2836-462c-bfa6-47629be2a766 101084 3 2023-03-02 01:53:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 81d2ef54-d97a-4667-ae0e-376763f7edbb 0xc005933a47 0xc005933a48}] [] [{kube-controller-manager Update apps/v1 2023-03-02 01:53:27 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-03-02 01:53:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"81d2ef54-d97a-4667-ae0e-376763f7edbb\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005933b08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar  2 01:53:29.596: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Mar  2 01:53:29.596: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-3600  0bb5bac4-556d-4e6b-817b-3933d14af795 101081 3 2023-03-02 01:53:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 81d2ef54-d97a-4667-ae0e-376763f7edbb 0xc005933b77 0xc005933b78}] [] [{kube-controller-manager Update apps/v1 2023-03-02 01:53:27 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-03-02 01:53:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"81d2ef54-d97a-4667-ae0e-376763f7edbb\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005933c18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Mar  2 01:53:29.674: INFO: Pod "webserver-deployment-69b7448995-22h4v" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-22h4v webserver-deployment-69b7448995- deployment-3600  5fb555c7-8ec4-4f41-8a6f-d28e47aa62c7 101066 0 2023-03-02 01:53:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:575edc14c55d04d5c0fabf2b18c8ad3017cb532491ac06d96f63da2a03be4b9d cni.projectcalico.org/podIP:172.30.201.238/32 cni.projectcalico.org/podIPs:172.30.201.238/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.201.238"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.201.238"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 266ba74a-2836-462c-bfa6-47629be2a766 0xc0059903a7 0xc0059903a8}] [] [{kube-controller-manager Update v1 2023-03-02 01:53:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"266ba74a-2836-462c-bfa6-47629be2a766\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-02 01:53:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-02 01:53:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 01:53:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2glc8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2glc8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.132.92.188,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c48,c42,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6ngst,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.132.92.188,PodIP:,StartTime:2023-03-02 01:53:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 01:53:29.675: INFO: Pod "webserver-deployment-69b7448995-429s9" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-429s9 webserver-deployment-69b7448995- deployment-3600  0c7bc8d1-4696-4f80-b40e-69eb46794660 101050 0 2023-03-02 01:53:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:3add0dc917f9ef9afd017b06d24d288e2cab77e5a87b2ed8cca567fee2996d5b cni.projectcalico.org/podIP:172.30.156.89/32 cni.projectcalico.org/podIPs:172.30.156.89/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.156.89"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.156.89"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 266ba74a-2836-462c-bfa6-47629be2a766 0xc0059907b7 0xc0059907b8}] [] [{kube-controller-manager Update v1 2023-03-02 01:53:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"266ba74a-2836-462c-bfa6-47629be2a766\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-02 01:53:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-02 01:53:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 01:53:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2vstd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2vstd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.132.92.143,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c48,c42,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6ngst,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.132.92.143,PodIP:,StartTime:2023-03-02 01:53:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 01:53:29.675: INFO: Pod "webserver-deployment-69b7448995-86f65" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-86f65 webserver-deployment-69b7448995- deployment-3600  0fe2babc-bd01-4cad-af27-b7368837d973 101051 0 2023-03-02 01:53:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:f836a68a1c1f83b04f6e47d0d38df1b80d7b04ed17840f74f0287d51483f914f cni.projectcalico.org/podIP:172.30.201.222/32 cni.projectcalico.org/podIPs:172.30.201.222/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.201.222"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.201.222"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 266ba74a-2836-462c-bfa6-47629be2a766 0xc005990bb7 0xc005990bb8}] [] [{kube-controller-manager Update v1 2023-03-02 01:53:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"266ba74a-2836-462c-bfa6-47629be2a766\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-02 01:53:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-02 01:53:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 01:53:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j964c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j964c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.132.92.188,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c48,c42,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6ngst,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.132.92.188,PodIP:,StartTime:2023-03-02 01:53:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 01:53:29.676: INFO: Pod "webserver-deployment-69b7448995-cz6r5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-cz6r5 webserver-deployment-69b7448995- deployment-3600  2483b1c7-88de-4fb5-93be-2d6261a4f5ab 101091 0 2023-03-02 01:53:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 266ba74a-2836-462c-bfa6-47629be2a766 0xc005990ff7 0xc005990ff8}] [] [{kube-controller-manager Update v1 2023-03-02 01:53:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"266ba74a-2836-462c-bfa6-47629be2a766\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z8lh7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z8lh7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c48,c42,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6ngst,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 01:53:29.676: INFO: Pod "webserver-deployment-69b7448995-f4789" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-f4789 webserver-deployment-69b7448995- deployment-3600  7e8ce471-76fa-497b-b628-c384cd63e213 101069 0 2023-03-02 01:53:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:d52e641602f156ac30a9b24be4c99ecb199cab883928239ada03b56ba6db676a cni.projectcalico.org/podIP:172.30.156.125/32 cni.projectcalico.org/podIPs:172.30.156.125/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.156.125"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.156.125"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 266ba74a-2836-462c-bfa6-47629be2a766 0xc0059911f7 0xc0059911f8}] [] [{kube-controller-manager Update v1 2023-03-02 01:53:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"266ba74a-2836-462c-bfa6-47629be2a766\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-02 01:53:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-02 01:53:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 01:53:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ntvcp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ntvcp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.132.92.143,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c48,c42,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6ngst,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.132.92.143,PodIP:,StartTime:2023-03-02 01:53:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 01:53:29.677: INFO: Pod "webserver-deployment-69b7448995-sc96t" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-sc96t webserver-deployment-69b7448995- deployment-3600  4c4179c6-51f1-4dc3-a379-662cc0ce0fe8 101070 0 2023-03-02 01:53:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:d9d93ece27b21048524d4a0e403abd849b1aff980cd8c36221bad2a6af25a006 cni.projectcalico.org/podIP:172.30.62.199/32 cni.projectcalico.org/podIPs:172.30.62.199/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.62.199"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.62.199"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 266ba74a-2836-462c-bfa6-47629be2a766 0xc005991557 0xc005991558}] [] [{kube-controller-manager Update v1 2023-03-02 01:53:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"266ba74a-2836-462c-bfa6-47629be2a766\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-02 01:53:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-02 01:53:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 01:53:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w55qw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w55qw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.132.92.186,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c48,c42,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6ngst,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.132.92.186,PodIP:,StartTime:2023-03-02 01:53:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 01:53:29.677: INFO: Pod "webserver-deployment-845c8977d9-646p8" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-646p8 webserver-deployment-845c8977d9- deployment-3600  bff0ae6c-9e4f-413e-80f1-70cfc20bfe7e 100966 0 2023-03-02 01:53:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:8828be3db0b0fd2d12185ce3505858488dfa0b0e2fdf0a532e1de7ada0c5caf9 cni.projectcalico.org/podIP:172.30.62.250/32 cni.projectcalico.org/podIPs:172.30.62.250/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.62.250"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.62.250"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 0bb5bac4-556d-4e6b-817b-3933d14af795 0xc005991847 0xc005991848}] [] [{kube-controller-manager Update v1 2023-03-02 01:53:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0bb5bac4-556d-4e6b-817b-3933d14af795\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 01:53:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 01:53:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-02 01:53:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.62.250\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-slmww,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-slmww,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.132.92.186,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c48,c42,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6ngst,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.132.92.186,PodIP:172.30.62.250,StartTime:2023-03-02 01:53:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 01:53:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://eff9ed292dbfd8d879d81d8f004fbc6a7a0fc11201057f602b52b66d477e9af1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.62.250,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 01:53:29.678: INFO: Pod "webserver-deployment-845c8977d9-65g2v" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-65g2v webserver-deployment-845c8977d9- deployment-3600  6af5be19-26cf-477c-aff3-dc3d0f929351 100956 0 2023-03-02 01:53:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:46abc12e556806360767985f845a170652d4130e9c013a45ae28515a324821f6 cni.projectcalico.org/podIP:172.30.62.255/32 cni.projectcalico.org/podIPs:172.30.62.255/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.62.255"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.62.255"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 0bb5bac4-556d-4e6b-817b-3933d14af795 0xc005991ad7 0xc005991ad8}] [] [{kube-controller-manager Update v1 2023-03-02 01:53:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0bb5bac4-556d-4e6b-817b-3933d14af795\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 01:53:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 01:53:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-02 01:53:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.62.255\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vgk94,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vgk94,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.132.92.186,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c48,c42,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6ngst,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.132.92.186,PodIP:172.30.62.255,StartTime:2023-03-02 01:53:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 01:53:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://0416b7c6181e76a9786bd3043b8bee18c82c1c71636ef5c7ef5ee5bb952cb853,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.62.255,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 01:53:29.678: INFO: Pod "webserver-deployment-845c8977d9-6vdgp" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-6vdgp webserver-deployment-845c8977d9- deployment-3600  f4090298-78b6-4a13-835b-104da592fc7a 101094 0 2023-03-02 01:53:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 0bb5bac4-556d-4e6b-817b-3933d14af795 0xc005991de7 0xc005991de8}] [] [{kube-controller-manager Update v1 2023-03-02 01:53:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0bb5bac4-556d-4e6b-817b-3933d14af795\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qxqv5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qxqv5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c48,c42,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6ngst,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 01:53:29.679: INFO: Pod "webserver-deployment-845c8977d9-7bnjb" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-7bnjb webserver-deployment-845c8977d9- deployment-3600  31734c11-3f8d-4e9f-b14f-25e02bf67b64 100936 0 2023-03-02 01:53:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:9aafc04d8ae29bfc9dc0112352f20aed104a833a6104b9224eda4ace83ce1cdf cni.projectcalico.org/podIP:172.30.156.97/32 cni.projectcalico.org/podIPs:172.30.156.97/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.156.97"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.156.97"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 0bb5bac4-556d-4e6b-817b-3933d14af795 0xc005991fd7 0xc005991fd8}] [] [{kube-controller-manager Update v1 2023-03-02 01:53:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0bb5bac4-556d-4e6b-817b-3933d14af795\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 01:53:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 01:53:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-02 01:53:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.156.97\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h8zth,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h8zth,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.132.92.143,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c48,c42,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.132.92.143,PodIP:172.30.156.97,StartTime:2023-03-02 01:53:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 01:53:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://18c457641d20ee51990a5aa430cfc6e88eb9c868ceb2a5e8997aa8e4b67ebf10,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.156.97,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 01:53:29.679: INFO: Pod "webserver-deployment-845c8977d9-hbvrv" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-hbvrv webserver-deployment-845c8977d9- deployment-3600  7b43e592-7374-4ca0-818a-2e72d490c944 100974 0 2023-03-02 01:53:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:10562bb2af33119c4b220663ffd94d9a687ff05f6fc61af1ed24259589de4959 cni.projectcalico.org/podIP:172.30.201.255/32 cni.projectcalico.org/podIPs:172.30.201.255/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.201.255"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.201.255"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 0bb5bac4-556d-4e6b-817b-3933d14af795 0xc0059ba327 0xc0059ba328}] [] [{kube-controller-manager Update v1 2023-03-02 01:53:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0bb5bac4-556d-4e6b-817b-3933d14af795\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 01:53:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 01:53:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-02 01:53:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.201.255\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d8fhj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d8fhj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.132.92.188,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c48,c42,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6ngst,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.132.92.188,PodIP:172.30.201.255,StartTime:2023-03-02 01:53:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 01:53:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://89346603bb12f2915f36bb8e05205955dc44396ce2d4de700ad19cf02782005a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.201.255,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 01:53:29.680: INFO: Pod "webserver-deployment-845c8977d9-jcn9t" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-jcn9t webserver-deployment-845c8977d9- deployment-3600  68ea6d86-833f-4735-91f3-c094af3322f8 100976 0 2023-03-02 01:53:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:98a7afd1cfe5f762b1463fa626a4f0b22aff0ec0f1adfdb89347c909647d30b5 cni.projectcalico.org/podIP:172.30.156.126/32 cni.projectcalico.org/podIPs:172.30.156.126/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.156.126"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.156.126"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 0bb5bac4-556d-4e6b-817b-3933d14af795 0xc0059ba7a7 0xc0059ba7a8}] [] [{kube-controller-manager Update v1 2023-03-02 01:53:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0bb5bac4-556d-4e6b-817b-3933d14af795\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 01:53:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 01:53:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-02 01:53:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.156.126\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rfklp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rfklp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.132.92.143,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c48,c42,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6ngst,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.132.92.143,PodIP:172.30.156.126,StartTime:2023-03-02 01:53:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 01:53:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://50176c60027a93c47b0eeb733faa9b528045893bd21e979d7e20d003f46cde09,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.156.126,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 01:53:29.680: INFO: Pod "webserver-deployment-845c8977d9-jkbb6" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-jkbb6 webserver-deployment-845c8977d9- deployment-3600  1dd255bd-3dd0-444f-a06f-971ffc51fa1e 100939 0 2023-03-02 01:53:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:47b0b0abeb117c003a15b811fa26ff964653cf324dfc80eff126342594e5a9ba cni.projectcalico.org/podIP:172.30.201.236/32 cni.projectcalico.org/podIPs:172.30.201.236/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.201.236"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.201.236"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 0bb5bac4-556d-4e6b-817b-3933d14af795 0xc0059babb7 0xc0059babb8}] [] [{kube-controller-manager Update v1 2023-03-02 01:53:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0bb5bac4-556d-4e6b-817b-3933d14af795\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 01:53:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 01:53:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-02 01:53:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.201.236\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g4sgt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g4sgt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.132.92.188,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c48,c42,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6ngst,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.132.92.188,PodIP:172.30.201.236,StartTime:2023-03-02 01:53:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 01:53:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://79ed638691ea0f4f66547b1889a7a4d68bbbdae1efae217dd74d8d4db0e2bc81,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.201.236,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 01:53:29.680: INFO: Pod "webserver-deployment-845c8977d9-jz7nf" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-jz7nf webserver-deployment-845c8977d9- deployment-3600  e99451ca-3b54-4fa5-9898-79e34ee15cbe 101090 0 2023-03-02 01:53:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 0bb5bac4-556d-4e6b-817b-3933d14af795 0xc0059baeb7 0xc0059baeb8}] [] [{kube-controller-manager Update v1 2023-03-02 01:53:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0bb5bac4-556d-4e6b-817b-3933d14af795\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bb7wf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bb7wf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.132.92.143,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c48,c42,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6ngst,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 01:53:29.681: INFO: Pod "webserver-deployment-845c8977d9-n6jhm" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-n6jhm webserver-deployment-845c8977d9- deployment-3600  c90898ce-d785-473c-b806-d7aef4e90f34 101096 0 2023-03-02 01:53:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 0bb5bac4-556d-4e6b-817b-3933d14af795 0xc0059bb117 0xc0059bb118}] [] [{kube-controller-manager Update v1 2023-03-02 01:53:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0bb5bac4-556d-4e6b-817b-3933d14af795\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mcrnk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mcrnk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c48,c42,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6ngst,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 01:53:29.681: INFO: Pod "webserver-deployment-845c8977d9-rvcch" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-rvcch webserver-deployment-845c8977d9- deployment-3600  6ad41549-a116-4bee-8cc1-c7d75f01984e 100934 0 2023-03-02 01:53:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:07fa70138941e7a0a2e18379ecd7572f203f8bdf287cc6497a18e266beea29b3 cni.projectcalico.org/podIP:172.30.201.221/32 cni.projectcalico.org/podIPs:172.30.201.221/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.201.221"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.201.221"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 0bb5bac4-556d-4e6b-817b-3933d14af795 0xc0059bb2e7 0xc0059bb2e8}] [] [{kube-controller-manager Update v1 2023-03-02 01:53:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0bb5bac4-556d-4e6b-817b-3933d14af795\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 01:53:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 01:53:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-02 01:53:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.201.221\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qbtnb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qbtnb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.132.92.188,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c48,c42,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6ngst,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.132.92.188,PodIP:172.30.201.221,StartTime:2023-03-02 01:53:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 01:53:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://1c95ca8a1aaaa639954239810fee59f9fee5e3267e141cd634d29b421f9aa66e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.201.221,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 01:53:29.682: INFO: Pod "webserver-deployment-845c8977d9-xh5zb" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-xh5zb webserver-deployment-845c8977d9- deployment-3600  587197f1-3014-447c-a845-873ec68e1255 100960 0 2023-03-02 01:53:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:beb2a85291bf324159b699e6baf5d75eb36b458e93403fe933a2c8679b21eaf1 cni.projectcalico.org/podIP:172.30.62.237/32 cni.projectcalico.org/podIPs:172.30.62.237/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.62.237"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.62.237"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 0bb5bac4-556d-4e6b-817b-3933d14af795 0xc0059bb647 0xc0059bb648}] [] [{kube-controller-manager Update v1 2023-03-02 01:53:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0bb5bac4-556d-4e6b-817b-3933d14af795\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 01:53:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 01:53:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-02 01:53:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.62.237\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gvv9z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gvv9z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.132.92.186,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c48,c42,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6ngst,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:53:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.132.92.186,PodIP:172.30.62.237,StartTime:2023-03-02 01:53:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 01:53:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://e7c23458820a99a59bf67aa5989af01b91415122ca986a11bc2ccefe939abfd4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.62.237,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar  2 01:53:29.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-3600" for this suite. 03/02/23 01:53:29.726
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:53:29.792
Mar  2 01:53:29.792: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename emptydir 03/02/23 01:53:29.793
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:53:29.893
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:53:29.919
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
STEP: Creating a pod to test emptydir volume type on node default medium 03/02/23 01:53:29.934
Mar  2 01:53:30.070: INFO: Waiting up to 5m0s for pod "pod-0a145df3-f44e-4830-af31-c021dc3bbaf7" in namespace "emptydir-143" to be "Succeeded or Failed"
Mar  2 01:53:30.084: INFO: Pod "pod-0a145df3-f44e-4830-af31-c021dc3bbaf7": Phase="Pending", Reason="", readiness=false. Elapsed: 14.689873ms
Mar  2 01:53:32.098: INFO: Pod "pod-0a145df3-f44e-4830-af31-c021dc3bbaf7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028200017s
Mar  2 01:53:34.099: INFO: Pod "pod-0a145df3-f44e-4830-af31-c021dc3bbaf7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029219618s
Mar  2 01:53:36.106: INFO: Pod "pod-0a145df3-f44e-4830-af31-c021dc3bbaf7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.036627795s
STEP: Saw pod success 03/02/23 01:53:36.106
Mar  2 01:53:36.107: INFO: Pod "pod-0a145df3-f44e-4830-af31-c021dc3bbaf7" satisfied condition "Succeeded or Failed"
Mar  2 01:53:36.150: INFO: Trying to get logs from node 10.132.92.143 pod pod-0a145df3-f44e-4830-af31-c021dc3bbaf7 container test-container: <nil>
STEP: delete the pod 03/02/23 01:53:36.305
Mar  2 01:53:36.346: INFO: Waiting for pod pod-0a145df3-f44e-4830-af31-c021dc3bbaf7 to disappear
Mar  2 01:53:36.361: INFO: Pod pod-0a145df3-f44e-4830-af31-c021dc3bbaf7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  2 01:53:36.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-143" for this suite. 03/02/23 01:53:36.405
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":142,"skipped":2748,"failed":0}
------------------------------
• [SLOW TEST] [6.654 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:53:29.792
    Mar  2 01:53:29.792: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename emptydir 03/02/23 01:53:29.793
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:53:29.893
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:53:29.919
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:156
    STEP: Creating a pod to test emptydir volume type on node default medium 03/02/23 01:53:29.934
    Mar  2 01:53:30.070: INFO: Waiting up to 5m0s for pod "pod-0a145df3-f44e-4830-af31-c021dc3bbaf7" in namespace "emptydir-143" to be "Succeeded or Failed"
    Mar  2 01:53:30.084: INFO: Pod "pod-0a145df3-f44e-4830-af31-c021dc3bbaf7": Phase="Pending", Reason="", readiness=false. Elapsed: 14.689873ms
    Mar  2 01:53:32.098: INFO: Pod "pod-0a145df3-f44e-4830-af31-c021dc3bbaf7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028200017s
    Mar  2 01:53:34.099: INFO: Pod "pod-0a145df3-f44e-4830-af31-c021dc3bbaf7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029219618s
    Mar  2 01:53:36.106: INFO: Pod "pod-0a145df3-f44e-4830-af31-c021dc3bbaf7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.036627795s
    STEP: Saw pod success 03/02/23 01:53:36.106
    Mar  2 01:53:36.107: INFO: Pod "pod-0a145df3-f44e-4830-af31-c021dc3bbaf7" satisfied condition "Succeeded or Failed"
    Mar  2 01:53:36.150: INFO: Trying to get logs from node 10.132.92.143 pod pod-0a145df3-f44e-4830-af31-c021dc3bbaf7 container test-container: <nil>
    STEP: delete the pod 03/02/23 01:53:36.305
    Mar  2 01:53:36.346: INFO: Waiting for pod pod-0a145df3-f44e-4830-af31-c021dc3bbaf7 to disappear
    Mar  2 01:53:36.361: INFO: Pod pod-0a145df3-f44e-4830-af31-c021dc3bbaf7 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  2 01:53:36.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-143" for this suite. 03/02/23 01:53:36.405
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:53:36.447
Mar  2 01:53:36.447: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename sched-preemption 03/02/23 01:53:36.449
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:53:36.567
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:53:36.586
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Mar  2 01:53:36.852: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  2 01:54:37.446: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
STEP: Create pods that use 4/5 of node resources. 03/02/23 01:54:37.501
Mar  2 01:54:37.724: INFO: Created pod: pod0-0-sched-preemption-low-priority
Mar  2 01:54:37.776: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Mar  2 01:54:37.864: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Mar  2 01:54:37.917: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Mar  2 01:54:38.020: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Mar  2 01:54:38.101: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 03/02/23 01:54:38.101
Mar  2 01:54:38.102: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-2088" to be "running"
Mar  2 01:54:38.116: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 14.755513ms
Mar  2 01:54:40.133: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031351447s
Mar  2 01:54:42.134: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032099087s
Mar  2 01:54:44.131: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.029795802s
Mar  2 01:54:46.141: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.038937337s
Mar  2 01:54:48.132: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 10.030226835s
Mar  2 01:54:48.132: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Mar  2 01:54:48.132: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-2088" to be "running"
Mar  2 01:54:48.147: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 14.859432ms
Mar  2 01:54:48.147: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Mar  2 01:54:48.147: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-2088" to be "running"
Mar  2 01:54:48.167: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 20.088778ms
Mar  2 01:54:48.167: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Mar  2 01:54:48.167: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-2088" to be "running"
Mar  2 01:54:48.186: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 19.156469ms
Mar  2 01:54:48.186: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Mar  2 01:54:48.186: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-2088" to be "running"
Mar  2 01:54:48.207: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 20.15333ms
Mar  2 01:54:48.207: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Mar  2 01:54:48.207: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-2088" to be "running"
Mar  2 01:54:48.225: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 18.10664ms
Mar  2 01:54:48.225: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 03/02/23 01:54:48.225
Mar  2 01:54:48.270: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-2088" to be "running"
Mar  2 01:54:48.282: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 11.899049ms
Mar  2 01:54:50.300: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029768142s
Mar  2 01:54:52.324: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054217667s
Mar  2 01:54:54.300: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.03010235s
Mar  2 01:54:54.300: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Mar  2 01:54:54.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-2088" for this suite. 03/02/23 01:54:54.513
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","completed":143,"skipped":2757,"failed":0}
------------------------------
• [SLOW TEST] [78.312 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:53:36.447
    Mar  2 01:53:36.447: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename sched-preemption 03/02/23 01:53:36.449
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:53:36.567
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:53:36.586
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Mar  2 01:53:36.852: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar  2 01:54:37.446: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:125
    STEP: Create pods that use 4/5 of node resources. 03/02/23 01:54:37.501
    Mar  2 01:54:37.724: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Mar  2 01:54:37.776: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Mar  2 01:54:37.864: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Mar  2 01:54:37.917: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Mar  2 01:54:38.020: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Mar  2 01:54:38.101: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 03/02/23 01:54:38.101
    Mar  2 01:54:38.102: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-2088" to be "running"
    Mar  2 01:54:38.116: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 14.755513ms
    Mar  2 01:54:40.133: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031351447s
    Mar  2 01:54:42.134: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032099087s
    Mar  2 01:54:44.131: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.029795802s
    Mar  2 01:54:46.141: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.038937337s
    Mar  2 01:54:48.132: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 10.030226835s
    Mar  2 01:54:48.132: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Mar  2 01:54:48.132: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-2088" to be "running"
    Mar  2 01:54:48.147: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 14.859432ms
    Mar  2 01:54:48.147: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Mar  2 01:54:48.147: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-2088" to be "running"
    Mar  2 01:54:48.167: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 20.088778ms
    Mar  2 01:54:48.167: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Mar  2 01:54:48.167: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-2088" to be "running"
    Mar  2 01:54:48.186: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 19.156469ms
    Mar  2 01:54:48.186: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Mar  2 01:54:48.186: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-2088" to be "running"
    Mar  2 01:54:48.207: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 20.15333ms
    Mar  2 01:54:48.207: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Mar  2 01:54:48.207: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-2088" to be "running"
    Mar  2 01:54:48.225: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 18.10664ms
    Mar  2 01:54:48.225: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 03/02/23 01:54:48.225
    Mar  2 01:54:48.270: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-2088" to be "running"
    Mar  2 01:54:48.282: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 11.899049ms
    Mar  2 01:54:50.300: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029768142s
    Mar  2 01:54:52.324: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054217667s
    Mar  2 01:54:54.300: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.03010235s
    Mar  2 01:54:54.300: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Mar  2 01:54:54.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-2088" for this suite. 03/02/23 01:54:54.513
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:54:54.76
Mar  2 01:54:54.760: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename statefulset 03/02/23 01:54:54.761
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:54:54.872
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:54:54.898
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-7828 03/02/23 01:54:54.911
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
STEP: Creating a new StatefulSet 03/02/23 01:54:54.961
Mar  2 01:54:55.032: INFO: Found 0 stateful pods, waiting for 3
Mar  2 01:55:05.059: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 01:55:05.059: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 01:55:05.059: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 03/02/23 01:55:05.106
Mar  2 01:55:05.161: INFO: Updating stateful set ss2
STEP: Creating a new revision 03/02/23 01:55:05.161
STEP: Not applying an update when the partition is greater than the number of replicas 03/02/23 01:55:15.281
STEP: Performing a canary update 03/02/23 01:55:15.281
Mar  2 01:55:15.376: INFO: Updating stateful set ss2
Mar  2 01:55:15.416: INFO: Waiting for Pod statefulset-7828/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
STEP: Restoring Pods to the correct revision when they are deleted 03/02/23 01:55:25.457
Mar  2 01:55:25.588: INFO: Found 1 stateful pods, waiting for 3
Mar  2 01:55:35.608: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 01:55:35.608: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 01:55:35.608: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 03/02/23 01:55:35.639
Mar  2 01:55:35.695: INFO: Updating stateful set ss2
Mar  2 01:55:35.767: INFO: Waiting for Pod statefulset-7828/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Mar  2 01:55:45.855: INFO: Updating stateful set ss2
Mar  2 01:55:45.893: INFO: Waiting for StatefulSet statefulset-7828/ss2 to complete update
Mar  2 01:55:45.894: INFO: Waiting for Pod statefulset-7828/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Mar  2 01:55:55.933: INFO: Waiting for StatefulSet statefulset-7828/ss2 to complete update
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar  2 01:56:05.935: INFO: Deleting all statefulset in ns statefulset-7828
Mar  2 01:56:05.950: INFO: Scaling statefulset ss2 to 0
Mar  2 01:56:16.013: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 01:56:16.042: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar  2 01:56:16.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7828" for this suite. 03/02/23 01:56:16.175
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","completed":144,"skipped":2765,"failed":0}
------------------------------
• [SLOW TEST] [81.457 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:315

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:54:54.76
    Mar  2 01:54:54.760: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename statefulset 03/02/23 01:54:54.761
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:54:54.872
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:54:54.898
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-7828 03/02/23 01:54:54.911
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:315
    STEP: Creating a new StatefulSet 03/02/23 01:54:54.961
    Mar  2 01:54:55.032: INFO: Found 0 stateful pods, waiting for 3
    Mar  2 01:55:05.059: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar  2 01:55:05.059: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar  2 01:55:05.059: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 03/02/23 01:55:05.106
    Mar  2 01:55:05.161: INFO: Updating stateful set ss2
    STEP: Creating a new revision 03/02/23 01:55:05.161
    STEP: Not applying an update when the partition is greater than the number of replicas 03/02/23 01:55:15.281
    STEP: Performing a canary update 03/02/23 01:55:15.281
    Mar  2 01:55:15.376: INFO: Updating stateful set ss2
    Mar  2 01:55:15.416: INFO: Waiting for Pod statefulset-7828/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    STEP: Restoring Pods to the correct revision when they are deleted 03/02/23 01:55:25.457
    Mar  2 01:55:25.588: INFO: Found 1 stateful pods, waiting for 3
    Mar  2 01:55:35.608: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar  2 01:55:35.608: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar  2 01:55:35.608: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 03/02/23 01:55:35.639
    Mar  2 01:55:35.695: INFO: Updating stateful set ss2
    Mar  2 01:55:35.767: INFO: Waiting for Pod statefulset-7828/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Mar  2 01:55:45.855: INFO: Updating stateful set ss2
    Mar  2 01:55:45.893: INFO: Waiting for StatefulSet statefulset-7828/ss2 to complete update
    Mar  2 01:55:45.894: INFO: Waiting for Pod statefulset-7828/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Mar  2 01:55:55.933: INFO: Waiting for StatefulSet statefulset-7828/ss2 to complete update
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar  2 01:56:05.935: INFO: Deleting all statefulset in ns statefulset-7828
    Mar  2 01:56:05.950: INFO: Scaling statefulset ss2 to 0
    Mar  2 01:56:16.013: INFO: Waiting for statefulset status.replicas updated to 0
    Mar  2 01:56:16.042: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar  2 01:56:16.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-7828" for this suite. 03/02/23 01:56:16.175
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:56:16.218
Mar  2 01:56:16.218: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename svcaccounts 03/02/23 01:56:16.22
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:56:16.291
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:56:16.305
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
STEP: Creating a pod to test service account token:  03/02/23 01:56:16.335
Mar  2 01:56:16.427: INFO: Waiting up to 5m0s for pod "test-pod-bf45890e-433c-4115-aeeb-6ba5899b52e5" in namespace "svcaccounts-5463" to be "Succeeded or Failed"
Mar  2 01:56:16.452: INFO: Pod "test-pod-bf45890e-433c-4115-aeeb-6ba5899b52e5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.349699ms
Mar  2 01:56:18.468: INFO: Pod "test-pod-bf45890e-433c-4115-aeeb-6ba5899b52e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040465175s
Mar  2 01:56:20.473: INFO: Pod "test-pod-bf45890e-433c-4115-aeeb-6ba5899b52e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045570265s
STEP: Saw pod success 03/02/23 01:56:20.473
Mar  2 01:56:20.473: INFO: Pod "test-pod-bf45890e-433c-4115-aeeb-6ba5899b52e5" satisfied condition "Succeeded or Failed"
Mar  2 01:56:20.491: INFO: Trying to get logs from node 10.132.92.143 pod test-pod-bf45890e-433c-4115-aeeb-6ba5899b52e5 container agnhost-container: <nil>
STEP: delete the pod 03/02/23 01:56:20.583
Mar  2 01:56:20.626: INFO: Waiting for pod test-pod-bf45890e-433c-4115-aeeb-6ba5899b52e5 to disappear
Mar  2 01:56:20.641: INFO: Pod test-pod-bf45890e-433c-4115-aeeb-6ba5899b52e5 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Mar  2 01:56:20.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5463" for this suite. 03/02/23 01:56:20.68
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","completed":145,"skipped":2787,"failed":0}
------------------------------
• [4.500 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:56:16.218
    Mar  2 01:56:16.218: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename svcaccounts 03/02/23 01:56:16.22
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:56:16.291
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:56:16.305
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:272
    STEP: Creating a pod to test service account token:  03/02/23 01:56:16.335
    Mar  2 01:56:16.427: INFO: Waiting up to 5m0s for pod "test-pod-bf45890e-433c-4115-aeeb-6ba5899b52e5" in namespace "svcaccounts-5463" to be "Succeeded or Failed"
    Mar  2 01:56:16.452: INFO: Pod "test-pod-bf45890e-433c-4115-aeeb-6ba5899b52e5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.349699ms
    Mar  2 01:56:18.468: INFO: Pod "test-pod-bf45890e-433c-4115-aeeb-6ba5899b52e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040465175s
    Mar  2 01:56:20.473: INFO: Pod "test-pod-bf45890e-433c-4115-aeeb-6ba5899b52e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045570265s
    STEP: Saw pod success 03/02/23 01:56:20.473
    Mar  2 01:56:20.473: INFO: Pod "test-pod-bf45890e-433c-4115-aeeb-6ba5899b52e5" satisfied condition "Succeeded or Failed"
    Mar  2 01:56:20.491: INFO: Trying to get logs from node 10.132.92.143 pod test-pod-bf45890e-433c-4115-aeeb-6ba5899b52e5 container agnhost-container: <nil>
    STEP: delete the pod 03/02/23 01:56:20.583
    Mar  2 01:56:20.626: INFO: Waiting for pod test-pod-bf45890e-433c-4115-aeeb-6ba5899b52e5 to disappear
    Mar  2 01:56:20.641: INFO: Pod test-pod-bf45890e-433c-4115-aeeb-6ba5899b52e5 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Mar  2 01:56:20.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-5463" for this suite. 03/02/23 01:56:20.68
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:56:20.722
Mar  2 01:56:20.722: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename endpointslice 03/02/23 01:56:20.723
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:56:20.8
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:56:20.814
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
Mar  2 01:56:20.898: INFO: Endpoints addresses: [172.20.0.1] , ports: [2040]
Mar  2 01:56:20.898: INFO: EndpointSlices addresses: [172.20.0.1] , ports: [2040]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Mar  2 01:56:20.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-4550" for this suite. 03/02/23 01:56:20.924
{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","completed":146,"skipped":2847,"failed":0}
------------------------------
• [0.236 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:56:20.722
    Mar  2 01:56:20.722: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename endpointslice 03/02/23 01:56:20.723
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:56:20.8
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:56:20.814
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:65
    Mar  2 01:56:20.898: INFO: Endpoints addresses: [172.20.0.1] , ports: [2040]
    Mar  2 01:56:20.898: INFO: EndpointSlices addresses: [172.20.0.1] , ports: [2040]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Mar  2 01:56:20.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-4550" for this suite. 03/02/23 01:56:20.924
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 01:56:20.961
Mar  2 01:56:20.961: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename container-probe 03/02/23 01:56:20.962
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:56:21.022
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:56:21.035
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
STEP: Creating pod test-webserver-ae9f84c0-38db-4dea-9b30-b642227a673a in namespace container-probe-6674 03/02/23 01:56:21.061
W0302 01:56:21.141773      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Mar  2 01:56:21.142: INFO: Waiting up to 5m0s for pod "test-webserver-ae9f84c0-38db-4dea-9b30-b642227a673a" in namespace "container-probe-6674" to be "not pending"
Mar  2 01:56:21.165: INFO: Pod "test-webserver-ae9f84c0-38db-4dea-9b30-b642227a673a": Phase="Pending", Reason="", readiness=false. Elapsed: 23.237025ms
Mar  2 01:56:23.200: INFO: Pod "test-webserver-ae9f84c0-38db-4dea-9b30-b642227a673a": Phase="Running", Reason="", readiness=true. Elapsed: 2.057805524s
Mar  2 01:56:23.200: INFO: Pod "test-webserver-ae9f84c0-38db-4dea-9b30-b642227a673a" satisfied condition "not pending"
Mar  2 01:56:23.200: INFO: Started pod test-webserver-ae9f84c0-38db-4dea-9b30-b642227a673a in namespace container-probe-6674
STEP: checking the pod's current state and verifying that restartCount is present 03/02/23 01:56:23.2
Mar  2 01:56:23.233: INFO: Initial restart count of pod test-webserver-ae9f84c0-38db-4dea-9b30-b642227a673a is 0
STEP: deleting the pod 03/02/23 02:00:23.629
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar  2 02:00:23.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6674" for this suite. 03/02/23 02:00:23.693
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":147,"skipped":2867,"failed":0}
------------------------------
• [SLOW TEST] [242.757 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 01:56:20.961
    Mar  2 01:56:20.961: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename container-probe 03/02/23 01:56:20.962
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 01:56:21.022
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 01:56:21.035
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:211
    STEP: Creating pod test-webserver-ae9f84c0-38db-4dea-9b30-b642227a673a in namespace container-probe-6674 03/02/23 01:56:21.061
    W0302 01:56:21.141773      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Mar  2 01:56:21.142: INFO: Waiting up to 5m0s for pod "test-webserver-ae9f84c0-38db-4dea-9b30-b642227a673a" in namespace "container-probe-6674" to be "not pending"
    Mar  2 01:56:21.165: INFO: Pod "test-webserver-ae9f84c0-38db-4dea-9b30-b642227a673a": Phase="Pending", Reason="", readiness=false. Elapsed: 23.237025ms
    Mar  2 01:56:23.200: INFO: Pod "test-webserver-ae9f84c0-38db-4dea-9b30-b642227a673a": Phase="Running", Reason="", readiness=true. Elapsed: 2.057805524s
    Mar  2 01:56:23.200: INFO: Pod "test-webserver-ae9f84c0-38db-4dea-9b30-b642227a673a" satisfied condition "not pending"
    Mar  2 01:56:23.200: INFO: Started pod test-webserver-ae9f84c0-38db-4dea-9b30-b642227a673a in namespace container-probe-6674
    STEP: checking the pod's current state and verifying that restartCount is present 03/02/23 01:56:23.2
    Mar  2 01:56:23.233: INFO: Initial restart count of pod test-webserver-ae9f84c0-38db-4dea-9b30-b642227a673a is 0
    STEP: deleting the pod 03/02/23 02:00:23.629
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar  2 02:00:23.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-6674" for this suite. 03/02/23 02:00:23.693
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:00:23.72
Mar  2 02:00:23.720: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename downward-api 03/02/23 02:00:23.721
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:00:23.777
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:00:23.792
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
STEP: Creating a pod to test downward API volume plugin 03/02/23 02:00:23.806
W0302 02:00:23.917187      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Mar  2 02:00:23.917: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9441d5e5-83ad-4124-9b7f-42c3e8f515cf" in namespace "downward-api-7802" to be "Succeeded or Failed"
Mar  2 02:00:23.942: INFO: Pod "downwardapi-volume-9441d5e5-83ad-4124-9b7f-42c3e8f515cf": Phase="Pending", Reason="", readiness=false. Elapsed: 24.73389ms
Mar  2 02:00:25.957: INFO: Pod "downwardapi-volume-9441d5e5-83ad-4124-9b7f-42c3e8f515cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040195465s
Mar  2 02:00:27.959: INFO: Pod "downwardapi-volume-9441d5e5-83ad-4124-9b7f-42c3e8f515cf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041548888s
Mar  2 02:00:29.961: INFO: Pod "downwardapi-volume-9441d5e5-83ad-4124-9b7f-42c3e8f515cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043529962s
STEP: Saw pod success 03/02/23 02:00:29.961
Mar  2 02:00:29.961: INFO: Pod "downwardapi-volume-9441d5e5-83ad-4124-9b7f-42c3e8f515cf" satisfied condition "Succeeded or Failed"
Mar  2 02:00:29.981: INFO: Trying to get logs from node 10.132.92.143 pod downwardapi-volume-9441d5e5-83ad-4124-9b7f-42c3e8f515cf container client-container: <nil>
STEP: delete the pod 03/02/23 02:00:30.045
Mar  2 02:00:30.093: INFO: Waiting for pod downwardapi-volume-9441d5e5-83ad-4124-9b7f-42c3e8f515cf to disappear
Mar  2 02:00:30.106: INFO: Pod downwardapi-volume-9441d5e5-83ad-4124-9b7f-42c3e8f515cf no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar  2 02:00:30.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7802" for this suite. 03/02/23 02:00:30.127
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":148,"skipped":2880,"failed":0}
------------------------------
• [SLOW TEST] [6.451 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:00:23.72
    Mar  2 02:00:23.720: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename downward-api 03/02/23 02:00:23.721
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:00:23.777
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:00:23.792
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:83
    STEP: Creating a pod to test downward API volume plugin 03/02/23 02:00:23.806
    W0302 02:00:23.917187      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Mar  2 02:00:23.917: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9441d5e5-83ad-4124-9b7f-42c3e8f515cf" in namespace "downward-api-7802" to be "Succeeded or Failed"
    Mar  2 02:00:23.942: INFO: Pod "downwardapi-volume-9441d5e5-83ad-4124-9b7f-42c3e8f515cf": Phase="Pending", Reason="", readiness=false. Elapsed: 24.73389ms
    Mar  2 02:00:25.957: INFO: Pod "downwardapi-volume-9441d5e5-83ad-4124-9b7f-42c3e8f515cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040195465s
    Mar  2 02:00:27.959: INFO: Pod "downwardapi-volume-9441d5e5-83ad-4124-9b7f-42c3e8f515cf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041548888s
    Mar  2 02:00:29.961: INFO: Pod "downwardapi-volume-9441d5e5-83ad-4124-9b7f-42c3e8f515cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043529962s
    STEP: Saw pod success 03/02/23 02:00:29.961
    Mar  2 02:00:29.961: INFO: Pod "downwardapi-volume-9441d5e5-83ad-4124-9b7f-42c3e8f515cf" satisfied condition "Succeeded or Failed"
    Mar  2 02:00:29.981: INFO: Trying to get logs from node 10.132.92.143 pod downwardapi-volume-9441d5e5-83ad-4124-9b7f-42c3e8f515cf container client-container: <nil>
    STEP: delete the pod 03/02/23 02:00:30.045
    Mar  2 02:00:30.093: INFO: Waiting for pod downwardapi-volume-9441d5e5-83ad-4124-9b7f-42c3e8f515cf to disappear
    Mar  2 02:00:30.106: INFO: Pod downwardapi-volume-9441d5e5-83ad-4124-9b7f-42c3e8f515cf no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar  2 02:00:30.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7802" for this suite. 03/02/23 02:00:30.127
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:00:30.172
Mar  2 02:00:30.172: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename dns 03/02/23 02:00:30.175
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:00:30.252
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:00:30.279
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 03/02/23 02:00:30.321
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 03/02/23 02:00:30.321
STEP: creating a pod to probe DNS 03/02/23 02:00:30.321
STEP: submitting the pod to kubernetes 03/02/23 02:00:30.321
Mar  2 02:00:30.463: INFO: Waiting up to 15m0s for pod "dns-test-73143c72-b313-45c3-8662-eac220a10bbc" in namespace "dns-2453" to be "running"
Mar  2 02:00:30.539: INFO: Pod "dns-test-73143c72-b313-45c3-8662-eac220a10bbc": Phase="Pending", Reason="", readiness=false. Elapsed: 76.082682ms
Mar  2 02:00:32.558: INFO: Pod "dns-test-73143c72-b313-45c3-8662-eac220a10bbc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.095423518s
Mar  2 02:00:34.556: INFO: Pod "dns-test-73143c72-b313-45c3-8662-eac220a10bbc": Phase="Running", Reason="", readiness=true. Elapsed: 4.093492611s
Mar  2 02:00:34.556: INFO: Pod "dns-test-73143c72-b313-45c3-8662-eac220a10bbc" satisfied condition "running"
STEP: retrieving the pod 03/02/23 02:00:34.556
STEP: looking for the results for each expected name from probers 03/02/23 02:00:34.584
Mar  2 02:00:34.688: INFO: DNS probes using dns-2453/dns-test-73143c72-b313-45c3-8662-eac220a10bbc succeeded

STEP: deleting the pod 03/02/23 02:00:34.688
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar  2 02:00:34.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2453" for this suite. 03/02/23 02:00:34.755
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","completed":149,"skipped":2885,"failed":0}
------------------------------
• [4.610 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:00:30.172
    Mar  2 02:00:30.172: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename dns 03/02/23 02:00:30.175
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:00:30.252
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:00:30.279
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     03/02/23 02:00:30.321
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     03/02/23 02:00:30.321
    STEP: creating a pod to probe DNS 03/02/23 02:00:30.321
    STEP: submitting the pod to kubernetes 03/02/23 02:00:30.321
    Mar  2 02:00:30.463: INFO: Waiting up to 15m0s for pod "dns-test-73143c72-b313-45c3-8662-eac220a10bbc" in namespace "dns-2453" to be "running"
    Mar  2 02:00:30.539: INFO: Pod "dns-test-73143c72-b313-45c3-8662-eac220a10bbc": Phase="Pending", Reason="", readiness=false. Elapsed: 76.082682ms
    Mar  2 02:00:32.558: INFO: Pod "dns-test-73143c72-b313-45c3-8662-eac220a10bbc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.095423518s
    Mar  2 02:00:34.556: INFO: Pod "dns-test-73143c72-b313-45c3-8662-eac220a10bbc": Phase="Running", Reason="", readiness=true. Elapsed: 4.093492611s
    Mar  2 02:00:34.556: INFO: Pod "dns-test-73143c72-b313-45c3-8662-eac220a10bbc" satisfied condition "running"
    STEP: retrieving the pod 03/02/23 02:00:34.556
    STEP: looking for the results for each expected name from probers 03/02/23 02:00:34.584
    Mar  2 02:00:34.688: INFO: DNS probes using dns-2453/dns-test-73143c72-b313-45c3-8662-eac220a10bbc succeeded

    STEP: deleting the pod 03/02/23 02:00:34.688
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar  2 02:00:34.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-2453" for this suite. 03/02/23 02:00:34.755
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:00:34.783
Mar  2 02:00:34.784: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename services 03/02/23 02:00:34.785
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:00:34.839
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:00:34.855
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  2 02:00:34.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3690" for this suite. 03/02/23 02:00:34.905
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","completed":150,"skipped":2888,"failed":0}
------------------------------
• [0.155 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:00:34.783
    Mar  2 02:00:34.784: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename services 03/02/23 02:00:34.785
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:00:34.839
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:00:34.855
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:781
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  2 02:00:34.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3690" for this suite. 03/02/23 02:00:34.905
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:00:34.94
Mar  2 02:00:34.940: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename statefulset 03/02/23 02:00:34.942
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:00:35.002
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:00:35.021
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-6590 03/02/23 02:00:35.045
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
STEP: Looking for a node to schedule stateful set and pod 03/02/23 02:00:35.133
STEP: Creating pod with conflicting port in namespace statefulset-6590 03/02/23 02:00:35.157
STEP: Waiting until pod test-pod will start running in namespace statefulset-6590 03/02/23 02:00:35.228
Mar  2 02:00:35.228: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-6590" to be "running"
Mar  2 02:00:35.243: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.97126ms
Mar  2 02:00:37.259: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031018102s
Mar  2 02:00:39.259: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=false. Elapsed: 4.03134146s
Mar  2 02:00:39.260: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-6590 03/02/23 02:00:39.26
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6590 03/02/23 02:00:39.284
Mar  2 02:00:39.363: INFO: Observed stateful pod in namespace: statefulset-6590, name: ss-0, uid: f1c631b4-862e-4c7f-bed8-ffa8f5f0c8fc, status phase: Pending. Waiting for statefulset controller to delete.
Mar  2 02:00:39.411: INFO: Observed stateful pod in namespace: statefulset-6590, name: ss-0, uid: f1c631b4-862e-4c7f-bed8-ffa8f5f0c8fc, status phase: Failed. Waiting for statefulset controller to delete.
Mar  2 02:00:39.475: INFO: Observed stateful pod in namespace: statefulset-6590, name: ss-0, uid: f1c631b4-862e-4c7f-bed8-ffa8f5f0c8fc, status phase: Failed. Waiting for statefulset controller to delete.
Mar  2 02:00:39.505: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6590
STEP: Removing pod with conflicting port in namespace statefulset-6590 03/02/23 02:00:39.505
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6590 and will be in running state 03/02/23 02:00:39.554
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar  2 02:00:43.651: INFO: Deleting all statefulset in ns statefulset-6590
Mar  2 02:00:43.671: INFO: Scaling statefulset ss to 0
Mar  2 02:00:53.754: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 02:00:53.777: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar  2 02:00:53.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6590" for this suite. 03/02/23 02:00:53.889
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","completed":151,"skipped":2894,"failed":0}
------------------------------
• [SLOW TEST] [18.976 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:737

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:00:34.94
    Mar  2 02:00:34.940: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename statefulset 03/02/23 02:00:34.942
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:00:35.002
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:00:35.021
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-6590 03/02/23 02:00:35.045
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:737
    STEP: Looking for a node to schedule stateful set and pod 03/02/23 02:00:35.133
    STEP: Creating pod with conflicting port in namespace statefulset-6590 03/02/23 02:00:35.157
    STEP: Waiting until pod test-pod will start running in namespace statefulset-6590 03/02/23 02:00:35.228
    Mar  2 02:00:35.228: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-6590" to be "running"
    Mar  2 02:00:35.243: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.97126ms
    Mar  2 02:00:37.259: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031018102s
    Mar  2 02:00:39.259: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=false. Elapsed: 4.03134146s
    Mar  2 02:00:39.260: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-6590 03/02/23 02:00:39.26
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6590 03/02/23 02:00:39.284
    Mar  2 02:00:39.363: INFO: Observed stateful pod in namespace: statefulset-6590, name: ss-0, uid: f1c631b4-862e-4c7f-bed8-ffa8f5f0c8fc, status phase: Pending. Waiting for statefulset controller to delete.
    Mar  2 02:00:39.411: INFO: Observed stateful pod in namespace: statefulset-6590, name: ss-0, uid: f1c631b4-862e-4c7f-bed8-ffa8f5f0c8fc, status phase: Failed. Waiting for statefulset controller to delete.
    Mar  2 02:00:39.475: INFO: Observed stateful pod in namespace: statefulset-6590, name: ss-0, uid: f1c631b4-862e-4c7f-bed8-ffa8f5f0c8fc, status phase: Failed. Waiting for statefulset controller to delete.
    Mar  2 02:00:39.505: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6590
    STEP: Removing pod with conflicting port in namespace statefulset-6590 03/02/23 02:00:39.505
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6590 and will be in running state 03/02/23 02:00:39.554
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar  2 02:00:43.651: INFO: Deleting all statefulset in ns statefulset-6590
    Mar  2 02:00:43.671: INFO: Scaling statefulset ss to 0
    Mar  2 02:00:53.754: INFO: Waiting for statefulset status.replicas updated to 0
    Mar  2 02:00:53.777: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar  2 02:00:53.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-6590" for this suite. 03/02/23 02:00:53.889
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:00:53.917
Mar  2 02:00:53.917: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename kubectl 03/02/23 02:00:53.918
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:00:53.975
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:00:53.987
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
Mar  2 02:00:54.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-8521 version'
Mar  2 02:00:54.101: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Mar  2 02:00:54.101: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.4\", GitCommit:\"872a965c6c6526caa949f0c6ac028ef7aff3fb78\", GitTreeState:\"clean\", BuildDate:\"2022-11-09T13:36:36Z\", GoVersion:\"go1.19.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.4+a34b9e9\", GitCommit:\"b6d1f054747e9886f61dd85316deac3415e2726f\", GitTreeState:\"clean\", BuildDate:\"2023-01-10T15:55:28Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  2 02:00:54.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8521" for this suite. 03/02/23 02:00:54.131
{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","completed":152,"skipped":2895,"failed":0}
------------------------------
• [0.257 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1677
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1683

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:00:53.917
    Mar  2 02:00:53.917: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename kubectl 03/02/23 02:00:53.918
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:00:53.975
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:00:53.987
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1683
    Mar  2 02:00:54.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-8521 version'
    Mar  2 02:00:54.101: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Mar  2 02:00:54.101: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.4\", GitCommit:\"872a965c6c6526caa949f0c6ac028ef7aff3fb78\", GitTreeState:\"clean\", BuildDate:\"2022-11-09T13:36:36Z\", GoVersion:\"go1.19.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.4+a34b9e9\", GitCommit:\"b6d1f054747e9886f61dd85316deac3415e2726f\", GitTreeState:\"clean\", BuildDate:\"2023-01-10T15:55:28Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  2 02:00:54.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8521" for this suite. 03/02/23 02:00:54.131
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:00:54.177
Mar  2 02:00:54.177: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename projected 03/02/23 02:00:54.178
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:00:54.239
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:00:54.256
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
STEP: Creating projection with secret that has name projected-secret-test-949bf476-6973-4446-a8ab-81d3658567d3 03/02/23 02:00:54.27
STEP: Creating a pod to test consume secrets 03/02/23 02:00:54.294
Mar  2 02:00:54.375: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-299f2cee-6bb1-45b0-89fc-1ef4e0b4b57b" in namespace "projected-8072" to be "Succeeded or Failed"
Mar  2 02:00:54.398: INFO: Pod "pod-projected-secrets-299f2cee-6bb1-45b0-89fc-1ef4e0b4b57b": Phase="Pending", Reason="", readiness=false. Elapsed: 22.822889ms
Mar  2 02:00:56.441: INFO: Pod "pod-projected-secrets-299f2cee-6bb1-45b0-89fc-1ef4e0b4b57b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.065680843s
Mar  2 02:00:58.412: INFO: Pod "pod-projected-secrets-299f2cee-6bb1-45b0-89fc-1ef4e0b4b57b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036568373s
Mar  2 02:01:00.416: INFO: Pod "pod-projected-secrets-299f2cee-6bb1-45b0-89fc-1ef4e0b4b57b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040979777s
STEP: Saw pod success 03/02/23 02:01:00.416
Mar  2 02:01:00.416: INFO: Pod "pod-projected-secrets-299f2cee-6bb1-45b0-89fc-1ef4e0b4b57b" satisfied condition "Succeeded or Failed"
Mar  2 02:01:00.436: INFO: Trying to get logs from node 10.132.92.143 pod pod-projected-secrets-299f2cee-6bb1-45b0-89fc-1ef4e0b4b57b container projected-secret-volume-test: <nil>
STEP: delete the pod 03/02/23 02:01:00.468
Mar  2 02:01:00.503: INFO: Waiting for pod pod-projected-secrets-299f2cee-6bb1-45b0-89fc-1ef4e0b4b57b to disappear
Mar  2 02:01:00.515: INFO: Pod pod-projected-secrets-299f2cee-6bb1-45b0-89fc-1ef4e0b4b57b no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Mar  2 02:01:00.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8072" for this suite. 03/02/23 02:01:00.543
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","completed":153,"skipped":2948,"failed":0}
------------------------------
• [SLOW TEST] [6.402 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:00:54.177
    Mar  2 02:00:54.177: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename projected 03/02/23 02:00:54.178
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:00:54.239
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:00:54.256
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:45
    STEP: Creating projection with secret that has name projected-secret-test-949bf476-6973-4446-a8ab-81d3658567d3 03/02/23 02:00:54.27
    STEP: Creating a pod to test consume secrets 03/02/23 02:00:54.294
    Mar  2 02:00:54.375: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-299f2cee-6bb1-45b0-89fc-1ef4e0b4b57b" in namespace "projected-8072" to be "Succeeded or Failed"
    Mar  2 02:00:54.398: INFO: Pod "pod-projected-secrets-299f2cee-6bb1-45b0-89fc-1ef4e0b4b57b": Phase="Pending", Reason="", readiness=false. Elapsed: 22.822889ms
    Mar  2 02:00:56.441: INFO: Pod "pod-projected-secrets-299f2cee-6bb1-45b0-89fc-1ef4e0b4b57b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.065680843s
    Mar  2 02:00:58.412: INFO: Pod "pod-projected-secrets-299f2cee-6bb1-45b0-89fc-1ef4e0b4b57b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036568373s
    Mar  2 02:01:00.416: INFO: Pod "pod-projected-secrets-299f2cee-6bb1-45b0-89fc-1ef4e0b4b57b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040979777s
    STEP: Saw pod success 03/02/23 02:01:00.416
    Mar  2 02:01:00.416: INFO: Pod "pod-projected-secrets-299f2cee-6bb1-45b0-89fc-1ef4e0b4b57b" satisfied condition "Succeeded or Failed"
    Mar  2 02:01:00.436: INFO: Trying to get logs from node 10.132.92.143 pod pod-projected-secrets-299f2cee-6bb1-45b0-89fc-1ef4e0b4b57b container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/02/23 02:01:00.468
    Mar  2 02:01:00.503: INFO: Waiting for pod pod-projected-secrets-299f2cee-6bb1-45b0-89fc-1ef4e0b4b57b to disappear
    Mar  2 02:01:00.515: INFO: Pod pod-projected-secrets-299f2cee-6bb1-45b0-89fc-1ef4e0b4b57b no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Mar  2 02:01:00.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8072" for this suite. 03/02/23 02:01:00.543
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:01:00.579
Mar  2 02:01:00.579: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename runtimeclass 03/02/23 02:01:00.58
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:01:00.639
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:01:00.651
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-2290-delete-me 03/02/23 02:01:00.682
STEP: Waiting for the RuntimeClass to disappear 03/02/23 02:01:00.712
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Mar  2 02:01:00.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-2290" for this suite. 03/02/23 02:01:00.824
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]","completed":154,"skipped":2950,"failed":0}
------------------------------
• [0.281 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:01:00.579
    Mar  2 02:01:00.579: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename runtimeclass 03/02/23 02:01:00.58
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:01:00.639
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:01:00.651
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-2290-delete-me 03/02/23 02:01:00.682
    STEP: Waiting for the RuntimeClass to disappear 03/02/23 02:01:00.712
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Mar  2 02:01:00.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-2290" for this suite. 03/02/23 02:01:00.824
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:01:00.86
Mar  2 02:01:00.860: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename subpath 03/02/23 02:01:00.861
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:01:00.921
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:01:00.934
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/02/23 02:01:00.947
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-rmph 03/02/23 02:01:00.996
STEP: Creating a pod to test atomic-volume-subpath 03/02/23 02:01:00.996
Mar  2 02:01:01.082: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-rmph" in namespace "subpath-3951" to be "Succeeded or Failed"
Mar  2 02:01:01.111: INFO: Pod "pod-subpath-test-projected-rmph": Phase="Pending", Reason="", readiness=false. Elapsed: 28.827458ms
Mar  2 02:01:03.127: INFO: Pod "pod-subpath-test-projected-rmph": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044190591s
Mar  2 02:01:05.130: INFO: Pod "pod-subpath-test-projected-rmph": Phase="Running", Reason="", readiness=true. Elapsed: 4.047320201s
Mar  2 02:01:07.125: INFO: Pod "pod-subpath-test-projected-rmph": Phase="Running", Reason="", readiness=true. Elapsed: 6.042262558s
Mar  2 02:01:09.127: INFO: Pod "pod-subpath-test-projected-rmph": Phase="Running", Reason="", readiness=true. Elapsed: 8.044924302s
Mar  2 02:01:11.128: INFO: Pod "pod-subpath-test-projected-rmph": Phase="Running", Reason="", readiness=true. Elapsed: 10.045174314s
Mar  2 02:01:13.186: INFO: Pod "pod-subpath-test-projected-rmph": Phase="Running", Reason="", readiness=true. Elapsed: 12.103677489s
Mar  2 02:01:15.131: INFO: Pod "pod-subpath-test-projected-rmph": Phase="Running", Reason="", readiness=true. Elapsed: 14.048172598s
Mar  2 02:01:17.128: INFO: Pod "pod-subpath-test-projected-rmph": Phase="Running", Reason="", readiness=true. Elapsed: 16.0451604s
Mar  2 02:01:19.128: INFO: Pod "pod-subpath-test-projected-rmph": Phase="Running", Reason="", readiness=true. Elapsed: 18.045881872s
Mar  2 02:01:21.132: INFO: Pod "pod-subpath-test-projected-rmph": Phase="Running", Reason="", readiness=true. Elapsed: 20.049408388s
Mar  2 02:01:23.168: INFO: Pod "pod-subpath-test-projected-rmph": Phase="Running", Reason="", readiness=true. Elapsed: 22.085131074s
Mar  2 02:01:25.129: INFO: Pod "pod-subpath-test-projected-rmph": Phase="Running", Reason="", readiness=false. Elapsed: 24.046694861s
Mar  2 02:01:27.167: INFO: Pod "pod-subpath-test-projected-rmph": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.084884532s
STEP: Saw pod success 03/02/23 02:01:27.167
Mar  2 02:01:27.168: INFO: Pod "pod-subpath-test-projected-rmph" satisfied condition "Succeeded or Failed"
Mar  2 02:01:27.205: INFO: Trying to get logs from node 10.132.92.143 pod pod-subpath-test-projected-rmph container test-container-subpath-projected-rmph: <nil>
STEP: delete the pod 03/02/23 02:01:27.318
Mar  2 02:01:27.495: INFO: Waiting for pod pod-subpath-test-projected-rmph to disappear
Mar  2 02:01:27.584: INFO: Pod pod-subpath-test-projected-rmph no longer exists
STEP: Deleting pod pod-subpath-test-projected-rmph 03/02/23 02:01:27.584
Mar  2 02:01:27.632: INFO: Deleting pod "pod-subpath-test-projected-rmph" in namespace "subpath-3951"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Mar  2 02:01:27.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3951" for this suite. 03/02/23 02:01:27.782
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]","completed":155,"skipped":2960,"failed":0}
------------------------------
• [SLOW TEST] [26.984 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:01:00.86
    Mar  2 02:01:00.860: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename subpath 03/02/23 02:01:00.861
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:01:00.921
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:01:00.934
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/02/23 02:01:00.947
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-rmph 03/02/23 02:01:00.996
    STEP: Creating a pod to test atomic-volume-subpath 03/02/23 02:01:00.996
    Mar  2 02:01:01.082: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-rmph" in namespace "subpath-3951" to be "Succeeded or Failed"
    Mar  2 02:01:01.111: INFO: Pod "pod-subpath-test-projected-rmph": Phase="Pending", Reason="", readiness=false. Elapsed: 28.827458ms
    Mar  2 02:01:03.127: INFO: Pod "pod-subpath-test-projected-rmph": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044190591s
    Mar  2 02:01:05.130: INFO: Pod "pod-subpath-test-projected-rmph": Phase="Running", Reason="", readiness=true. Elapsed: 4.047320201s
    Mar  2 02:01:07.125: INFO: Pod "pod-subpath-test-projected-rmph": Phase="Running", Reason="", readiness=true. Elapsed: 6.042262558s
    Mar  2 02:01:09.127: INFO: Pod "pod-subpath-test-projected-rmph": Phase="Running", Reason="", readiness=true. Elapsed: 8.044924302s
    Mar  2 02:01:11.128: INFO: Pod "pod-subpath-test-projected-rmph": Phase="Running", Reason="", readiness=true. Elapsed: 10.045174314s
    Mar  2 02:01:13.186: INFO: Pod "pod-subpath-test-projected-rmph": Phase="Running", Reason="", readiness=true. Elapsed: 12.103677489s
    Mar  2 02:01:15.131: INFO: Pod "pod-subpath-test-projected-rmph": Phase="Running", Reason="", readiness=true. Elapsed: 14.048172598s
    Mar  2 02:01:17.128: INFO: Pod "pod-subpath-test-projected-rmph": Phase="Running", Reason="", readiness=true. Elapsed: 16.0451604s
    Mar  2 02:01:19.128: INFO: Pod "pod-subpath-test-projected-rmph": Phase="Running", Reason="", readiness=true. Elapsed: 18.045881872s
    Mar  2 02:01:21.132: INFO: Pod "pod-subpath-test-projected-rmph": Phase="Running", Reason="", readiness=true. Elapsed: 20.049408388s
    Mar  2 02:01:23.168: INFO: Pod "pod-subpath-test-projected-rmph": Phase="Running", Reason="", readiness=true. Elapsed: 22.085131074s
    Mar  2 02:01:25.129: INFO: Pod "pod-subpath-test-projected-rmph": Phase="Running", Reason="", readiness=false. Elapsed: 24.046694861s
    Mar  2 02:01:27.167: INFO: Pod "pod-subpath-test-projected-rmph": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.084884532s
    STEP: Saw pod success 03/02/23 02:01:27.167
    Mar  2 02:01:27.168: INFO: Pod "pod-subpath-test-projected-rmph" satisfied condition "Succeeded or Failed"
    Mar  2 02:01:27.205: INFO: Trying to get logs from node 10.132.92.143 pod pod-subpath-test-projected-rmph container test-container-subpath-projected-rmph: <nil>
    STEP: delete the pod 03/02/23 02:01:27.318
    Mar  2 02:01:27.495: INFO: Waiting for pod pod-subpath-test-projected-rmph to disappear
    Mar  2 02:01:27.584: INFO: Pod pod-subpath-test-projected-rmph no longer exists
    STEP: Deleting pod pod-subpath-test-projected-rmph 03/02/23 02:01:27.584
    Mar  2 02:01:27.632: INFO: Deleting pod "pod-subpath-test-projected-rmph" in namespace "subpath-3951"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Mar  2 02:01:27.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-3951" for this suite. 03/02/23 02:01:27.782
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:01:27.914
Mar  2 02:01:27.981: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename sched-preemption 03/02/23 02:01:27.991
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:01:28.081
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:01:28.138
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Mar  2 02:01:28.280: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  2 02:02:28.688: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:02:28.744
Mar  2 02:02:28.745: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename sched-preemption-path 03/02/23 02:02:28.745
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:02:28.869
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:02:28.895
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:690
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
Mar  2 02:02:29.024: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Mar  2 02:02:29.064: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/framework.go:187
Mar  2 02:02:29.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-5229" for this suite. 03/02/23 02:02:29.199
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:706
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Mar  2 02:02:29.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-8983" for this suite. 03/02/23 02:02:29.316
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","completed":156,"skipped":2979,"failed":0}
------------------------------
• [SLOW TEST] [61.661 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:683
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:733

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:01:27.914
    Mar  2 02:01:27.981: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename sched-preemption 03/02/23 02:01:27.991
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:01:28.081
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:01:28.138
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Mar  2 02:01:28.280: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar  2 02:02:28.688: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:02:28.744
    Mar  2 02:02:28.745: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename sched-preemption-path 03/02/23 02:02:28.745
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:02:28.869
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:02:28.895
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:690
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:733
    Mar  2 02:02:29.024: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Mar  2 02:02:29.064: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/framework.go:187
    Mar  2 02:02:29.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-5229" for this suite. 03/02/23 02:02:29.199
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:706
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Mar  2 02:02:29.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-8983" for this suite. 03/02/23 02:02:29.316
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:02:29.58
Mar  2 02:02:29.580: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename secrets 03/02/23 02:02:29.581
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:02:29.655
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:02:29.667
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
STEP: Creating secret with name secret-test-fedbb071-6729-40f7-8dbb-99a745908249 03/02/23 02:02:29.685
STEP: Creating a pod to test consume secrets 03/02/23 02:02:29.703
Mar  2 02:02:29.784: INFO: Waiting up to 5m0s for pod "pod-secrets-2845e62f-2f71-4c21-8472-ebc4debb70cf" in namespace "secrets-16" to be "Succeeded or Failed"
Mar  2 02:02:29.804: INFO: Pod "pod-secrets-2845e62f-2f71-4c21-8472-ebc4debb70cf": Phase="Pending", Reason="", readiness=false. Elapsed: 20.082768ms
Mar  2 02:02:31.829: INFO: Pod "pod-secrets-2845e62f-2f71-4c21-8472-ebc4debb70cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045019952s
Mar  2 02:02:33.819: INFO: Pod "pod-secrets-2845e62f-2f71-4c21-8472-ebc4debb70cf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035053145s
Mar  2 02:02:35.834: INFO: Pod "pod-secrets-2845e62f-2f71-4c21-8472-ebc4debb70cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.050456469s
STEP: Saw pod success 03/02/23 02:02:35.834
Mar  2 02:02:35.834: INFO: Pod "pod-secrets-2845e62f-2f71-4c21-8472-ebc4debb70cf" satisfied condition "Succeeded or Failed"
Mar  2 02:02:35.854: INFO: Trying to get logs from node 10.132.92.143 pod pod-secrets-2845e62f-2f71-4c21-8472-ebc4debb70cf container secret-volume-test: <nil>
STEP: delete the pod 03/02/23 02:02:35.917
Mar  2 02:02:35.978: INFO: Waiting for pod pod-secrets-2845e62f-2f71-4c21-8472-ebc4debb70cf to disappear
Mar  2 02:02:35.992: INFO: Pod pod-secrets-2845e62f-2f71-4c21-8472-ebc4debb70cf no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar  2 02:02:35.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-16" for this suite. 03/02/23 02:02:36.019
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":157,"skipped":2992,"failed":0}
------------------------------
• [SLOW TEST] [6.477 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:02:29.58
    Mar  2 02:02:29.580: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename secrets 03/02/23 02:02:29.581
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:02:29.655
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:02:29.667
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:124
    STEP: Creating secret with name secret-test-fedbb071-6729-40f7-8dbb-99a745908249 03/02/23 02:02:29.685
    STEP: Creating a pod to test consume secrets 03/02/23 02:02:29.703
    Mar  2 02:02:29.784: INFO: Waiting up to 5m0s for pod "pod-secrets-2845e62f-2f71-4c21-8472-ebc4debb70cf" in namespace "secrets-16" to be "Succeeded or Failed"
    Mar  2 02:02:29.804: INFO: Pod "pod-secrets-2845e62f-2f71-4c21-8472-ebc4debb70cf": Phase="Pending", Reason="", readiness=false. Elapsed: 20.082768ms
    Mar  2 02:02:31.829: INFO: Pod "pod-secrets-2845e62f-2f71-4c21-8472-ebc4debb70cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045019952s
    Mar  2 02:02:33.819: INFO: Pod "pod-secrets-2845e62f-2f71-4c21-8472-ebc4debb70cf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035053145s
    Mar  2 02:02:35.834: INFO: Pod "pod-secrets-2845e62f-2f71-4c21-8472-ebc4debb70cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.050456469s
    STEP: Saw pod success 03/02/23 02:02:35.834
    Mar  2 02:02:35.834: INFO: Pod "pod-secrets-2845e62f-2f71-4c21-8472-ebc4debb70cf" satisfied condition "Succeeded or Failed"
    Mar  2 02:02:35.854: INFO: Trying to get logs from node 10.132.92.143 pod pod-secrets-2845e62f-2f71-4c21-8472-ebc4debb70cf container secret-volume-test: <nil>
    STEP: delete the pod 03/02/23 02:02:35.917
    Mar  2 02:02:35.978: INFO: Waiting for pod pod-secrets-2845e62f-2f71-4c21-8472-ebc4debb70cf to disappear
    Mar  2 02:02:35.992: INFO: Pod pod-secrets-2845e62f-2f71-4c21-8472-ebc4debb70cf no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar  2 02:02:35.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-16" for this suite. 03/02/23 02:02:36.019
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:02:36.057
Mar  2 02:02:36.057: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename kubectl 03/02/23 02:02:36.059
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:02:36.148
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:02:36.165
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
STEP: creating a replication controller 03/02/23 02:02:36.182
Mar  2 02:02:36.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-6663 create -f -'
Mar  2 02:02:37.318: INFO: stderr: ""
Mar  2 02:02:37.318: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 03/02/23 02:02:37.318
Mar  2 02:02:37.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-6663 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 02:02:37.431: INFO: stderr: ""
Mar  2 02:02:37.431: INFO: stdout: "update-demo-nautilus-k7tlq "
STEP: Replicas for name=update-demo: expected=2 actual=1 03/02/23 02:02:37.431
Mar  2 02:02:42.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-6663 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 02:02:42.653: INFO: stderr: ""
Mar  2 02:02:42.653: INFO: stdout: "update-demo-nautilus-k7tlq update-demo-nautilus-r9bjq "
Mar  2 02:02:42.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-6663 get pods update-demo-nautilus-k7tlq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 02:02:42.895: INFO: stderr: ""
Mar  2 02:02:42.895: INFO: stdout: ""
Mar  2 02:02:42.895: INFO: update-demo-nautilus-k7tlq is created but not running
Mar  2 02:02:47.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-6663 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 02:02:48.035: INFO: stderr: ""
Mar  2 02:02:48.035: INFO: stdout: "update-demo-nautilus-k7tlq update-demo-nautilus-r9bjq "
Mar  2 02:02:48.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-6663 get pods update-demo-nautilus-k7tlq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 02:02:48.175: INFO: stderr: ""
Mar  2 02:02:48.175: INFO: stdout: "true"
Mar  2 02:02:48.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-6663 get pods update-demo-nautilus-k7tlq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  2 02:02:48.286: INFO: stderr: ""
Mar  2 02:02:48.286: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar  2 02:02:48.286: INFO: validating pod update-demo-nautilus-k7tlq
Mar  2 02:02:48.314: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 02:02:48.314: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 02:02:48.314: INFO: update-demo-nautilus-k7tlq is verified up and running
Mar  2 02:02:48.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-6663 get pods update-demo-nautilus-r9bjq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 02:02:48.441: INFO: stderr: ""
Mar  2 02:02:48.441: INFO: stdout: "true"
Mar  2 02:02:48.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-6663 get pods update-demo-nautilus-r9bjq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  2 02:02:48.557: INFO: stderr: ""
Mar  2 02:02:48.557: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar  2 02:02:48.557: INFO: validating pod update-demo-nautilus-r9bjq
Mar  2 02:02:48.603: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 02:02:48.603: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 02:02:48.603: INFO: update-demo-nautilus-r9bjq is verified up and running
STEP: using delete to clean up resources 03/02/23 02:02:48.603
Mar  2 02:02:48.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-6663 delete --grace-period=0 --force -f -'
Mar  2 02:02:48.748: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 02:02:48.748: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar  2 02:02:48.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-6663 get rc,svc -l name=update-demo --no-headers'
Mar  2 02:02:48.934: INFO: stderr: "No resources found in kubectl-6663 namespace.\n"
Mar  2 02:02:48.934: INFO: stdout: ""
Mar  2 02:02:48.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-6663 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  2 02:02:49.113: INFO: stderr: ""
Mar  2 02:02:49.113: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  2 02:02:49.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6663" for this suite. 03/02/23 02:02:49.149
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","completed":158,"skipped":2994,"failed":0}
------------------------------
• [SLOW TEST] [13.124 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:337

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:02:36.057
    Mar  2 02:02:36.057: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename kubectl 03/02/23 02:02:36.059
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:02:36.148
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:02:36.165
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:337
    STEP: creating a replication controller 03/02/23 02:02:36.182
    Mar  2 02:02:36.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-6663 create -f -'
    Mar  2 02:02:37.318: INFO: stderr: ""
    Mar  2 02:02:37.318: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 03/02/23 02:02:37.318
    Mar  2 02:02:37.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-6663 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar  2 02:02:37.431: INFO: stderr: ""
    Mar  2 02:02:37.431: INFO: stdout: "update-demo-nautilus-k7tlq "
    STEP: Replicas for name=update-demo: expected=2 actual=1 03/02/23 02:02:37.431
    Mar  2 02:02:42.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-6663 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar  2 02:02:42.653: INFO: stderr: ""
    Mar  2 02:02:42.653: INFO: stdout: "update-demo-nautilus-k7tlq update-demo-nautilus-r9bjq "
    Mar  2 02:02:42.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-6663 get pods update-demo-nautilus-k7tlq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar  2 02:02:42.895: INFO: stderr: ""
    Mar  2 02:02:42.895: INFO: stdout: ""
    Mar  2 02:02:42.895: INFO: update-demo-nautilus-k7tlq is created but not running
    Mar  2 02:02:47.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-6663 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar  2 02:02:48.035: INFO: stderr: ""
    Mar  2 02:02:48.035: INFO: stdout: "update-demo-nautilus-k7tlq update-demo-nautilus-r9bjq "
    Mar  2 02:02:48.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-6663 get pods update-demo-nautilus-k7tlq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar  2 02:02:48.175: INFO: stderr: ""
    Mar  2 02:02:48.175: INFO: stdout: "true"
    Mar  2 02:02:48.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-6663 get pods update-demo-nautilus-k7tlq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar  2 02:02:48.286: INFO: stderr: ""
    Mar  2 02:02:48.286: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar  2 02:02:48.286: INFO: validating pod update-demo-nautilus-k7tlq
    Mar  2 02:02:48.314: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar  2 02:02:48.314: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar  2 02:02:48.314: INFO: update-demo-nautilus-k7tlq is verified up and running
    Mar  2 02:02:48.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-6663 get pods update-demo-nautilus-r9bjq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar  2 02:02:48.441: INFO: stderr: ""
    Mar  2 02:02:48.441: INFO: stdout: "true"
    Mar  2 02:02:48.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-6663 get pods update-demo-nautilus-r9bjq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar  2 02:02:48.557: INFO: stderr: ""
    Mar  2 02:02:48.557: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar  2 02:02:48.557: INFO: validating pod update-demo-nautilus-r9bjq
    Mar  2 02:02:48.603: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar  2 02:02:48.603: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar  2 02:02:48.603: INFO: update-demo-nautilus-r9bjq is verified up and running
    STEP: using delete to clean up resources 03/02/23 02:02:48.603
    Mar  2 02:02:48.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-6663 delete --grace-period=0 --force -f -'
    Mar  2 02:02:48.748: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar  2 02:02:48.748: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Mar  2 02:02:48.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-6663 get rc,svc -l name=update-demo --no-headers'
    Mar  2 02:02:48.934: INFO: stderr: "No resources found in kubectl-6663 namespace.\n"
    Mar  2 02:02:48.934: INFO: stdout: ""
    Mar  2 02:02:48.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-6663 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Mar  2 02:02:49.113: INFO: stderr: ""
    Mar  2 02:02:49.113: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  2 02:02:49.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6663" for this suite. 03/02/23 02:02:49.149
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:02:49.181
Mar  2 02:02:49.181: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename configmap 03/02/23 02:02:49.182
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:02:49.271
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:02:49.318
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
Mar  2 02:02:49.419: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-898849dc-9b90-435f-8e63-018ae7a7959b 03/02/23 02:02:49.419
STEP: Creating the pod 03/02/23 02:02:49.44
Mar  2 02:02:49.496: INFO: Waiting up to 5m0s for pod "pod-configmaps-beaced90-0af5-4d84-8dea-75c2de3a9462" in namespace "configmap-494" to be "running"
Mar  2 02:02:49.530: INFO: Pod "pod-configmaps-beaced90-0af5-4d84-8dea-75c2de3a9462": Phase="Pending", Reason="", readiness=false. Elapsed: 34.344773ms
Mar  2 02:02:51.548: INFO: Pod "pod-configmaps-beaced90-0af5-4d84-8dea-75c2de3a9462": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052050546s
Mar  2 02:02:53.546: INFO: Pod "pod-configmaps-beaced90-0af5-4d84-8dea-75c2de3a9462": Phase="Running", Reason="", readiness=false. Elapsed: 4.049572336s
Mar  2 02:02:53.546: INFO: Pod "pod-configmaps-beaced90-0af5-4d84-8dea-75c2de3a9462" satisfied condition "running"
STEP: Waiting for pod with text data 03/02/23 02:02:53.546
STEP: Waiting for pod with binary data 03/02/23 02:02:53.577
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar  2 02:02:53.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-494" for this suite. 03/02/23 02:02:53.637
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","completed":159,"skipped":2998,"failed":0}
------------------------------
• [4.482 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:02:49.181
    Mar  2 02:02:49.181: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename configmap 03/02/23 02:02:49.182
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:02:49.271
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:02:49.318
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:174
    Mar  2 02:02:49.419: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating configMap with name configmap-test-upd-898849dc-9b90-435f-8e63-018ae7a7959b 03/02/23 02:02:49.419
    STEP: Creating the pod 03/02/23 02:02:49.44
    Mar  2 02:02:49.496: INFO: Waiting up to 5m0s for pod "pod-configmaps-beaced90-0af5-4d84-8dea-75c2de3a9462" in namespace "configmap-494" to be "running"
    Mar  2 02:02:49.530: INFO: Pod "pod-configmaps-beaced90-0af5-4d84-8dea-75c2de3a9462": Phase="Pending", Reason="", readiness=false. Elapsed: 34.344773ms
    Mar  2 02:02:51.548: INFO: Pod "pod-configmaps-beaced90-0af5-4d84-8dea-75c2de3a9462": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052050546s
    Mar  2 02:02:53.546: INFO: Pod "pod-configmaps-beaced90-0af5-4d84-8dea-75c2de3a9462": Phase="Running", Reason="", readiness=false. Elapsed: 4.049572336s
    Mar  2 02:02:53.546: INFO: Pod "pod-configmaps-beaced90-0af5-4d84-8dea-75c2de3a9462" satisfied condition "running"
    STEP: Waiting for pod with text data 03/02/23 02:02:53.546
    STEP: Waiting for pod with binary data 03/02/23 02:02:53.577
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  2 02:02:53.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-494" for this suite. 03/02/23 02:02:53.637
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:02:53.68
Mar  2 02:02:53.680: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename pod-network-test 03/02/23 02:02:53.681
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:02:53.74
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:02:53.754
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-2256 03/02/23 02:02:53.768
STEP: creating a selector 03/02/23 02:02:53.768
STEP: Creating the service pods in kubernetes 03/02/23 02:02:53.768
Mar  2 02:02:53.768: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar  2 02:02:54.063: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2256" to be "running and ready"
Mar  2 02:02:54.085: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 22.63958ms
Mar  2 02:02:54.085: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 02:02:56.109: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045969868s
Mar  2 02:02:56.109: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 02:02:58.103: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.040584742s
Mar  2 02:02:58.103: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 02:03:00.105: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.042004409s
Mar  2 02:03:00.105: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 02:03:02.103: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.040260241s
Mar  2 02:03:02.103: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 02:03:04.102: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.038896567s
Mar  2 02:03:04.102: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 02:03:06.102: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.039346968s
Mar  2 02:03:06.102: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 02:03:08.103: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.040518161s
Mar  2 02:03:08.103: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 02:03:10.105: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.042086628s
Mar  2 02:03:10.105: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 02:03:12.100: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.037125716s
Mar  2 02:03:12.100: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 02:03:14.102: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.038952765s
Mar  2 02:03:14.102: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 02:03:16.101: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.038178973s
Mar  2 02:03:16.101: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Mar  2 02:03:16.101: INFO: Pod "netserver-0" satisfied condition "running and ready"
Mar  2 02:03:16.115: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2256" to be "running and ready"
Mar  2 02:03:16.136: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 20.297285ms
Mar  2 02:03:16.136: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Mar  2 02:03:16.136: INFO: Pod "netserver-1" satisfied condition "running and ready"
Mar  2 02:03:16.150: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2256" to be "running and ready"
Mar  2 02:03:16.164: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 13.674569ms
Mar  2 02:03:16.164: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Mar  2 02:03:16.164: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 03/02/23 02:03:16.178
Mar  2 02:03:16.261: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2256" to be "running"
Mar  2 02:03:16.275: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.168511ms
Mar  2 02:03:18.291: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029887421s
Mar  2 02:03:20.291: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.030474483s
Mar  2 02:03:20.291: INFO: Pod "test-container-pod" satisfied condition "running"
Mar  2 02:03:20.306: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-2256" to be "running"
Mar  2 02:03:20.320: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 14.28094ms
Mar  2 02:03:20.320: INFO: Pod "host-test-container-pod" satisfied condition "running"
Mar  2 02:03:20.335: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar  2 02:03:20.335: INFO: Going to poll 172.30.156.112 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Mar  2 02:03:20.349: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.156.112:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2256 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 02:03:20.349: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
Mar  2 02:03:20.349: INFO: ExecWithOptions: Clientset creation
Mar  2 02:03:20.349: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-2256/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.156.112%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar  2 02:03:20.621: INFO: Found all 1 expected endpoints: [netserver-0]
Mar  2 02:03:20.621: INFO: Going to poll 172.30.62.238 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Mar  2 02:03:20.646: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.62.238:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2256 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 02:03:20.646: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
Mar  2 02:03:20.646: INFO: ExecWithOptions: Clientset creation
Mar  2 02:03:20.647: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-2256/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.62.238%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar  2 02:03:21.003: INFO: Found all 1 expected endpoints: [netserver-1]
Mar  2 02:03:21.003: INFO: Going to poll 172.30.201.231 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Mar  2 02:03:21.017: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.201.231:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2256 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 02:03:21.017: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
Mar  2 02:03:21.018: INFO: ExecWithOptions: Clientset creation
Mar  2 02:03:21.018: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-2256/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.201.231%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar  2 02:03:21.264: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Mar  2 02:03:21.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2256" for this suite. 03/02/23 02:03:21.307
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","completed":160,"skipped":3076,"failed":0}
------------------------------
• [SLOW TEST] [27.655 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:02:53.68
    Mar  2 02:02:53.680: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename pod-network-test 03/02/23 02:02:53.681
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:02:53.74
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:02:53.754
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-2256 03/02/23 02:02:53.768
    STEP: creating a selector 03/02/23 02:02:53.768
    STEP: Creating the service pods in kubernetes 03/02/23 02:02:53.768
    Mar  2 02:02:53.768: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Mar  2 02:02:54.063: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2256" to be "running and ready"
    Mar  2 02:02:54.085: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 22.63958ms
    Mar  2 02:02:54.085: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 02:02:56.109: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045969868s
    Mar  2 02:02:56.109: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 02:02:58.103: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.040584742s
    Mar  2 02:02:58.103: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 02:03:00.105: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.042004409s
    Mar  2 02:03:00.105: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 02:03:02.103: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.040260241s
    Mar  2 02:03:02.103: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 02:03:04.102: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.038896567s
    Mar  2 02:03:04.102: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 02:03:06.102: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.039346968s
    Mar  2 02:03:06.102: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 02:03:08.103: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.040518161s
    Mar  2 02:03:08.103: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 02:03:10.105: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.042086628s
    Mar  2 02:03:10.105: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 02:03:12.100: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.037125716s
    Mar  2 02:03:12.100: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 02:03:14.102: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.038952765s
    Mar  2 02:03:14.102: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 02:03:16.101: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.038178973s
    Mar  2 02:03:16.101: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Mar  2 02:03:16.101: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Mar  2 02:03:16.115: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2256" to be "running and ready"
    Mar  2 02:03:16.136: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 20.297285ms
    Mar  2 02:03:16.136: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Mar  2 02:03:16.136: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Mar  2 02:03:16.150: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2256" to be "running and ready"
    Mar  2 02:03:16.164: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 13.674569ms
    Mar  2 02:03:16.164: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Mar  2 02:03:16.164: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 03/02/23 02:03:16.178
    Mar  2 02:03:16.261: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2256" to be "running"
    Mar  2 02:03:16.275: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.168511ms
    Mar  2 02:03:18.291: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029887421s
    Mar  2 02:03:20.291: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.030474483s
    Mar  2 02:03:20.291: INFO: Pod "test-container-pod" satisfied condition "running"
    Mar  2 02:03:20.306: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-2256" to be "running"
    Mar  2 02:03:20.320: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 14.28094ms
    Mar  2 02:03:20.320: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Mar  2 02:03:20.335: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Mar  2 02:03:20.335: INFO: Going to poll 172.30.156.112 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Mar  2 02:03:20.349: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.156.112:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2256 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 02:03:20.349: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    Mar  2 02:03:20.349: INFO: ExecWithOptions: Clientset creation
    Mar  2 02:03:20.349: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-2256/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.156.112%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar  2 02:03:20.621: INFO: Found all 1 expected endpoints: [netserver-0]
    Mar  2 02:03:20.621: INFO: Going to poll 172.30.62.238 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Mar  2 02:03:20.646: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.62.238:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2256 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 02:03:20.646: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    Mar  2 02:03:20.646: INFO: ExecWithOptions: Clientset creation
    Mar  2 02:03:20.647: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-2256/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.62.238%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar  2 02:03:21.003: INFO: Found all 1 expected endpoints: [netserver-1]
    Mar  2 02:03:21.003: INFO: Going to poll 172.30.201.231 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Mar  2 02:03:21.017: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.201.231:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2256 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 02:03:21.017: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    Mar  2 02:03:21.018: INFO: ExecWithOptions: Clientset creation
    Mar  2 02:03:21.018: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-2256/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.201.231%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar  2 02:03:21.264: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Mar  2 02:03:21.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-2256" for this suite. 03/02/23 02:03:21.307
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:03:21.337
Mar  2 02:03:21.338: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename custom-resource-definition 03/02/23 02:03:21.339
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:03:21.435
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:03:21.454
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Mar  2 02:03:21.467: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 02:03:22.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9273" for this suite. 03/02/23 02:03:22.405
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","completed":161,"skipped":3094,"failed":0}
------------------------------
• [1.138 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:03:21.337
    Mar  2 02:03:21.338: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename custom-resource-definition 03/02/23 02:03:21.339
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:03:21.435
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:03:21.454
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Mar  2 02:03:21.467: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 02:03:22.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-9273" for this suite. 03/02/23 02:03:22.405
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:03:22.478
Mar  2 02:03:22.478: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename crd-webhook 03/02/23 02:03:22.479
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:03:22.596
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:03:22.629
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 03/02/23 02:03:22.682
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 03/02/23 02:03:22.967
STEP: Deploying the custom resource conversion webhook pod 03/02/23 02:03:23.014
STEP: Wait for the deployment to be ready 03/02/23 02:03:23.084
Mar  2 02:03:23.201: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Mar  2 02:03:25.275: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 2, 3, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 3, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 2, 3, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 3, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/02/23 02:03:27.292
STEP: Verifying the service has paired with the endpoint 03/02/23 02:03:27.346
Mar  2 02:03:28.346: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Mar  2 02:03:28.366: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Creating a v1 custom resource 03/02/23 02:03:31.568
STEP: v2 custom resource should be converted 03/02/23 02:03:31.596
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 02:03:32.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-1601" for this suite. 03/02/23 02:03:32.26
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","completed":162,"skipped":3115,"failed":0}
------------------------------
• [SLOW TEST] [10.231 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:03:22.478
    Mar  2 02:03:22.478: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename crd-webhook 03/02/23 02:03:22.479
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:03:22.596
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:03:22.629
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 03/02/23 02:03:22.682
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 03/02/23 02:03:22.967
    STEP: Deploying the custom resource conversion webhook pod 03/02/23 02:03:23.014
    STEP: Wait for the deployment to be ready 03/02/23 02:03:23.084
    Mar  2 02:03:23.201: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    Mar  2 02:03:25.275: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 2, 3, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 3, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 2, 3, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 3, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/02/23 02:03:27.292
    STEP: Verifying the service has paired with the endpoint 03/02/23 02:03:27.346
    Mar  2 02:03:28.346: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Mar  2 02:03:28.366: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Creating a v1 custom resource 03/02/23 02:03:31.568
    STEP: v2 custom resource should be converted 03/02/23 02:03:31.596
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 02:03:32.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-1601" for this suite. 03/02/23 02:03:32.26
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:03:32.739
Mar  2 02:03:32.739: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename pods 03/02/23 02:03:32.741
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:03:32.87
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:03:32.895
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
STEP: creating the pod 03/02/23 02:03:32.964
STEP: setting up watch 03/02/23 02:03:32.965
STEP: submitting the pod to kubernetes 03/02/23 02:03:33.123
STEP: verifying the pod is in kubernetes 03/02/23 02:03:33.189
STEP: verifying pod creation was observed 03/02/23 02:03:33.236
Mar  2 02:03:33.237: INFO: Waiting up to 5m0s for pod "pod-submit-remove-914fd04c-3f92-4936-b21f-5dece5f366cf" in namespace "pods-1127" to be "running"
Mar  2 02:03:33.251: INFO: Pod "pod-submit-remove-914fd04c-3f92-4936-b21f-5dece5f366cf": Phase="Pending", Reason="", readiness=false. Elapsed: 14.701152ms
Mar  2 02:03:35.276: INFO: Pod "pod-submit-remove-914fd04c-3f92-4936-b21f-5dece5f366cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039042242s
Mar  2 02:03:37.266: INFO: Pod "pod-submit-remove-914fd04c-3f92-4936-b21f-5dece5f366cf": Phase="Running", Reason="", readiness=true. Elapsed: 4.029808719s
Mar  2 02:03:37.266: INFO: Pod "pod-submit-remove-914fd04c-3f92-4936-b21f-5dece5f366cf" satisfied condition "running"
STEP: deleting the pod gracefully 03/02/23 02:03:37.283
STEP: verifying pod deletion was observed 03/02/23 02:03:37.312
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar  2 02:03:39.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1127" for this suite. 03/02/23 02:03:39.653
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","completed":163,"skipped":3147,"failed":0}
------------------------------
• [SLOW TEST] [6.961 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:03:32.739
    Mar  2 02:03:32.739: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename pods 03/02/23 02:03:32.741
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:03:32.87
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:03:32.895
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:225
    STEP: creating the pod 03/02/23 02:03:32.964
    STEP: setting up watch 03/02/23 02:03:32.965
    STEP: submitting the pod to kubernetes 03/02/23 02:03:33.123
    STEP: verifying the pod is in kubernetes 03/02/23 02:03:33.189
    STEP: verifying pod creation was observed 03/02/23 02:03:33.236
    Mar  2 02:03:33.237: INFO: Waiting up to 5m0s for pod "pod-submit-remove-914fd04c-3f92-4936-b21f-5dece5f366cf" in namespace "pods-1127" to be "running"
    Mar  2 02:03:33.251: INFO: Pod "pod-submit-remove-914fd04c-3f92-4936-b21f-5dece5f366cf": Phase="Pending", Reason="", readiness=false. Elapsed: 14.701152ms
    Mar  2 02:03:35.276: INFO: Pod "pod-submit-remove-914fd04c-3f92-4936-b21f-5dece5f366cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039042242s
    Mar  2 02:03:37.266: INFO: Pod "pod-submit-remove-914fd04c-3f92-4936-b21f-5dece5f366cf": Phase="Running", Reason="", readiness=true. Elapsed: 4.029808719s
    Mar  2 02:03:37.266: INFO: Pod "pod-submit-remove-914fd04c-3f92-4936-b21f-5dece5f366cf" satisfied condition "running"
    STEP: deleting the pod gracefully 03/02/23 02:03:37.283
    STEP: verifying pod deletion was observed 03/02/23 02:03:37.312
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar  2 02:03:39.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-1127" for this suite. 03/02/23 02:03:39.653
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:03:39.701
Mar  2 02:03:39.702: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename pod-network-test 03/02/23 02:03:39.703
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:03:39.759
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:03:39.77
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-9080 03/02/23 02:03:39.783
STEP: creating a selector 03/02/23 02:03:39.783
STEP: Creating the service pods in kubernetes 03/02/23 02:03:39.784
Mar  2 02:03:39.784: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar  2 02:03:40.004: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9080" to be "running and ready"
Mar  2 02:03:40.018: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 13.592929ms
Mar  2 02:03:40.018: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 02:03:42.046: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041301201s
Mar  2 02:03:42.046: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 02:03:44.036: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.031791568s
Mar  2 02:03:44.036: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 02:03:46.037: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.032218189s
Mar  2 02:03:46.037: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 02:03:48.036: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.031688812s
Mar  2 02:03:48.036: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 02:03:50.038: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.03372079s
Mar  2 02:03:50.038: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 02:03:52.040: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.035185327s
Mar  2 02:03:52.040: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 02:03:54.045: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.040399316s
Mar  2 02:03:54.045: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 02:03:56.034: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.029798691s
Mar  2 02:03:56.034: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 02:03:58.033: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.028835681s
Mar  2 02:03:58.033: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 02:04:00.037: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.032977419s
Mar  2 02:04:00.037: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 02:04:02.034: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.029434954s
Mar  2 02:04:02.034: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Mar  2 02:04:02.034: INFO: Pod "netserver-0" satisfied condition "running and ready"
Mar  2 02:04:02.048: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9080" to be "running and ready"
Mar  2 02:04:02.064: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 15.328849ms
Mar  2 02:04:02.064: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Mar  2 02:04:02.064: INFO: Pod "netserver-1" satisfied condition "running and ready"
Mar  2 02:04:02.092: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-9080" to be "running and ready"
Mar  2 02:04:02.106: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 14.51199ms
Mar  2 02:04:02.107: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Mar  2 02:04:02.107: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 03/02/23 02:04:02.159
Mar  2 02:04:02.293: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9080" to be "running"
Mar  2 02:04:02.310: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 16.908941ms
Mar  2 02:04:04.327: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034070146s
Mar  2 02:04:06.335: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.041754626s
Mar  2 02:04:06.335: INFO: Pod "test-container-pod" satisfied condition "running"
Mar  2 02:04:06.359: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-9080" to be "running"
Mar  2 02:04:06.378: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 18.996618ms
Mar  2 02:04:06.378: INFO: Pod "host-test-container-pod" satisfied condition "running"
Mar  2 02:04:06.398: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar  2 02:04:06.398: INFO: Going to poll 172.30.156.111 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar  2 02:04:06.415: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.156.111 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9080 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 02:04:06.415: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
Mar  2 02:04:06.415: INFO: ExecWithOptions: Clientset creation
Mar  2 02:04:06.416: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-9080/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.156.111+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar  2 02:04:07.725: INFO: Found all 1 expected endpoints: [netserver-0]
Mar  2 02:04:07.725: INFO: Going to poll 172.30.62.198 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar  2 02:04:07.740: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.62.198 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9080 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 02:04:07.741: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
Mar  2 02:04:07.741: INFO: ExecWithOptions: Clientset creation
Mar  2 02:04:07.742: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-9080/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.62.198+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar  2 02:04:08.978: INFO: Found all 1 expected endpoints: [netserver-1]
Mar  2 02:04:08.979: INFO: Going to poll 172.30.201.248 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar  2 02:04:08.994: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.201.248 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9080 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 02:04:08.994: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
Mar  2 02:04:08.995: INFO: ExecWithOptions: Clientset creation
Mar  2 02:04:08.995: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-9080/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.201.248+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar  2 02:04:10.317: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Mar  2 02:04:10.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9080" for this suite. 03/02/23 02:04:10.341
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","completed":164,"skipped":3152,"failed":0}
------------------------------
• [SLOW TEST] [30.667 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:03:39.701
    Mar  2 02:03:39.702: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename pod-network-test 03/02/23 02:03:39.703
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:03:39.759
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:03:39.77
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-9080 03/02/23 02:03:39.783
    STEP: creating a selector 03/02/23 02:03:39.783
    STEP: Creating the service pods in kubernetes 03/02/23 02:03:39.784
    Mar  2 02:03:39.784: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Mar  2 02:03:40.004: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9080" to be "running and ready"
    Mar  2 02:03:40.018: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 13.592929ms
    Mar  2 02:03:40.018: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 02:03:42.046: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041301201s
    Mar  2 02:03:42.046: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 02:03:44.036: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.031791568s
    Mar  2 02:03:44.036: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 02:03:46.037: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.032218189s
    Mar  2 02:03:46.037: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 02:03:48.036: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.031688812s
    Mar  2 02:03:48.036: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 02:03:50.038: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.03372079s
    Mar  2 02:03:50.038: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 02:03:52.040: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.035185327s
    Mar  2 02:03:52.040: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 02:03:54.045: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.040399316s
    Mar  2 02:03:54.045: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 02:03:56.034: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.029798691s
    Mar  2 02:03:56.034: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 02:03:58.033: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.028835681s
    Mar  2 02:03:58.033: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 02:04:00.037: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.032977419s
    Mar  2 02:04:00.037: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 02:04:02.034: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.029434954s
    Mar  2 02:04:02.034: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Mar  2 02:04:02.034: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Mar  2 02:04:02.048: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9080" to be "running and ready"
    Mar  2 02:04:02.064: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 15.328849ms
    Mar  2 02:04:02.064: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Mar  2 02:04:02.064: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Mar  2 02:04:02.092: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-9080" to be "running and ready"
    Mar  2 02:04:02.106: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 14.51199ms
    Mar  2 02:04:02.107: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Mar  2 02:04:02.107: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 03/02/23 02:04:02.159
    Mar  2 02:04:02.293: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9080" to be "running"
    Mar  2 02:04:02.310: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 16.908941ms
    Mar  2 02:04:04.327: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034070146s
    Mar  2 02:04:06.335: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.041754626s
    Mar  2 02:04:06.335: INFO: Pod "test-container-pod" satisfied condition "running"
    Mar  2 02:04:06.359: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-9080" to be "running"
    Mar  2 02:04:06.378: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 18.996618ms
    Mar  2 02:04:06.378: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Mar  2 02:04:06.398: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Mar  2 02:04:06.398: INFO: Going to poll 172.30.156.111 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Mar  2 02:04:06.415: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.156.111 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9080 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 02:04:06.415: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    Mar  2 02:04:06.415: INFO: ExecWithOptions: Clientset creation
    Mar  2 02:04:06.416: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-9080/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.156.111+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar  2 02:04:07.725: INFO: Found all 1 expected endpoints: [netserver-0]
    Mar  2 02:04:07.725: INFO: Going to poll 172.30.62.198 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Mar  2 02:04:07.740: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.62.198 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9080 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 02:04:07.741: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    Mar  2 02:04:07.741: INFO: ExecWithOptions: Clientset creation
    Mar  2 02:04:07.742: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-9080/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.62.198+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar  2 02:04:08.978: INFO: Found all 1 expected endpoints: [netserver-1]
    Mar  2 02:04:08.979: INFO: Going to poll 172.30.201.248 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Mar  2 02:04:08.994: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.201.248 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9080 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 02:04:08.994: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    Mar  2 02:04:08.995: INFO: ExecWithOptions: Clientset creation
    Mar  2 02:04:08.995: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-9080/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.201.248+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar  2 02:04:10.317: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Mar  2 02:04:10.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-9080" for this suite. 03/02/23 02:04:10.341
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:04:10.373
Mar  2 02:04:10.374: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename container-probe 03/02/23 02:04:10.375
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:04:10.424
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:04:10.439
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
STEP: Creating pod liveness-1cd7b12d-69fd-4f32-b8fb-d20e1fbad316 in namespace container-probe-5024 03/02/23 02:04:10.454
Mar  2 02:04:10.547: INFO: Waiting up to 5m0s for pod "liveness-1cd7b12d-69fd-4f32-b8fb-d20e1fbad316" in namespace "container-probe-5024" to be "not pending"
Mar  2 02:04:10.594: INFO: Pod "liveness-1cd7b12d-69fd-4f32-b8fb-d20e1fbad316": Phase="Pending", Reason="", readiness=false. Elapsed: 46.142108ms
Mar  2 02:04:12.631: INFO: Pod "liveness-1cd7b12d-69fd-4f32-b8fb-d20e1fbad316": Phase="Running", Reason="", readiness=true. Elapsed: 2.084063992s
Mar  2 02:04:12.631: INFO: Pod "liveness-1cd7b12d-69fd-4f32-b8fb-d20e1fbad316" satisfied condition "not pending"
Mar  2 02:04:12.632: INFO: Started pod liveness-1cd7b12d-69fd-4f32-b8fb-d20e1fbad316 in namespace container-probe-5024
STEP: checking the pod's current state and verifying that restartCount is present 03/02/23 02:04:12.632
Mar  2 02:04:12.649: INFO: Initial restart count of pod liveness-1cd7b12d-69fd-4f32-b8fb-d20e1fbad316 is 0
Mar  2 02:04:32.882: INFO: Restart count of pod container-probe-5024/liveness-1cd7b12d-69fd-4f32-b8fb-d20e1fbad316 is now 1 (20.232890193s elapsed)
STEP: deleting the pod 03/02/23 02:04:32.882
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar  2 02:04:32.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5024" for this suite. 03/02/23 02:04:32.974
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":165,"skipped":3191,"failed":0}
------------------------------
• [SLOW TEST] [22.628 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:04:10.373
    Mar  2 02:04:10.374: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename container-probe 03/02/23 02:04:10.375
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:04:10.424
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:04:10.439
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:165
    STEP: Creating pod liveness-1cd7b12d-69fd-4f32-b8fb-d20e1fbad316 in namespace container-probe-5024 03/02/23 02:04:10.454
    Mar  2 02:04:10.547: INFO: Waiting up to 5m0s for pod "liveness-1cd7b12d-69fd-4f32-b8fb-d20e1fbad316" in namespace "container-probe-5024" to be "not pending"
    Mar  2 02:04:10.594: INFO: Pod "liveness-1cd7b12d-69fd-4f32-b8fb-d20e1fbad316": Phase="Pending", Reason="", readiness=false. Elapsed: 46.142108ms
    Mar  2 02:04:12.631: INFO: Pod "liveness-1cd7b12d-69fd-4f32-b8fb-d20e1fbad316": Phase="Running", Reason="", readiness=true. Elapsed: 2.084063992s
    Mar  2 02:04:12.631: INFO: Pod "liveness-1cd7b12d-69fd-4f32-b8fb-d20e1fbad316" satisfied condition "not pending"
    Mar  2 02:04:12.632: INFO: Started pod liveness-1cd7b12d-69fd-4f32-b8fb-d20e1fbad316 in namespace container-probe-5024
    STEP: checking the pod's current state and verifying that restartCount is present 03/02/23 02:04:12.632
    Mar  2 02:04:12.649: INFO: Initial restart count of pod liveness-1cd7b12d-69fd-4f32-b8fb-d20e1fbad316 is 0
    Mar  2 02:04:32.882: INFO: Restart count of pod container-probe-5024/liveness-1cd7b12d-69fd-4f32-b8fb-d20e1fbad316 is now 1 (20.232890193s elapsed)
    STEP: deleting the pod 03/02/23 02:04:32.882
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar  2 02:04:32.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-5024" for this suite. 03/02/23 02:04:32.974
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:04:33.028
Mar  2 02:04:33.028: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename emptydir-wrapper 03/02/23 02:04:33.03
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:04:33.093
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:04:33.105
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 03/02/23 02:04:33.118
STEP: Creating RC which spawns configmap-volume pods 03/02/23 02:04:34.268
Mar  2 02:04:34.320: INFO: Pod name wrapped-volume-race-9f3e12f8-a260-4de1-a150-8229f68d764d: Found 0 pods out of 5
Mar  2 02:04:39.352: INFO: Pod name wrapped-volume-race-9f3e12f8-a260-4de1-a150-8229f68d764d: Found 5 pods out of 5
STEP: Ensuring each pod is running 03/02/23 02:04:39.352
Mar  2 02:04:39.353: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9f3e12f8-a260-4de1-a150-8229f68d764d-5br74" in namespace "emptydir-wrapper-5071" to be "running"
Mar  2 02:04:39.411: INFO: Pod "wrapped-volume-race-9f3e12f8-a260-4de1-a150-8229f68d764d-5br74": Phase="Running", Reason="", readiness=true. Elapsed: 58.167364ms
Mar  2 02:04:39.411: INFO: Pod "wrapped-volume-race-9f3e12f8-a260-4de1-a150-8229f68d764d-5br74" satisfied condition "running"
Mar  2 02:04:39.411: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9f3e12f8-a260-4de1-a150-8229f68d764d-bbpc2" in namespace "emptydir-wrapper-5071" to be "running"
Mar  2 02:04:39.430: INFO: Pod "wrapped-volume-race-9f3e12f8-a260-4de1-a150-8229f68d764d-bbpc2": Phase="Running", Reason="", readiness=true. Elapsed: 18.940848ms
Mar  2 02:04:39.430: INFO: Pod "wrapped-volume-race-9f3e12f8-a260-4de1-a150-8229f68d764d-bbpc2" satisfied condition "running"
Mar  2 02:04:39.430: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9f3e12f8-a260-4de1-a150-8229f68d764d-dmvrf" in namespace "emptydir-wrapper-5071" to be "running"
Mar  2 02:04:39.446: INFO: Pod "wrapped-volume-race-9f3e12f8-a260-4de1-a150-8229f68d764d-dmvrf": Phase="Running", Reason="", readiness=true. Elapsed: 15.768204ms
Mar  2 02:04:39.446: INFO: Pod "wrapped-volume-race-9f3e12f8-a260-4de1-a150-8229f68d764d-dmvrf" satisfied condition "running"
Mar  2 02:04:39.446: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9f3e12f8-a260-4de1-a150-8229f68d764d-tnc6l" in namespace "emptydir-wrapper-5071" to be "running"
Mar  2 02:04:39.461: INFO: Pod "wrapped-volume-race-9f3e12f8-a260-4de1-a150-8229f68d764d-tnc6l": Phase="Running", Reason="", readiness=true. Elapsed: 15.366516ms
Mar  2 02:04:39.461: INFO: Pod "wrapped-volume-race-9f3e12f8-a260-4de1-a150-8229f68d764d-tnc6l" satisfied condition "running"
Mar  2 02:04:39.461: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9f3e12f8-a260-4de1-a150-8229f68d764d-w2mhk" in namespace "emptydir-wrapper-5071" to be "running"
Mar  2 02:04:39.477: INFO: Pod "wrapped-volume-race-9f3e12f8-a260-4de1-a150-8229f68d764d-w2mhk": Phase="Running", Reason="", readiness=true. Elapsed: 16.110618ms
Mar  2 02:04:39.477: INFO: Pod "wrapped-volume-race-9f3e12f8-a260-4de1-a150-8229f68d764d-w2mhk" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-9f3e12f8-a260-4de1-a150-8229f68d764d in namespace emptydir-wrapper-5071, will wait for the garbage collector to delete the pods 03/02/23 02:04:39.477
Mar  2 02:04:39.575: INFO: Deleting ReplicationController wrapped-volume-race-9f3e12f8-a260-4de1-a150-8229f68d764d took: 30.809093ms
Mar  2 02:04:39.775: INFO: Terminating ReplicationController wrapped-volume-race-9f3e12f8-a260-4de1-a150-8229f68d764d pods took: 200.227266ms
STEP: Creating RC which spawns configmap-volume pods 03/02/23 02:04:44.396
Mar  2 02:04:44.463: INFO: Pod name wrapped-volume-race-9846599f-292f-4c8e-9dc2-2e3b24d3899e: Found 0 pods out of 5
Mar  2 02:04:49.488: INFO: Pod name wrapped-volume-race-9846599f-292f-4c8e-9dc2-2e3b24d3899e: Found 5 pods out of 5
STEP: Ensuring each pod is running 03/02/23 02:04:49.488
Mar  2 02:04:49.488: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9846599f-292f-4c8e-9dc2-2e3b24d3899e-6b6k6" in namespace "emptydir-wrapper-5071" to be "running"
Mar  2 02:04:49.502: INFO: Pod "wrapped-volume-race-9846599f-292f-4c8e-9dc2-2e3b24d3899e-6b6k6": Phase="Running", Reason="", readiness=true. Elapsed: 14.279765ms
Mar  2 02:04:49.502: INFO: Pod "wrapped-volume-race-9846599f-292f-4c8e-9dc2-2e3b24d3899e-6b6k6" satisfied condition "running"
Mar  2 02:04:49.502: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9846599f-292f-4c8e-9dc2-2e3b24d3899e-7hrjd" in namespace "emptydir-wrapper-5071" to be "running"
Mar  2 02:04:49.519: INFO: Pod "wrapped-volume-race-9846599f-292f-4c8e-9dc2-2e3b24d3899e-7hrjd": Phase="Running", Reason="", readiness=true. Elapsed: 16.693451ms
Mar  2 02:04:49.519: INFO: Pod "wrapped-volume-race-9846599f-292f-4c8e-9dc2-2e3b24d3899e-7hrjd" satisfied condition "running"
Mar  2 02:04:49.520: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9846599f-292f-4c8e-9dc2-2e3b24d3899e-g8dx5" in namespace "emptydir-wrapper-5071" to be "running"
Mar  2 02:04:49.533: INFO: Pod "wrapped-volume-race-9846599f-292f-4c8e-9dc2-2e3b24d3899e-g8dx5": Phase="Running", Reason="", readiness=true. Elapsed: 13.211317ms
Mar  2 02:04:49.533: INFO: Pod "wrapped-volume-race-9846599f-292f-4c8e-9dc2-2e3b24d3899e-g8dx5" satisfied condition "running"
Mar  2 02:04:49.533: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9846599f-292f-4c8e-9dc2-2e3b24d3899e-httrm" in namespace "emptydir-wrapper-5071" to be "running"
Mar  2 02:04:49.547: INFO: Pod "wrapped-volume-race-9846599f-292f-4c8e-9dc2-2e3b24d3899e-httrm": Phase="Running", Reason="", readiness=true. Elapsed: 13.806118ms
Mar  2 02:04:49.547: INFO: Pod "wrapped-volume-race-9846599f-292f-4c8e-9dc2-2e3b24d3899e-httrm" satisfied condition "running"
Mar  2 02:04:49.548: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9846599f-292f-4c8e-9dc2-2e3b24d3899e-wtv8c" in namespace "emptydir-wrapper-5071" to be "running"
Mar  2 02:04:49.562: INFO: Pod "wrapped-volume-race-9846599f-292f-4c8e-9dc2-2e3b24d3899e-wtv8c": Phase="Running", Reason="", readiness=true. Elapsed: 14.557928ms
Mar  2 02:04:49.562: INFO: Pod "wrapped-volume-race-9846599f-292f-4c8e-9dc2-2e3b24d3899e-wtv8c" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-9846599f-292f-4c8e-9dc2-2e3b24d3899e in namespace emptydir-wrapper-5071, will wait for the garbage collector to delete the pods 03/02/23 02:04:49.562
Mar  2 02:04:49.665: INFO: Deleting ReplicationController wrapped-volume-race-9846599f-292f-4c8e-9dc2-2e3b24d3899e took: 34.790765ms
Mar  2 02:04:49.765: INFO: Terminating ReplicationController wrapped-volume-race-9846599f-292f-4c8e-9dc2-2e3b24d3899e pods took: 100.157791ms
STEP: Creating RC which spawns configmap-volume pods 03/02/23 02:04:54.793
Mar  2 02:04:54.891: INFO: Pod name wrapped-volume-race-5f690cbf-4330-4d16-9069-f54dfe092c5f: Found 0 pods out of 5
Mar  2 02:04:59.919: INFO: Pod name wrapped-volume-race-5f690cbf-4330-4d16-9069-f54dfe092c5f: Found 5 pods out of 5
STEP: Ensuring each pod is running 03/02/23 02:04:59.919
Mar  2 02:04:59.919: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5f690cbf-4330-4d16-9069-f54dfe092c5f-6vltj" in namespace "emptydir-wrapper-5071" to be "running"
Mar  2 02:04:59.932: INFO: Pod "wrapped-volume-race-5f690cbf-4330-4d16-9069-f54dfe092c5f-6vltj": Phase="Running", Reason="", readiness=true. Elapsed: 13.213516ms
Mar  2 02:04:59.933: INFO: Pod "wrapped-volume-race-5f690cbf-4330-4d16-9069-f54dfe092c5f-6vltj" satisfied condition "running"
Mar  2 02:04:59.933: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5f690cbf-4330-4d16-9069-f54dfe092c5f-f9ktn" in namespace "emptydir-wrapper-5071" to be "running"
Mar  2 02:04:59.946: INFO: Pod "wrapped-volume-race-5f690cbf-4330-4d16-9069-f54dfe092c5f-f9ktn": Phase="Pending", Reason="", readiness=false. Elapsed: 13.655292ms
Mar  2 02:05:01.967: INFO: Pod "wrapped-volume-race-5f690cbf-4330-4d16-9069-f54dfe092c5f-f9ktn": Phase="Running", Reason="", readiness=true. Elapsed: 2.03397372s
Mar  2 02:05:01.967: INFO: Pod "wrapped-volume-race-5f690cbf-4330-4d16-9069-f54dfe092c5f-f9ktn" satisfied condition "running"
Mar  2 02:05:01.967: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5f690cbf-4330-4d16-9069-f54dfe092c5f-q8q28" in namespace "emptydir-wrapper-5071" to be "running"
Mar  2 02:05:01.990: INFO: Pod "wrapped-volume-race-5f690cbf-4330-4d16-9069-f54dfe092c5f-q8q28": Phase="Running", Reason="", readiness=true. Elapsed: 22.956453ms
Mar  2 02:05:01.990: INFO: Pod "wrapped-volume-race-5f690cbf-4330-4d16-9069-f54dfe092c5f-q8q28" satisfied condition "running"
Mar  2 02:05:01.990: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5f690cbf-4330-4d16-9069-f54dfe092c5f-rmvr6" in namespace "emptydir-wrapper-5071" to be "running"
Mar  2 02:05:02.033: INFO: Pod "wrapped-volume-race-5f690cbf-4330-4d16-9069-f54dfe092c5f-rmvr6": Phase="Running", Reason="", readiness=true. Elapsed: 43.67229ms
Mar  2 02:05:02.033: INFO: Pod "wrapped-volume-race-5f690cbf-4330-4d16-9069-f54dfe092c5f-rmvr6" satisfied condition "running"
Mar  2 02:05:02.033: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5f690cbf-4330-4d16-9069-f54dfe092c5f-rtn52" in namespace "emptydir-wrapper-5071" to be "running"
Mar  2 02:05:02.054: INFO: Pod "wrapped-volume-race-5f690cbf-4330-4d16-9069-f54dfe092c5f-rtn52": Phase="Running", Reason="", readiness=true. Elapsed: 21.0696ms
Mar  2 02:05:02.055: INFO: Pod "wrapped-volume-race-5f690cbf-4330-4d16-9069-f54dfe092c5f-rtn52" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-5f690cbf-4330-4d16-9069-f54dfe092c5f in namespace emptydir-wrapper-5071, will wait for the garbage collector to delete the pods 03/02/23 02:05:02.055
Mar  2 02:05:02.164: INFO: Deleting ReplicationController wrapped-volume-race-5f690cbf-4330-4d16-9069-f54dfe092c5f took: 36.295445ms
Mar  2 02:05:02.364: INFO: Terminating ReplicationController wrapped-volume-race-5f690cbf-4330-4d16-9069-f54dfe092c5f pods took: 200.4486ms
STEP: Cleaning up the configMaps 03/02/23 02:05:06.164
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Mar  2 02:05:07.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5071" for this suite. 03/02/23 02:05:07.997
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","completed":166,"skipped":3227,"failed":0}
------------------------------
• [SLOW TEST] [34.997 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:04:33.028
    Mar  2 02:04:33.028: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename emptydir-wrapper 03/02/23 02:04:33.03
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:04:33.093
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:04:33.105
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 03/02/23 02:04:33.118
    STEP: Creating RC which spawns configmap-volume pods 03/02/23 02:04:34.268
    Mar  2 02:04:34.320: INFO: Pod name wrapped-volume-race-9f3e12f8-a260-4de1-a150-8229f68d764d: Found 0 pods out of 5
    Mar  2 02:04:39.352: INFO: Pod name wrapped-volume-race-9f3e12f8-a260-4de1-a150-8229f68d764d: Found 5 pods out of 5
    STEP: Ensuring each pod is running 03/02/23 02:04:39.352
    Mar  2 02:04:39.353: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9f3e12f8-a260-4de1-a150-8229f68d764d-5br74" in namespace "emptydir-wrapper-5071" to be "running"
    Mar  2 02:04:39.411: INFO: Pod "wrapped-volume-race-9f3e12f8-a260-4de1-a150-8229f68d764d-5br74": Phase="Running", Reason="", readiness=true. Elapsed: 58.167364ms
    Mar  2 02:04:39.411: INFO: Pod "wrapped-volume-race-9f3e12f8-a260-4de1-a150-8229f68d764d-5br74" satisfied condition "running"
    Mar  2 02:04:39.411: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9f3e12f8-a260-4de1-a150-8229f68d764d-bbpc2" in namespace "emptydir-wrapper-5071" to be "running"
    Mar  2 02:04:39.430: INFO: Pod "wrapped-volume-race-9f3e12f8-a260-4de1-a150-8229f68d764d-bbpc2": Phase="Running", Reason="", readiness=true. Elapsed: 18.940848ms
    Mar  2 02:04:39.430: INFO: Pod "wrapped-volume-race-9f3e12f8-a260-4de1-a150-8229f68d764d-bbpc2" satisfied condition "running"
    Mar  2 02:04:39.430: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9f3e12f8-a260-4de1-a150-8229f68d764d-dmvrf" in namespace "emptydir-wrapper-5071" to be "running"
    Mar  2 02:04:39.446: INFO: Pod "wrapped-volume-race-9f3e12f8-a260-4de1-a150-8229f68d764d-dmvrf": Phase="Running", Reason="", readiness=true. Elapsed: 15.768204ms
    Mar  2 02:04:39.446: INFO: Pod "wrapped-volume-race-9f3e12f8-a260-4de1-a150-8229f68d764d-dmvrf" satisfied condition "running"
    Mar  2 02:04:39.446: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9f3e12f8-a260-4de1-a150-8229f68d764d-tnc6l" in namespace "emptydir-wrapper-5071" to be "running"
    Mar  2 02:04:39.461: INFO: Pod "wrapped-volume-race-9f3e12f8-a260-4de1-a150-8229f68d764d-tnc6l": Phase="Running", Reason="", readiness=true. Elapsed: 15.366516ms
    Mar  2 02:04:39.461: INFO: Pod "wrapped-volume-race-9f3e12f8-a260-4de1-a150-8229f68d764d-tnc6l" satisfied condition "running"
    Mar  2 02:04:39.461: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9f3e12f8-a260-4de1-a150-8229f68d764d-w2mhk" in namespace "emptydir-wrapper-5071" to be "running"
    Mar  2 02:04:39.477: INFO: Pod "wrapped-volume-race-9f3e12f8-a260-4de1-a150-8229f68d764d-w2mhk": Phase="Running", Reason="", readiness=true. Elapsed: 16.110618ms
    Mar  2 02:04:39.477: INFO: Pod "wrapped-volume-race-9f3e12f8-a260-4de1-a150-8229f68d764d-w2mhk" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-9f3e12f8-a260-4de1-a150-8229f68d764d in namespace emptydir-wrapper-5071, will wait for the garbage collector to delete the pods 03/02/23 02:04:39.477
    Mar  2 02:04:39.575: INFO: Deleting ReplicationController wrapped-volume-race-9f3e12f8-a260-4de1-a150-8229f68d764d took: 30.809093ms
    Mar  2 02:04:39.775: INFO: Terminating ReplicationController wrapped-volume-race-9f3e12f8-a260-4de1-a150-8229f68d764d pods took: 200.227266ms
    STEP: Creating RC which spawns configmap-volume pods 03/02/23 02:04:44.396
    Mar  2 02:04:44.463: INFO: Pod name wrapped-volume-race-9846599f-292f-4c8e-9dc2-2e3b24d3899e: Found 0 pods out of 5
    Mar  2 02:04:49.488: INFO: Pod name wrapped-volume-race-9846599f-292f-4c8e-9dc2-2e3b24d3899e: Found 5 pods out of 5
    STEP: Ensuring each pod is running 03/02/23 02:04:49.488
    Mar  2 02:04:49.488: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9846599f-292f-4c8e-9dc2-2e3b24d3899e-6b6k6" in namespace "emptydir-wrapper-5071" to be "running"
    Mar  2 02:04:49.502: INFO: Pod "wrapped-volume-race-9846599f-292f-4c8e-9dc2-2e3b24d3899e-6b6k6": Phase="Running", Reason="", readiness=true. Elapsed: 14.279765ms
    Mar  2 02:04:49.502: INFO: Pod "wrapped-volume-race-9846599f-292f-4c8e-9dc2-2e3b24d3899e-6b6k6" satisfied condition "running"
    Mar  2 02:04:49.502: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9846599f-292f-4c8e-9dc2-2e3b24d3899e-7hrjd" in namespace "emptydir-wrapper-5071" to be "running"
    Mar  2 02:04:49.519: INFO: Pod "wrapped-volume-race-9846599f-292f-4c8e-9dc2-2e3b24d3899e-7hrjd": Phase="Running", Reason="", readiness=true. Elapsed: 16.693451ms
    Mar  2 02:04:49.519: INFO: Pod "wrapped-volume-race-9846599f-292f-4c8e-9dc2-2e3b24d3899e-7hrjd" satisfied condition "running"
    Mar  2 02:04:49.520: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9846599f-292f-4c8e-9dc2-2e3b24d3899e-g8dx5" in namespace "emptydir-wrapper-5071" to be "running"
    Mar  2 02:04:49.533: INFO: Pod "wrapped-volume-race-9846599f-292f-4c8e-9dc2-2e3b24d3899e-g8dx5": Phase="Running", Reason="", readiness=true. Elapsed: 13.211317ms
    Mar  2 02:04:49.533: INFO: Pod "wrapped-volume-race-9846599f-292f-4c8e-9dc2-2e3b24d3899e-g8dx5" satisfied condition "running"
    Mar  2 02:04:49.533: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9846599f-292f-4c8e-9dc2-2e3b24d3899e-httrm" in namespace "emptydir-wrapper-5071" to be "running"
    Mar  2 02:04:49.547: INFO: Pod "wrapped-volume-race-9846599f-292f-4c8e-9dc2-2e3b24d3899e-httrm": Phase="Running", Reason="", readiness=true. Elapsed: 13.806118ms
    Mar  2 02:04:49.547: INFO: Pod "wrapped-volume-race-9846599f-292f-4c8e-9dc2-2e3b24d3899e-httrm" satisfied condition "running"
    Mar  2 02:04:49.548: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9846599f-292f-4c8e-9dc2-2e3b24d3899e-wtv8c" in namespace "emptydir-wrapper-5071" to be "running"
    Mar  2 02:04:49.562: INFO: Pod "wrapped-volume-race-9846599f-292f-4c8e-9dc2-2e3b24d3899e-wtv8c": Phase="Running", Reason="", readiness=true. Elapsed: 14.557928ms
    Mar  2 02:04:49.562: INFO: Pod "wrapped-volume-race-9846599f-292f-4c8e-9dc2-2e3b24d3899e-wtv8c" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-9846599f-292f-4c8e-9dc2-2e3b24d3899e in namespace emptydir-wrapper-5071, will wait for the garbage collector to delete the pods 03/02/23 02:04:49.562
    Mar  2 02:04:49.665: INFO: Deleting ReplicationController wrapped-volume-race-9846599f-292f-4c8e-9dc2-2e3b24d3899e took: 34.790765ms
    Mar  2 02:04:49.765: INFO: Terminating ReplicationController wrapped-volume-race-9846599f-292f-4c8e-9dc2-2e3b24d3899e pods took: 100.157791ms
    STEP: Creating RC which spawns configmap-volume pods 03/02/23 02:04:54.793
    Mar  2 02:04:54.891: INFO: Pod name wrapped-volume-race-5f690cbf-4330-4d16-9069-f54dfe092c5f: Found 0 pods out of 5
    Mar  2 02:04:59.919: INFO: Pod name wrapped-volume-race-5f690cbf-4330-4d16-9069-f54dfe092c5f: Found 5 pods out of 5
    STEP: Ensuring each pod is running 03/02/23 02:04:59.919
    Mar  2 02:04:59.919: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5f690cbf-4330-4d16-9069-f54dfe092c5f-6vltj" in namespace "emptydir-wrapper-5071" to be "running"
    Mar  2 02:04:59.932: INFO: Pod "wrapped-volume-race-5f690cbf-4330-4d16-9069-f54dfe092c5f-6vltj": Phase="Running", Reason="", readiness=true. Elapsed: 13.213516ms
    Mar  2 02:04:59.933: INFO: Pod "wrapped-volume-race-5f690cbf-4330-4d16-9069-f54dfe092c5f-6vltj" satisfied condition "running"
    Mar  2 02:04:59.933: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5f690cbf-4330-4d16-9069-f54dfe092c5f-f9ktn" in namespace "emptydir-wrapper-5071" to be "running"
    Mar  2 02:04:59.946: INFO: Pod "wrapped-volume-race-5f690cbf-4330-4d16-9069-f54dfe092c5f-f9ktn": Phase="Pending", Reason="", readiness=false. Elapsed: 13.655292ms
    Mar  2 02:05:01.967: INFO: Pod "wrapped-volume-race-5f690cbf-4330-4d16-9069-f54dfe092c5f-f9ktn": Phase="Running", Reason="", readiness=true. Elapsed: 2.03397372s
    Mar  2 02:05:01.967: INFO: Pod "wrapped-volume-race-5f690cbf-4330-4d16-9069-f54dfe092c5f-f9ktn" satisfied condition "running"
    Mar  2 02:05:01.967: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5f690cbf-4330-4d16-9069-f54dfe092c5f-q8q28" in namespace "emptydir-wrapper-5071" to be "running"
    Mar  2 02:05:01.990: INFO: Pod "wrapped-volume-race-5f690cbf-4330-4d16-9069-f54dfe092c5f-q8q28": Phase="Running", Reason="", readiness=true. Elapsed: 22.956453ms
    Mar  2 02:05:01.990: INFO: Pod "wrapped-volume-race-5f690cbf-4330-4d16-9069-f54dfe092c5f-q8q28" satisfied condition "running"
    Mar  2 02:05:01.990: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5f690cbf-4330-4d16-9069-f54dfe092c5f-rmvr6" in namespace "emptydir-wrapper-5071" to be "running"
    Mar  2 02:05:02.033: INFO: Pod "wrapped-volume-race-5f690cbf-4330-4d16-9069-f54dfe092c5f-rmvr6": Phase="Running", Reason="", readiness=true. Elapsed: 43.67229ms
    Mar  2 02:05:02.033: INFO: Pod "wrapped-volume-race-5f690cbf-4330-4d16-9069-f54dfe092c5f-rmvr6" satisfied condition "running"
    Mar  2 02:05:02.033: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5f690cbf-4330-4d16-9069-f54dfe092c5f-rtn52" in namespace "emptydir-wrapper-5071" to be "running"
    Mar  2 02:05:02.054: INFO: Pod "wrapped-volume-race-5f690cbf-4330-4d16-9069-f54dfe092c5f-rtn52": Phase="Running", Reason="", readiness=true. Elapsed: 21.0696ms
    Mar  2 02:05:02.055: INFO: Pod "wrapped-volume-race-5f690cbf-4330-4d16-9069-f54dfe092c5f-rtn52" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-5f690cbf-4330-4d16-9069-f54dfe092c5f in namespace emptydir-wrapper-5071, will wait for the garbage collector to delete the pods 03/02/23 02:05:02.055
    Mar  2 02:05:02.164: INFO: Deleting ReplicationController wrapped-volume-race-5f690cbf-4330-4d16-9069-f54dfe092c5f took: 36.295445ms
    Mar  2 02:05:02.364: INFO: Terminating ReplicationController wrapped-volume-race-5f690cbf-4330-4d16-9069-f54dfe092c5f pods took: 200.4486ms
    STEP: Cleaning up the configMaps 03/02/23 02:05:06.164
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Mar  2 02:05:07.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-5071" for this suite. 03/02/23 02:05:07.997
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:05:08.026
Mar  2 02:05:08.026: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename proxy 03/02/23 02:05:08.028
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:05:08.085
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:05:08.103
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 03/02/23 02:05:08.158
STEP: creating replication controller proxy-service-bvrqw in namespace proxy-7314 03/02/23 02:05:08.164
I0302 02:05:08.249165      20 runners.go:193] Created replication controller with name: proxy-service-bvrqw, namespace: proxy-7314, replica count: 1
I0302 02:05:09.300896      20 runners.go:193] proxy-service-bvrqw Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 02:05:10.301135      20 runners.go:193] proxy-service-bvrqw Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 02:05:11.301872      20 runners.go:193] proxy-service-bvrqw Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0302 02:05:12.302144      20 runners.go:193] proxy-service-bvrqw Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 02:05:12.325: INFO: setup took 4.207832929s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 03/02/23 02:05:12.325
Mar  2 02:05:12.373: INFO: (0) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 46.66212ms)
Mar  2 02:05:12.377: INFO: (0) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">test<... (200; 51.388089ms)
Mar  2 02:05:12.378: INFO: (0) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 52.397814ms)
Mar  2 02:05:12.378: INFO: (0) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">... (200; 52.784162ms)
Mar  2 02:05:12.379: INFO: (0) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 53.051147ms)
Mar  2 02:05:12.379: INFO: (0) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname2/proxy/: bar (200; 54.207273ms)
Mar  2 02:05:12.380: INFO: (0) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/rewriteme">test</a> (200; 54.824063ms)
Mar  2 02:05:12.380: INFO: (0) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 54.651323ms)
Mar  2 02:05:12.380: INFO: (0) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname1/proxy/: foo (200; 55.308545ms)
Mar  2 02:05:12.381: INFO: (0) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname1/proxy/: foo (200; 55.142287ms)
Mar  2 02:05:12.385: INFO: (0) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname2/proxy/: bar (200; 59.40071ms)
Mar  2 02:05:12.394: INFO: (0) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:462/proxy/: tls qux (200; 68.735037ms)
Mar  2 02:05:12.394: INFO: (0) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/tlsrewritem... (200; 68.345465ms)
Mar  2 02:05:12.395: INFO: (0) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname2/proxy/: tls qux (200; 68.915718ms)
Mar  2 02:05:12.402: INFO: (0) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname1/proxy/: tls baz (200; 75.803025ms)
Mar  2 02:05:12.402: INFO: (0) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:460/proxy/: tls baz (200; 76.295444ms)
Mar  2 02:05:12.452: INFO: (1) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/rewriteme">test</a> (200; 46.624969ms)
Mar  2 02:05:12.452: INFO: (1) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:462/proxy/: tls qux (200; 46.814686ms)
Mar  2 02:05:12.452: INFO: (1) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 47.528637ms)
Mar  2 02:05:12.452: INFO: (1) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 48.459682ms)
Mar  2 02:05:12.452: INFO: (1) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">... (200; 47.055315ms)
Mar  2 02:05:12.453: INFO: (1) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/tlsrewritem... (200; 48.936531ms)
Mar  2 02:05:12.453: INFO: (1) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:460/proxy/: tls baz (200; 48.023135ms)
Mar  2 02:05:12.453: INFO: (1) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 49.24714ms)
Mar  2 02:05:12.453: INFO: (1) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">test<... (200; 48.577921ms)
Mar  2 02:05:12.453: INFO: (1) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 49.888437ms)
Mar  2 02:05:12.459: INFO: (1) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname1/proxy/: foo (200; 54.070326ms)
Mar  2 02:05:12.460: INFO: (1) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname2/proxy/: bar (200; 55.556528ms)
Mar  2 02:05:12.461: INFO: (1) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname2/proxy/: tls qux (200; 55.634993ms)
Mar  2 02:05:12.461: INFO: (1) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname1/proxy/: tls baz (200; 57.156756ms)
Mar  2 02:05:12.462: INFO: (1) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname2/proxy/: bar (200; 57.792667ms)
Mar  2 02:05:12.462: INFO: (1) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname1/proxy/: foo (200; 57.341912ms)
Mar  2 02:05:12.517: INFO: (2) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 55.039998ms)
Mar  2 02:05:12.518: INFO: (2) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">test<... (200; 55.677188ms)
Mar  2 02:05:12.518: INFO: (2) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname2/proxy/: tls qux (200; 55.576691ms)
Mar  2 02:05:12.519: INFO: (2) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 56.337173ms)
Mar  2 02:05:12.524: INFO: (2) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname1/proxy/: tls baz (200; 61.929059ms)
Mar  2 02:05:12.533: INFO: (2) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/tlsrewritem... (200; 70.942012ms)
Mar  2 02:05:12.564: INFO: (2) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:462/proxy/: tls qux (200; 101.910571ms)
Mar  2 02:05:12.564: INFO: (2) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:460/proxy/: tls baz (200; 101.985894ms)
Mar  2 02:05:12.564: INFO: (2) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 101.938298ms)
Mar  2 02:05:12.565: INFO: (2) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/rewriteme">test</a> (200; 102.533476ms)
Mar  2 02:05:12.565: INFO: (2) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">... (200; 102.720297ms)
Mar  2 02:05:12.565: INFO: (2) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 103.10115ms)
Mar  2 02:05:12.598: INFO: (2) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname2/proxy/: bar (200; 136.005824ms)
Mar  2 02:05:12.619: INFO: (2) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname1/proxy/: foo (200; 157.065096ms)
Mar  2 02:05:12.619: INFO: (2) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname2/proxy/: bar (200; 156.784735ms)
Mar  2 02:05:12.620: INFO: (2) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname1/proxy/: foo (200; 156.82872ms)
Mar  2 02:05:12.668: INFO: (3) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:462/proxy/: tls qux (200; 47.822259ms)
Mar  2 02:05:12.694: INFO: (3) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 74.33504ms)
Mar  2 02:05:12.694: INFO: (3) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 74.431551ms)
Mar  2 02:05:12.694: INFO: (3) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/rewriteme">test</a> (200; 74.697704ms)
Mar  2 02:05:12.695: INFO: (3) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:460/proxy/: tls baz (200; 74.624243ms)
Mar  2 02:05:12.695: INFO: (3) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">... (200; 75.04857ms)
Mar  2 02:05:12.697: INFO: (3) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/tlsrewritem... (200; 76.876617ms)
Mar  2 02:05:12.697: INFO: (3) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">test<... (200; 76.971791ms)
Mar  2 02:05:12.697: INFO: (3) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 77.567026ms)
Mar  2 02:05:12.713: INFO: (3) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 93.061861ms)
Mar  2 02:05:12.732: INFO: (3) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname1/proxy/: tls baz (200; 112.255443ms)
Mar  2 02:05:12.733: INFO: (3) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname2/proxy/: bar (200; 112.766041ms)
Mar  2 02:05:12.733: INFO: (3) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname2/proxy/: bar (200; 112.389759ms)
Mar  2 02:05:12.733: INFO: (3) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname2/proxy/: tls qux (200; 112.669799ms)
Mar  2 02:05:12.733: INFO: (3) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname1/proxy/: foo (200; 112.542546ms)
Mar  2 02:05:12.742: INFO: (3) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname1/proxy/: foo (200; 122.641934ms)
Mar  2 02:05:12.812: INFO: (4) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/tlsrewritem... (200; 68.806306ms)
Mar  2 02:05:12.812: INFO: (4) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">test<... (200; 68.914161ms)
Mar  2 02:05:12.812: INFO: (4) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:460/proxy/: tls baz (200; 68.705665ms)
Mar  2 02:05:12.812: INFO: (4) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 69.669625ms)
Mar  2 02:05:12.837: INFO: (4) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">... (200; 94.090409ms)
Mar  2 02:05:12.838: INFO: (4) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/rewriteme">test</a> (200; 94.367873ms)
Mar  2 02:05:12.838: INFO: (4) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 95.203717ms)
Mar  2 02:05:12.838: INFO: (4) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 95.105552ms)
Mar  2 02:05:12.838: INFO: (4) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:462/proxy/: tls qux (200; 94.740974ms)
Mar  2 02:05:12.838: INFO: (4) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 95.37665ms)
Mar  2 02:05:12.850: INFO: (4) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname1/proxy/: foo (200; 106.424084ms)
Mar  2 02:05:12.850: INFO: (4) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname1/proxy/: foo (200; 106.057238ms)
Mar  2 02:05:12.850: INFO: (4) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname2/proxy/: bar (200; 106.771337ms)
Mar  2 02:05:12.850: INFO: (4) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname2/proxy/: tls qux (200; 106.257726ms)
Mar  2 02:05:12.850: INFO: (4) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname2/proxy/: bar (200; 106.293389ms)
Mar  2 02:05:12.850: INFO: (4) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname1/proxy/: tls baz (200; 106.910128ms)
Mar  2 02:05:12.908: INFO: (5) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:462/proxy/: tls qux (200; 57.184354ms)
Mar  2 02:05:12.908: INFO: (5) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 57.571085ms)
Mar  2 02:05:12.908: INFO: (5) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/rewriteme">test</a> (200; 57.114005ms)
Mar  2 02:05:12.908: INFO: (5) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 57.706556ms)
Mar  2 02:05:12.908: INFO: (5) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">test<... (200; 57.776676ms)
Mar  2 02:05:12.908: INFO: (5) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 57.941645ms)
Mar  2 02:05:12.908: INFO: (5) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:460/proxy/: tls baz (200; 57.440748ms)
Mar  2 02:05:12.911: INFO: (5) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 60.176372ms)
Mar  2 02:05:12.911: INFO: (5) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/tlsrewritem... (200; 60.577394ms)
Mar  2 02:05:12.911: INFO: (5) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">... (200; 60.609468ms)
Mar  2 02:05:12.913: INFO: (5) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname1/proxy/: foo (200; 62.231286ms)
Mar  2 02:05:12.913: INFO: (5) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname2/proxy/: bar (200; 62.623545ms)
Mar  2 02:05:12.913: INFO: (5) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname1/proxy/: tls baz (200; 62.706222ms)
Mar  2 02:05:12.913: INFO: (5) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname2/proxy/: tls qux (200; 62.171971ms)
Mar  2 02:05:12.917: INFO: (5) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname2/proxy/: bar (200; 65.593171ms)
Mar  2 02:05:12.917: INFO: (5) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname1/proxy/: foo (200; 66.259413ms)
Mar  2 02:05:12.951: INFO: (6) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 33.878614ms)
Mar  2 02:05:12.952: INFO: (6) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:462/proxy/: tls qux (200; 34.229386ms)
Mar  2 02:05:12.952: INFO: (6) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 34.219553ms)
Mar  2 02:05:12.952: INFO: (6) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/tlsrewritem... (200; 34.368669ms)
Mar  2 02:05:12.952: INFO: (6) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 34.419981ms)
Mar  2 02:05:12.952: INFO: (6) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/rewriteme">test</a> (200; 34.302676ms)
Mar  2 02:05:12.952: INFO: (6) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">test<... (200; 34.38491ms)
Mar  2 02:05:12.952: INFO: (6) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">... (200; 34.441404ms)
Mar  2 02:05:12.952: INFO: (6) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:460/proxy/: tls baz (200; 34.520759ms)
Mar  2 02:05:12.952: INFO: (6) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 34.678075ms)
Mar  2 02:05:12.978: INFO: (6) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname2/proxy/: bar (200; 60.724816ms)
Mar  2 02:05:12.978: INFO: (6) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname1/proxy/: tls baz (200; 60.688946ms)
Mar  2 02:05:12.978: INFO: (6) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname1/proxy/: foo (200; 61.045285ms)
Mar  2 02:05:12.978: INFO: (6) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname1/proxy/: foo (200; 60.985363ms)
Mar  2 02:05:12.981: INFO: (6) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname2/proxy/: tls qux (200; 63.394549ms)
Mar  2 02:05:12.981: INFO: (6) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname2/proxy/: bar (200; 63.393475ms)
Mar  2 02:05:13.005: INFO: (7) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:462/proxy/: tls qux (200; 23.274622ms)
Mar  2 02:05:13.038: INFO: (7) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 56.626452ms)
Mar  2 02:05:13.039: INFO: (7) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 56.874452ms)
Mar  2 02:05:13.039: INFO: (7) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">test<... (200; 57.755413ms)
Mar  2 02:05:13.039: INFO: (7) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 57.607951ms)
Mar  2 02:05:13.041: INFO: (7) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/rewriteme">test</a> (200; 58.97993ms)
Mar  2 02:05:13.042: INFO: (7) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 59.87892ms)
Mar  2 02:05:13.042: INFO: (7) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/tlsrewritem... (200; 60.892698ms)
Mar  2 02:05:13.042: INFO: (7) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:460/proxy/: tls baz (200; 60.981848ms)
Mar  2 02:05:13.042: INFO: (7) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">... (200; 61.177078ms)
Mar  2 02:05:13.058: INFO: (7) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname2/proxy/: bar (200; 76.136626ms)
Mar  2 02:05:13.058: INFO: (7) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname1/proxy/: tls baz (200; 75.963349ms)
Mar  2 02:05:13.058: INFO: (7) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname1/proxy/: foo (200; 76.216982ms)
Mar  2 02:05:13.058: INFO: (7) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname2/proxy/: bar (200; 76.051904ms)
Mar  2 02:05:13.058: INFO: (7) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname2/proxy/: tls qux (200; 76.196879ms)
Mar  2 02:05:13.058: INFO: (7) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname1/proxy/: foo (200; 76.399148ms)
Mar  2 02:05:13.093: INFO: (8) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">test<... (200; 34.068216ms)
Mar  2 02:05:13.093: INFO: (8) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 34.29032ms)
Mar  2 02:05:13.093: INFO: (8) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 34.623317ms)
Mar  2 02:05:13.093: INFO: (8) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/rewriteme">test</a> (200; 34.954906ms)
Mar  2 02:05:13.093: INFO: (8) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/tlsrewritem... (200; 34.822379ms)
Mar  2 02:05:13.095: INFO: (8) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:460/proxy/: tls baz (200; 36.371645ms)
Mar  2 02:05:13.095: INFO: (8) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 36.991864ms)
Mar  2 02:05:13.095: INFO: (8) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:462/proxy/: tls qux (200; 37.132208ms)
Mar  2 02:05:13.095: INFO: (8) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 37.503345ms)
Mar  2 02:05:13.095: INFO: (8) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">... (200; 37.575581ms)
Mar  2 02:05:13.128: INFO: (8) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname1/proxy/: tls baz (200; 69.320584ms)
Mar  2 02:05:13.137: INFO: (8) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname2/proxy/: bar (200; 78.363052ms)
Mar  2 02:05:13.137: INFO: (8) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname2/proxy/: bar (200; 78.26309ms)
Mar  2 02:05:13.137: INFO: (8) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname2/proxy/: tls qux (200; 79.087025ms)
Mar  2 02:05:13.138: INFO: (8) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname1/proxy/: foo (200; 79.424629ms)
Mar  2 02:05:13.138: INFO: (8) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname1/proxy/: foo (200; 79.594285ms)
Mar  2 02:05:13.164: INFO: (9) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/rewriteme">test</a> (200; 26.096822ms)
Mar  2 02:05:13.186: INFO: (9) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:462/proxy/: tls qux (200; 48.449249ms)
Mar  2 02:05:13.186: INFO: (9) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 47.453627ms)
Mar  2 02:05:13.186: INFO: (9) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/tlsrewritem... (200; 47.751727ms)
Mar  2 02:05:13.186: INFO: (9) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 47.89085ms)
Mar  2 02:05:13.187: INFO: (9) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 47.835827ms)
Mar  2 02:05:13.187: INFO: (9) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">test<... (200; 47.971355ms)
Mar  2 02:05:13.187: INFO: (9) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:460/proxy/: tls baz (200; 48.03058ms)
Mar  2 02:05:13.187: INFO: (9) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname2/proxy/: tls qux (200; 48.70966ms)
Mar  2 02:05:13.187: INFO: (9) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">... (200; 48.603545ms)
Mar  2 02:05:13.187: INFO: (9) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname1/proxy/: tls baz (200; 48.170493ms)
Mar  2 02:05:13.187: INFO: (9) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname1/proxy/: foo (200; 48.195731ms)
Mar  2 02:05:13.187: INFO: (9) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname1/proxy/: foo (200; 49.115922ms)
Mar  2 02:05:13.187: INFO: (9) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname2/proxy/: bar (200; 48.620523ms)
Mar  2 02:05:13.187: INFO: (9) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname2/proxy/: bar (200; 49.189278ms)
Mar  2 02:05:13.187: INFO: (9) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 48.854476ms)
Mar  2 02:05:13.240: INFO: (10) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">... (200; 52.194354ms)
Mar  2 02:05:13.240: INFO: (10) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">test<... (200; 52.109925ms)
Mar  2 02:05:13.240: INFO: (10) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:462/proxy/: tls qux (200; 52.257334ms)
Mar  2 02:05:13.240: INFO: (10) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/rewriteme">test</a> (200; 52.545715ms)
Mar  2 02:05:13.240: INFO: (10) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 52.925878ms)
Mar  2 02:05:13.240: INFO: (10) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 52.704627ms)
Mar  2 02:05:13.241: INFO: (10) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 52.838809ms)
Mar  2 02:05:13.241: INFO: (10) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/tlsrewritem... (200; 52.697673ms)
Mar  2 02:05:13.241: INFO: (10) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:460/proxy/: tls baz (200; 53.029883ms)
Mar  2 02:05:13.241: INFO: (10) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname1/proxy/: tls baz (200; 52.776285ms)
Mar  2 02:05:13.241: INFO: (10) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname2/proxy/: bar (200; 53.080534ms)
Mar  2 02:05:13.241: INFO: (10) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 52.762333ms)
Mar  2 02:05:13.241: INFO: (10) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname2/proxy/: bar (200; 52.708325ms)
Mar  2 02:05:13.241: INFO: (10) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname1/proxy/: foo (200; 53.178154ms)
Mar  2 02:05:13.241: INFO: (10) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname1/proxy/: foo (200; 52.879248ms)
Mar  2 02:05:13.265: INFO: (10) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname2/proxy/: tls qux (200; 77.089158ms)
Mar  2 02:05:13.295: INFO: (11) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 29.279479ms)
Mar  2 02:05:13.296: INFO: (11) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">test<... (200; 30.367901ms)
Mar  2 02:05:13.296: INFO: (11) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 30.312102ms)
Mar  2 02:05:13.316: INFO: (11) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/tlsrewritem... (200; 51.458154ms)
Mar  2 02:05:13.317: INFO: (11) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/rewriteme">test</a> (200; 51.591574ms)
Mar  2 02:05:13.317: INFO: (11) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 51.548287ms)
Mar  2 02:05:13.317: INFO: (11) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:460/proxy/: tls baz (200; 51.957269ms)
Mar  2 02:05:13.317: INFO: (11) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">... (200; 52.073773ms)
Mar  2 02:05:13.317: INFO: (11) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 51.958138ms)
Mar  2 02:05:13.317: INFO: (11) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname2/proxy/: tls qux (200; 51.94688ms)
Mar  2 02:05:13.317: INFO: (11) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:462/proxy/: tls qux (200; 51.84264ms)
Mar  2 02:05:13.317: INFO: (11) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname1/proxy/: tls baz (200; 51.967016ms)
Mar  2 02:05:13.317: INFO: (11) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname1/proxy/: foo (200; 50.451272ms)
Mar  2 02:05:13.317: INFO: (11) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname2/proxy/: bar (200; 52.170201ms)
Mar  2 02:05:13.317: INFO: (11) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname1/proxy/: foo (200; 51.918578ms)
Mar  2 02:05:13.317: INFO: (11) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname2/proxy/: bar (200; 52.207697ms)
Mar  2 02:05:13.344: INFO: (12) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:462/proxy/: tls qux (200; 27.025743ms)
Mar  2 02:05:13.347: INFO: (12) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:460/proxy/: tls baz (200; 28.534457ms)
Mar  2 02:05:13.348: INFO: (12) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 28.969114ms)
Mar  2 02:05:13.348: INFO: (12) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/tlsrewritem... (200; 29.561945ms)
Mar  2 02:05:13.349: INFO: (12) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">... (200; 30.308444ms)
Mar  2 02:05:13.349: INFO: (12) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">test<... (200; 29.672506ms)
Mar  2 02:05:13.349: INFO: (12) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 29.773955ms)
Mar  2 02:05:13.349: INFO: (12) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/rewriteme">test</a> (200; 30.985834ms)
Mar  2 02:05:13.349: INFO: (12) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 30.304342ms)
Mar  2 02:05:13.349: INFO: (12) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 31.092733ms)
Mar  2 02:05:13.373: INFO: (12) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname1/proxy/: foo (200; 54.217507ms)
Mar  2 02:05:13.374: INFO: (12) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname2/proxy/: bar (200; 56.652446ms)
Mar  2 02:05:13.375: INFO: (12) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname1/proxy/: foo (200; 57.009784ms)
Mar  2 02:05:13.375: INFO: (12) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname2/proxy/: bar (200; 56.135609ms)
Mar  2 02:05:13.375: INFO: (12) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname2/proxy/: tls qux (200; 57.364186ms)
Mar  2 02:05:13.375: INFO: (12) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname1/proxy/: tls baz (200; 56.762606ms)
Mar  2 02:05:13.412: INFO: (13) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">... (200; 34.555409ms)
Mar  2 02:05:13.414: INFO: (13) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/rewriteme">test</a> (200; 36.801587ms)
Mar  2 02:05:13.414: INFO: (13) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">test<... (200; 38.071464ms)
Mar  2 02:05:13.415: INFO: (13) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 38.809297ms)
Mar  2 02:05:13.415: INFO: (13) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 38.851436ms)
Mar  2 02:05:13.415: INFO: (13) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 38.705116ms)
Mar  2 02:05:13.415: INFO: (13) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 39.799383ms)
Mar  2 02:05:13.415: INFO: (13) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:462/proxy/: tls qux (200; 38.689126ms)
Mar  2 02:05:13.416: INFO: (13) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/tlsrewritem... (200; 39.929124ms)
Mar  2 02:05:13.416: INFO: (13) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:460/proxy/: tls baz (200; 39.38614ms)
Mar  2 02:05:13.417: INFO: (13) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname2/proxy/: tls qux (200; 40.188558ms)
Mar  2 02:05:13.423: INFO: (13) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname1/proxy/: foo (200; 46.968493ms)
Mar  2 02:05:13.423: INFO: (13) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname2/proxy/: bar (200; 46.337114ms)
Mar  2 02:05:13.424: INFO: (13) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname1/proxy/: foo (200; 46.490846ms)
Mar  2 02:05:13.424: INFO: (13) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname1/proxy/: tls baz (200; 47.370491ms)
Mar  2 02:05:13.424: INFO: (13) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname2/proxy/: bar (200; 47.474169ms)
Mar  2 02:05:13.444: INFO: (14) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:462/proxy/: tls qux (200; 20.165161ms)
Mar  2 02:05:13.451: INFO: (14) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">... (200; 26.692631ms)
Mar  2 02:05:13.451: INFO: (14) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:460/proxy/: tls baz (200; 25.80712ms)
Mar  2 02:05:13.452: INFO: (14) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 27.031458ms)
Mar  2 02:05:13.452: INFO: (14) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/rewriteme">test</a> (200; 28.066509ms)
Mar  2 02:05:13.452: INFO: (14) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 27.004223ms)
Mar  2 02:05:13.453: INFO: (14) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/tlsrewritem... (200; 28.358876ms)
Mar  2 02:05:13.454: INFO: (14) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 29.077872ms)
Mar  2 02:05:13.454: INFO: (14) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 29.118809ms)
Mar  2 02:05:13.454: INFO: (14) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">test<... (200; 28.745335ms)
Mar  2 02:05:13.464: INFO: (14) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname1/proxy/: foo (200; 40.410567ms)
Mar  2 02:05:13.464: INFO: (14) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname2/proxy/: bar (200; 40.398459ms)
Mar  2 02:05:13.464: INFO: (14) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname1/proxy/: foo (200; 38.977868ms)
Mar  2 02:05:13.465: INFO: (14) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname2/proxy/: bar (200; 40.283577ms)
Mar  2 02:05:13.465: INFO: (14) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname2/proxy/: tls qux (200; 41.368273ms)
Mar  2 02:05:13.466: INFO: (14) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname1/proxy/: tls baz (200; 40.720761ms)
Mar  2 02:05:13.512: INFO: (15) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 44.73823ms)
Mar  2 02:05:13.513: INFO: (15) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">test<... (200; 45.946636ms)
Mar  2 02:05:13.513: INFO: (15) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 45.689268ms)
Mar  2 02:05:13.513: INFO: (15) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 45.929963ms)
Mar  2 02:05:13.514: INFO: (15) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/tlsrewritem... (200; 46.321703ms)
Mar  2 02:05:13.514: INFO: (15) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">... (200; 47.397613ms)
Mar  2 02:05:13.514: INFO: (15) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/rewriteme">test</a> (200; 47.605855ms)
Mar  2 02:05:13.515: INFO: (15) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:460/proxy/: tls baz (200; 48.453921ms)
Mar  2 02:05:13.515: INFO: (15) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:462/proxy/: tls qux (200; 48.851595ms)
Mar  2 02:05:13.515: INFO: (15) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 47.757657ms)
Mar  2 02:05:13.520: INFO: (15) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname2/proxy/: bar (200; 52.684711ms)
Mar  2 02:05:13.521: INFO: (15) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname2/proxy/: bar (200; 53.778313ms)
Mar  2 02:05:13.521: INFO: (15) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname1/proxy/: foo (200; 54.833783ms)
Mar  2 02:05:13.521: INFO: (15) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname1/proxy/: tls baz (200; 55.27013ms)
Mar  2 02:05:13.521: INFO: (15) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname2/proxy/: tls qux (200; 54.826959ms)
Mar  2 02:05:13.525: INFO: (15) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname1/proxy/: foo (200; 58.701913ms)
Mar  2 02:05:13.547: INFO: (16) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/rewriteme">test</a> (200; 21.301134ms)
Mar  2 02:05:13.552: INFO: (16) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 26.486007ms)
Mar  2 02:05:13.552: INFO: (16) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 26.358779ms)
Mar  2 02:05:13.553: INFO: (16) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:462/proxy/: tls qux (200; 26.891542ms)
Mar  2 02:05:13.553: INFO: (16) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:460/proxy/: tls baz (200; 27.137743ms)
Mar  2 02:05:13.553: INFO: (16) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">... (200; 27.280188ms)
Mar  2 02:05:13.553: INFO: (16) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 26.879344ms)
Mar  2 02:05:13.553: INFO: (16) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">test<... (200; 26.995294ms)
Mar  2 02:05:13.553: INFO: (16) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 27.62321ms)
Mar  2 02:05:13.553: INFO: (16) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/tlsrewritem... (200; 27.690466ms)
Mar  2 02:05:13.558: INFO: (16) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname1/proxy/: tls baz (200; 31.757067ms)
Mar  2 02:05:13.568: INFO: (16) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname1/proxy/: foo (200; 42.431032ms)
Mar  2 02:05:13.569: INFO: (16) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname2/proxy/: tls qux (200; 42.844001ms)
Mar  2 02:05:13.569: INFO: (16) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname2/proxy/: bar (200; 43.4137ms)
Mar  2 02:05:13.570: INFO: (16) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname1/proxy/: foo (200; 44.323097ms)
Mar  2 02:05:13.571: INFO: (16) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname2/proxy/: bar (200; 44.700324ms)
Mar  2 02:05:13.625: INFO: (17) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:462/proxy/: tls qux (200; 54.165818ms)
Mar  2 02:05:13.627: INFO: (17) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 55.224968ms)
Mar  2 02:05:13.627: INFO: (17) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 55.348412ms)
Mar  2 02:05:13.627: INFO: (17) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">... (200; 55.43065ms)
Mar  2 02:05:13.627: INFO: (17) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 55.758239ms)
Mar  2 02:05:13.627: INFO: (17) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/rewriteme">test</a> (200; 55.819402ms)
Mar  2 02:05:13.627: INFO: (17) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">test<... (200; 56.178199ms)
Mar  2 02:05:13.627: INFO: (17) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/tlsrewritem... (200; 56.345707ms)
Mar  2 02:05:13.627: INFO: (17) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:460/proxy/: tls baz (200; 56.47202ms)
Mar  2 02:05:13.627: INFO: (17) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 56.366562ms)
Mar  2 02:05:13.639: INFO: (17) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname2/proxy/: bar (200; 67.730084ms)
Mar  2 02:05:13.639: INFO: (17) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname2/proxy/: bar (200; 67.962828ms)
Mar  2 02:05:13.639: INFO: (17) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname2/proxy/: tls qux (200; 67.905788ms)
Mar  2 02:05:13.639: INFO: (17) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname1/proxy/: foo (200; 67.697888ms)
Mar  2 02:05:13.644: INFO: (17) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname1/proxy/: foo (200; 72.758369ms)
Mar  2 02:05:13.644: INFO: (17) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname1/proxy/: tls baz (200; 72.963092ms)
Mar  2 02:05:13.748: INFO: (18) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 103.182149ms)
Mar  2 02:05:13.748: INFO: (18) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/tlsrewritem... (200; 103.035712ms)
Mar  2 02:05:13.748: INFO: (18) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">test<... (200; 103.014768ms)
Mar  2 02:05:13.748: INFO: (18) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 102.947586ms)
Mar  2 02:05:13.748: INFO: (18) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:462/proxy/: tls qux (200; 104.164612ms)
Mar  2 02:05:13.749: INFO: (18) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 103.825294ms)
Mar  2 02:05:13.750: INFO: (18) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname1/proxy/: foo (200; 105.583906ms)
Mar  2 02:05:13.751: INFO: (18) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">... (200; 106.457514ms)
Mar  2 02:05:13.751: INFO: (18) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/rewriteme">test</a> (200; 106.31296ms)
Mar  2 02:05:13.751: INFO: (18) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:460/proxy/: tls baz (200; 106.573635ms)
Mar  2 02:05:13.762: INFO: (18) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname1/proxy/: foo (200; 117.829397ms)
Mar  2 02:05:13.762: INFO: (18) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname2/proxy/: bar (200; 117.544682ms)
Mar  2 02:05:13.762: INFO: (18) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 118.013109ms)
Mar  2 02:05:13.766: INFO: (18) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname1/proxy/: tls baz (200; 121.484261ms)
Mar  2 02:05:13.767: INFO: (18) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname2/proxy/: tls qux (200; 122.083491ms)
Mar  2 02:05:13.767: INFO: (18) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname2/proxy/: bar (200; 122.161548ms)
Mar  2 02:05:13.797: INFO: (19) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 28.55903ms)
Mar  2 02:05:13.798: INFO: (19) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/tlsrewritem... (200; 29.601206ms)
Mar  2 02:05:13.798: INFO: (19) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">... (200; 30.136155ms)
Mar  2 02:05:13.799: INFO: (19) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:460/proxy/: tls baz (200; 31.037924ms)
Mar  2 02:05:13.799: INFO: (19) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 31.209885ms)
Mar  2 02:05:13.799: INFO: (19) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 31.909182ms)
Mar  2 02:05:13.799: INFO: (19) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">test<... (200; 32.13782ms)
Mar  2 02:05:13.799: INFO: (19) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/rewriteme">test</a> (200; 31.520235ms)
Mar  2 02:05:13.801: INFO: (19) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:462/proxy/: tls qux (200; 34.413018ms)
Mar  2 02:05:13.801: INFO: (19) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 34.167429ms)
Mar  2 02:05:13.807: INFO: (19) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname2/proxy/: bar (200; 40.2961ms)
Mar  2 02:05:13.842: INFO: (19) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname2/proxy/: tls qux (200; 74.399233ms)
Mar  2 02:05:13.842: INFO: (19) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname2/proxy/: bar (200; 74.595623ms)
Mar  2 02:05:13.842: INFO: (19) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname1/proxy/: foo (200; 74.861642ms)
Mar  2 02:05:13.842: INFO: (19) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname1/proxy/: foo (200; 75.489301ms)
Mar  2 02:05:13.842: INFO: (19) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname1/proxy/: tls baz (200; 75.282315ms)
STEP: deleting ReplicationController proxy-service-bvrqw in namespace proxy-7314, will wait for the garbage collector to delete the pods 03/02/23 02:05:13.842
Mar  2 02:05:13.964: INFO: Deleting ReplicationController proxy-service-bvrqw took: 52.210104ms
Mar  2 02:05:14.065: INFO: Terminating ReplicationController proxy-service-bvrqw pods took: 101.465065ms
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Mar  2 02:05:16.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7314" for this suite. 03/02/23 02:05:16.843
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","completed":167,"skipped":3234,"failed":0}
------------------------------
• [SLOW TEST] [8.876 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:05:08.026
    Mar  2 02:05:08.026: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename proxy 03/02/23 02:05:08.028
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:05:08.085
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:05:08.103
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 03/02/23 02:05:08.158
    STEP: creating replication controller proxy-service-bvrqw in namespace proxy-7314 03/02/23 02:05:08.164
    I0302 02:05:08.249165      20 runners.go:193] Created replication controller with name: proxy-service-bvrqw, namespace: proxy-7314, replica count: 1
    I0302 02:05:09.300896      20 runners.go:193] proxy-service-bvrqw Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0302 02:05:10.301135      20 runners.go:193] proxy-service-bvrqw Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0302 02:05:11.301872      20 runners.go:193] proxy-service-bvrqw Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I0302 02:05:12.302144      20 runners.go:193] proxy-service-bvrqw Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar  2 02:05:12.325: INFO: setup took 4.207832929s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 03/02/23 02:05:12.325
    Mar  2 02:05:12.373: INFO: (0) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 46.66212ms)
    Mar  2 02:05:12.377: INFO: (0) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">test<... (200; 51.388089ms)
    Mar  2 02:05:12.378: INFO: (0) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 52.397814ms)
    Mar  2 02:05:12.378: INFO: (0) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">... (200; 52.784162ms)
    Mar  2 02:05:12.379: INFO: (0) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 53.051147ms)
    Mar  2 02:05:12.379: INFO: (0) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname2/proxy/: bar (200; 54.207273ms)
    Mar  2 02:05:12.380: INFO: (0) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/rewriteme">test</a> (200; 54.824063ms)
    Mar  2 02:05:12.380: INFO: (0) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 54.651323ms)
    Mar  2 02:05:12.380: INFO: (0) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname1/proxy/: foo (200; 55.308545ms)
    Mar  2 02:05:12.381: INFO: (0) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname1/proxy/: foo (200; 55.142287ms)
    Mar  2 02:05:12.385: INFO: (0) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname2/proxy/: bar (200; 59.40071ms)
    Mar  2 02:05:12.394: INFO: (0) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:462/proxy/: tls qux (200; 68.735037ms)
    Mar  2 02:05:12.394: INFO: (0) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/tlsrewritem... (200; 68.345465ms)
    Mar  2 02:05:12.395: INFO: (0) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname2/proxy/: tls qux (200; 68.915718ms)
    Mar  2 02:05:12.402: INFO: (0) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname1/proxy/: tls baz (200; 75.803025ms)
    Mar  2 02:05:12.402: INFO: (0) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:460/proxy/: tls baz (200; 76.295444ms)
    Mar  2 02:05:12.452: INFO: (1) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/rewriteme">test</a> (200; 46.624969ms)
    Mar  2 02:05:12.452: INFO: (1) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:462/proxy/: tls qux (200; 46.814686ms)
    Mar  2 02:05:12.452: INFO: (1) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 47.528637ms)
    Mar  2 02:05:12.452: INFO: (1) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 48.459682ms)
    Mar  2 02:05:12.452: INFO: (1) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">... (200; 47.055315ms)
    Mar  2 02:05:12.453: INFO: (1) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/tlsrewritem... (200; 48.936531ms)
    Mar  2 02:05:12.453: INFO: (1) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:460/proxy/: tls baz (200; 48.023135ms)
    Mar  2 02:05:12.453: INFO: (1) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 49.24714ms)
    Mar  2 02:05:12.453: INFO: (1) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">test<... (200; 48.577921ms)
    Mar  2 02:05:12.453: INFO: (1) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 49.888437ms)
    Mar  2 02:05:12.459: INFO: (1) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname1/proxy/: foo (200; 54.070326ms)
    Mar  2 02:05:12.460: INFO: (1) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname2/proxy/: bar (200; 55.556528ms)
    Mar  2 02:05:12.461: INFO: (1) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname2/proxy/: tls qux (200; 55.634993ms)
    Mar  2 02:05:12.461: INFO: (1) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname1/proxy/: tls baz (200; 57.156756ms)
    Mar  2 02:05:12.462: INFO: (1) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname2/proxy/: bar (200; 57.792667ms)
    Mar  2 02:05:12.462: INFO: (1) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname1/proxy/: foo (200; 57.341912ms)
    Mar  2 02:05:12.517: INFO: (2) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 55.039998ms)
    Mar  2 02:05:12.518: INFO: (2) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">test<... (200; 55.677188ms)
    Mar  2 02:05:12.518: INFO: (2) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname2/proxy/: tls qux (200; 55.576691ms)
    Mar  2 02:05:12.519: INFO: (2) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 56.337173ms)
    Mar  2 02:05:12.524: INFO: (2) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname1/proxy/: tls baz (200; 61.929059ms)
    Mar  2 02:05:12.533: INFO: (2) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/tlsrewritem... (200; 70.942012ms)
    Mar  2 02:05:12.564: INFO: (2) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:462/proxy/: tls qux (200; 101.910571ms)
    Mar  2 02:05:12.564: INFO: (2) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:460/proxy/: tls baz (200; 101.985894ms)
    Mar  2 02:05:12.564: INFO: (2) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 101.938298ms)
    Mar  2 02:05:12.565: INFO: (2) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/rewriteme">test</a> (200; 102.533476ms)
    Mar  2 02:05:12.565: INFO: (2) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">... (200; 102.720297ms)
    Mar  2 02:05:12.565: INFO: (2) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 103.10115ms)
    Mar  2 02:05:12.598: INFO: (2) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname2/proxy/: bar (200; 136.005824ms)
    Mar  2 02:05:12.619: INFO: (2) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname1/proxy/: foo (200; 157.065096ms)
    Mar  2 02:05:12.619: INFO: (2) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname2/proxy/: bar (200; 156.784735ms)
    Mar  2 02:05:12.620: INFO: (2) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname1/proxy/: foo (200; 156.82872ms)
    Mar  2 02:05:12.668: INFO: (3) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:462/proxy/: tls qux (200; 47.822259ms)
    Mar  2 02:05:12.694: INFO: (3) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 74.33504ms)
    Mar  2 02:05:12.694: INFO: (3) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 74.431551ms)
    Mar  2 02:05:12.694: INFO: (3) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/rewriteme">test</a> (200; 74.697704ms)
    Mar  2 02:05:12.695: INFO: (3) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:460/proxy/: tls baz (200; 74.624243ms)
    Mar  2 02:05:12.695: INFO: (3) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">... (200; 75.04857ms)
    Mar  2 02:05:12.697: INFO: (3) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/tlsrewritem... (200; 76.876617ms)
    Mar  2 02:05:12.697: INFO: (3) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">test<... (200; 76.971791ms)
    Mar  2 02:05:12.697: INFO: (3) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 77.567026ms)
    Mar  2 02:05:12.713: INFO: (3) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 93.061861ms)
    Mar  2 02:05:12.732: INFO: (3) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname1/proxy/: tls baz (200; 112.255443ms)
    Mar  2 02:05:12.733: INFO: (3) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname2/proxy/: bar (200; 112.766041ms)
    Mar  2 02:05:12.733: INFO: (3) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname2/proxy/: bar (200; 112.389759ms)
    Mar  2 02:05:12.733: INFO: (3) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname2/proxy/: tls qux (200; 112.669799ms)
    Mar  2 02:05:12.733: INFO: (3) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname1/proxy/: foo (200; 112.542546ms)
    Mar  2 02:05:12.742: INFO: (3) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname1/proxy/: foo (200; 122.641934ms)
    Mar  2 02:05:12.812: INFO: (4) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/tlsrewritem... (200; 68.806306ms)
    Mar  2 02:05:12.812: INFO: (4) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">test<... (200; 68.914161ms)
    Mar  2 02:05:12.812: INFO: (4) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:460/proxy/: tls baz (200; 68.705665ms)
    Mar  2 02:05:12.812: INFO: (4) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 69.669625ms)
    Mar  2 02:05:12.837: INFO: (4) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">... (200; 94.090409ms)
    Mar  2 02:05:12.838: INFO: (4) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/rewriteme">test</a> (200; 94.367873ms)
    Mar  2 02:05:12.838: INFO: (4) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 95.203717ms)
    Mar  2 02:05:12.838: INFO: (4) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 95.105552ms)
    Mar  2 02:05:12.838: INFO: (4) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:462/proxy/: tls qux (200; 94.740974ms)
    Mar  2 02:05:12.838: INFO: (4) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 95.37665ms)
    Mar  2 02:05:12.850: INFO: (4) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname1/proxy/: foo (200; 106.424084ms)
    Mar  2 02:05:12.850: INFO: (4) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname1/proxy/: foo (200; 106.057238ms)
    Mar  2 02:05:12.850: INFO: (4) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname2/proxy/: bar (200; 106.771337ms)
    Mar  2 02:05:12.850: INFO: (4) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname2/proxy/: tls qux (200; 106.257726ms)
    Mar  2 02:05:12.850: INFO: (4) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname2/proxy/: bar (200; 106.293389ms)
    Mar  2 02:05:12.850: INFO: (4) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname1/proxy/: tls baz (200; 106.910128ms)
    Mar  2 02:05:12.908: INFO: (5) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:462/proxy/: tls qux (200; 57.184354ms)
    Mar  2 02:05:12.908: INFO: (5) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 57.571085ms)
    Mar  2 02:05:12.908: INFO: (5) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/rewriteme">test</a> (200; 57.114005ms)
    Mar  2 02:05:12.908: INFO: (5) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 57.706556ms)
    Mar  2 02:05:12.908: INFO: (5) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">test<... (200; 57.776676ms)
    Mar  2 02:05:12.908: INFO: (5) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 57.941645ms)
    Mar  2 02:05:12.908: INFO: (5) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:460/proxy/: tls baz (200; 57.440748ms)
    Mar  2 02:05:12.911: INFO: (5) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 60.176372ms)
    Mar  2 02:05:12.911: INFO: (5) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/tlsrewritem... (200; 60.577394ms)
    Mar  2 02:05:12.911: INFO: (5) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">... (200; 60.609468ms)
    Mar  2 02:05:12.913: INFO: (5) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname1/proxy/: foo (200; 62.231286ms)
    Mar  2 02:05:12.913: INFO: (5) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname2/proxy/: bar (200; 62.623545ms)
    Mar  2 02:05:12.913: INFO: (5) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname1/proxy/: tls baz (200; 62.706222ms)
    Mar  2 02:05:12.913: INFO: (5) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname2/proxy/: tls qux (200; 62.171971ms)
    Mar  2 02:05:12.917: INFO: (5) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname2/proxy/: bar (200; 65.593171ms)
    Mar  2 02:05:12.917: INFO: (5) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname1/proxy/: foo (200; 66.259413ms)
    Mar  2 02:05:12.951: INFO: (6) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 33.878614ms)
    Mar  2 02:05:12.952: INFO: (6) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:462/proxy/: tls qux (200; 34.229386ms)
    Mar  2 02:05:12.952: INFO: (6) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 34.219553ms)
    Mar  2 02:05:12.952: INFO: (6) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/tlsrewritem... (200; 34.368669ms)
    Mar  2 02:05:12.952: INFO: (6) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 34.419981ms)
    Mar  2 02:05:12.952: INFO: (6) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/rewriteme">test</a> (200; 34.302676ms)
    Mar  2 02:05:12.952: INFO: (6) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">test<... (200; 34.38491ms)
    Mar  2 02:05:12.952: INFO: (6) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">... (200; 34.441404ms)
    Mar  2 02:05:12.952: INFO: (6) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:460/proxy/: tls baz (200; 34.520759ms)
    Mar  2 02:05:12.952: INFO: (6) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 34.678075ms)
    Mar  2 02:05:12.978: INFO: (6) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname2/proxy/: bar (200; 60.724816ms)
    Mar  2 02:05:12.978: INFO: (6) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname1/proxy/: tls baz (200; 60.688946ms)
    Mar  2 02:05:12.978: INFO: (6) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname1/proxy/: foo (200; 61.045285ms)
    Mar  2 02:05:12.978: INFO: (6) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname1/proxy/: foo (200; 60.985363ms)
    Mar  2 02:05:12.981: INFO: (6) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname2/proxy/: tls qux (200; 63.394549ms)
    Mar  2 02:05:12.981: INFO: (6) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname2/proxy/: bar (200; 63.393475ms)
    Mar  2 02:05:13.005: INFO: (7) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:462/proxy/: tls qux (200; 23.274622ms)
    Mar  2 02:05:13.038: INFO: (7) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 56.626452ms)
    Mar  2 02:05:13.039: INFO: (7) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 56.874452ms)
    Mar  2 02:05:13.039: INFO: (7) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">test<... (200; 57.755413ms)
    Mar  2 02:05:13.039: INFO: (7) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 57.607951ms)
    Mar  2 02:05:13.041: INFO: (7) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/rewriteme">test</a> (200; 58.97993ms)
    Mar  2 02:05:13.042: INFO: (7) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 59.87892ms)
    Mar  2 02:05:13.042: INFO: (7) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/tlsrewritem... (200; 60.892698ms)
    Mar  2 02:05:13.042: INFO: (7) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:460/proxy/: tls baz (200; 60.981848ms)
    Mar  2 02:05:13.042: INFO: (7) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">... (200; 61.177078ms)
    Mar  2 02:05:13.058: INFO: (7) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname2/proxy/: bar (200; 76.136626ms)
    Mar  2 02:05:13.058: INFO: (7) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname1/proxy/: tls baz (200; 75.963349ms)
    Mar  2 02:05:13.058: INFO: (7) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname1/proxy/: foo (200; 76.216982ms)
    Mar  2 02:05:13.058: INFO: (7) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname2/proxy/: bar (200; 76.051904ms)
    Mar  2 02:05:13.058: INFO: (7) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname2/proxy/: tls qux (200; 76.196879ms)
    Mar  2 02:05:13.058: INFO: (7) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname1/proxy/: foo (200; 76.399148ms)
    Mar  2 02:05:13.093: INFO: (8) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">test<... (200; 34.068216ms)
    Mar  2 02:05:13.093: INFO: (8) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 34.29032ms)
    Mar  2 02:05:13.093: INFO: (8) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 34.623317ms)
    Mar  2 02:05:13.093: INFO: (8) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/rewriteme">test</a> (200; 34.954906ms)
    Mar  2 02:05:13.093: INFO: (8) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/tlsrewritem... (200; 34.822379ms)
    Mar  2 02:05:13.095: INFO: (8) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:460/proxy/: tls baz (200; 36.371645ms)
    Mar  2 02:05:13.095: INFO: (8) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 36.991864ms)
    Mar  2 02:05:13.095: INFO: (8) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:462/proxy/: tls qux (200; 37.132208ms)
    Mar  2 02:05:13.095: INFO: (8) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 37.503345ms)
    Mar  2 02:05:13.095: INFO: (8) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">... (200; 37.575581ms)
    Mar  2 02:05:13.128: INFO: (8) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname1/proxy/: tls baz (200; 69.320584ms)
    Mar  2 02:05:13.137: INFO: (8) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname2/proxy/: bar (200; 78.363052ms)
    Mar  2 02:05:13.137: INFO: (8) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname2/proxy/: bar (200; 78.26309ms)
    Mar  2 02:05:13.137: INFO: (8) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname2/proxy/: tls qux (200; 79.087025ms)
    Mar  2 02:05:13.138: INFO: (8) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname1/proxy/: foo (200; 79.424629ms)
    Mar  2 02:05:13.138: INFO: (8) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname1/proxy/: foo (200; 79.594285ms)
    Mar  2 02:05:13.164: INFO: (9) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/rewriteme">test</a> (200; 26.096822ms)
    Mar  2 02:05:13.186: INFO: (9) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:462/proxy/: tls qux (200; 48.449249ms)
    Mar  2 02:05:13.186: INFO: (9) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 47.453627ms)
    Mar  2 02:05:13.186: INFO: (9) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/tlsrewritem... (200; 47.751727ms)
    Mar  2 02:05:13.186: INFO: (9) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 47.89085ms)
    Mar  2 02:05:13.187: INFO: (9) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 47.835827ms)
    Mar  2 02:05:13.187: INFO: (9) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">test<... (200; 47.971355ms)
    Mar  2 02:05:13.187: INFO: (9) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:460/proxy/: tls baz (200; 48.03058ms)
    Mar  2 02:05:13.187: INFO: (9) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname2/proxy/: tls qux (200; 48.70966ms)
    Mar  2 02:05:13.187: INFO: (9) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">... (200; 48.603545ms)
    Mar  2 02:05:13.187: INFO: (9) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname1/proxy/: tls baz (200; 48.170493ms)
    Mar  2 02:05:13.187: INFO: (9) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname1/proxy/: foo (200; 48.195731ms)
    Mar  2 02:05:13.187: INFO: (9) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname1/proxy/: foo (200; 49.115922ms)
    Mar  2 02:05:13.187: INFO: (9) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname2/proxy/: bar (200; 48.620523ms)
    Mar  2 02:05:13.187: INFO: (9) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname2/proxy/: bar (200; 49.189278ms)
    Mar  2 02:05:13.187: INFO: (9) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 48.854476ms)
    Mar  2 02:05:13.240: INFO: (10) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">... (200; 52.194354ms)
    Mar  2 02:05:13.240: INFO: (10) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">test<... (200; 52.109925ms)
    Mar  2 02:05:13.240: INFO: (10) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:462/proxy/: tls qux (200; 52.257334ms)
    Mar  2 02:05:13.240: INFO: (10) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/rewriteme">test</a> (200; 52.545715ms)
    Mar  2 02:05:13.240: INFO: (10) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 52.925878ms)
    Mar  2 02:05:13.240: INFO: (10) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 52.704627ms)
    Mar  2 02:05:13.241: INFO: (10) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 52.838809ms)
    Mar  2 02:05:13.241: INFO: (10) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/tlsrewritem... (200; 52.697673ms)
    Mar  2 02:05:13.241: INFO: (10) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:460/proxy/: tls baz (200; 53.029883ms)
    Mar  2 02:05:13.241: INFO: (10) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname1/proxy/: tls baz (200; 52.776285ms)
    Mar  2 02:05:13.241: INFO: (10) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname2/proxy/: bar (200; 53.080534ms)
    Mar  2 02:05:13.241: INFO: (10) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 52.762333ms)
    Mar  2 02:05:13.241: INFO: (10) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname2/proxy/: bar (200; 52.708325ms)
    Mar  2 02:05:13.241: INFO: (10) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname1/proxy/: foo (200; 53.178154ms)
    Mar  2 02:05:13.241: INFO: (10) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname1/proxy/: foo (200; 52.879248ms)
    Mar  2 02:05:13.265: INFO: (10) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname2/proxy/: tls qux (200; 77.089158ms)
    Mar  2 02:05:13.295: INFO: (11) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 29.279479ms)
    Mar  2 02:05:13.296: INFO: (11) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">test<... (200; 30.367901ms)
    Mar  2 02:05:13.296: INFO: (11) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 30.312102ms)
    Mar  2 02:05:13.316: INFO: (11) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/tlsrewritem... (200; 51.458154ms)
    Mar  2 02:05:13.317: INFO: (11) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/rewriteme">test</a> (200; 51.591574ms)
    Mar  2 02:05:13.317: INFO: (11) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 51.548287ms)
    Mar  2 02:05:13.317: INFO: (11) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:460/proxy/: tls baz (200; 51.957269ms)
    Mar  2 02:05:13.317: INFO: (11) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">... (200; 52.073773ms)
    Mar  2 02:05:13.317: INFO: (11) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 51.958138ms)
    Mar  2 02:05:13.317: INFO: (11) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname2/proxy/: tls qux (200; 51.94688ms)
    Mar  2 02:05:13.317: INFO: (11) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:462/proxy/: tls qux (200; 51.84264ms)
    Mar  2 02:05:13.317: INFO: (11) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname1/proxy/: tls baz (200; 51.967016ms)
    Mar  2 02:05:13.317: INFO: (11) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname1/proxy/: foo (200; 50.451272ms)
    Mar  2 02:05:13.317: INFO: (11) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname2/proxy/: bar (200; 52.170201ms)
    Mar  2 02:05:13.317: INFO: (11) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname1/proxy/: foo (200; 51.918578ms)
    Mar  2 02:05:13.317: INFO: (11) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname2/proxy/: bar (200; 52.207697ms)
    Mar  2 02:05:13.344: INFO: (12) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:462/proxy/: tls qux (200; 27.025743ms)
    Mar  2 02:05:13.347: INFO: (12) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:460/proxy/: tls baz (200; 28.534457ms)
    Mar  2 02:05:13.348: INFO: (12) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 28.969114ms)
    Mar  2 02:05:13.348: INFO: (12) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/tlsrewritem... (200; 29.561945ms)
    Mar  2 02:05:13.349: INFO: (12) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">... (200; 30.308444ms)
    Mar  2 02:05:13.349: INFO: (12) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">test<... (200; 29.672506ms)
    Mar  2 02:05:13.349: INFO: (12) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 29.773955ms)
    Mar  2 02:05:13.349: INFO: (12) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/rewriteme">test</a> (200; 30.985834ms)
    Mar  2 02:05:13.349: INFO: (12) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 30.304342ms)
    Mar  2 02:05:13.349: INFO: (12) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 31.092733ms)
    Mar  2 02:05:13.373: INFO: (12) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname1/proxy/: foo (200; 54.217507ms)
    Mar  2 02:05:13.374: INFO: (12) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname2/proxy/: bar (200; 56.652446ms)
    Mar  2 02:05:13.375: INFO: (12) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname1/proxy/: foo (200; 57.009784ms)
    Mar  2 02:05:13.375: INFO: (12) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname2/proxy/: bar (200; 56.135609ms)
    Mar  2 02:05:13.375: INFO: (12) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname2/proxy/: tls qux (200; 57.364186ms)
    Mar  2 02:05:13.375: INFO: (12) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname1/proxy/: tls baz (200; 56.762606ms)
    Mar  2 02:05:13.412: INFO: (13) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">... (200; 34.555409ms)
    Mar  2 02:05:13.414: INFO: (13) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/rewriteme">test</a> (200; 36.801587ms)
    Mar  2 02:05:13.414: INFO: (13) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">test<... (200; 38.071464ms)
    Mar  2 02:05:13.415: INFO: (13) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 38.809297ms)
    Mar  2 02:05:13.415: INFO: (13) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 38.851436ms)
    Mar  2 02:05:13.415: INFO: (13) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 38.705116ms)
    Mar  2 02:05:13.415: INFO: (13) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 39.799383ms)
    Mar  2 02:05:13.415: INFO: (13) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:462/proxy/: tls qux (200; 38.689126ms)
    Mar  2 02:05:13.416: INFO: (13) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/tlsrewritem... (200; 39.929124ms)
    Mar  2 02:05:13.416: INFO: (13) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:460/proxy/: tls baz (200; 39.38614ms)
    Mar  2 02:05:13.417: INFO: (13) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname2/proxy/: tls qux (200; 40.188558ms)
    Mar  2 02:05:13.423: INFO: (13) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname1/proxy/: foo (200; 46.968493ms)
    Mar  2 02:05:13.423: INFO: (13) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname2/proxy/: bar (200; 46.337114ms)
    Mar  2 02:05:13.424: INFO: (13) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname1/proxy/: foo (200; 46.490846ms)
    Mar  2 02:05:13.424: INFO: (13) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname1/proxy/: tls baz (200; 47.370491ms)
    Mar  2 02:05:13.424: INFO: (13) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname2/proxy/: bar (200; 47.474169ms)
    Mar  2 02:05:13.444: INFO: (14) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:462/proxy/: tls qux (200; 20.165161ms)
    Mar  2 02:05:13.451: INFO: (14) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">... (200; 26.692631ms)
    Mar  2 02:05:13.451: INFO: (14) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:460/proxy/: tls baz (200; 25.80712ms)
    Mar  2 02:05:13.452: INFO: (14) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 27.031458ms)
    Mar  2 02:05:13.452: INFO: (14) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/rewriteme">test</a> (200; 28.066509ms)
    Mar  2 02:05:13.452: INFO: (14) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 27.004223ms)
    Mar  2 02:05:13.453: INFO: (14) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/tlsrewritem... (200; 28.358876ms)
    Mar  2 02:05:13.454: INFO: (14) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 29.077872ms)
    Mar  2 02:05:13.454: INFO: (14) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 29.118809ms)
    Mar  2 02:05:13.454: INFO: (14) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">test<... (200; 28.745335ms)
    Mar  2 02:05:13.464: INFO: (14) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname1/proxy/: foo (200; 40.410567ms)
    Mar  2 02:05:13.464: INFO: (14) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname2/proxy/: bar (200; 40.398459ms)
    Mar  2 02:05:13.464: INFO: (14) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname1/proxy/: foo (200; 38.977868ms)
    Mar  2 02:05:13.465: INFO: (14) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname2/proxy/: bar (200; 40.283577ms)
    Mar  2 02:05:13.465: INFO: (14) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname2/proxy/: tls qux (200; 41.368273ms)
    Mar  2 02:05:13.466: INFO: (14) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname1/proxy/: tls baz (200; 40.720761ms)
    Mar  2 02:05:13.512: INFO: (15) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 44.73823ms)
    Mar  2 02:05:13.513: INFO: (15) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">test<... (200; 45.946636ms)
    Mar  2 02:05:13.513: INFO: (15) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 45.689268ms)
    Mar  2 02:05:13.513: INFO: (15) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 45.929963ms)
    Mar  2 02:05:13.514: INFO: (15) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/tlsrewritem... (200; 46.321703ms)
    Mar  2 02:05:13.514: INFO: (15) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">... (200; 47.397613ms)
    Mar  2 02:05:13.514: INFO: (15) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/rewriteme">test</a> (200; 47.605855ms)
    Mar  2 02:05:13.515: INFO: (15) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:460/proxy/: tls baz (200; 48.453921ms)
    Mar  2 02:05:13.515: INFO: (15) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:462/proxy/: tls qux (200; 48.851595ms)
    Mar  2 02:05:13.515: INFO: (15) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 47.757657ms)
    Mar  2 02:05:13.520: INFO: (15) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname2/proxy/: bar (200; 52.684711ms)
    Mar  2 02:05:13.521: INFO: (15) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname2/proxy/: bar (200; 53.778313ms)
    Mar  2 02:05:13.521: INFO: (15) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname1/proxy/: foo (200; 54.833783ms)
    Mar  2 02:05:13.521: INFO: (15) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname1/proxy/: tls baz (200; 55.27013ms)
    Mar  2 02:05:13.521: INFO: (15) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname2/proxy/: tls qux (200; 54.826959ms)
    Mar  2 02:05:13.525: INFO: (15) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname1/proxy/: foo (200; 58.701913ms)
    Mar  2 02:05:13.547: INFO: (16) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/rewriteme">test</a> (200; 21.301134ms)
    Mar  2 02:05:13.552: INFO: (16) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 26.486007ms)
    Mar  2 02:05:13.552: INFO: (16) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 26.358779ms)
    Mar  2 02:05:13.553: INFO: (16) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:462/proxy/: tls qux (200; 26.891542ms)
    Mar  2 02:05:13.553: INFO: (16) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:460/proxy/: tls baz (200; 27.137743ms)
    Mar  2 02:05:13.553: INFO: (16) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">... (200; 27.280188ms)
    Mar  2 02:05:13.553: INFO: (16) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 26.879344ms)
    Mar  2 02:05:13.553: INFO: (16) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">test<... (200; 26.995294ms)
    Mar  2 02:05:13.553: INFO: (16) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 27.62321ms)
    Mar  2 02:05:13.553: INFO: (16) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/tlsrewritem... (200; 27.690466ms)
    Mar  2 02:05:13.558: INFO: (16) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname1/proxy/: tls baz (200; 31.757067ms)
    Mar  2 02:05:13.568: INFO: (16) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname1/proxy/: foo (200; 42.431032ms)
    Mar  2 02:05:13.569: INFO: (16) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname2/proxy/: tls qux (200; 42.844001ms)
    Mar  2 02:05:13.569: INFO: (16) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname2/proxy/: bar (200; 43.4137ms)
    Mar  2 02:05:13.570: INFO: (16) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname1/proxy/: foo (200; 44.323097ms)
    Mar  2 02:05:13.571: INFO: (16) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname2/proxy/: bar (200; 44.700324ms)
    Mar  2 02:05:13.625: INFO: (17) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:462/proxy/: tls qux (200; 54.165818ms)
    Mar  2 02:05:13.627: INFO: (17) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 55.224968ms)
    Mar  2 02:05:13.627: INFO: (17) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 55.348412ms)
    Mar  2 02:05:13.627: INFO: (17) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">... (200; 55.43065ms)
    Mar  2 02:05:13.627: INFO: (17) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 55.758239ms)
    Mar  2 02:05:13.627: INFO: (17) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/rewriteme">test</a> (200; 55.819402ms)
    Mar  2 02:05:13.627: INFO: (17) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">test<... (200; 56.178199ms)
    Mar  2 02:05:13.627: INFO: (17) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/tlsrewritem... (200; 56.345707ms)
    Mar  2 02:05:13.627: INFO: (17) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:460/proxy/: tls baz (200; 56.47202ms)
    Mar  2 02:05:13.627: INFO: (17) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 56.366562ms)
    Mar  2 02:05:13.639: INFO: (17) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname2/proxy/: bar (200; 67.730084ms)
    Mar  2 02:05:13.639: INFO: (17) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname2/proxy/: bar (200; 67.962828ms)
    Mar  2 02:05:13.639: INFO: (17) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname2/proxy/: tls qux (200; 67.905788ms)
    Mar  2 02:05:13.639: INFO: (17) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname1/proxy/: foo (200; 67.697888ms)
    Mar  2 02:05:13.644: INFO: (17) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname1/proxy/: foo (200; 72.758369ms)
    Mar  2 02:05:13.644: INFO: (17) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname1/proxy/: tls baz (200; 72.963092ms)
    Mar  2 02:05:13.748: INFO: (18) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 103.182149ms)
    Mar  2 02:05:13.748: INFO: (18) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/tlsrewritem... (200; 103.035712ms)
    Mar  2 02:05:13.748: INFO: (18) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">test<... (200; 103.014768ms)
    Mar  2 02:05:13.748: INFO: (18) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 102.947586ms)
    Mar  2 02:05:13.748: INFO: (18) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:462/proxy/: tls qux (200; 104.164612ms)
    Mar  2 02:05:13.749: INFO: (18) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 103.825294ms)
    Mar  2 02:05:13.750: INFO: (18) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname1/proxy/: foo (200; 105.583906ms)
    Mar  2 02:05:13.751: INFO: (18) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">... (200; 106.457514ms)
    Mar  2 02:05:13.751: INFO: (18) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/rewriteme">test</a> (200; 106.31296ms)
    Mar  2 02:05:13.751: INFO: (18) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:460/proxy/: tls baz (200; 106.573635ms)
    Mar  2 02:05:13.762: INFO: (18) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname1/proxy/: foo (200; 117.829397ms)
    Mar  2 02:05:13.762: INFO: (18) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname2/proxy/: bar (200; 117.544682ms)
    Mar  2 02:05:13.762: INFO: (18) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 118.013109ms)
    Mar  2 02:05:13.766: INFO: (18) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname1/proxy/: tls baz (200; 121.484261ms)
    Mar  2 02:05:13.767: INFO: (18) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname2/proxy/: tls qux (200; 122.083491ms)
    Mar  2 02:05:13.767: INFO: (18) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname2/proxy/: bar (200; 122.161548ms)
    Mar  2 02:05:13.797: INFO: (19) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 28.55903ms)
    Mar  2 02:05:13.798: INFO: (19) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:443/proxy/tlsrewritem... (200; 29.601206ms)
    Mar  2 02:05:13.798: INFO: (19) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">... (200; 30.136155ms)
    Mar  2 02:05:13.799: INFO: (19) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:460/proxy/: tls baz (200; 31.037924ms)
    Mar  2 02:05:13.799: INFO: (19) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:162/proxy/: bar (200; 31.209885ms)
    Mar  2 02:05:13.799: INFO: (19) /api/v1/namespaces/proxy-7314/pods/http:proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 31.909182ms)
    Mar  2 02:05:13.799: INFO: (19) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:1080/proxy/rewriteme">test<... (200; 32.13782ms)
    Mar  2 02:05:13.799: INFO: (19) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/: <a href="/api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl/proxy/rewriteme">test</a> (200; 31.520235ms)
    Mar  2 02:05:13.801: INFO: (19) /api/v1/namespaces/proxy-7314/pods/https:proxy-service-bvrqw-sj8dl:462/proxy/: tls qux (200; 34.413018ms)
    Mar  2 02:05:13.801: INFO: (19) /api/v1/namespaces/proxy-7314/pods/proxy-service-bvrqw-sj8dl:160/proxy/: foo (200; 34.167429ms)
    Mar  2 02:05:13.807: INFO: (19) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname2/proxy/: bar (200; 40.2961ms)
    Mar  2 02:05:13.842: INFO: (19) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname2/proxy/: tls qux (200; 74.399233ms)
    Mar  2 02:05:13.842: INFO: (19) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname2/proxy/: bar (200; 74.595623ms)
    Mar  2 02:05:13.842: INFO: (19) /api/v1/namespaces/proxy-7314/services/proxy-service-bvrqw:portname1/proxy/: foo (200; 74.861642ms)
    Mar  2 02:05:13.842: INFO: (19) /api/v1/namespaces/proxy-7314/services/http:proxy-service-bvrqw:portname1/proxy/: foo (200; 75.489301ms)
    Mar  2 02:05:13.842: INFO: (19) /api/v1/namespaces/proxy-7314/services/https:proxy-service-bvrqw:tlsportname1/proxy/: tls baz (200; 75.282315ms)
    STEP: deleting ReplicationController proxy-service-bvrqw in namespace proxy-7314, will wait for the garbage collector to delete the pods 03/02/23 02:05:13.842
    Mar  2 02:05:13.964: INFO: Deleting ReplicationController proxy-service-bvrqw took: 52.210104ms
    Mar  2 02:05:14.065: INFO: Terminating ReplicationController proxy-service-bvrqw pods took: 101.465065ms
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Mar  2 02:05:16.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-7314" for this suite. 03/02/23 02:05:16.843
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:05:16.904
Mar  2 02:05:16.905: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename kubectl 03/02/23 02:05:16.907
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:05:17.121
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:05:17.139
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1570
STEP: creating an pod 03/02/23 02:05:17.161
Mar  2 02:05:17.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-4989 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Mar  2 02:05:17.395: INFO: stderr: ""
Mar  2 02:05:17.395: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
STEP: Waiting for log generator to start. 03/02/23 02:05:17.395
Mar  2 02:05:17.396: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Mar  2 02:05:17.396: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-4989" to be "running and ready, or succeeded"
Mar  2 02:05:17.424: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 28.305542ms
Mar  2 02:05:17.424: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on '10.132.92.143' to be 'Running' but was 'Pending'
Mar  2 02:05:19.454: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057883s
Mar  2 02:05:19.454: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on '10.132.92.143' to be 'Running' but was 'Pending'
Mar  2 02:05:21.438: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.042620097s
Mar  2 02:05:21.438: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Mar  2 02:05:21.438: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 03/02/23 02:05:21.438
Mar  2 02:05:21.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-4989 logs logs-generator logs-generator'
Mar  2 02:05:21.685: INFO: stderr: ""
Mar  2 02:05:21.685: INFO: stdout: "I0302 02:05:19.108293       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/mqms 368\nI0302 02:05:19.308561       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/fkt 535\nI0302 02:05:19.509205       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/wx6 405\nI0302 02:05:19.708635       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/cg6 440\nI0302 02:05:19.909137       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/6cnf 271\nI0302 02:05:20.108486       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/zg5 342\nI0302 02:05:20.309138       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/dmt 283\nI0302 02:05:20.508684       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/nln 324\nI0302 02:05:20.709140       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/42d 389\nI0302 02:05:20.911514       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/rntn 501\nI0302 02:05:21.108879       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/tz8 237\nI0302 02:05:21.309289       1 logs_generator.go:76] 11 GET /api/v1/namespaces/default/pods/wkw 362\nI0302 02:05:21.508503       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/2pd 240\n"
STEP: limiting log lines 03/02/23 02:05:21.685
Mar  2 02:05:21.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-4989 logs logs-generator logs-generator --tail=1'
Mar  2 02:05:21.857: INFO: stderr: ""
Mar  2 02:05:21.857: INFO: stdout: "I0302 02:05:21.709316       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/pcrm 511\n"
Mar  2 02:05:21.858: INFO: got output "I0302 02:05:21.709316       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/pcrm 511\n"
STEP: limiting log bytes 03/02/23 02:05:21.858
Mar  2 02:05:21.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-4989 logs logs-generator logs-generator --limit-bytes=1'
Mar  2 02:05:22.023: INFO: stderr: ""
Mar  2 02:05:22.023: INFO: stdout: "I"
Mar  2 02:05:22.023: INFO: got output "I"
STEP: exposing timestamps 03/02/23 02:05:22.023
Mar  2 02:05:22.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-4989 logs logs-generator logs-generator --tail=1 --timestamps'
Mar  2 02:05:22.228: INFO: stderr: ""
Mar  2 02:05:22.228: INFO: stdout: "2023-03-01T20:05:22.108925280-06:00 I0302 02:05:22.108839       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/4wb 547\n"
Mar  2 02:05:22.228: INFO: got output "2023-03-01T20:05:22.108925280-06:00 I0302 02:05:22.108839       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/4wb 547\n"
STEP: restricting to a time range 03/02/23 02:05:22.228
Mar  2 02:05:24.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-4989 logs logs-generator logs-generator --since=1s'
Mar  2 02:05:24.900: INFO: stderr: ""
Mar  2 02:05:24.900: INFO: stdout: "I0302 02:05:23.908518       1 logs_generator.go:76] 24 GET /api/v1/namespaces/ns/pods/dqpw 416\nI0302 02:05:24.108960       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/default/pods/dbv 499\nI0302 02:05:24.309428       1 logs_generator.go:76] 26 GET /api/v1/namespaces/ns/pods/46v 305\nI0302 02:05:24.508884       1 logs_generator.go:76] 27 GET /api/v1/namespaces/kube-system/pods/qlq 426\nI0302 02:05:24.709260       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/ns/pods/6d95 350\n"
Mar  2 02:05:24.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-4989 logs logs-generator logs-generator --since=24h'
Mar  2 02:05:25.128: INFO: stderr: ""
Mar  2 02:05:25.128: INFO: stdout: "I0302 02:05:19.108293       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/mqms 368\nI0302 02:05:19.308561       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/fkt 535\nI0302 02:05:19.509205       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/wx6 405\nI0302 02:05:19.708635       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/cg6 440\nI0302 02:05:19.909137       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/6cnf 271\nI0302 02:05:20.108486       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/zg5 342\nI0302 02:05:20.309138       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/dmt 283\nI0302 02:05:20.508684       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/nln 324\nI0302 02:05:20.709140       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/42d 389\nI0302 02:05:20.911514       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/rntn 501\nI0302 02:05:21.108879       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/tz8 237\nI0302 02:05:21.309289       1 logs_generator.go:76] 11 GET /api/v1/namespaces/default/pods/wkw 362\nI0302 02:05:21.508503       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/2pd 240\nI0302 02:05:21.709316       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/pcrm 511\nI0302 02:05:21.908690       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/6bh 457\nI0302 02:05:22.108839       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/4wb 547\nI0302 02:05:22.309407       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/9bx 529\nI0302 02:05:22.508731       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/kn6h 243\nI0302 02:05:22.708381       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/vdv 416\nI0302 02:05:22.908719       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/7vg5 559\nI0302 02:05:23.109661       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/dz7 380\nI0302 02:05:23.309508       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/default/pods/jrk 213\nI0302 02:05:23.509029       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/kube-system/pods/2lfm 359\nI0302 02:05:23.708371       1 logs_generator.go:76] 23 GET /api/v1/namespaces/default/pods/n8z 363\nI0302 02:05:23.908518       1 logs_generator.go:76] 24 GET /api/v1/namespaces/ns/pods/dqpw 416\nI0302 02:05:24.108960       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/default/pods/dbv 499\nI0302 02:05:24.309428       1 logs_generator.go:76] 26 GET /api/v1/namespaces/ns/pods/46v 305\nI0302 02:05:24.508884       1 logs_generator.go:76] 27 GET /api/v1/namespaces/kube-system/pods/qlq 426\nI0302 02:05:24.709260       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/ns/pods/6d95 350\nI0302 02:05:24.908569       1 logs_generator.go:76] 29 POST /api/v1/namespaces/kube-system/pods/92v 532\nI0302 02:05:25.109036       1 logs_generator.go:76] 30 GET /api/v1/namespaces/kube-system/pods/rk8 475\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1575
Mar  2 02:05:25.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-4989 delete pod logs-generator'
Mar  2 02:05:26.854: INFO: stderr: ""
Mar  2 02:05:26.854: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  2 02:05:26.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4989" for this suite. 03/02/23 02:05:26.879
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","completed":168,"skipped":3248,"failed":0}
------------------------------
• [SLOW TEST] [10.029 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1567
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1590

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:05:16.904
    Mar  2 02:05:16.905: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename kubectl 03/02/23 02:05:16.907
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:05:17.121
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:05:17.139
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1570
    STEP: creating an pod 03/02/23 02:05:17.161
    Mar  2 02:05:17.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-4989 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Mar  2 02:05:17.395: INFO: stderr: ""
    Mar  2 02:05:17.395: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1590
    STEP: Waiting for log generator to start. 03/02/23 02:05:17.395
    Mar  2 02:05:17.396: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Mar  2 02:05:17.396: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-4989" to be "running and ready, or succeeded"
    Mar  2 02:05:17.424: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 28.305542ms
    Mar  2 02:05:17.424: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on '10.132.92.143' to be 'Running' but was 'Pending'
    Mar  2 02:05:19.454: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057883s
    Mar  2 02:05:19.454: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on '10.132.92.143' to be 'Running' but was 'Pending'
    Mar  2 02:05:21.438: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.042620097s
    Mar  2 02:05:21.438: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Mar  2 02:05:21.438: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 03/02/23 02:05:21.438
    Mar  2 02:05:21.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-4989 logs logs-generator logs-generator'
    Mar  2 02:05:21.685: INFO: stderr: ""
    Mar  2 02:05:21.685: INFO: stdout: "I0302 02:05:19.108293       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/mqms 368\nI0302 02:05:19.308561       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/fkt 535\nI0302 02:05:19.509205       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/wx6 405\nI0302 02:05:19.708635       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/cg6 440\nI0302 02:05:19.909137       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/6cnf 271\nI0302 02:05:20.108486       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/zg5 342\nI0302 02:05:20.309138       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/dmt 283\nI0302 02:05:20.508684       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/nln 324\nI0302 02:05:20.709140       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/42d 389\nI0302 02:05:20.911514       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/rntn 501\nI0302 02:05:21.108879       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/tz8 237\nI0302 02:05:21.309289       1 logs_generator.go:76] 11 GET /api/v1/namespaces/default/pods/wkw 362\nI0302 02:05:21.508503       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/2pd 240\n"
    STEP: limiting log lines 03/02/23 02:05:21.685
    Mar  2 02:05:21.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-4989 logs logs-generator logs-generator --tail=1'
    Mar  2 02:05:21.857: INFO: stderr: ""
    Mar  2 02:05:21.857: INFO: stdout: "I0302 02:05:21.709316       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/pcrm 511\n"
    Mar  2 02:05:21.858: INFO: got output "I0302 02:05:21.709316       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/pcrm 511\n"
    STEP: limiting log bytes 03/02/23 02:05:21.858
    Mar  2 02:05:21.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-4989 logs logs-generator logs-generator --limit-bytes=1'
    Mar  2 02:05:22.023: INFO: stderr: ""
    Mar  2 02:05:22.023: INFO: stdout: "I"
    Mar  2 02:05:22.023: INFO: got output "I"
    STEP: exposing timestamps 03/02/23 02:05:22.023
    Mar  2 02:05:22.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-4989 logs logs-generator logs-generator --tail=1 --timestamps'
    Mar  2 02:05:22.228: INFO: stderr: ""
    Mar  2 02:05:22.228: INFO: stdout: "2023-03-01T20:05:22.108925280-06:00 I0302 02:05:22.108839       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/4wb 547\n"
    Mar  2 02:05:22.228: INFO: got output "2023-03-01T20:05:22.108925280-06:00 I0302 02:05:22.108839       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/4wb 547\n"
    STEP: restricting to a time range 03/02/23 02:05:22.228
    Mar  2 02:05:24.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-4989 logs logs-generator logs-generator --since=1s'
    Mar  2 02:05:24.900: INFO: stderr: ""
    Mar  2 02:05:24.900: INFO: stdout: "I0302 02:05:23.908518       1 logs_generator.go:76] 24 GET /api/v1/namespaces/ns/pods/dqpw 416\nI0302 02:05:24.108960       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/default/pods/dbv 499\nI0302 02:05:24.309428       1 logs_generator.go:76] 26 GET /api/v1/namespaces/ns/pods/46v 305\nI0302 02:05:24.508884       1 logs_generator.go:76] 27 GET /api/v1/namespaces/kube-system/pods/qlq 426\nI0302 02:05:24.709260       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/ns/pods/6d95 350\n"
    Mar  2 02:05:24.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-4989 logs logs-generator logs-generator --since=24h'
    Mar  2 02:05:25.128: INFO: stderr: ""
    Mar  2 02:05:25.128: INFO: stdout: "I0302 02:05:19.108293       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/mqms 368\nI0302 02:05:19.308561       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/fkt 535\nI0302 02:05:19.509205       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/wx6 405\nI0302 02:05:19.708635       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/cg6 440\nI0302 02:05:19.909137       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/6cnf 271\nI0302 02:05:20.108486       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/zg5 342\nI0302 02:05:20.309138       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/dmt 283\nI0302 02:05:20.508684       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/nln 324\nI0302 02:05:20.709140       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/42d 389\nI0302 02:05:20.911514       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/rntn 501\nI0302 02:05:21.108879       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/tz8 237\nI0302 02:05:21.309289       1 logs_generator.go:76] 11 GET /api/v1/namespaces/default/pods/wkw 362\nI0302 02:05:21.508503       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/2pd 240\nI0302 02:05:21.709316       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/pcrm 511\nI0302 02:05:21.908690       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/6bh 457\nI0302 02:05:22.108839       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/4wb 547\nI0302 02:05:22.309407       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/9bx 529\nI0302 02:05:22.508731       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/kn6h 243\nI0302 02:05:22.708381       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/vdv 416\nI0302 02:05:22.908719       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/7vg5 559\nI0302 02:05:23.109661       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/dz7 380\nI0302 02:05:23.309508       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/default/pods/jrk 213\nI0302 02:05:23.509029       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/kube-system/pods/2lfm 359\nI0302 02:05:23.708371       1 logs_generator.go:76] 23 GET /api/v1/namespaces/default/pods/n8z 363\nI0302 02:05:23.908518       1 logs_generator.go:76] 24 GET /api/v1/namespaces/ns/pods/dqpw 416\nI0302 02:05:24.108960       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/default/pods/dbv 499\nI0302 02:05:24.309428       1 logs_generator.go:76] 26 GET /api/v1/namespaces/ns/pods/46v 305\nI0302 02:05:24.508884       1 logs_generator.go:76] 27 GET /api/v1/namespaces/kube-system/pods/qlq 426\nI0302 02:05:24.709260       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/ns/pods/6d95 350\nI0302 02:05:24.908569       1 logs_generator.go:76] 29 POST /api/v1/namespaces/kube-system/pods/92v 532\nI0302 02:05:25.109036       1 logs_generator.go:76] 30 GET /api/v1/namespaces/kube-system/pods/rk8 475\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1575
    Mar  2 02:05:25.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-4989 delete pod logs-generator'
    Mar  2 02:05:26.854: INFO: stderr: ""
    Mar  2 02:05:26.854: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  2 02:05:26.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4989" for this suite. 03/02/23 02:05:26.879
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:05:26.934
Mar  2 02:05:26.934: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename podtemplate 03/02/23 02:05:26.936
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:05:27.016
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:05:27.03
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 03/02/23 02:05:27.074
W0302 02:05:27.096408      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "e2e-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "e2e-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "e2e-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "e2e-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Replace a pod template 03/02/23 02:05:27.096
Mar  2 02:05:27.132: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Mar  2 02:05:27.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-796" for this suite. 03/02/23 02:05:27.195
{"msg":"PASSED [sig-node] PodTemplates should replace a pod template [Conformance]","completed":169,"skipped":3253,"failed":0}
------------------------------
• [0.287 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:05:26.934
    Mar  2 02:05:26.934: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename podtemplate 03/02/23 02:05:26.936
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:05:27.016
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:05:27.03
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 03/02/23 02:05:27.074
    W0302 02:05:27.096408      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "e2e-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "e2e-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "e2e-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "e2e-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Replace a pod template 03/02/23 02:05:27.096
    Mar  2 02:05:27.132: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Mar  2 02:05:27.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-796" for this suite. 03/02/23 02:05:27.195
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:05:27.222
Mar  2 02:05:27.223: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename events 03/02/23 02:05:27.224
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:05:27.298
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:05:27.312
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 03/02/23 02:05:27.326
STEP: listing all events in all namespaces 03/02/23 02:05:27.351
STEP: patching the test event 03/02/23 02:05:27.444
STEP: fetching the test event 03/02/23 02:05:27.489
STEP: updating the test event 03/02/23 02:05:27.502
STEP: getting the test event 03/02/23 02:05:27.556
STEP: deleting the test event 03/02/23 02:05:27.574
STEP: listing all events in all namespaces 03/02/23 02:05:27.621
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Mar  2 02:05:27.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2880" for this suite. 03/02/23 02:05:27.707
{"msg":"PASSED [sig-instrumentation] Events should manage the lifecycle of an event [Conformance]","completed":170,"skipped":3275,"failed":0}
------------------------------
• [0.513 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:05:27.222
    Mar  2 02:05:27.223: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename events 03/02/23 02:05:27.224
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:05:27.298
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:05:27.312
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 03/02/23 02:05:27.326
    STEP: listing all events in all namespaces 03/02/23 02:05:27.351
    STEP: patching the test event 03/02/23 02:05:27.444
    STEP: fetching the test event 03/02/23 02:05:27.489
    STEP: updating the test event 03/02/23 02:05:27.502
    STEP: getting the test event 03/02/23 02:05:27.556
    STEP: deleting the test event 03/02/23 02:05:27.574
    STEP: listing all events in all namespaces 03/02/23 02:05:27.621
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Mar  2 02:05:27.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-2880" for this suite. 03/02/23 02:05:27.707
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:05:27.737
Mar  2 02:05:27.737: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename emptydir 03/02/23 02:05:27.739
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:05:27.805
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:05:27.817
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
STEP: Creating a pod to test emptydir 0666 on tmpfs 03/02/23 02:05:27.83
Mar  2 02:05:27.924: INFO: Waiting up to 5m0s for pod "pod-50125e3e-b319-4954-90e7-f1ca7c1b7b7f" in namespace "emptydir-5169" to be "Succeeded or Failed"
Mar  2 02:05:27.941: INFO: Pod "pod-50125e3e-b319-4954-90e7-f1ca7c1b7b7f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.959207ms
Mar  2 02:05:29.971: INFO: Pod "pod-50125e3e-b319-4954-90e7-f1ca7c1b7b7f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046818555s
Mar  2 02:05:31.961: INFO: Pod "pod-50125e3e-b319-4954-90e7-f1ca7c1b7b7f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036480397s
Mar  2 02:05:33.961: INFO: Pod "pod-50125e3e-b319-4954-90e7-f1ca7c1b7b7f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.03687174s
STEP: Saw pod success 03/02/23 02:05:33.961
Mar  2 02:05:33.961: INFO: Pod "pod-50125e3e-b319-4954-90e7-f1ca7c1b7b7f" satisfied condition "Succeeded or Failed"
Mar  2 02:05:33.986: INFO: Trying to get logs from node 10.132.92.143 pod pod-50125e3e-b319-4954-90e7-f1ca7c1b7b7f container test-container: <nil>
STEP: delete the pod 03/02/23 02:05:34.047
Mar  2 02:05:34.117: INFO: Waiting for pod pod-50125e3e-b319-4954-90e7-f1ca7c1b7b7f to disappear
Mar  2 02:05:34.140: INFO: Pod pod-50125e3e-b319-4954-90e7-f1ca7c1b7b7f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  2 02:05:34.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5169" for this suite. 03/02/23 02:05:34.169
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":171,"skipped":3292,"failed":0}
------------------------------
• [SLOW TEST] [6.462 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:05:27.737
    Mar  2 02:05:27.737: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename emptydir 03/02/23 02:05:27.739
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:05:27.805
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:05:27.817
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:136
    STEP: Creating a pod to test emptydir 0666 on tmpfs 03/02/23 02:05:27.83
    Mar  2 02:05:27.924: INFO: Waiting up to 5m0s for pod "pod-50125e3e-b319-4954-90e7-f1ca7c1b7b7f" in namespace "emptydir-5169" to be "Succeeded or Failed"
    Mar  2 02:05:27.941: INFO: Pod "pod-50125e3e-b319-4954-90e7-f1ca7c1b7b7f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.959207ms
    Mar  2 02:05:29.971: INFO: Pod "pod-50125e3e-b319-4954-90e7-f1ca7c1b7b7f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046818555s
    Mar  2 02:05:31.961: INFO: Pod "pod-50125e3e-b319-4954-90e7-f1ca7c1b7b7f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036480397s
    Mar  2 02:05:33.961: INFO: Pod "pod-50125e3e-b319-4954-90e7-f1ca7c1b7b7f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.03687174s
    STEP: Saw pod success 03/02/23 02:05:33.961
    Mar  2 02:05:33.961: INFO: Pod "pod-50125e3e-b319-4954-90e7-f1ca7c1b7b7f" satisfied condition "Succeeded or Failed"
    Mar  2 02:05:33.986: INFO: Trying to get logs from node 10.132.92.143 pod pod-50125e3e-b319-4954-90e7-f1ca7c1b7b7f container test-container: <nil>
    STEP: delete the pod 03/02/23 02:05:34.047
    Mar  2 02:05:34.117: INFO: Waiting for pod pod-50125e3e-b319-4954-90e7-f1ca7c1b7b7f to disappear
    Mar  2 02:05:34.140: INFO: Pod pod-50125e3e-b319-4954-90e7-f1ca7c1b7b7f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  2 02:05:34.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5169" for this suite. 03/02/23 02:05:34.169
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:05:34.204
Mar  2 02:05:34.204: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename replicaset 03/02/23 02:05:34.205
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:05:34.278
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:05:34.291
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Mar  2 02:05:34.428: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar  2 02:05:39.451: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/02/23 02:05:39.451
STEP: Scaling up "test-rs" replicaset  03/02/23 02:05:39.452
Mar  2 02:05:39.498: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 03/02/23 02:05:39.498
W0302 02:05:39.525453      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Mar  2 02:05:39.533: INFO: observed ReplicaSet test-rs in namespace replicaset-7200 with ReadyReplicas 1, AvailableReplicas 1
Mar  2 02:05:39.611: INFO: observed ReplicaSet test-rs in namespace replicaset-7200 with ReadyReplicas 1, AvailableReplicas 1
Mar  2 02:05:39.691: INFO: observed ReplicaSet test-rs in namespace replicaset-7200 with ReadyReplicas 1, AvailableReplicas 1
Mar  2 02:05:39.721: INFO: observed ReplicaSet test-rs in namespace replicaset-7200 with ReadyReplicas 1, AvailableReplicas 1
Mar  2 02:05:41.968: INFO: observed ReplicaSet test-rs in namespace replicaset-7200 with ReadyReplicas 2, AvailableReplicas 2
Mar  2 02:05:42.072: INFO: observed Replicaset test-rs in namespace replicaset-7200 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Mar  2 02:05:42.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7200" for this suite. 03/02/23 02:05:42.094
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","completed":172,"skipped":3319,"failed":0}
------------------------------
• [SLOW TEST] [7.917 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:05:34.204
    Mar  2 02:05:34.204: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename replicaset 03/02/23 02:05:34.205
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:05:34.278
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:05:34.291
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Mar  2 02:05:34.428: INFO: Pod name sample-pod: Found 0 pods out of 1
    Mar  2 02:05:39.451: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/02/23 02:05:39.451
    STEP: Scaling up "test-rs" replicaset  03/02/23 02:05:39.452
    Mar  2 02:05:39.498: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 03/02/23 02:05:39.498
    W0302 02:05:39.525453      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Mar  2 02:05:39.533: INFO: observed ReplicaSet test-rs in namespace replicaset-7200 with ReadyReplicas 1, AvailableReplicas 1
    Mar  2 02:05:39.611: INFO: observed ReplicaSet test-rs in namespace replicaset-7200 with ReadyReplicas 1, AvailableReplicas 1
    Mar  2 02:05:39.691: INFO: observed ReplicaSet test-rs in namespace replicaset-7200 with ReadyReplicas 1, AvailableReplicas 1
    Mar  2 02:05:39.721: INFO: observed ReplicaSet test-rs in namespace replicaset-7200 with ReadyReplicas 1, AvailableReplicas 1
    Mar  2 02:05:41.968: INFO: observed ReplicaSet test-rs in namespace replicaset-7200 with ReadyReplicas 2, AvailableReplicas 2
    Mar  2 02:05:42.072: INFO: observed Replicaset test-rs in namespace replicaset-7200 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Mar  2 02:05:42.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-7200" for this suite. 03/02/23 02:05:42.094
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:05:42.122
Mar  2 02:05:42.122: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename deployment 03/02/23 02:05:42.123
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:05:42.186
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:05:42.2
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 03/02/23 02:05:42.244
STEP: waiting for Deployment to be created 03/02/23 02:05:42.272
STEP: waiting for all Replicas to be Ready 03/02/23 02:05:42.279
Mar  2 02:05:42.286: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  2 02:05:42.286: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  2 02:05:42.318: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  2 02:05:42.318: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  2 02:05:42.355: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  2 02:05:42.355: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  2 02:05:42.452: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  2 02:05:42.452: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  2 02:05:44.899: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Mar  2 02:05:44.899: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Mar  2 02:05:45.007: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 03/02/23 02:05:45.007
W0302 02:05:45.024787      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Mar  2 02:05:45.030: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 03/02/23 02:05:45.03
Mar  2 02:05:45.037: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 0
Mar  2 02:05:45.037: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 0
Mar  2 02:05:45.037: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 0
Mar  2 02:05:45.037: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 0
Mar  2 02:05:45.037: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 0
Mar  2 02:05:45.037: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 0
Mar  2 02:05:45.037: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 0
Mar  2 02:05:45.037: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 0
Mar  2 02:05:45.038: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 1
Mar  2 02:05:45.038: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 1
Mar  2 02:05:45.038: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 2
Mar  2 02:05:45.038: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 2
Mar  2 02:05:45.038: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 2
Mar  2 02:05:45.038: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 2
Mar  2 02:05:45.079: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 2
Mar  2 02:05:45.079: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 2
Mar  2 02:05:45.141: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 2
Mar  2 02:05:45.141: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 2
Mar  2 02:05:45.213: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 1
Mar  2 02:05:45.213: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 1
Mar  2 02:05:45.247: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 1
Mar  2 02:05:45.247: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 1
Mar  2 02:05:46.936: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 2
Mar  2 02:05:46.936: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 2
Mar  2 02:05:47.020: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 1
STEP: listing Deployments 03/02/23 02:05:47.02
Mar  2 02:05:47.051: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 03/02/23 02:05:47.051
Mar  2 02:05:47.141: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 03/02/23 02:05:47.142
Mar  2 02:05:47.227: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  2 02:05:47.227: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  2 02:05:47.227: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  2 02:05:47.227: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  2 02:05:47.242: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  2 02:05:47.266: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  2 02:05:50.006: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar  2 02:05:50.136: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar  2 02:05:50.173: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar  2 02:05:52.112: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 03/02/23 02:05:52.177
STEP: fetching the DeploymentStatus 03/02/23 02:05:52.203
Mar  2 02:05:52.225: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 1
Mar  2 02:05:52.225: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 1
Mar  2 02:05:52.225: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 1
Mar  2 02:05:52.225: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 1
Mar  2 02:05:52.226: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 1
Mar  2 02:05:52.226: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 1
Mar  2 02:05:52.226: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 2
Mar  2 02:05:52.226: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 2
Mar  2 02:05:52.226: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 2
Mar  2 02:05:52.226: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 3
STEP: deleting the Deployment 03/02/23 02:05:52.226
Mar  2 02:05:52.264: INFO: observed event type MODIFIED
Mar  2 02:05:52.265: INFO: observed event type MODIFIED
Mar  2 02:05:52.265: INFO: observed event type MODIFIED
Mar  2 02:05:52.265: INFO: observed event type MODIFIED
Mar  2 02:05:52.265: INFO: observed event type MODIFIED
Mar  2 02:05:52.265: INFO: observed event type MODIFIED
Mar  2 02:05:52.265: INFO: observed event type MODIFIED
Mar  2 02:05:52.266: INFO: observed event type MODIFIED
Mar  2 02:05:52.266: INFO: observed event type MODIFIED
Mar  2 02:05:52.266: INFO: observed event type MODIFIED
Mar  2 02:05:52.266: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar  2 02:05:52.280: INFO: Log out all the ReplicaSets if there is no deployment created
Mar  2 02:05:52.292: INFO: ReplicaSet "test-deployment-54cc775c4b":
&ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-2435  b3d261a6-e0d0-4cac-90e1-c666b75280ee 109463 4 2023-03-02 02:05:45 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 06147802-c6e3-4877-9758-3a48f9438b0c 0xc006dca967 0xc006dca968}] [] [{kube-controller-manager Update apps/v1 2023-03-02 02:05:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06147802-c6e3-4877-9758-3a48f9438b0c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 02:05:52 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006dca9f0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Mar  2 02:05:52.310: INFO: pod: "test-deployment-54cc775c4b-mrrsm":
&Pod{ObjectMeta:{test-deployment-54cc775c4b-mrrsm test-deployment-54cc775c4b- deployment-2435  3792c532-3e82-4be8-a0c1-23c0fd8236f7 109412 0 2023-03-02 02:05:47 +0000 UTC 2023-03-02 02:05:51 +0000 UTC 0xc0036d5a38 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:cb3a1265b2ec6ec97bd114be10e1ab4a94fbb0fa4eaec2f2d1b5e46b415b3e70 cni.projectcalico.org/podIP:172.30.201.244/32 cni.projectcalico.org/podIPs:172.30.201.244/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.201.244"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.201.244"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-54cc775c4b b3d261a6-e0d0-4cac-90e1-c666b75280ee 0xc0036d5a77 0xc0036d5a78}] [] [{kube-controller-manager Update v1 2023-03-02 02:05:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b3d261a6-e0d0-4cac-90e1-c666b75280ee\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 02:05:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 02:05:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-02 02:05:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.201.244\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lv6rr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lv6rr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.132.92.188,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c9,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-n5hp7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:05:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:05:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:05:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:05:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.132.92.188,PodIP:172.30.201.244,StartTime:2023-03-02 02:05:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 02:05:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:cri-o://7c16e88c128cba8b88bd7da14402e4d3a194e3d16644c071bafeb52b85987151,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.201.244,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Mar  2 02:05:52.310: INFO: pod: "test-deployment-54cc775c4b-sbq6r":
&Pod{ObjectMeta:{test-deployment-54cc775c4b-sbq6r test-deployment-54cc775c4b- deployment-2435  bf9b1b28-ad9b-4cf6-ab56-ca3873c3f824 109458 0 2023-03-02 02:05:45 +0000 UTC 2023-03-02 02:05:53 +0000 UTC 0xc0036d5cc0 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:3f84eebe5e1c6c4446e3bbc6862d76b18e02ef1bf82af1b70ecb350030611d5b cni.projectcalico.org/podIP:172.30.156.127/32 cni.projectcalico.org/podIPs:172.30.156.127/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.156.127"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.156.127"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-54cc775c4b b3d261a6-e0d0-4cac-90e1-c666b75280ee 0xc0036d5d07 0xc0036d5d08}] [] [{kube-controller-manager Update v1 2023-03-02 02:05:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b3d261a6-e0d0-4cac-90e1-c666b75280ee\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 02:05:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-02 02:05:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.156.127\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-03-02 02:05:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w8p8c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w8p8c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.132.92.143,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c9,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-n5hp7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:05:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:05:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:05:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:05:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.132.92.143,PodIP:172.30.156.127,StartTime:2023-03-02 02:05:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 02:05:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:cri-o://caa2ff447964afccb3f19fba87ba09067f21adb586460bdd4b25ad674077448b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.156.127,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Mar  2 02:05:52.311: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
&ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-2435  95cbf100-540d-423b-a35e-49a6084bc213 109454 2 2023-03-02 02:05:47 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 06147802-c6e3-4877-9758-3a48f9438b0c 0xc006dcaa57 0xc006dcaa58}] [] [{kube-controller-manager Update apps/v1 2023-03-02 02:05:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06147802-c6e3-4877-9758-3a48f9438b0c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 02:05:52 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006dcaae0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Mar  2 02:05:52.331: INFO: pod: "test-deployment-7c7d8d58c8-2lgp9":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-2lgp9 test-deployment-7c7d8d58c8- deployment-2435  9188a93d-5b3d-4181-8cc5-de18f878b88f 109397 0 2023-03-02 02:05:47 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:7ff8416ed512e640556440da1338f71ad1400c22c37b5d6d0ba6c421fa698210 cni.projectcalico.org/podIP:172.30.156.121/32 cni.projectcalico.org/podIPs:172.30.156.121/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.156.121"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.156.121"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 95cbf100-540d-423b-a35e-49a6084bc213 0xc006dcaec7 0xc006dcaec8}] [] [{kube-controller-manager Update v1 2023-03-02 02:05:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"95cbf100-540d-423b-a35e-49a6084bc213\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 02:05:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 02:05:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-02 02:05:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.156.121\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-922th,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-922th,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.132.92.143,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c9,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-n5hp7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:05:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:05:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:05:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:05:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.132.92.143,PodIP:172.30.156.121,StartTime:2023-03-02 02:05:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 02:05:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://0766e2901658132db5d73bd2ec07f1c5a0cbbd4b89bbcd48a10b5551a800caf3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.156.121,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Mar  2 02:05:52.331: INFO: pod: "test-deployment-7c7d8d58c8-cmgdv":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-cmgdv test-deployment-7c7d8d58c8- deployment-2435  b3c52421-d88f-4881-8991-5448d0b92136 109453 0 2023-03-02 02:05:50 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:d59d0b395a34acf25609b6028f88753e4a6f34b944bec9f5aa2eb925a11a5736 cni.projectcalico.org/podIP:172.30.201.210/32 cni.projectcalico.org/podIPs:172.30.201.210/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.201.210"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.201.210"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 95cbf100-540d-423b-a35e-49a6084bc213 0xc006dcb147 0xc006dcb148}] [] [{kube-controller-manager Update v1 2023-03-02 02:05:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"95cbf100-540d-423b-a35e-49a6084bc213\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 02:05:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 02:05:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-02 02:05:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.201.210\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sf4rr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sf4rr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.132.92.188,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c9,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-n5hp7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:05:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:05:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:05:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:05:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.132.92.188,PodIP:172.30.201.210,StartTime:2023-03-02 02:05:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 02:05:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://242c363d90b5c8f0193a5f27035e81964b2717816577332798a3cdee90849eb4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.201.210,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Mar  2 02:05:52.332: INFO: ReplicaSet "test-deployment-8594bb6fdd":
&ReplicaSet{ObjectMeta:{test-deployment-8594bb6fdd  deployment-2435  f20c2df6-f8fd-49b4-b3b2-ae783924a41a 109284 3 2023-03-02 02:05:42 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 06147802-c6e3-4877-9758-3a48f9438b0c 0xc006dcab57 0xc006dcab58}] [] [{kube-controller-manager Update apps/v1 2023-03-02 02:05:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06147802-c6e3-4877-9758-3a48f9438b0c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 02:05:46 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8594bb6fdd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006dcabe0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar  2 02:05:52.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2435" for this suite. 03/02/23 02:05:52.387
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","completed":173,"skipped":3342,"failed":0}
------------------------------
• [SLOW TEST] [10.294 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:05:42.122
    Mar  2 02:05:42.122: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename deployment 03/02/23 02:05:42.123
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:05:42.186
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:05:42.2
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 03/02/23 02:05:42.244
    STEP: waiting for Deployment to be created 03/02/23 02:05:42.272
    STEP: waiting for all Replicas to be Ready 03/02/23 02:05:42.279
    Mar  2 02:05:42.286: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar  2 02:05:42.286: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar  2 02:05:42.318: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar  2 02:05:42.318: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar  2 02:05:42.355: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar  2 02:05:42.355: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar  2 02:05:42.452: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar  2 02:05:42.452: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar  2 02:05:44.899: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Mar  2 02:05:44.899: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Mar  2 02:05:45.007: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 03/02/23 02:05:45.007
    W0302 02:05:45.024787      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Mar  2 02:05:45.030: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 03/02/23 02:05:45.03
    Mar  2 02:05:45.037: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 0
    Mar  2 02:05:45.037: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 0
    Mar  2 02:05:45.037: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 0
    Mar  2 02:05:45.037: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 0
    Mar  2 02:05:45.037: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 0
    Mar  2 02:05:45.037: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 0
    Mar  2 02:05:45.037: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 0
    Mar  2 02:05:45.037: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 0
    Mar  2 02:05:45.038: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 1
    Mar  2 02:05:45.038: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 1
    Mar  2 02:05:45.038: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 2
    Mar  2 02:05:45.038: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 2
    Mar  2 02:05:45.038: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 2
    Mar  2 02:05:45.038: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 2
    Mar  2 02:05:45.079: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 2
    Mar  2 02:05:45.079: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 2
    Mar  2 02:05:45.141: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 2
    Mar  2 02:05:45.141: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 2
    Mar  2 02:05:45.213: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 1
    Mar  2 02:05:45.213: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 1
    Mar  2 02:05:45.247: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 1
    Mar  2 02:05:45.247: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 1
    Mar  2 02:05:46.936: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 2
    Mar  2 02:05:46.936: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 2
    Mar  2 02:05:47.020: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 1
    STEP: listing Deployments 03/02/23 02:05:47.02
    Mar  2 02:05:47.051: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 03/02/23 02:05:47.051
    Mar  2 02:05:47.141: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 03/02/23 02:05:47.142
    Mar  2 02:05:47.227: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar  2 02:05:47.227: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar  2 02:05:47.227: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar  2 02:05:47.227: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar  2 02:05:47.242: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar  2 02:05:47.266: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar  2 02:05:50.006: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Mar  2 02:05:50.136: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Mar  2 02:05:50.173: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Mar  2 02:05:52.112: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 03/02/23 02:05:52.177
    STEP: fetching the DeploymentStatus 03/02/23 02:05:52.203
    Mar  2 02:05:52.225: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 1
    Mar  2 02:05:52.225: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 1
    Mar  2 02:05:52.225: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 1
    Mar  2 02:05:52.225: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 1
    Mar  2 02:05:52.226: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 1
    Mar  2 02:05:52.226: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 1
    Mar  2 02:05:52.226: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 2
    Mar  2 02:05:52.226: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 2
    Mar  2 02:05:52.226: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 2
    Mar  2 02:05:52.226: INFO: observed Deployment test-deployment in namespace deployment-2435 with ReadyReplicas 3
    STEP: deleting the Deployment 03/02/23 02:05:52.226
    Mar  2 02:05:52.264: INFO: observed event type MODIFIED
    Mar  2 02:05:52.265: INFO: observed event type MODIFIED
    Mar  2 02:05:52.265: INFO: observed event type MODIFIED
    Mar  2 02:05:52.265: INFO: observed event type MODIFIED
    Mar  2 02:05:52.265: INFO: observed event type MODIFIED
    Mar  2 02:05:52.265: INFO: observed event type MODIFIED
    Mar  2 02:05:52.265: INFO: observed event type MODIFIED
    Mar  2 02:05:52.266: INFO: observed event type MODIFIED
    Mar  2 02:05:52.266: INFO: observed event type MODIFIED
    Mar  2 02:05:52.266: INFO: observed event type MODIFIED
    Mar  2 02:05:52.266: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar  2 02:05:52.280: INFO: Log out all the ReplicaSets if there is no deployment created
    Mar  2 02:05:52.292: INFO: ReplicaSet "test-deployment-54cc775c4b":
    &ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-2435  b3d261a6-e0d0-4cac-90e1-c666b75280ee 109463 4 2023-03-02 02:05:45 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 06147802-c6e3-4877-9758-3a48f9438b0c 0xc006dca967 0xc006dca968}] [] [{kube-controller-manager Update apps/v1 2023-03-02 02:05:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06147802-c6e3-4877-9758-3a48f9438b0c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 02:05:52 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006dca9f0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Mar  2 02:05:52.310: INFO: pod: "test-deployment-54cc775c4b-mrrsm":
    &Pod{ObjectMeta:{test-deployment-54cc775c4b-mrrsm test-deployment-54cc775c4b- deployment-2435  3792c532-3e82-4be8-a0c1-23c0fd8236f7 109412 0 2023-03-02 02:05:47 +0000 UTC 2023-03-02 02:05:51 +0000 UTC 0xc0036d5a38 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:cb3a1265b2ec6ec97bd114be10e1ab4a94fbb0fa4eaec2f2d1b5e46b415b3e70 cni.projectcalico.org/podIP:172.30.201.244/32 cni.projectcalico.org/podIPs:172.30.201.244/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.201.244"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.201.244"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-54cc775c4b b3d261a6-e0d0-4cac-90e1-c666b75280ee 0xc0036d5a77 0xc0036d5a78}] [] [{kube-controller-manager Update v1 2023-03-02 02:05:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b3d261a6-e0d0-4cac-90e1-c666b75280ee\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 02:05:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 02:05:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-02 02:05:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.201.244\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lv6rr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lv6rr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.132.92.188,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c9,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-n5hp7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:05:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:05:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:05:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:05:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.132.92.188,PodIP:172.30.201.244,StartTime:2023-03-02 02:05:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 02:05:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:cri-o://7c16e88c128cba8b88bd7da14402e4d3a194e3d16644c071bafeb52b85987151,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.201.244,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Mar  2 02:05:52.310: INFO: pod: "test-deployment-54cc775c4b-sbq6r":
    &Pod{ObjectMeta:{test-deployment-54cc775c4b-sbq6r test-deployment-54cc775c4b- deployment-2435  bf9b1b28-ad9b-4cf6-ab56-ca3873c3f824 109458 0 2023-03-02 02:05:45 +0000 UTC 2023-03-02 02:05:53 +0000 UTC 0xc0036d5cc0 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:3f84eebe5e1c6c4446e3bbc6862d76b18e02ef1bf82af1b70ecb350030611d5b cni.projectcalico.org/podIP:172.30.156.127/32 cni.projectcalico.org/podIPs:172.30.156.127/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.156.127"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.156.127"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-54cc775c4b b3d261a6-e0d0-4cac-90e1-c666b75280ee 0xc0036d5d07 0xc0036d5d08}] [] [{kube-controller-manager Update v1 2023-03-02 02:05:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b3d261a6-e0d0-4cac-90e1-c666b75280ee\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 02:05:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-02 02:05:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.156.127\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-03-02 02:05:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w8p8c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w8p8c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.132.92.143,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c9,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-n5hp7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:05:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:05:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:05:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:05:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.132.92.143,PodIP:172.30.156.127,StartTime:2023-03-02 02:05:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 02:05:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:cri-o://caa2ff447964afccb3f19fba87ba09067f21adb586460bdd4b25ad674077448b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.156.127,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Mar  2 02:05:52.311: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
    &ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-2435  95cbf100-540d-423b-a35e-49a6084bc213 109454 2 2023-03-02 02:05:47 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 06147802-c6e3-4877-9758-3a48f9438b0c 0xc006dcaa57 0xc006dcaa58}] [] [{kube-controller-manager Update apps/v1 2023-03-02 02:05:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06147802-c6e3-4877-9758-3a48f9438b0c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 02:05:52 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006dcaae0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Mar  2 02:05:52.331: INFO: pod: "test-deployment-7c7d8d58c8-2lgp9":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-2lgp9 test-deployment-7c7d8d58c8- deployment-2435  9188a93d-5b3d-4181-8cc5-de18f878b88f 109397 0 2023-03-02 02:05:47 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:7ff8416ed512e640556440da1338f71ad1400c22c37b5d6d0ba6c421fa698210 cni.projectcalico.org/podIP:172.30.156.121/32 cni.projectcalico.org/podIPs:172.30.156.121/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.156.121"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.156.121"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 95cbf100-540d-423b-a35e-49a6084bc213 0xc006dcaec7 0xc006dcaec8}] [] [{kube-controller-manager Update v1 2023-03-02 02:05:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"95cbf100-540d-423b-a35e-49a6084bc213\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 02:05:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 02:05:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-02 02:05:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.156.121\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-922th,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-922th,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.132.92.143,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c9,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-n5hp7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:05:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:05:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:05:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:05:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.132.92.143,PodIP:172.30.156.121,StartTime:2023-03-02 02:05:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 02:05:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://0766e2901658132db5d73bd2ec07f1c5a0cbbd4b89bbcd48a10b5551a800caf3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.156.121,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Mar  2 02:05:52.331: INFO: pod: "test-deployment-7c7d8d58c8-cmgdv":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-cmgdv test-deployment-7c7d8d58c8- deployment-2435  b3c52421-d88f-4881-8991-5448d0b92136 109453 0 2023-03-02 02:05:50 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:d59d0b395a34acf25609b6028f88753e4a6f34b944bec9f5aa2eb925a11a5736 cni.projectcalico.org/podIP:172.30.201.210/32 cni.projectcalico.org/podIPs:172.30.201.210/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.201.210"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.201.210"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 95cbf100-540d-423b-a35e-49a6084bc213 0xc006dcb147 0xc006dcb148}] [] [{kube-controller-manager Update v1 2023-03-02 02:05:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"95cbf100-540d-423b-a35e-49a6084bc213\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 02:05:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 02:05:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-02 02:05:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.201.210\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sf4rr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sf4rr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.132.92.188,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c9,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-n5hp7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:05:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:05:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:05:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:05:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.132.92.188,PodIP:172.30.201.210,StartTime:2023-03-02 02:05:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 02:05:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://242c363d90b5c8f0193a5f27035e81964b2717816577332798a3cdee90849eb4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.201.210,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Mar  2 02:05:52.332: INFO: ReplicaSet "test-deployment-8594bb6fdd":
    &ReplicaSet{ObjectMeta:{test-deployment-8594bb6fdd  deployment-2435  f20c2df6-f8fd-49b4-b3b2-ae783924a41a 109284 3 2023-03-02 02:05:42 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 06147802-c6e3-4877-9758-3a48f9438b0c 0xc006dcab57 0xc006dcab58}] [] [{kube-controller-manager Update apps/v1 2023-03-02 02:05:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06147802-c6e3-4877-9758-3a48f9438b0c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 02:05:46 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8594bb6fdd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006dcabe0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar  2 02:05:52.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-2435" for this suite. 03/02/23 02:05:52.387
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:05:52.424
Mar  2 02:05:52.424: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename configmap 03/02/23 02:05:52.426
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:05:52.494
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:05:52.551
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
STEP: creating a ConfigMap 03/02/23 02:05:52.568
STEP: fetching the ConfigMap 03/02/23 02:05:52.601
STEP: patching the ConfigMap 03/02/23 02:05:52.617
STEP: listing all ConfigMaps in all namespaces with a label selector 03/02/23 02:05:52.651
STEP: deleting the ConfigMap by collection with a label selector 03/02/23 02:05:52.773
STEP: listing all ConfigMaps in test namespace 03/02/23 02:05:52.826
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Mar  2 02:05:52.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6275" for this suite. 03/02/23 02:05:52.866
{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","completed":174,"skipped":3360,"failed":0}
------------------------------
• [0.481 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:05:52.424
    Mar  2 02:05:52.424: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename configmap 03/02/23 02:05:52.426
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:05:52.494
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:05:52.551
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:168
    STEP: creating a ConfigMap 03/02/23 02:05:52.568
    STEP: fetching the ConfigMap 03/02/23 02:05:52.601
    STEP: patching the ConfigMap 03/02/23 02:05:52.617
    STEP: listing all ConfigMaps in all namespaces with a label selector 03/02/23 02:05:52.651
    STEP: deleting the ConfigMap by collection with a label selector 03/02/23 02:05:52.773
    STEP: listing all ConfigMaps in test namespace 03/02/23 02:05:52.826
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  2 02:05:52.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6275" for this suite. 03/02/23 02:05:52.866
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:05:52.907
Mar  2 02:05:52.907: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename projected 03/02/23 02:05:52.91
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:05:52.996
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:05:53.016
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
STEP: Creating configMap with name projected-configmap-test-volume-map-9cceb165-3b48-409a-9079-7d90d89d2853 03/02/23 02:05:53.032
STEP: Creating a pod to test consume configMaps 03/02/23 02:05:53.057
Mar  2 02:05:53.141: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d431da7d-c232-49c3-8aed-ad4c3735d9fd" in namespace "projected-4237" to be "Succeeded or Failed"
Mar  2 02:05:53.154: INFO: Pod "pod-projected-configmaps-d431da7d-c232-49c3-8aed-ad4c3735d9fd": Phase="Pending", Reason="", readiness=false. Elapsed: 12.423664ms
Mar  2 02:05:55.180: INFO: Pod "pod-projected-configmaps-d431da7d-c232-49c3-8aed-ad4c3735d9fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03923421s
Mar  2 02:05:57.170: INFO: Pod "pod-projected-configmaps-d431da7d-c232-49c3-8aed-ad4c3735d9fd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028322533s
Mar  2 02:05:59.169: INFO: Pod "pod-projected-configmaps-d431da7d-c232-49c3-8aed-ad4c3735d9fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027668339s
STEP: Saw pod success 03/02/23 02:05:59.169
Mar  2 02:05:59.169: INFO: Pod "pod-projected-configmaps-d431da7d-c232-49c3-8aed-ad4c3735d9fd" satisfied condition "Succeeded or Failed"
Mar  2 02:05:59.182: INFO: Trying to get logs from node 10.132.92.143 pod pod-projected-configmaps-d431da7d-c232-49c3-8aed-ad4c3735d9fd container agnhost-container: <nil>
STEP: delete the pod 03/02/23 02:05:59.217
Mar  2 02:05:59.274: INFO: Waiting for pod pod-projected-configmaps-d431da7d-c232-49c3-8aed-ad4c3735d9fd to disappear
Mar  2 02:05:59.287: INFO: Pod pod-projected-configmaps-d431da7d-c232-49c3-8aed-ad4c3735d9fd no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar  2 02:05:59.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4237" for this suite. 03/02/23 02:05:59.307
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":175,"skipped":3360,"failed":0}
------------------------------
• [SLOW TEST] [6.426 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:05:52.907
    Mar  2 02:05:52.907: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename projected 03/02/23 02:05:52.91
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:05:52.996
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:05:53.016
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:98
    STEP: Creating configMap with name projected-configmap-test-volume-map-9cceb165-3b48-409a-9079-7d90d89d2853 03/02/23 02:05:53.032
    STEP: Creating a pod to test consume configMaps 03/02/23 02:05:53.057
    Mar  2 02:05:53.141: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d431da7d-c232-49c3-8aed-ad4c3735d9fd" in namespace "projected-4237" to be "Succeeded or Failed"
    Mar  2 02:05:53.154: INFO: Pod "pod-projected-configmaps-d431da7d-c232-49c3-8aed-ad4c3735d9fd": Phase="Pending", Reason="", readiness=false. Elapsed: 12.423664ms
    Mar  2 02:05:55.180: INFO: Pod "pod-projected-configmaps-d431da7d-c232-49c3-8aed-ad4c3735d9fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03923421s
    Mar  2 02:05:57.170: INFO: Pod "pod-projected-configmaps-d431da7d-c232-49c3-8aed-ad4c3735d9fd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028322533s
    Mar  2 02:05:59.169: INFO: Pod "pod-projected-configmaps-d431da7d-c232-49c3-8aed-ad4c3735d9fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027668339s
    STEP: Saw pod success 03/02/23 02:05:59.169
    Mar  2 02:05:59.169: INFO: Pod "pod-projected-configmaps-d431da7d-c232-49c3-8aed-ad4c3735d9fd" satisfied condition "Succeeded or Failed"
    Mar  2 02:05:59.182: INFO: Trying to get logs from node 10.132.92.143 pod pod-projected-configmaps-d431da7d-c232-49c3-8aed-ad4c3735d9fd container agnhost-container: <nil>
    STEP: delete the pod 03/02/23 02:05:59.217
    Mar  2 02:05:59.274: INFO: Waiting for pod pod-projected-configmaps-d431da7d-c232-49c3-8aed-ad4c3735d9fd to disappear
    Mar  2 02:05:59.287: INFO: Pod pod-projected-configmaps-d431da7d-c232-49c3-8aed-ad4c3735d9fd no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar  2 02:05:59.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4237" for this suite. 03/02/23 02:05:59.307
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:05:59.338
Mar  2 02:05:59.338: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename projected 03/02/23 02:05:59.339
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:05:59.4
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:05:59.413
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
STEP: Creating a pod to test downward API volume plugin 03/02/23 02:05:59.426
Mar  2 02:05:59.490: INFO: Waiting up to 5m0s for pod "downwardapi-volume-872d38fc-4599-4f72-ae25-454a331517aa" in namespace "projected-8443" to be "Succeeded or Failed"
Mar  2 02:05:59.508: INFO: Pod "downwardapi-volume-872d38fc-4599-4f72-ae25-454a331517aa": Phase="Pending", Reason="", readiness=false. Elapsed: 18.117752ms
Mar  2 02:06:01.524: INFO: Pod "downwardapi-volume-872d38fc-4599-4f72-ae25-454a331517aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033433163s
Mar  2 02:06:03.522: INFO: Pod "downwardapi-volume-872d38fc-4599-4f72-ae25-454a331517aa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031635254s
Mar  2 02:06:05.543: INFO: Pod "downwardapi-volume-872d38fc-4599-4f72-ae25-454a331517aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.053053789s
STEP: Saw pod success 03/02/23 02:06:05.543
Mar  2 02:06:05.544: INFO: Pod "downwardapi-volume-872d38fc-4599-4f72-ae25-454a331517aa" satisfied condition "Succeeded or Failed"
Mar  2 02:06:05.564: INFO: Trying to get logs from node 10.132.92.143 pod downwardapi-volume-872d38fc-4599-4f72-ae25-454a331517aa container client-container: <nil>
STEP: delete the pod 03/02/23 02:06:05.632
Mar  2 02:06:05.704: INFO: Waiting for pod downwardapi-volume-872d38fc-4599-4f72-ae25-454a331517aa to disappear
Mar  2 02:06:05.748: INFO: Pod downwardapi-volume-872d38fc-4599-4f72-ae25-454a331517aa no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar  2 02:06:05.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8443" for this suite. 03/02/23 02:06:05.799
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","completed":176,"skipped":3373,"failed":0}
------------------------------
• [SLOW TEST] [6.510 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:05:59.338
    Mar  2 02:05:59.338: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename projected 03/02/23 02:05:59.339
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:05:59.4
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:05:59.413
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:206
    STEP: Creating a pod to test downward API volume plugin 03/02/23 02:05:59.426
    Mar  2 02:05:59.490: INFO: Waiting up to 5m0s for pod "downwardapi-volume-872d38fc-4599-4f72-ae25-454a331517aa" in namespace "projected-8443" to be "Succeeded or Failed"
    Mar  2 02:05:59.508: INFO: Pod "downwardapi-volume-872d38fc-4599-4f72-ae25-454a331517aa": Phase="Pending", Reason="", readiness=false. Elapsed: 18.117752ms
    Mar  2 02:06:01.524: INFO: Pod "downwardapi-volume-872d38fc-4599-4f72-ae25-454a331517aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033433163s
    Mar  2 02:06:03.522: INFO: Pod "downwardapi-volume-872d38fc-4599-4f72-ae25-454a331517aa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031635254s
    Mar  2 02:06:05.543: INFO: Pod "downwardapi-volume-872d38fc-4599-4f72-ae25-454a331517aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.053053789s
    STEP: Saw pod success 03/02/23 02:06:05.543
    Mar  2 02:06:05.544: INFO: Pod "downwardapi-volume-872d38fc-4599-4f72-ae25-454a331517aa" satisfied condition "Succeeded or Failed"
    Mar  2 02:06:05.564: INFO: Trying to get logs from node 10.132.92.143 pod downwardapi-volume-872d38fc-4599-4f72-ae25-454a331517aa container client-container: <nil>
    STEP: delete the pod 03/02/23 02:06:05.632
    Mar  2 02:06:05.704: INFO: Waiting for pod downwardapi-volume-872d38fc-4599-4f72-ae25-454a331517aa to disappear
    Mar  2 02:06:05.748: INFO: Pod downwardapi-volume-872d38fc-4599-4f72-ae25-454a331517aa no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar  2 02:06:05.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8443" for this suite. 03/02/23 02:06:05.799
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:06:05.849
Mar  2 02:06:05.849: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename custom-resource-definition 03/02/23 02:06:05.851
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:06:05.95
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:06:05.963
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Mar  2 02:06:05.977: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 02:06:14.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3873" for this suite. 03/02/23 02:06:14.924
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","completed":177,"skipped":3387,"failed":0}
------------------------------
• [SLOW TEST] [9.104 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:06:05.849
    Mar  2 02:06:05.849: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename custom-resource-definition 03/02/23 02:06:05.851
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:06:05.95
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:06:05.963
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Mar  2 02:06:05.977: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 02:06:14.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-3873" for this suite. 03/02/23 02:06:14.924
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:06:14.956
Mar  2 02:06:14.956: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename var-expansion 03/02/23 02:06:14.959
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:06:15.06
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:06:15.087
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
Mar  2 02:06:15.355: INFO: Waiting up to 2m0s for pod "var-expansion-14ff7788-3ea3-4af2-afc2-3264da075c25" in namespace "var-expansion-1105" to be "container 0 failed with reason CreateContainerConfigError"
Mar  2 02:06:15.404: INFO: Pod "var-expansion-14ff7788-3ea3-4af2-afc2-3264da075c25": Phase="Pending", Reason="", readiness=false. Elapsed: 49.766562ms
Mar  2 02:06:17.418: INFO: Pod "var-expansion-14ff7788-3ea3-4af2-afc2-3264da075c25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.063198436s
Mar  2 02:06:17.418: INFO: Pod "var-expansion-14ff7788-3ea3-4af2-afc2-3264da075c25" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Mar  2 02:06:17.418: INFO: Deleting pod "var-expansion-14ff7788-3ea3-4af2-afc2-3264da075c25" in namespace "var-expansion-1105"
Mar  2 02:06:17.450: INFO: Wait up to 5m0s for pod "var-expansion-14ff7788-3ea3-4af2-afc2-3264da075c25" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar  2 02:06:21.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1105" for this suite. 03/02/23 02:06:21.547
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","completed":178,"skipped":3406,"failed":0}
------------------------------
• [SLOW TEST] [6.644 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:06:14.956
    Mar  2 02:06:14.956: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename var-expansion 03/02/23 02:06:14.959
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:06:15.06
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:06:15.087
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:185
    Mar  2 02:06:15.355: INFO: Waiting up to 2m0s for pod "var-expansion-14ff7788-3ea3-4af2-afc2-3264da075c25" in namespace "var-expansion-1105" to be "container 0 failed with reason CreateContainerConfigError"
    Mar  2 02:06:15.404: INFO: Pod "var-expansion-14ff7788-3ea3-4af2-afc2-3264da075c25": Phase="Pending", Reason="", readiness=false. Elapsed: 49.766562ms
    Mar  2 02:06:17.418: INFO: Pod "var-expansion-14ff7788-3ea3-4af2-afc2-3264da075c25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.063198436s
    Mar  2 02:06:17.418: INFO: Pod "var-expansion-14ff7788-3ea3-4af2-afc2-3264da075c25" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Mar  2 02:06:17.418: INFO: Deleting pod "var-expansion-14ff7788-3ea3-4af2-afc2-3264da075c25" in namespace "var-expansion-1105"
    Mar  2 02:06:17.450: INFO: Wait up to 5m0s for pod "var-expansion-14ff7788-3ea3-4af2-afc2-3264da075c25" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar  2 02:06:21.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-1105" for this suite. 03/02/23 02:06:21.547
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:06:21.601
Mar  2 02:06:21.601: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename var-expansion 03/02/23 02:06:21.602
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:06:21.707
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:06:21.726
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
STEP: Creating a pod to test substitution in container's args 03/02/23 02:06:21.74
Mar  2 02:06:21.896: INFO: Waiting up to 5m0s for pod "var-expansion-000e3d16-78e8-4ef6-9a49-6bad9ad7b0f0" in namespace "var-expansion-6657" to be "Succeeded or Failed"
Mar  2 02:06:21.913: INFO: Pod "var-expansion-000e3d16-78e8-4ef6-9a49-6bad9ad7b0f0": Phase="Pending", Reason="", readiness=false. Elapsed: 16.764149ms
Mar  2 02:06:23.928: INFO: Pod "var-expansion-000e3d16-78e8-4ef6-9a49-6bad9ad7b0f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032127923s
Mar  2 02:06:25.931: INFO: Pod "var-expansion-000e3d16-78e8-4ef6-9a49-6bad9ad7b0f0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034627773s
Mar  2 02:06:28.076: INFO: Pod "var-expansion-000e3d16-78e8-4ef6-9a49-6bad9ad7b0f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.179774546s
STEP: Saw pod success 03/02/23 02:06:28.076
Mar  2 02:06:28.076: INFO: Pod "var-expansion-000e3d16-78e8-4ef6-9a49-6bad9ad7b0f0" satisfied condition "Succeeded or Failed"
Mar  2 02:06:28.098: INFO: Trying to get logs from node 10.132.92.143 pod var-expansion-000e3d16-78e8-4ef6-9a49-6bad9ad7b0f0 container dapi-container: <nil>
STEP: delete the pod 03/02/23 02:06:28.148
Mar  2 02:06:28.234: INFO: Waiting for pod var-expansion-000e3d16-78e8-4ef6-9a49-6bad9ad7b0f0 to disappear
Mar  2 02:06:28.249: INFO: Pod var-expansion-000e3d16-78e8-4ef6-9a49-6bad9ad7b0f0 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar  2 02:06:28.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6657" for this suite. 03/02/23 02:06:28.279
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","completed":179,"skipped":3417,"failed":0}
------------------------------
• [SLOW TEST] [6.702 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:06:21.601
    Mar  2 02:06:21.601: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename var-expansion 03/02/23 02:06:21.602
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:06:21.707
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:06:21.726
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:91
    STEP: Creating a pod to test substitution in container's args 03/02/23 02:06:21.74
    Mar  2 02:06:21.896: INFO: Waiting up to 5m0s for pod "var-expansion-000e3d16-78e8-4ef6-9a49-6bad9ad7b0f0" in namespace "var-expansion-6657" to be "Succeeded or Failed"
    Mar  2 02:06:21.913: INFO: Pod "var-expansion-000e3d16-78e8-4ef6-9a49-6bad9ad7b0f0": Phase="Pending", Reason="", readiness=false. Elapsed: 16.764149ms
    Mar  2 02:06:23.928: INFO: Pod "var-expansion-000e3d16-78e8-4ef6-9a49-6bad9ad7b0f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032127923s
    Mar  2 02:06:25.931: INFO: Pod "var-expansion-000e3d16-78e8-4ef6-9a49-6bad9ad7b0f0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034627773s
    Mar  2 02:06:28.076: INFO: Pod "var-expansion-000e3d16-78e8-4ef6-9a49-6bad9ad7b0f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.179774546s
    STEP: Saw pod success 03/02/23 02:06:28.076
    Mar  2 02:06:28.076: INFO: Pod "var-expansion-000e3d16-78e8-4ef6-9a49-6bad9ad7b0f0" satisfied condition "Succeeded or Failed"
    Mar  2 02:06:28.098: INFO: Trying to get logs from node 10.132.92.143 pod var-expansion-000e3d16-78e8-4ef6-9a49-6bad9ad7b0f0 container dapi-container: <nil>
    STEP: delete the pod 03/02/23 02:06:28.148
    Mar  2 02:06:28.234: INFO: Waiting for pod var-expansion-000e3d16-78e8-4ef6-9a49-6bad9ad7b0f0 to disappear
    Mar  2 02:06:28.249: INFO: Pod var-expansion-000e3d16-78e8-4ef6-9a49-6bad9ad7b0f0 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar  2 02:06:28.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-6657" for this suite. 03/02/23 02:06:28.279
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:06:28.306
Mar  2 02:06:28.306: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename downward-api 03/02/23 02:06:28.309
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:06:28.38
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:06:28.419
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
STEP: Creating the pod 03/02/23 02:06:28.441
Mar  2 02:06:28.609: INFO: Waiting up to 5m0s for pod "labelsupdate18c66332-4a1f-40f0-af88-68185b364ec9" in namespace "downward-api-6795" to be "running and ready"
Mar  2 02:06:28.733: INFO: Pod "labelsupdate18c66332-4a1f-40f0-af88-68185b364ec9": Phase="Pending", Reason="", readiness=false. Elapsed: 123.762069ms
Mar  2 02:06:28.733: INFO: The phase of Pod labelsupdate18c66332-4a1f-40f0-af88-68185b364ec9 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 02:06:30.752: INFO: Pod "labelsupdate18c66332-4a1f-40f0-af88-68185b364ec9": Phase="Running", Reason="", readiness=true. Elapsed: 2.142217536s
Mar  2 02:06:30.752: INFO: The phase of Pod labelsupdate18c66332-4a1f-40f0-af88-68185b364ec9 is Running (Ready = true)
Mar  2 02:06:30.752: INFO: Pod "labelsupdate18c66332-4a1f-40f0-af88-68185b364ec9" satisfied condition "running and ready"
Mar  2 02:06:31.380: INFO: Successfully updated pod "labelsupdate18c66332-4a1f-40f0-af88-68185b364ec9"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar  2 02:06:35.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6795" for this suite. 03/02/23 02:06:35.546
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","completed":180,"skipped":3431,"failed":0}
------------------------------
• [SLOW TEST] [7.270 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:06:28.306
    Mar  2 02:06:28.306: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename downward-api 03/02/23 02:06:28.309
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:06:28.38
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:06:28.419
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:129
    STEP: Creating the pod 03/02/23 02:06:28.441
    Mar  2 02:06:28.609: INFO: Waiting up to 5m0s for pod "labelsupdate18c66332-4a1f-40f0-af88-68185b364ec9" in namespace "downward-api-6795" to be "running and ready"
    Mar  2 02:06:28.733: INFO: Pod "labelsupdate18c66332-4a1f-40f0-af88-68185b364ec9": Phase="Pending", Reason="", readiness=false. Elapsed: 123.762069ms
    Mar  2 02:06:28.733: INFO: The phase of Pod labelsupdate18c66332-4a1f-40f0-af88-68185b364ec9 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 02:06:30.752: INFO: Pod "labelsupdate18c66332-4a1f-40f0-af88-68185b364ec9": Phase="Running", Reason="", readiness=true. Elapsed: 2.142217536s
    Mar  2 02:06:30.752: INFO: The phase of Pod labelsupdate18c66332-4a1f-40f0-af88-68185b364ec9 is Running (Ready = true)
    Mar  2 02:06:30.752: INFO: Pod "labelsupdate18c66332-4a1f-40f0-af88-68185b364ec9" satisfied condition "running and ready"
    Mar  2 02:06:31.380: INFO: Successfully updated pod "labelsupdate18c66332-4a1f-40f0-af88-68185b364ec9"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar  2 02:06:35.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6795" for this suite. 03/02/23 02:06:35.546
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:06:35.577
Mar  2 02:06:35.577: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename daemonsets 03/02/23 02:06:35.578
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:06:35.655
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:06:35.668
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
STEP: Creating simple DaemonSet "daemon-set" 03/02/23 02:06:35.97
STEP: Check that daemon pods launch on every node of the cluster. 03/02/23 02:06:35.999
Mar  2 02:06:36.043: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 02:06:36.043: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
Mar  2 02:06:37.090: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 02:06:37.090: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
Mar  2 02:06:38.087: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 02:06:38.087: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
Mar  2 02:06:39.079: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar  2 02:06:39.079: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status 03/02/23 02:06:39.095
Mar  2 02:06:39.110: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 03/02/23 02:06:39.11
Mar  2 02:06:39.150: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 03/02/23 02:06:39.15
Mar  2 02:06:39.157: INFO: Observed &DaemonSet event: ADDED
Mar  2 02:06:39.157: INFO: Observed &DaemonSet event: MODIFIED
Mar  2 02:06:39.157: INFO: Observed &DaemonSet event: MODIFIED
Mar  2 02:06:39.157: INFO: Observed &DaemonSet event: MODIFIED
Mar  2 02:06:39.157: INFO: Observed &DaemonSet event: MODIFIED
Mar  2 02:06:39.157: INFO: Found daemon set daemon-set in namespace daemonsets-1649 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar  2 02:06:39.157: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 03/02/23 02:06:39.157
STEP: watching for the daemon set status to be patched 03/02/23 02:06:39.182
Mar  2 02:06:39.189: INFO: Observed &DaemonSet event: ADDED
Mar  2 02:06:39.189: INFO: Observed &DaemonSet event: MODIFIED
Mar  2 02:06:39.190: INFO: Observed &DaemonSet event: MODIFIED
Mar  2 02:06:39.190: INFO: Observed &DaemonSet event: MODIFIED
Mar  2 02:06:39.190: INFO: Observed &DaemonSet event: MODIFIED
Mar  2 02:06:39.191: INFO: Observed daemon set daemon-set in namespace daemonsets-1649 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar  2 02:06:39.191: INFO: Observed &DaemonSet event: MODIFIED
Mar  2 02:06:39.191: INFO: Found daemon set daemon-set in namespace daemonsets-1649 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Mar  2 02:06:39.191: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 03/02/23 02:06:39.208
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1649, will wait for the garbage collector to delete the pods 03/02/23 02:06:39.209
Mar  2 02:06:39.301: INFO: Deleting DaemonSet.extensions daemon-set took: 28.994993ms
Mar  2 02:06:39.502: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.834909ms
Mar  2 02:06:43.418: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 02:06:43.418: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar  2 02:06:43.440: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"110597"},"items":null}

Mar  2 02:06:43.455: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"110597"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Mar  2 02:06:43.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1649" for this suite. 03/02/23 02:06:43.543
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","completed":181,"skipped":3446,"failed":0}
------------------------------
• [SLOW TEST] [7.992 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:06:35.577
    Mar  2 02:06:35.577: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename daemonsets 03/02/23 02:06:35.578
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:06:35.655
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:06:35.668
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:861
    STEP: Creating simple DaemonSet "daemon-set" 03/02/23 02:06:35.97
    STEP: Check that daemon pods launch on every node of the cluster. 03/02/23 02:06:35.999
    Mar  2 02:06:36.043: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 02:06:36.043: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
    Mar  2 02:06:37.090: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 02:06:37.090: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
    Mar  2 02:06:38.087: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 02:06:38.087: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
    Mar  2 02:06:39.079: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar  2 02:06:39.079: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Getting /status 03/02/23 02:06:39.095
    Mar  2 02:06:39.110: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 03/02/23 02:06:39.11
    Mar  2 02:06:39.150: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 03/02/23 02:06:39.15
    Mar  2 02:06:39.157: INFO: Observed &DaemonSet event: ADDED
    Mar  2 02:06:39.157: INFO: Observed &DaemonSet event: MODIFIED
    Mar  2 02:06:39.157: INFO: Observed &DaemonSet event: MODIFIED
    Mar  2 02:06:39.157: INFO: Observed &DaemonSet event: MODIFIED
    Mar  2 02:06:39.157: INFO: Observed &DaemonSet event: MODIFIED
    Mar  2 02:06:39.157: INFO: Found daemon set daemon-set in namespace daemonsets-1649 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Mar  2 02:06:39.157: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 03/02/23 02:06:39.157
    STEP: watching for the daemon set status to be patched 03/02/23 02:06:39.182
    Mar  2 02:06:39.189: INFO: Observed &DaemonSet event: ADDED
    Mar  2 02:06:39.189: INFO: Observed &DaemonSet event: MODIFIED
    Mar  2 02:06:39.190: INFO: Observed &DaemonSet event: MODIFIED
    Mar  2 02:06:39.190: INFO: Observed &DaemonSet event: MODIFIED
    Mar  2 02:06:39.190: INFO: Observed &DaemonSet event: MODIFIED
    Mar  2 02:06:39.191: INFO: Observed daemon set daemon-set in namespace daemonsets-1649 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Mar  2 02:06:39.191: INFO: Observed &DaemonSet event: MODIFIED
    Mar  2 02:06:39.191: INFO: Found daemon set daemon-set in namespace daemonsets-1649 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Mar  2 02:06:39.191: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 03/02/23 02:06:39.208
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1649, will wait for the garbage collector to delete the pods 03/02/23 02:06:39.209
    Mar  2 02:06:39.301: INFO: Deleting DaemonSet.extensions daemon-set took: 28.994993ms
    Mar  2 02:06:39.502: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.834909ms
    Mar  2 02:06:43.418: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 02:06:43.418: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar  2 02:06:43.440: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"110597"},"items":null}

    Mar  2 02:06:43.455: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"110597"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Mar  2 02:06:43.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-1649" for this suite. 03/02/23 02:06:43.543
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:06:43.57
Mar  2 02:06:43.570: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename projected 03/02/23 02:06:43.571
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:06:43.63
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:06:43.644
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
STEP: Creating a pod to test downward API volume plugin 03/02/23 02:06:43.662
Mar  2 02:06:43.749: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ea422ce8-4bf8-4e32-b186-df958cb7f320" in namespace "projected-9925" to be "Succeeded or Failed"
Mar  2 02:06:43.774: INFO: Pod "downwardapi-volume-ea422ce8-4bf8-4e32-b186-df958cb7f320": Phase="Pending", Reason="", readiness=false. Elapsed: 24.987019ms
Mar  2 02:06:45.794: INFO: Pod "downwardapi-volume-ea422ce8-4bf8-4e32-b186-df958cb7f320": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044614494s
Mar  2 02:06:47.790: INFO: Pod "downwardapi-volume-ea422ce8-4bf8-4e32-b186-df958cb7f320": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041482801s
Mar  2 02:06:49.789: INFO: Pod "downwardapi-volume-ea422ce8-4bf8-4e32-b186-df958cb7f320": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.03977252s
STEP: Saw pod success 03/02/23 02:06:49.789
Mar  2 02:06:49.789: INFO: Pod "downwardapi-volume-ea422ce8-4bf8-4e32-b186-df958cb7f320" satisfied condition "Succeeded or Failed"
Mar  2 02:06:49.802: INFO: Trying to get logs from node 10.132.92.143 pod downwardapi-volume-ea422ce8-4bf8-4e32-b186-df958cb7f320 container client-container: <nil>
STEP: delete the pod 03/02/23 02:06:49.88
Mar  2 02:06:49.953: INFO: Waiting for pod downwardapi-volume-ea422ce8-4bf8-4e32-b186-df958cb7f320 to disappear
Mar  2 02:06:49.974: INFO: Pod downwardapi-volume-ea422ce8-4bf8-4e32-b186-df958cb7f320 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar  2 02:06:49.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9925" for this suite. 03/02/23 02:06:50.029
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":182,"skipped":3451,"failed":0}
------------------------------
• [SLOW TEST] [6.521 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:06:43.57
    Mar  2 02:06:43.570: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename projected 03/02/23 02:06:43.571
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:06:43.63
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:06:43.644
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:83
    STEP: Creating a pod to test downward API volume plugin 03/02/23 02:06:43.662
    Mar  2 02:06:43.749: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ea422ce8-4bf8-4e32-b186-df958cb7f320" in namespace "projected-9925" to be "Succeeded or Failed"
    Mar  2 02:06:43.774: INFO: Pod "downwardapi-volume-ea422ce8-4bf8-4e32-b186-df958cb7f320": Phase="Pending", Reason="", readiness=false. Elapsed: 24.987019ms
    Mar  2 02:06:45.794: INFO: Pod "downwardapi-volume-ea422ce8-4bf8-4e32-b186-df958cb7f320": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044614494s
    Mar  2 02:06:47.790: INFO: Pod "downwardapi-volume-ea422ce8-4bf8-4e32-b186-df958cb7f320": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041482801s
    Mar  2 02:06:49.789: INFO: Pod "downwardapi-volume-ea422ce8-4bf8-4e32-b186-df958cb7f320": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.03977252s
    STEP: Saw pod success 03/02/23 02:06:49.789
    Mar  2 02:06:49.789: INFO: Pod "downwardapi-volume-ea422ce8-4bf8-4e32-b186-df958cb7f320" satisfied condition "Succeeded or Failed"
    Mar  2 02:06:49.802: INFO: Trying to get logs from node 10.132.92.143 pod downwardapi-volume-ea422ce8-4bf8-4e32-b186-df958cb7f320 container client-container: <nil>
    STEP: delete the pod 03/02/23 02:06:49.88
    Mar  2 02:06:49.953: INFO: Waiting for pod downwardapi-volume-ea422ce8-4bf8-4e32-b186-df958cb7f320 to disappear
    Mar  2 02:06:49.974: INFO: Pod downwardapi-volume-ea422ce8-4bf8-4e32-b186-df958cb7f320 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar  2 02:06:49.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9925" for this suite. 03/02/23 02:06:50.029
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:06:50.091
Mar  2 02:06:50.092: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename replication-controller 03/02/23 02:06:50.093
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:06:50.173
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:06:50.191
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
STEP: creating a ReplicationController 03/02/23 02:06:50.222
W0302 02:06:50.262394      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "rc-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "rc-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "rc-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "rc-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: waiting for RC to be added 03/02/23 02:06:50.262
STEP: waiting for available Replicas 03/02/23 02:06:50.262
STEP: patching ReplicationController 03/02/23 02:06:52.349
STEP: waiting for RC to be modified 03/02/23 02:06:52.378
STEP: patching ReplicationController status 03/02/23 02:06:52.378
STEP: waiting for RC to be modified 03/02/23 02:06:52.404
STEP: waiting for available Replicas 03/02/23 02:06:52.405
STEP: fetching ReplicationController status 03/02/23 02:06:52.419
STEP: patching ReplicationController scale 03/02/23 02:06:52.435
STEP: waiting for RC to be modified 03/02/23 02:06:52.459
STEP: waiting for ReplicationController's scale to be the max amount 03/02/23 02:06:52.459
STEP: fetching ReplicationController; ensuring that it's patched 03/02/23 02:06:54.535
STEP: updating ReplicationController status 03/02/23 02:06:54.55
STEP: waiting for RC to be modified 03/02/23 02:06:54.577
STEP: listing all ReplicationControllers 03/02/23 02:06:54.577
STEP: checking that ReplicationController has expected values 03/02/23 02:06:54.591
STEP: deleting ReplicationControllers by collection 03/02/23 02:06:54.591
STEP: waiting for ReplicationController to have a DELETED watchEvent 03/02/23 02:06:54.632
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Mar  2 02:06:54.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3909" for this suite. 03/02/23 02:06:54.793
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","completed":183,"skipped":3455,"failed":0}
------------------------------
• [4.726 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:06:50.091
    Mar  2 02:06:50.092: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename replication-controller 03/02/23 02:06:50.093
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:06:50.173
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:06:50.191
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:109
    STEP: creating a ReplicationController 03/02/23 02:06:50.222
    W0302 02:06:50.262394      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "rc-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "rc-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "rc-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "rc-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: waiting for RC to be added 03/02/23 02:06:50.262
    STEP: waiting for available Replicas 03/02/23 02:06:50.262
    STEP: patching ReplicationController 03/02/23 02:06:52.349
    STEP: waiting for RC to be modified 03/02/23 02:06:52.378
    STEP: patching ReplicationController status 03/02/23 02:06:52.378
    STEP: waiting for RC to be modified 03/02/23 02:06:52.404
    STEP: waiting for available Replicas 03/02/23 02:06:52.405
    STEP: fetching ReplicationController status 03/02/23 02:06:52.419
    STEP: patching ReplicationController scale 03/02/23 02:06:52.435
    STEP: waiting for RC to be modified 03/02/23 02:06:52.459
    STEP: waiting for ReplicationController's scale to be the max amount 03/02/23 02:06:52.459
    STEP: fetching ReplicationController; ensuring that it's patched 03/02/23 02:06:54.535
    STEP: updating ReplicationController status 03/02/23 02:06:54.55
    STEP: waiting for RC to be modified 03/02/23 02:06:54.577
    STEP: listing all ReplicationControllers 03/02/23 02:06:54.577
    STEP: checking that ReplicationController has expected values 03/02/23 02:06:54.591
    STEP: deleting ReplicationControllers by collection 03/02/23 02:06:54.591
    STEP: waiting for ReplicationController to have a DELETED watchEvent 03/02/23 02:06:54.632
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Mar  2 02:06:54.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-3909" for this suite. 03/02/23 02:06:54.793
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:06:54.826
Mar  2 02:06:54.827: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename watch 03/02/23 02:06:54.828
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:06:54.904
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:06:54.916
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 03/02/23 02:06:54.943
STEP: creating a new configmap 03/02/23 02:06:54.95
STEP: modifying the configmap once 03/02/23 02:06:54.977
STEP: changing the label value of the configmap 03/02/23 02:06:55.051
STEP: Expecting to observe a delete notification for the watched object 03/02/23 02:06:55.094
Mar  2 02:06:55.094: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2073  7845d591-1a3a-4759-8352-377872071f7b 110846 0 2023-03-02 02:06:54 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-02 02:06:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 02:06:55.095: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2073  7845d591-1a3a-4759-8352-377872071f7b 110852 0 2023-03-02 02:06:54 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-02 02:06:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 02:06:55.095: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2073  7845d591-1a3a-4759-8352-377872071f7b 110857 0 2023-03-02 02:06:54 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-02 02:06:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 03/02/23 02:06:55.095
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 03/02/23 02:06:55.152
STEP: changing the label value of the configmap back 03/02/23 02:07:05.152
STEP: modifying the configmap a third time 03/02/23 02:07:05.198
STEP: deleting the configmap 03/02/23 02:07:05.24
STEP: Expecting to observe an add notification for the watched object when the label value was restored 03/02/23 02:07:05.277
Mar  2 02:07:05.277: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2073  7845d591-1a3a-4759-8352-377872071f7b 110986 0 2023-03-02 02:06:54 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-02 02:07:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 02:07:05.277: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2073  7845d591-1a3a-4759-8352-377872071f7b 110987 0 2023-03-02 02:06:54 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-02 02:07:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 02:07:05.277: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2073  7845d591-1a3a-4759-8352-377872071f7b 110988 0 2023-03-02 02:06:54 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-02 02:07:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Mar  2 02:07:05.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2073" for this suite. 03/02/23 02:07:05.302
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","completed":184,"skipped":3519,"failed":0}
------------------------------
• [SLOW TEST] [10.505 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:06:54.826
    Mar  2 02:06:54.827: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename watch 03/02/23 02:06:54.828
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:06:54.904
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:06:54.916
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 03/02/23 02:06:54.943
    STEP: creating a new configmap 03/02/23 02:06:54.95
    STEP: modifying the configmap once 03/02/23 02:06:54.977
    STEP: changing the label value of the configmap 03/02/23 02:06:55.051
    STEP: Expecting to observe a delete notification for the watched object 03/02/23 02:06:55.094
    Mar  2 02:06:55.094: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2073  7845d591-1a3a-4759-8352-377872071f7b 110846 0 2023-03-02 02:06:54 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-02 02:06:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar  2 02:06:55.095: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2073  7845d591-1a3a-4759-8352-377872071f7b 110852 0 2023-03-02 02:06:54 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-02 02:06:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar  2 02:06:55.095: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2073  7845d591-1a3a-4759-8352-377872071f7b 110857 0 2023-03-02 02:06:54 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-02 02:06:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 03/02/23 02:06:55.095
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 03/02/23 02:06:55.152
    STEP: changing the label value of the configmap back 03/02/23 02:07:05.152
    STEP: modifying the configmap a third time 03/02/23 02:07:05.198
    STEP: deleting the configmap 03/02/23 02:07:05.24
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 03/02/23 02:07:05.277
    Mar  2 02:07:05.277: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2073  7845d591-1a3a-4759-8352-377872071f7b 110986 0 2023-03-02 02:06:54 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-02 02:07:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar  2 02:07:05.277: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2073  7845d591-1a3a-4759-8352-377872071f7b 110987 0 2023-03-02 02:06:54 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-02 02:07:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar  2 02:07:05.277: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2073  7845d591-1a3a-4759-8352-377872071f7b 110988 0 2023-03-02 02:06:54 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-02 02:07:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Mar  2 02:07:05.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-2073" for this suite. 03/02/23 02:07:05.302
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:07:05.336
Mar  2 02:07:05.336: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename kubectl 03/02/23 02:07:05.338
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:07:05.396
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:07:05.409
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
STEP: validating api versions 03/02/23 02:07:05.427
Mar  2 02:07:05.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-720 api-versions'
Mar  2 02:07:05.567: INFO: stderr: ""
Mar  2 02:07:05.567: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napiserver.openshift.io/v1\napps.openshift.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nauthorization.openshift.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\nbuild.openshift.io/v1\ncertificates.k8s.io/v1\ncloudcredential.openshift.io/v1\nconfig.openshift.io/v1\nconsole.openshift.io/v1\nconsole.openshift.io/v1alpha1\ncontrolplane.operator.openshift.io/v1alpha1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nhelm.openshift.io/v1beta1\nibm.com/v1alpha1\nimage.openshift.io/v1\nimageregistry.operator.openshift.io/v1\ningress.operator.openshift.io/v1\nk8s.cni.cncf.io/v1\nmachineconfiguration.openshift.io/v1\nmetrics.k8s.io/v1beta1\nmigration.k8s.io/v1alpha1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nmonitoring.coreos.com/v1beta1\nnetwork.operator.openshift.io/v1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\noauth.openshift.io/v1\noperator.openshift.io/v1\noperator.openshift.io/v1alpha1\noperator.tigera.io/v1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\noperators.coreos.com/v2\npackages.operators.coreos.com/v1\nperformance.openshift.io/v1\nperformance.openshift.io/v1alpha1\nperformance.openshift.io/v2\npolicy/v1\nproject.openshift.io/v1\nquota.openshift.io/v1\nrbac.authorization.k8s.io/v1\nroute.openshift.io/v1\nsamples.operator.openshift.io/v1\nscheduling.k8s.io/v1\nsecurity.internal.openshift.io/v1\nsecurity.openshift.io/v1\nsnapshot.storage.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplate.openshift.io/v1\ntuned.openshift.io/v1\nuser.openshift.io/v1\nv1\nwhereabouts.cni.cncf.io/v1alpha1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  2 02:07:05.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-720" for this suite. 03/02/23 02:07:05.617
{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","completed":185,"skipped":3538,"failed":0}
------------------------------
• [0.309 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:816
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:07:05.336
    Mar  2 02:07:05.336: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename kubectl 03/02/23 02:07:05.338
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:07:05.396
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:07:05.409
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:822
    STEP: validating api versions 03/02/23 02:07:05.427
    Mar  2 02:07:05.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-720 api-versions'
    Mar  2 02:07:05.567: INFO: stderr: ""
    Mar  2 02:07:05.567: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napiserver.openshift.io/v1\napps.openshift.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nauthorization.openshift.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\nbuild.openshift.io/v1\ncertificates.k8s.io/v1\ncloudcredential.openshift.io/v1\nconfig.openshift.io/v1\nconsole.openshift.io/v1\nconsole.openshift.io/v1alpha1\ncontrolplane.operator.openshift.io/v1alpha1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nhelm.openshift.io/v1beta1\nibm.com/v1alpha1\nimage.openshift.io/v1\nimageregistry.operator.openshift.io/v1\ningress.operator.openshift.io/v1\nk8s.cni.cncf.io/v1\nmachineconfiguration.openshift.io/v1\nmetrics.k8s.io/v1beta1\nmigration.k8s.io/v1alpha1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nmonitoring.coreos.com/v1beta1\nnetwork.operator.openshift.io/v1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\noauth.openshift.io/v1\noperator.openshift.io/v1\noperator.openshift.io/v1alpha1\noperator.tigera.io/v1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\noperators.coreos.com/v2\npackages.operators.coreos.com/v1\nperformance.openshift.io/v1\nperformance.openshift.io/v1alpha1\nperformance.openshift.io/v2\npolicy/v1\nproject.openshift.io/v1\nquota.openshift.io/v1\nrbac.authorization.k8s.io/v1\nroute.openshift.io/v1\nsamples.operator.openshift.io/v1\nscheduling.k8s.io/v1\nsecurity.internal.openshift.io/v1\nsecurity.openshift.io/v1\nsnapshot.storage.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplate.openshift.io/v1\ntuned.openshift.io/v1\nuser.openshift.io/v1\nv1\nwhereabouts.cni.cncf.io/v1alpha1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  2 02:07:05.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-720" for this suite. 03/02/23 02:07:05.617
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:07:05.645
Mar  2 02:07:05.645: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename runtimeclass 03/02/23 02:07:05.648
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:07:05.744
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:07:05.761
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Mar  2 02:07:05.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-6359" for this suite. 03/02/23 02:07:05.915
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]","completed":186,"skipped":3538,"failed":0}
------------------------------
• [0.305 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:07:05.645
    Mar  2 02:07:05.645: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename runtimeclass 03/02/23 02:07:05.648
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:07:05.744
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:07:05.761
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Mar  2 02:07:05.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-6359" for this suite. 03/02/23 02:07:05.915
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:07:05.953
Mar  2 02:07:05.953: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename security-context-test 03/02/23 02:07:05.954
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:07:06.054
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:07:06.067
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
Mar  2 02:07:06.153: INFO: Waiting up to 5m0s for pod "busybox-user-65534-4cb5bfb5-c94e-45ef-93a6-cfc1a57146be" in namespace "security-context-test-5144" to be "Succeeded or Failed"
Mar  2 02:07:06.179: INFO: Pod "busybox-user-65534-4cb5bfb5-c94e-45ef-93a6-cfc1a57146be": Phase="Pending", Reason="", readiness=false. Elapsed: 26.735432ms
Mar  2 02:07:08.198: INFO: Pod "busybox-user-65534-4cb5bfb5-c94e-45ef-93a6-cfc1a57146be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04497104s
Mar  2 02:07:10.194: INFO: Pod "busybox-user-65534-4cb5bfb5-c94e-45ef-93a6-cfc1a57146be": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041845594s
Mar  2 02:07:12.202: INFO: Pod "busybox-user-65534-4cb5bfb5-c94e-45ef-93a6-cfc1a57146be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.049078952s
Mar  2 02:07:12.202: INFO: Pod "busybox-user-65534-4cb5bfb5-c94e-45ef-93a6-cfc1a57146be" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Mar  2 02:07:12.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5144" for this suite. 03/02/23 02:07:12.222
{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","completed":187,"skipped":3540,"failed":0}
------------------------------
• [SLOW TEST] [6.296 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:308
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:07:05.953
    Mar  2 02:07:05.953: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename security-context-test 03/02/23 02:07:05.954
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:07:06.054
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:07:06.067
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:346
    Mar  2 02:07:06.153: INFO: Waiting up to 5m0s for pod "busybox-user-65534-4cb5bfb5-c94e-45ef-93a6-cfc1a57146be" in namespace "security-context-test-5144" to be "Succeeded or Failed"
    Mar  2 02:07:06.179: INFO: Pod "busybox-user-65534-4cb5bfb5-c94e-45ef-93a6-cfc1a57146be": Phase="Pending", Reason="", readiness=false. Elapsed: 26.735432ms
    Mar  2 02:07:08.198: INFO: Pod "busybox-user-65534-4cb5bfb5-c94e-45ef-93a6-cfc1a57146be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04497104s
    Mar  2 02:07:10.194: INFO: Pod "busybox-user-65534-4cb5bfb5-c94e-45ef-93a6-cfc1a57146be": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041845594s
    Mar  2 02:07:12.202: INFO: Pod "busybox-user-65534-4cb5bfb5-c94e-45ef-93a6-cfc1a57146be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.049078952s
    Mar  2 02:07:12.202: INFO: Pod "busybox-user-65534-4cb5bfb5-c94e-45ef-93a6-cfc1a57146be" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Mar  2 02:07:12.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-5144" for this suite. 03/02/23 02:07:12.222
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:07:12.25
Mar  2 02:07:12.251: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename container-runtime 03/02/23 02:07:12.251
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:07:12.318
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:07:12.331
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
STEP: create the container 03/02/23 02:07:12.345
STEP: wait for the container to reach Succeeded 03/02/23 02:07:12.43
STEP: get the container status 03/02/23 02:07:17.553
STEP: the container should be terminated 03/02/23 02:07:17.573
STEP: the termination message should be set 03/02/23 02:07:17.573
Mar  2 02:07:17.573: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 03/02/23 02:07:17.573
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Mar  2 02:07:17.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2211" for this suite. 03/02/23 02:07:17.656
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":188,"skipped":3546,"failed":0}
------------------------------
• [SLOW TEST] [5.438 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:07:12.25
    Mar  2 02:07:12.251: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename container-runtime 03/02/23 02:07:12.251
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:07:12.318
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:07:12.331
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231
    STEP: create the container 03/02/23 02:07:12.345
    STEP: wait for the container to reach Succeeded 03/02/23 02:07:12.43
    STEP: get the container status 03/02/23 02:07:17.553
    STEP: the container should be terminated 03/02/23 02:07:17.573
    STEP: the termination message should be set 03/02/23 02:07:17.573
    Mar  2 02:07:17.573: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 03/02/23 02:07:17.573
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Mar  2 02:07:17.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-2211" for this suite. 03/02/23 02:07:17.656
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:07:17.691
Mar  2 02:07:17.691: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename endpointslicemirroring 03/02/23 02:07:17.692
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:07:17.78
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:07:17.802
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 03/02/23 02:07:17.946
Mar  2 02:07:18.008: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 03/02/23 02:07:20.029
Mar  2 02:07:20.097: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 03/02/23 02:07:22.118
Mar  2 02:07:22.156: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:187
Mar  2 02:07:24.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-793" for this suite. 03/02/23 02:07:24.197
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","completed":189,"skipped":3552,"failed":0}
------------------------------
• [SLOW TEST] [6.529 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:07:17.691
    Mar  2 02:07:17.691: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename endpointslicemirroring 03/02/23 02:07:17.692
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:07:17.78
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:07:17.802
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 03/02/23 02:07:17.946
    Mar  2 02:07:18.008: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 03/02/23 02:07:20.029
    Mar  2 02:07:20.097: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 03/02/23 02:07:22.118
    Mar  2 02:07:22.156: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:187
    Mar  2 02:07:24.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslicemirroring-793" for this suite. 03/02/23 02:07:24.197
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:07:24.223
Mar  2 02:07:24.224: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename security-context-test 03/02/23 02:07:24.225
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:07:24.292
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:07:24.337
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
Mar  2 02:07:24.412: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-59170665-6a36-4d63-826f-3ee4fe17cf96" in namespace "security-context-test-8535" to be "Succeeded or Failed"
Mar  2 02:07:24.426: INFO: Pod "busybox-privileged-false-59170665-6a36-4d63-826f-3ee4fe17cf96": Phase="Pending", Reason="", readiness=false. Elapsed: 13.919859ms
Mar  2 02:07:26.446: INFO: Pod "busybox-privileged-false-59170665-6a36-4d63-826f-3ee4fe17cf96": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034850998s
Mar  2 02:07:28.446: INFO: Pod "busybox-privileged-false-59170665-6a36-4d63-826f-3ee4fe17cf96": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03446764s
Mar  2 02:07:30.442: INFO: Pod "busybox-privileged-false-59170665-6a36-4d63-826f-3ee4fe17cf96": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030331791s
Mar  2 02:07:30.442: INFO: Pod "busybox-privileged-false-59170665-6a36-4d63-826f-3ee4fe17cf96" satisfied condition "Succeeded or Failed"
Mar  2 02:07:30.480: INFO: Got logs for pod "busybox-privileged-false-59170665-6a36-4d63-826f-3ee4fe17cf96": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Mar  2 02:07:30.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8535" for this suite. 03/02/23 02:07:30.503
{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","completed":190,"skipped":3558,"failed":0}
------------------------------
• [SLOW TEST] [6.307 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:490
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:527

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:07:24.223
    Mar  2 02:07:24.224: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename security-context-test 03/02/23 02:07:24.225
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:07:24.292
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:07:24.337
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:527
    Mar  2 02:07:24.412: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-59170665-6a36-4d63-826f-3ee4fe17cf96" in namespace "security-context-test-8535" to be "Succeeded or Failed"
    Mar  2 02:07:24.426: INFO: Pod "busybox-privileged-false-59170665-6a36-4d63-826f-3ee4fe17cf96": Phase="Pending", Reason="", readiness=false. Elapsed: 13.919859ms
    Mar  2 02:07:26.446: INFO: Pod "busybox-privileged-false-59170665-6a36-4d63-826f-3ee4fe17cf96": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034850998s
    Mar  2 02:07:28.446: INFO: Pod "busybox-privileged-false-59170665-6a36-4d63-826f-3ee4fe17cf96": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03446764s
    Mar  2 02:07:30.442: INFO: Pod "busybox-privileged-false-59170665-6a36-4d63-826f-3ee4fe17cf96": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030331791s
    Mar  2 02:07:30.442: INFO: Pod "busybox-privileged-false-59170665-6a36-4d63-826f-3ee4fe17cf96" satisfied condition "Succeeded or Failed"
    Mar  2 02:07:30.480: INFO: Got logs for pod "busybox-privileged-false-59170665-6a36-4d63-826f-3ee4fe17cf96": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Mar  2 02:07:30.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-8535" for this suite. 03/02/23 02:07:30.503
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:07:30.536
Mar  2 02:07:30.536: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename downward-api 03/02/23 02:07:30.538
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:07:30.592
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:07:30.605
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
STEP: Creating a pod to test downward API volume plugin 03/02/23 02:07:30.617
Mar  2 02:07:30.697: INFO: Waiting up to 5m0s for pod "downwardapi-volume-363f9397-90aa-43af-8244-0535103276a0" in namespace "downward-api-9626" to be "Succeeded or Failed"
Mar  2 02:07:30.731: INFO: Pod "downwardapi-volume-363f9397-90aa-43af-8244-0535103276a0": Phase="Pending", Reason="", readiness=false. Elapsed: 34.399997ms
Mar  2 02:07:32.749: INFO: Pod "downwardapi-volume-363f9397-90aa-43af-8244-0535103276a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051957022s
Mar  2 02:07:34.746: INFO: Pod "downwardapi-volume-363f9397-90aa-43af-8244-0535103276a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049278995s
STEP: Saw pod success 03/02/23 02:07:34.746
Mar  2 02:07:34.747: INFO: Pod "downwardapi-volume-363f9397-90aa-43af-8244-0535103276a0" satisfied condition "Succeeded or Failed"
Mar  2 02:07:34.762: INFO: Trying to get logs from node 10.132.92.143 pod downwardapi-volume-363f9397-90aa-43af-8244-0535103276a0 container client-container: <nil>
STEP: delete the pod 03/02/23 02:07:34.794
Mar  2 02:07:34.847: INFO: Waiting for pod downwardapi-volume-363f9397-90aa-43af-8244-0535103276a0 to disappear
Mar  2 02:07:34.861: INFO: Pod downwardapi-volume-363f9397-90aa-43af-8244-0535103276a0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar  2 02:07:34.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9626" for this suite. 03/02/23 02:07:34.889
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":191,"skipped":3568,"failed":0}
------------------------------
• [4.377 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:07:30.536
    Mar  2 02:07:30.536: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename downward-api 03/02/23 02:07:30.538
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:07:30.592
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:07:30.605
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:260
    STEP: Creating a pod to test downward API volume plugin 03/02/23 02:07:30.617
    Mar  2 02:07:30.697: INFO: Waiting up to 5m0s for pod "downwardapi-volume-363f9397-90aa-43af-8244-0535103276a0" in namespace "downward-api-9626" to be "Succeeded or Failed"
    Mar  2 02:07:30.731: INFO: Pod "downwardapi-volume-363f9397-90aa-43af-8244-0535103276a0": Phase="Pending", Reason="", readiness=false. Elapsed: 34.399997ms
    Mar  2 02:07:32.749: INFO: Pod "downwardapi-volume-363f9397-90aa-43af-8244-0535103276a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051957022s
    Mar  2 02:07:34.746: INFO: Pod "downwardapi-volume-363f9397-90aa-43af-8244-0535103276a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049278995s
    STEP: Saw pod success 03/02/23 02:07:34.746
    Mar  2 02:07:34.747: INFO: Pod "downwardapi-volume-363f9397-90aa-43af-8244-0535103276a0" satisfied condition "Succeeded or Failed"
    Mar  2 02:07:34.762: INFO: Trying to get logs from node 10.132.92.143 pod downwardapi-volume-363f9397-90aa-43af-8244-0535103276a0 container client-container: <nil>
    STEP: delete the pod 03/02/23 02:07:34.794
    Mar  2 02:07:34.847: INFO: Waiting for pod downwardapi-volume-363f9397-90aa-43af-8244-0535103276a0 to disappear
    Mar  2 02:07:34.861: INFO: Pod downwardapi-volume-363f9397-90aa-43af-8244-0535103276a0 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar  2 02:07:34.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9626" for this suite. 03/02/23 02:07:34.889
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:07:34.914
Mar  2 02:07:34.914: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename configmap 03/02/23 02:07:34.915
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:07:34.968
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:07:34.981
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
STEP: Creating configMap with name configmap-test-volume-map-37ae981c-fb64-4143-9992-581581a95876 03/02/23 02:07:34.996
STEP: Creating a pod to test consume configMaps 03/02/23 02:07:35.019
Mar  2 02:07:35.102: INFO: Waiting up to 5m0s for pod "pod-configmaps-0761ac6e-1f1d-4777-9c95-a33b241bb131" in namespace "configmap-397" to be "Succeeded or Failed"
Mar  2 02:07:35.116: INFO: Pod "pod-configmaps-0761ac6e-1f1d-4777-9c95-a33b241bb131": Phase="Pending", Reason="", readiness=false. Elapsed: 13.427757ms
Mar  2 02:07:37.135: INFO: Pod "pod-configmaps-0761ac6e-1f1d-4777-9c95-a33b241bb131": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032813931s
Mar  2 02:07:39.130: INFO: Pod "pod-configmaps-0761ac6e-1f1d-4777-9c95-a33b241bb131": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027823916s
Mar  2 02:07:41.131: INFO: Pod "pod-configmaps-0761ac6e-1f1d-4777-9c95-a33b241bb131": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028316938s
STEP: Saw pod success 03/02/23 02:07:41.131
Mar  2 02:07:41.131: INFO: Pod "pod-configmaps-0761ac6e-1f1d-4777-9c95-a33b241bb131" satisfied condition "Succeeded or Failed"
Mar  2 02:07:41.145: INFO: Trying to get logs from node 10.132.92.143 pod pod-configmaps-0761ac6e-1f1d-4777-9c95-a33b241bb131 container agnhost-container: <nil>
STEP: delete the pod 03/02/23 02:07:41.177
Mar  2 02:07:41.222: INFO: Waiting for pod pod-configmaps-0761ac6e-1f1d-4777-9c95-a33b241bb131 to disappear
Mar  2 02:07:41.240: INFO: Pod pod-configmaps-0761ac6e-1f1d-4777-9c95-a33b241bb131 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar  2 02:07:41.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-397" for this suite. 03/02/23 02:07:41.263
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":192,"skipped":3575,"failed":0}
------------------------------
• [SLOW TEST] [6.376 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:07:34.914
    Mar  2 02:07:34.914: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename configmap 03/02/23 02:07:34.915
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:07:34.968
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:07:34.981
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:108
    STEP: Creating configMap with name configmap-test-volume-map-37ae981c-fb64-4143-9992-581581a95876 03/02/23 02:07:34.996
    STEP: Creating a pod to test consume configMaps 03/02/23 02:07:35.019
    Mar  2 02:07:35.102: INFO: Waiting up to 5m0s for pod "pod-configmaps-0761ac6e-1f1d-4777-9c95-a33b241bb131" in namespace "configmap-397" to be "Succeeded or Failed"
    Mar  2 02:07:35.116: INFO: Pod "pod-configmaps-0761ac6e-1f1d-4777-9c95-a33b241bb131": Phase="Pending", Reason="", readiness=false. Elapsed: 13.427757ms
    Mar  2 02:07:37.135: INFO: Pod "pod-configmaps-0761ac6e-1f1d-4777-9c95-a33b241bb131": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032813931s
    Mar  2 02:07:39.130: INFO: Pod "pod-configmaps-0761ac6e-1f1d-4777-9c95-a33b241bb131": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027823916s
    Mar  2 02:07:41.131: INFO: Pod "pod-configmaps-0761ac6e-1f1d-4777-9c95-a33b241bb131": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028316938s
    STEP: Saw pod success 03/02/23 02:07:41.131
    Mar  2 02:07:41.131: INFO: Pod "pod-configmaps-0761ac6e-1f1d-4777-9c95-a33b241bb131" satisfied condition "Succeeded or Failed"
    Mar  2 02:07:41.145: INFO: Trying to get logs from node 10.132.92.143 pod pod-configmaps-0761ac6e-1f1d-4777-9c95-a33b241bb131 container agnhost-container: <nil>
    STEP: delete the pod 03/02/23 02:07:41.177
    Mar  2 02:07:41.222: INFO: Waiting for pod pod-configmaps-0761ac6e-1f1d-4777-9c95-a33b241bb131 to disappear
    Mar  2 02:07:41.240: INFO: Pod pod-configmaps-0761ac6e-1f1d-4777-9c95-a33b241bb131 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  2 02:07:41.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-397" for this suite. 03/02/23 02:07:41.263
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:07:41.293
Mar  2 02:07:41.294: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename sysctl 03/02/23 02:07:41.295
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:07:41.361
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:07:41.382
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 03/02/23 02:07:41.403
STEP: Watching for error events or started pod 03/02/23 02:07:41.48
STEP: Waiting for pod completion 03/02/23 02:07:43.498
Mar  2 02:07:43.498: INFO: Waiting up to 3m0s for pod "sysctl-1c241694-2dbc-44ad-b2b9-7455805cc26f" in namespace "sysctl-9801" to be "completed"
Mar  2 02:07:43.512: INFO: Pod "sysctl-1c241694-2dbc-44ad-b2b9-7455805cc26f": Phase="Pending", Reason="", readiness=false. Elapsed: 13.395646ms
Mar  2 02:07:45.528: INFO: Pod "sysctl-1c241694-2dbc-44ad-b2b9-7455805cc26f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030056138s
Mar  2 02:07:47.528: INFO: Pod "sysctl-1c241694-2dbc-44ad-b2b9-7455805cc26f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029592217s
Mar  2 02:07:47.528: INFO: Pod "sysctl-1c241694-2dbc-44ad-b2b9-7455805cc26f" satisfied condition "completed"
STEP: Checking that the pod succeeded 03/02/23 02:07:47.541
STEP: Getting logs from the pod 03/02/23 02:07:47.541
STEP: Checking that the sysctl is actually updated 03/02/23 02:07:47.573
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Mar  2 02:07:47.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-9801" for this suite. 03/02/23 02:07:47.6
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":193,"skipped":3592,"failed":0}
------------------------------
• [SLOW TEST] [6.333 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:07:41.293
    Mar  2 02:07:41.294: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename sysctl 03/02/23 02:07:41.295
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:07:41.361
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:07:41.382
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 03/02/23 02:07:41.403
    STEP: Watching for error events or started pod 03/02/23 02:07:41.48
    STEP: Waiting for pod completion 03/02/23 02:07:43.498
    Mar  2 02:07:43.498: INFO: Waiting up to 3m0s for pod "sysctl-1c241694-2dbc-44ad-b2b9-7455805cc26f" in namespace "sysctl-9801" to be "completed"
    Mar  2 02:07:43.512: INFO: Pod "sysctl-1c241694-2dbc-44ad-b2b9-7455805cc26f": Phase="Pending", Reason="", readiness=false. Elapsed: 13.395646ms
    Mar  2 02:07:45.528: INFO: Pod "sysctl-1c241694-2dbc-44ad-b2b9-7455805cc26f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030056138s
    Mar  2 02:07:47.528: INFO: Pod "sysctl-1c241694-2dbc-44ad-b2b9-7455805cc26f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029592217s
    Mar  2 02:07:47.528: INFO: Pod "sysctl-1c241694-2dbc-44ad-b2b9-7455805cc26f" satisfied condition "completed"
    STEP: Checking that the pod succeeded 03/02/23 02:07:47.541
    STEP: Getting logs from the pod 03/02/23 02:07:47.541
    STEP: Checking that the sysctl is actually updated 03/02/23 02:07:47.573
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Mar  2 02:07:47.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-9801" for this suite. 03/02/23 02:07:47.6
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:07:47.628
Mar  2 02:07:47.628: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename svc-latency 03/02/23 02:07:47.63
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:07:47.68
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:07:47.695
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Mar  2 02:07:47.707: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: creating replication controller svc-latency-rc in namespace svc-latency-5429 03/02/23 02:07:47.709
W0302 02:07:47.767972      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "svc-latency-rc" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "svc-latency-rc" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "svc-latency-rc" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "svc-latency-rc" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
I0302 02:07:47.768358      20 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5429, replica count: 1
I0302 02:07:48.819620      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 02:07:49.820698      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 02:07:49.957: INFO: Created: latency-svc-27sz4
Mar  2 02:07:49.976: INFO: Got endpoints: latency-svc-27sz4 [55.442132ms]
Mar  2 02:07:50.010: INFO: Created: latency-svc-gf7gp
Mar  2 02:07:50.026: INFO: Created: latency-svc-ttwxx
Mar  2 02:07:50.032: INFO: Got endpoints: latency-svc-gf7gp [54.98483ms]
Mar  2 02:07:50.042: INFO: Created: latency-svc-djx7v
Mar  2 02:07:50.043: INFO: Got endpoints: latency-svc-ttwxx [66.774183ms]
Mar  2 02:07:50.077: INFO: Got endpoints: latency-svc-djx7v [99.705672ms]
Mar  2 02:07:50.078: INFO: Created: latency-svc-jkq54
Mar  2 02:07:50.084: INFO: Created: latency-svc-g8p7j
Mar  2 02:07:50.091: INFO: Created: latency-svc-sgst5
Mar  2 02:07:50.106: INFO: Got endpoints: latency-svc-g8p7j [128.394998ms]
Mar  2 02:07:50.106: INFO: Got endpoints: latency-svc-jkq54 [129.213107ms]
Mar  2 02:07:50.111: INFO: Got endpoints: latency-svc-sgst5 [134.325975ms]
Mar  2 02:07:50.120: INFO: Created: latency-svc-b2z4c
Mar  2 02:07:50.141: INFO: Got endpoints: latency-svc-b2z4c [163.990999ms]
Mar  2 02:07:50.150: INFO: Created: latency-svc-lk488
Mar  2 02:07:50.162: INFO: Created: latency-svc-v8mbw
Mar  2 02:07:50.166: INFO: Got endpoints: latency-svc-lk488 [189.84487ms]
Mar  2 02:07:50.178: INFO: Got endpoints: latency-svc-v8mbw [200.832706ms]
Mar  2 02:07:50.180: INFO: Created: latency-svc-xsll2
Mar  2 02:07:50.209: INFO: Got endpoints: latency-svc-xsll2 [232.087715ms]
Mar  2 02:07:50.210: INFO: Created: latency-svc-vpdq9
Mar  2 02:07:50.211: INFO: Created: latency-svc-bmkhz
Mar  2 02:07:50.213: INFO: Got endpoints: latency-svc-vpdq9 [235.074399ms]
Mar  2 02:07:50.225: INFO: Created: latency-svc-xbg7x
Mar  2 02:07:50.227: INFO: Got endpoints: latency-svc-bmkhz [249.44867ms]
Mar  2 02:07:50.246: INFO: Got endpoints: latency-svc-xbg7x [268.360741ms]
Mar  2 02:07:50.247: INFO: Created: latency-svc-q97xn
Mar  2 02:07:50.265: INFO: Got endpoints: latency-svc-q97xn [287.455176ms]
Mar  2 02:07:50.266: INFO: Created: latency-svc-sf5kx
Mar  2 02:07:50.278: INFO: Got endpoints: latency-svc-sf5kx [300.274971ms]
Mar  2 02:07:50.281: INFO: Created: latency-svc-fsbhx
Mar  2 02:07:50.293: INFO: Got endpoints: latency-svc-fsbhx [261.63393ms]
Mar  2 02:07:50.299: INFO: Created: latency-svc-cg6n2
Mar  2 02:07:50.315: INFO: Got endpoints: latency-svc-cg6n2 [271.157891ms]
Mar  2 02:07:50.318: INFO: Created: latency-svc-fcpt7
Mar  2 02:07:50.332: INFO: Created: latency-svc-nfwh7
Mar  2 02:07:50.338: INFO: Got endpoints: latency-svc-fcpt7 [260.79015ms]
Mar  2 02:07:50.342: INFO: Got endpoints: latency-svc-nfwh7 [235.328497ms]
Mar  2 02:07:50.353: INFO: Created: latency-svc-wm5qv
Mar  2 02:07:50.368: INFO: Got endpoints: latency-svc-wm5qv [261.899294ms]
Mar  2 02:07:50.382: INFO: Created: latency-svc-ksk6z
Mar  2 02:07:50.385: INFO: Created: latency-svc-dnrgn
Mar  2 02:07:50.393: INFO: Got endpoints: latency-svc-ksk6z [281.569689ms]
Mar  2 02:07:50.401: INFO: Got endpoints: latency-svc-dnrgn [259.6497ms]
Mar  2 02:07:50.402: INFO: Created: latency-svc-b97ls
Mar  2 02:07:50.416: INFO: Created: latency-svc-2gjzb
Mar  2 02:07:50.416: INFO: Got endpoints: latency-svc-b97ls [249.245139ms]
Mar  2 02:07:50.431: INFO: Got endpoints: latency-svc-2gjzb [252.804234ms]
Mar  2 02:07:50.436: INFO: Created: latency-svc-k6k2g
Mar  2 02:07:50.451: INFO: Created: latency-svc-nrp98
Mar  2 02:07:50.454: INFO: Got endpoints: latency-svc-k6k2g [244.828315ms]
Mar  2 02:07:50.464: INFO: Got endpoints: latency-svc-nrp98 [251.210622ms]
Mar  2 02:07:50.471: INFO: Created: latency-svc-sk6lr
Mar  2 02:07:50.487: INFO: Got endpoints: latency-svc-sk6lr [259.772017ms]
Mar  2 02:07:50.491: INFO: Created: latency-svc-772sc
Mar  2 02:07:50.506: INFO: Got endpoints: latency-svc-772sc [260.185821ms]
Mar  2 02:07:50.527: INFO: Created: latency-svc-28v6h
Mar  2 02:07:50.549: INFO: Created: latency-svc-xv6lr
Mar  2 02:07:50.553: INFO: Got endpoints: latency-svc-28v6h [288.120788ms]
Mar  2 02:07:50.563: INFO: Got endpoints: latency-svc-xv6lr [285.059008ms]
Mar  2 02:07:50.581: INFO: Created: latency-svc-h7t66
Mar  2 02:07:50.590: INFO: Got endpoints: latency-svc-h7t66 [296.279987ms]
Mar  2 02:07:50.590: INFO: Created: latency-svc-cq9ms
Mar  2 02:07:50.614: INFO: Got endpoints: latency-svc-cq9ms [299.104509ms]
Mar  2 02:07:50.616: INFO: Created: latency-svc-gst5d
Mar  2 02:07:50.622: INFO: Got endpoints: latency-svc-gst5d [284.603804ms]
Mar  2 02:07:50.625: INFO: Created: latency-svc-547s9
Mar  2 02:07:50.643: INFO: Got endpoints: latency-svc-547s9 [300.98852ms]
Mar  2 02:07:50.645: INFO: Created: latency-svc-g9wc2
Mar  2 02:07:50.659: INFO: Created: latency-svc-5qrhg
Mar  2 02:07:50.662: INFO: Got endpoints: latency-svc-g9wc2 [294.004858ms]
Mar  2 02:07:50.690: INFO: Got endpoints: latency-svc-5qrhg [297.049814ms]
Mar  2 02:07:50.706: INFO: Created: latency-svc-54pdh
Mar  2 02:07:50.706: INFO: Created: latency-svc-blpwr
Mar  2 02:07:50.719: INFO: Created: latency-svc-hs9qx
Mar  2 02:07:50.719: INFO: Got endpoints: latency-svc-54pdh [302.8701ms]
Mar  2 02:07:50.719: INFO: Got endpoints: latency-svc-blpwr [318.49672ms]
Mar  2 02:07:50.734: INFO: Got endpoints: latency-svc-hs9qx [302.070782ms]
Mar  2 02:07:50.738: INFO: Created: latency-svc-67ff9
Mar  2 02:07:50.750: INFO: Got endpoints: latency-svc-67ff9 [295.388924ms]
Mar  2 02:07:50.754: INFO: Created: latency-svc-kcx9m
Mar  2 02:07:50.788: INFO: Got endpoints: latency-svc-kcx9m [323.916181ms]
Mar  2 02:07:50.791: INFO: Created: latency-svc-h2cgb
Mar  2 02:07:50.805: INFO: Created: latency-svc-sp5q2
Mar  2 02:07:50.806: INFO: Got endpoints: latency-svc-h2cgb [318.472624ms]
Mar  2 02:07:50.823: INFO: Got endpoints: latency-svc-sp5q2 [315.953004ms]
Mar  2 02:07:50.825: INFO: Created: latency-svc-gfjcv
Mar  2 02:07:50.838: INFO: Got endpoints: latency-svc-gfjcv [284.435854ms]
Mar  2 02:07:50.842: INFO: Created: latency-svc-smjdb
Mar  2 02:07:50.867: INFO: Got endpoints: latency-svc-smjdb [304.349919ms]
Mar  2 02:07:50.871: INFO: Created: latency-svc-cnmpb
Mar  2 02:07:50.883: INFO: Created: latency-svc-dhp8t
Mar  2 02:07:50.889: INFO: Got endpoints: latency-svc-cnmpb [299.239178ms]
Mar  2 02:07:50.897: INFO: Got endpoints: latency-svc-dhp8t [283.261259ms]
Mar  2 02:07:50.910: INFO: Created: latency-svc-b4xkf
Mar  2 02:07:50.926: INFO: Created: latency-svc-djbdq
Mar  2 02:07:50.939: INFO: Got endpoints: latency-svc-b4xkf [316.37035ms]
Mar  2 02:07:50.968: INFO: Got endpoints: latency-svc-djbdq [324.650637ms]
Mar  2 02:07:50.968: INFO: Created: latency-svc-nqf7j
Mar  2 02:07:50.979: INFO: Got endpoints: latency-svc-nqf7j [316.32033ms]
Mar  2 02:07:50.979: INFO: Created: latency-svc-pqsh2
Mar  2 02:07:50.992: INFO: Created: latency-svc-flb66
Mar  2 02:07:51.005: INFO: Got endpoints: latency-svc-pqsh2 [314.693671ms]
Mar  2 02:07:51.011: INFO: Got endpoints: latency-svc-flb66 [291.312431ms]
Mar  2 02:07:51.022: INFO: Created: latency-svc-nzqqj
Mar  2 02:07:51.035: INFO: Got endpoints: latency-svc-nzqqj [315.931431ms]
Mar  2 02:07:51.042: INFO: Created: latency-svc-tqk88
Mar  2 02:07:51.059: INFO: Got endpoints: latency-svc-tqk88 [324.618241ms]
Mar  2 02:07:51.059: INFO: Created: latency-svc-hzs7j
Mar  2 02:07:51.072: INFO: Got endpoints: latency-svc-hzs7j [321.930053ms]
Mar  2 02:07:51.073: INFO: Created: latency-svc-7bwfn
Mar  2 02:07:51.086: INFO: Got endpoints: latency-svc-7bwfn [297.358567ms]
Mar  2 02:07:51.095: INFO: Created: latency-svc-65plr
Mar  2 02:07:51.102: INFO: Got endpoints: latency-svc-65plr [295.836428ms]
Mar  2 02:07:51.104: INFO: Created: latency-svc-v5p67
Mar  2 02:07:51.118: INFO: Got endpoints: latency-svc-v5p67 [295.165291ms]
Mar  2 02:07:51.127: INFO: Created: latency-svc-szwlx
Mar  2 02:07:51.139: INFO: Got endpoints: latency-svc-szwlx [300.753634ms]
Mar  2 02:07:51.380: INFO: Created: latency-svc-2mhgm
Mar  2 02:07:51.381: INFO: Created: latency-svc-7gvmb
Mar  2 02:07:51.381: INFO: Created: latency-svc-ss66l
Mar  2 02:07:51.381: INFO: Created: latency-svc-j82dx
Mar  2 02:07:51.385: INFO: Created: latency-svc-n8bdn
Mar  2 02:07:51.391: INFO: Created: latency-svc-qcdcz
Mar  2 02:07:51.394: INFO: Created: latency-svc-kdtrf
Mar  2 02:07:51.394: INFO: Created: latency-svc-6ck7f
Mar  2 02:07:51.394: INFO: Created: latency-svc-d8d6s
Mar  2 02:07:51.395: INFO: Created: latency-svc-6qmxz
Mar  2 02:07:51.395: INFO: Created: latency-svc-5b4xf
Mar  2 02:07:51.395: INFO: Created: latency-svc-8sxbc
Mar  2 02:07:51.395: INFO: Created: latency-svc-vm446
Mar  2 02:07:51.398: INFO: Created: latency-svc-tgnk9
Mar  2 02:07:51.399: INFO: Created: latency-svc-mfhlz
Mar  2 02:07:51.405: INFO: Got endpoints: latency-svc-j82dx [507.229951ms]
Mar  2 02:07:51.407: INFO: Got endpoints: latency-svc-2mhgm [539.071266ms]
Mar  2 02:07:51.407: INFO: Got endpoints: latency-svc-ss66l [439.125838ms]
Mar  2 02:07:51.424: INFO: Got endpoints: latency-svc-n8bdn [285.367167ms]
Mar  2 02:07:51.424: INFO: Got endpoints: latency-svc-7gvmb [365.543132ms]
Mar  2 02:07:51.428: INFO: Got endpoints: latency-svc-8sxbc [392.873579ms]
Mar  2 02:07:51.428: INFO: Got endpoints: latency-svc-6qmxz [538.784984ms]
Mar  2 02:07:51.429: INFO: Got endpoints: latency-svc-d8d6s [489.405217ms]
Mar  2 02:07:51.447: INFO: Got endpoints: latency-svc-qcdcz [361.127227ms]
Mar  2 02:07:51.449: INFO: Got endpoints: latency-svc-6ck7f [444.319135ms]
Mar  2 02:07:51.450: INFO: Got endpoints: latency-svc-mfhlz [331.989864ms]
Mar  2 02:07:51.450: INFO: Got endpoints: latency-svc-vm446 [439.134496ms]
Mar  2 02:07:51.450: INFO: Got endpoints: latency-svc-tgnk9 [378.54865ms]
Mar  2 02:07:51.457: INFO: Created: latency-svc-x55pf
Mar  2 02:07:51.490: INFO: Got endpoints: latency-svc-x55pf [82.992079ms]
Mar  2 02:07:51.490: INFO: Got endpoints: latency-svc-5b4xf [388.080021ms]
Mar  2 02:07:51.490: INFO: Got endpoints: latency-svc-kdtrf [511.025441ms]
Mar  2 02:07:51.513: INFO: Created: latency-svc-b5f7n
Mar  2 02:07:51.520: INFO: Created: latency-svc-5tksk
Mar  2 02:07:51.526: INFO: Got endpoints: latency-svc-b5f7n [118.952404ms]
Mar  2 02:07:51.541: INFO: Got endpoints: latency-svc-5tksk [136.309145ms]
Mar  2 02:07:51.545: INFO: Created: latency-svc-7m7z6
Mar  2 02:07:51.569: INFO: Got endpoints: latency-svc-7m7z6 [144.657826ms]
Mar  2 02:07:51.578: INFO: Created: latency-svc-d4ws8
Mar  2 02:07:51.590: INFO: Created: latency-svc-nt8cn
Mar  2 02:07:51.601: INFO: Got endpoints: latency-svc-d4ws8 [176.785404ms]
Mar  2 02:07:51.606: INFO: Got endpoints: latency-svc-nt8cn [178.10951ms]
Mar  2 02:07:51.612: INFO: Created: latency-svc-5r9t8
Mar  2 02:07:51.625: INFO: Got endpoints: latency-svc-5r9t8 [196.936705ms]
Mar  2 02:07:51.626: INFO: Created: latency-svc-cwp95
Mar  2 02:07:51.640: INFO: Got endpoints: latency-svc-cwp95 [211.132204ms]
Mar  2 02:07:51.641: INFO: Created: latency-svc-lgvc6
Mar  2 02:07:51.668: INFO: Got endpoints: latency-svc-lgvc6 [220.328474ms]
Mar  2 02:07:51.669: INFO: Created: latency-svc-vhlxm
Mar  2 02:07:51.693: INFO: Got endpoints: latency-svc-vhlxm [243.538059ms]
Mar  2 02:07:51.706: INFO: Created: latency-svc-9td8f
Mar  2 02:07:51.720: INFO: Got endpoints: latency-svc-9td8f [270.094823ms]
Mar  2 02:07:51.729: INFO: Created: latency-svc-7vhgr
Mar  2 02:07:51.745: INFO: Got endpoints: latency-svc-7vhgr [294.634063ms]
Mar  2 02:07:51.747: INFO: Created: latency-svc-ttpth
Mar  2 02:07:51.766: INFO: Created: latency-svc-fgzt7
Mar  2 02:07:51.786: INFO: Got endpoints: latency-svc-fgzt7 [296.14312ms]
Mar  2 02:07:51.786: INFO: Got endpoints: latency-svc-ttpth [336.110065ms]
Mar  2 02:07:51.787: INFO: Created: latency-svc-gj4hq
Mar  2 02:07:51.804: INFO: Got endpoints: latency-svc-gj4hq [314.07165ms]
Mar  2 02:07:51.815: INFO: Created: latency-svc-j88r5
Mar  2 02:07:51.824: INFO: Created: latency-svc-sw5dr
Mar  2 02:07:51.831: INFO: Got endpoints: latency-svc-j88r5 [340.344034ms]
Mar  2 02:07:51.855: INFO: Got endpoints: latency-svc-sw5dr [329.003022ms]
Mar  2 02:07:51.857: INFO: Created: latency-svc-vjmqp
Mar  2 02:07:51.869: INFO: Created: latency-svc-gglpr
Mar  2 02:07:51.872: INFO: Got endpoints: latency-svc-vjmqp [330.385503ms]
Mar  2 02:07:51.885: INFO: Got endpoints: latency-svc-gglpr [315.874928ms]
Mar  2 02:07:51.886: INFO: Created: latency-svc-vwck4
Mar  2 02:07:51.902: INFO: Got endpoints: latency-svc-vwck4 [300.945156ms]
Mar  2 02:07:51.916: INFO: Created: latency-svc-jf4zh
Mar  2 02:07:51.936: INFO: Got endpoints: latency-svc-jf4zh [329.807525ms]
Mar  2 02:07:51.936: INFO: Created: latency-svc-znqcc
Mar  2 02:07:51.963: INFO: Got endpoints: latency-svc-znqcc [337.967525ms]
Mar  2 02:07:51.979: INFO: Created: latency-svc-pfx87
Mar  2 02:07:52.026: INFO: Created: latency-svc-zj4nr
Mar  2 02:07:52.026: INFO: Got endpoints: latency-svc-pfx87 [386.450153ms]
Mar  2 02:07:52.043: INFO: Got endpoints: latency-svc-zj4nr [375.657057ms]
Mar  2 02:07:52.044: INFO: Created: latency-svc-szvhc
Mar  2 02:07:52.046: INFO: Created: latency-svc-g7cwr
Mar  2 02:07:52.046: INFO: Created: latency-svc-pt872
Mar  2 02:07:52.055: INFO: Created: latency-svc-lxnps
Mar  2 02:07:52.068: INFO: Got endpoints: latency-svc-pt872 [321.605398ms]
Mar  2 02:07:52.068: INFO: Got endpoints: latency-svc-g7cwr [375.356766ms]
Mar  2 02:07:52.069: INFO: Got endpoints: latency-svc-szvhc [348.146236ms]
Mar  2 02:07:52.082: INFO: Got endpoints: latency-svc-lxnps [296.098299ms]
Mar  2 02:07:52.093: INFO: Created: latency-svc-xdxft
Mar  2 02:07:52.099: INFO: Created: latency-svc-ks27z
Mar  2 02:07:52.108: INFO: Got endpoints: latency-svc-xdxft [322.320831ms]
Mar  2 02:07:52.118: INFO: Got endpoints: latency-svc-ks27z [314.167036ms]
Mar  2 02:07:52.126: INFO: Created: latency-svc-rscmt
Mar  2 02:07:52.132: INFO: Got endpoints: latency-svc-rscmt [300.353758ms]
Mar  2 02:07:52.142: INFO: Created: latency-svc-c954z
Mar  2 02:07:52.149: INFO: Created: latency-svc-kg6r8
Mar  2 02:07:52.161: INFO: Got endpoints: latency-svc-c954z [306.720333ms]
Mar  2 02:07:52.164: INFO: Got endpoints: latency-svc-kg6r8 [291.917368ms]
Mar  2 02:07:52.171: INFO: Created: latency-svc-g8zxh
Mar  2 02:07:52.190: INFO: Created: latency-svc-6n78t
Mar  2 02:07:52.193: INFO: Got endpoints: latency-svc-g8zxh [308.246402ms]
Mar  2 02:07:52.211: INFO: Got endpoints: latency-svc-6n78t [309.383361ms]
Mar  2 02:07:52.212: INFO: Created: latency-svc-vczkj
Mar  2 02:07:52.226: INFO: Got endpoints: latency-svc-vczkj [289.422786ms]
Mar  2 02:07:52.229: INFO: Created: latency-svc-p2k5m
Mar  2 02:07:52.244: INFO: Got endpoints: latency-svc-p2k5m [280.131363ms]
Mar  2 02:07:52.245: INFO: Created: latency-svc-f9qns
Mar  2 02:07:52.264: INFO: Got endpoints: latency-svc-f9qns [237.493933ms]
Mar  2 02:07:52.273: INFO: Created: latency-svc-x5f2z
Mar  2 02:07:52.288: INFO: Got endpoints: latency-svc-x5f2z [244.262482ms]
Mar  2 02:07:52.324: INFO: Created: latency-svc-52m6d
Mar  2 02:07:52.324: INFO: Created: latency-svc-jw8c5
Mar  2 02:07:52.325: INFO: Created: latency-svc-sb7kn
Mar  2 02:07:52.326: INFO: Got endpoints: latency-svc-jw8c5 [257.895704ms]
Mar  2 02:07:52.326: INFO: Got endpoints: latency-svc-52m6d [257.465589ms]
Mar  2 02:07:52.337: INFO: Got endpoints: latency-svc-sb7kn [268.503657ms]
Mar  2 02:07:52.338: INFO: Created: latency-svc-25gsc
Mar  2 02:07:52.356: INFO: Got endpoints: latency-svc-25gsc [273.483432ms]
Mar  2 02:07:52.356: INFO: Created: latency-svc-rsccn
Mar  2 02:07:52.372: INFO: Created: latency-svc-4gkkw
Mar  2 02:07:52.376: INFO: Got endpoints: latency-svc-rsccn [257.484161ms]
Mar  2 02:07:52.384: INFO: Got endpoints: latency-svc-4gkkw [275.968364ms]
Mar  2 02:07:52.392: INFO: Created: latency-svc-d6sb8
Mar  2 02:07:52.407: INFO: Created: latency-svc-btbwn
Mar  2 02:07:52.408: INFO: Got endpoints: latency-svc-d6sb8 [276.573217ms]
Mar  2 02:07:52.421: INFO: Got endpoints: latency-svc-btbwn [258.839212ms]
Mar  2 02:07:52.431: INFO: Created: latency-svc-6rh24
Mar  2 02:07:52.445: INFO: Created: latency-svc-bb2mr
Mar  2 02:07:52.453: INFO: Got endpoints: latency-svc-6rh24 [289.311414ms]
Mar  2 02:07:52.458: INFO: Got endpoints: latency-svc-bb2mr [264.600784ms]
Mar  2 02:07:52.463: INFO: Created: latency-svc-dqlkb
Mar  2 02:07:52.472: INFO: Got endpoints: latency-svc-dqlkb [260.649955ms]
Mar  2 02:07:52.482: INFO: Created: latency-svc-dxnjk
Mar  2 02:07:52.496: INFO: Created: latency-svc-dl7jk
Mar  2 02:07:52.497: INFO: Got endpoints: latency-svc-dxnjk [270.813819ms]
Mar  2 02:07:52.509: INFO: Got endpoints: latency-svc-dl7jk [265.347184ms]
Mar  2 02:07:52.514: INFO: Created: latency-svc-cqpgb
Mar  2 02:07:52.539: INFO: Got endpoints: latency-svc-cqpgb [275.066605ms]
Mar  2 02:07:52.548: INFO: Created: latency-svc-gbvkt
Mar  2 02:07:52.564: INFO: Created: latency-svc-hlgbq
Mar  2 02:07:52.573: INFO: Got endpoints: latency-svc-gbvkt [285.217258ms]
Mar  2 02:07:52.577: INFO: Got endpoints: latency-svc-hlgbq [251.23958ms]
Mar  2 02:07:52.583: INFO: Created: latency-svc-qhqhh
Mar  2 02:07:52.621: INFO: Got endpoints: latency-svc-qhqhh [295.004119ms]
Mar  2 02:07:52.623: INFO: Created: latency-svc-5sxck
Mar  2 02:07:52.644: INFO: Got endpoints: latency-svc-5sxck [306.494057ms]
Mar  2 02:07:52.674: INFO: Created: latency-svc-2vqpx
Mar  2 02:07:52.690: INFO: Got endpoints: latency-svc-2vqpx [334.43465ms]
Mar  2 02:07:52.700: INFO: Created: latency-svc-cf7td
Mar  2 02:07:52.712: INFO: Got endpoints: latency-svc-cf7td [335.879891ms]
Mar  2 02:07:52.713: INFO: Created: latency-svc-tq4ht
Mar  2 02:07:52.730: INFO: Got endpoints: latency-svc-tq4ht [345.268137ms]
Mar  2 02:07:52.741: INFO: Created: latency-svc-lllwf
Mar  2 02:07:52.753: INFO: Created: latency-svc-nmnvr
Mar  2 02:07:52.759: INFO: Got endpoints: latency-svc-lllwf [351.196957ms]
Mar  2 02:07:52.768: INFO: Created: latency-svc-rb8mf
Mar  2 02:07:52.771: INFO: Got endpoints: latency-svc-nmnvr [350.056644ms]
Mar  2 02:07:52.782: INFO: Got endpoints: latency-svc-rb8mf [328.613641ms]
Mar  2 02:07:52.786: INFO: Created: latency-svc-ll56l
Mar  2 02:07:52.805: INFO: Got endpoints: latency-svc-ll56l [346.280538ms]
Mar  2 02:07:52.805: INFO: Created: latency-svc-m6jnw
Mar  2 02:07:52.819: INFO: Got endpoints: latency-svc-m6jnw [346.332142ms]
Mar  2 02:07:52.824: INFO: Created: latency-svc-8g48k
Mar  2 02:07:52.871: INFO: Created: latency-svc-fwxvz
Mar  2 02:07:52.871: INFO: Got endpoints: latency-svc-fwxvz [362.365457ms]
Mar  2 02:07:52.872: INFO: Created: latency-svc-sjspb
Mar  2 02:07:52.872: INFO: Got endpoints: latency-svc-8g48k [375.477313ms]
Mar  2 02:07:52.883: INFO: Got endpoints: latency-svc-sjspb [344.325624ms]
Mar  2 02:07:52.884: INFO: Created: latency-svc-kp2gv
Mar  2 02:07:52.904: INFO: Got endpoints: latency-svc-kp2gv [331.441376ms]
Mar  2 02:07:52.911: INFO: Created: latency-svc-q8kfl
Mar  2 02:07:52.923: INFO: Created: latency-svc-qwtpg
Mar  2 02:07:52.927: INFO: Got endpoints: latency-svc-q8kfl [349.513557ms]
Mar  2 02:07:52.941: INFO: Got endpoints: latency-svc-qwtpg [319.957204ms]
Mar  2 02:07:52.951: INFO: Created: latency-svc-tbnrd
Mar  2 02:07:52.967: INFO: Created: latency-svc-9qqrv
Mar  2 02:07:52.968: INFO: Got endpoints: latency-svc-tbnrd [324.357406ms]
Mar  2 02:07:52.983: INFO: Got endpoints: latency-svc-9qqrv [292.166292ms]
Mar  2 02:07:52.984: INFO: Created: latency-svc-d82j2
Mar  2 02:07:53.002: INFO: Created: latency-svc-grjzt
Mar  2 02:07:53.002: INFO: Got endpoints: latency-svc-d82j2 [290.405586ms]
Mar  2 02:07:53.024: INFO: Got endpoints: latency-svc-grjzt [294.597841ms]
Mar  2 02:07:53.026: INFO: Created: latency-svc-twqpx
Mar  2 02:07:53.039: INFO: Created: latency-svc-km4hg
Mar  2 02:07:53.040: INFO: Got endpoints: latency-svc-twqpx [280.428961ms]
Mar  2 02:07:53.049: INFO: Created: latency-svc-5gdcp
Mar  2 02:07:53.051: INFO: Got endpoints: latency-svc-km4hg [279.668481ms]
Mar  2 02:07:53.083: INFO: Created: latency-svc-8kf9h
Mar  2 02:07:53.083: INFO: Got endpoints: latency-svc-5gdcp [300.91068ms]
Mar  2 02:07:53.124: INFO: Got endpoints: latency-svc-8kf9h [319.078763ms]
Mar  2 02:07:53.124: INFO: Created: latency-svc-9wngc
Mar  2 02:07:53.125: INFO: Created: latency-svc-pkmmv
Mar  2 02:07:53.132: INFO: Created: latency-svc-h9pmr
Mar  2 02:07:53.140: INFO: Got endpoints: latency-svc-pkmmv [268.907571ms]
Mar  2 02:07:53.143: INFO: Got endpoints: latency-svc-9wngc [324.153659ms]
Mar  2 02:07:53.149: INFO: Got endpoints: latency-svc-h9pmr [276.975723ms]
Mar  2 02:07:53.156: INFO: Created: latency-svc-976ft
Mar  2 02:07:53.168: INFO: Created: latency-svc-g6skr
Mar  2 02:07:53.175: INFO: Got endpoints: latency-svc-976ft [292.030324ms]
Mar  2 02:07:53.185: INFO: Created: latency-svc-6fsdf
Mar  2 02:07:53.220: INFO: Created: latency-svc-plcfg
Mar  2 02:07:53.222: INFO: Got endpoints: latency-svc-6fsdf [295.118778ms]
Mar  2 02:07:53.223: INFO: Got endpoints: latency-svc-g6skr [318.085364ms]
Mar  2 02:07:53.224: INFO: Got endpoints: latency-svc-plcfg [282.537669ms]
Mar  2 02:07:53.226: INFO: Created: latency-svc-d7br5
Mar  2 02:07:53.248: INFO: Got endpoints: latency-svc-d7br5 [278.661017ms]
Mar  2 02:07:53.248: INFO: Created: latency-svc-2wr95
Mar  2 02:07:53.286: INFO: Created: latency-svc-gc5sx
Mar  2 02:07:53.287: INFO: Created: latency-svc-4d969
Mar  2 02:07:53.287: INFO: Got endpoints: latency-svc-gc5sx [285.113746ms]
Mar  2 02:07:53.288: INFO: Got endpoints: latency-svc-2wr95 [304.883837ms]
Mar  2 02:07:53.303: INFO: Created: latency-svc-wn4gg
Mar  2 02:07:53.303: INFO: Got endpoints: latency-svc-4d969 [278.761805ms]
Mar  2 02:07:53.314: INFO: Got endpoints: latency-svc-wn4gg [273.821893ms]
Mar  2 02:07:53.319: INFO: Created: latency-svc-clmql
Mar  2 02:07:53.335: INFO: Got endpoints: latency-svc-clmql [283.984628ms]
Mar  2 02:07:53.339: INFO: Created: latency-svc-5kts6
Mar  2 02:07:53.353: INFO: Got endpoints: latency-svc-5kts6 [270.264881ms]
Mar  2 02:07:53.358: INFO: Created: latency-svc-bwklw
Mar  2 02:07:53.379: INFO: Created: latency-svc-s75wx
Mar  2 02:07:53.398: INFO: Created: latency-svc-ss7xr
Mar  2 02:07:53.406: INFO: Got endpoints: latency-svc-bwklw [281.631145ms]
Mar  2 02:07:53.419: INFO: Got endpoints: latency-svc-s75wx [278.060521ms]
Mar  2 02:07:53.423: INFO: Got endpoints: latency-svc-ss7xr [279.867873ms]
Mar  2 02:07:53.432: INFO: Created: latency-svc-njnbm
Mar  2 02:07:53.448: INFO: Got endpoints: latency-svc-njnbm [298.186939ms]
Mar  2 02:07:53.455: INFO: Created: latency-svc-h68jv
Mar  2 02:07:53.480: INFO: Created: latency-svc-q5nds
Mar  2 02:07:53.490: INFO: Got endpoints: latency-svc-h68jv [314.884136ms]
Mar  2 02:07:53.499: INFO: Got endpoints: latency-svc-q5nds [277.057968ms]
Mar  2 02:07:53.500: INFO: Created: latency-svc-dkrmp
Mar  2 02:07:53.515: INFO: Created: latency-svc-r9nh7
Mar  2 02:07:53.519: INFO: Got endpoints: latency-svc-dkrmp [296.04072ms]
Mar  2 02:07:53.529: INFO: Got endpoints: latency-svc-r9nh7 [305.677976ms]
Mar  2 02:07:53.543: INFO: Created: latency-svc-t9dp5
Mar  2 02:07:53.557: INFO: Created: latency-svc-vljx8
Mar  2 02:07:53.561: INFO: Got endpoints: latency-svc-t9dp5 [312.859078ms]
Mar  2 02:07:53.571: INFO: Created: latency-svc-67hh2
Mar  2 02:07:53.573: INFO: Got endpoints: latency-svc-vljx8 [285.255763ms]
Mar  2 02:07:53.587: INFO: Got endpoints: latency-svc-67hh2 [300.05506ms]
Mar  2 02:07:53.589: INFO: Created: latency-svc-fhjsr
Mar  2 02:07:53.606: INFO: Got endpoints: latency-svc-fhjsr [302.810177ms]
Mar  2 02:07:53.607: INFO: Created: latency-svc-qq8sz
Mar  2 02:07:53.622: INFO: Got endpoints: latency-svc-qq8sz [308.399094ms]
Mar  2 02:07:53.634: INFO: Created: latency-svc-hw294
Mar  2 02:07:53.647: INFO: Created: latency-svc-nv6d7
Mar  2 02:07:53.650: INFO: Got endpoints: latency-svc-hw294 [314.447571ms]
Mar  2 02:07:53.664: INFO: Created: latency-svc-slhdr
Mar  2 02:07:53.673: INFO: Got endpoints: latency-svc-nv6d7 [319.819311ms]
Mar  2 02:07:53.678: INFO: Got endpoints: latency-svc-slhdr [272.301143ms]
Mar  2 02:07:53.681: INFO: Created: latency-svc-lvs7p
Mar  2 02:07:53.694: INFO: Got endpoints: latency-svc-lvs7p [275.628244ms]
Mar  2 02:07:53.704: INFO: Created: latency-svc-c6bnh
Mar  2 02:07:53.720: INFO: Got endpoints: latency-svc-c6bnh [297.302488ms]
Mar  2 02:07:53.725: INFO: Created: latency-svc-m575x
Mar  2 02:07:53.740: INFO: Got endpoints: latency-svc-m575x [292.417226ms]
Mar  2 02:07:53.744: INFO: Created: latency-svc-vtrkc
Mar  2 02:07:53.759: INFO: Got endpoints: latency-svc-vtrkc [268.201544ms]
Mar  2 02:07:53.771: INFO: Created: latency-svc-74z7l
Mar  2 02:07:53.786: INFO: Got endpoints: latency-svc-74z7l [286.21296ms]
Mar  2 02:07:53.794: INFO: Created: latency-svc-7pjnv
Mar  2 02:07:53.811: INFO: Created: latency-svc-wzk2d
Mar  2 02:07:53.816: INFO: Got endpoints: latency-svc-7pjnv [296.204844ms]
Mar  2 02:07:53.836: INFO: Created: latency-svc-gf459
Mar  2 02:07:53.842: INFO: Got endpoints: latency-svc-wzk2d [312.219033ms]
Mar  2 02:07:53.846: INFO: Got endpoints: latency-svc-gf459 [285.011548ms]
Mar  2 02:07:53.856: INFO: Created: latency-svc-mtxz8
Mar  2 02:07:53.869: INFO: Created: latency-svc-vfd9m
Mar  2 02:07:53.871: INFO: Got endpoints: latency-svc-mtxz8 [298.196648ms]
Mar  2 02:07:53.888: INFO: Got endpoints: latency-svc-vfd9m [300.142586ms]
Mar  2 02:07:53.888: INFO: Created: latency-svc-d74kw
Mar  2 02:07:53.906: INFO: Got endpoints: latency-svc-d74kw [299.389025ms]
Mar  2 02:07:53.907: INFO: Created: latency-svc-clp7f
Mar  2 02:07:53.920: INFO: Created: latency-svc-6cz6k
Mar  2 02:07:53.932: INFO: Got endpoints: latency-svc-clp7f [309.606707ms]
Mar  2 02:07:53.938: INFO: Got endpoints: latency-svc-6cz6k [287.949294ms]
Mar  2 02:07:53.946: INFO: Created: latency-svc-sv475
Mar  2 02:07:53.959: INFO: Created: latency-svc-bfz6f
Mar  2 02:07:53.963: INFO: Got endpoints: latency-svc-sv475 [289.586125ms]
Mar  2 02:07:53.973: INFO: Created: latency-svc-74gk8
Mar  2 02:07:53.976: INFO: Got endpoints: latency-svc-bfz6f [297.847558ms]
Mar  2 02:07:53.989: INFO: Created: latency-svc-x8xcx
Mar  2 02:07:53.999: INFO: Got endpoints: latency-svc-74gk8 [304.786935ms]
Mar  2 02:07:54.003: INFO: Got endpoints: latency-svc-x8xcx [282.444716ms]
Mar  2 02:07:54.003: INFO: Latencies: [54.98483ms 66.774183ms 82.992079ms 99.705672ms 118.952404ms 128.394998ms 129.213107ms 134.325975ms 136.309145ms 144.657826ms 163.990999ms 176.785404ms 178.10951ms 189.84487ms 196.936705ms 200.832706ms 211.132204ms 220.328474ms 232.087715ms 235.074399ms 235.328497ms 237.493933ms 243.538059ms 244.262482ms 244.828315ms 249.245139ms 249.44867ms 251.210622ms 251.23958ms 252.804234ms 257.465589ms 257.484161ms 257.895704ms 258.839212ms 259.6497ms 259.772017ms 260.185821ms 260.649955ms 260.79015ms 261.63393ms 261.899294ms 264.600784ms 265.347184ms 268.201544ms 268.360741ms 268.503657ms 268.907571ms 270.094823ms 270.264881ms 270.813819ms 271.157891ms 272.301143ms 273.483432ms 273.821893ms 275.066605ms 275.628244ms 275.968364ms 276.573217ms 276.975723ms 277.057968ms 278.060521ms 278.661017ms 278.761805ms 279.668481ms 279.867873ms 280.131363ms 280.428961ms 281.569689ms 281.631145ms 282.444716ms 282.537669ms 283.261259ms 283.984628ms 284.435854ms 284.603804ms 285.011548ms 285.059008ms 285.113746ms 285.217258ms 285.255763ms 285.367167ms 286.21296ms 287.455176ms 287.949294ms 288.120788ms 289.311414ms 289.422786ms 289.586125ms 290.405586ms 291.312431ms 291.917368ms 292.030324ms 292.166292ms 292.417226ms 294.004858ms 294.597841ms 294.634063ms 295.004119ms 295.118778ms 295.165291ms 295.388924ms 295.836428ms 296.04072ms 296.098299ms 296.14312ms 296.204844ms 296.279987ms 297.049814ms 297.302488ms 297.358567ms 297.847558ms 298.186939ms 298.196648ms 299.104509ms 299.239178ms 299.389025ms 300.05506ms 300.142586ms 300.274971ms 300.353758ms 300.753634ms 300.91068ms 300.945156ms 300.98852ms 302.070782ms 302.810177ms 302.8701ms 304.349919ms 304.786935ms 304.883837ms 305.677976ms 306.494057ms 306.720333ms 308.246402ms 308.399094ms 309.383361ms 309.606707ms 312.219033ms 312.859078ms 314.07165ms 314.167036ms 314.447571ms 314.693671ms 314.884136ms 315.874928ms 315.931431ms 315.953004ms 316.32033ms 316.37035ms 318.085364ms 318.472624ms 318.49672ms 319.078763ms 319.819311ms 319.957204ms 321.605398ms 321.930053ms 322.320831ms 323.916181ms 324.153659ms 324.357406ms 324.618241ms 324.650637ms 328.613641ms 329.003022ms 329.807525ms 330.385503ms 331.441376ms 331.989864ms 334.43465ms 335.879891ms 336.110065ms 337.967525ms 340.344034ms 344.325624ms 345.268137ms 346.280538ms 346.332142ms 348.146236ms 349.513557ms 350.056644ms 351.196957ms 361.127227ms 362.365457ms 365.543132ms 375.356766ms 375.477313ms 375.657057ms 378.54865ms 386.450153ms 388.080021ms 392.873579ms 439.125838ms 439.134496ms 444.319135ms 489.405217ms 507.229951ms 511.025441ms 538.784984ms 539.071266ms]
Mar  2 02:07:54.003: INFO: 50 %ile: 295.388924ms
Mar  2 02:07:54.003: INFO: 90 %ile: 350.056644ms
Mar  2 02:07:54.003: INFO: 99 %ile: 538.784984ms
Mar  2 02:07:54.003: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:187
Mar  2 02:07:54.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-5429" for this suite. 03/02/23 02:07:54.028
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","completed":194,"skipped":3603,"failed":0}
------------------------------
• [SLOW TEST] [6.424 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:07:47.628
    Mar  2 02:07:47.628: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename svc-latency 03/02/23 02:07:47.63
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:07:47.68
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:07:47.695
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Mar  2 02:07:47.707: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-5429 03/02/23 02:07:47.709
    W0302 02:07:47.767972      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "svc-latency-rc" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "svc-latency-rc" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "svc-latency-rc" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "svc-latency-rc" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    I0302 02:07:47.768358      20 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5429, replica count: 1
    I0302 02:07:48.819620      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0302 02:07:49.820698      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar  2 02:07:49.957: INFO: Created: latency-svc-27sz4
    Mar  2 02:07:49.976: INFO: Got endpoints: latency-svc-27sz4 [55.442132ms]
    Mar  2 02:07:50.010: INFO: Created: latency-svc-gf7gp
    Mar  2 02:07:50.026: INFO: Created: latency-svc-ttwxx
    Mar  2 02:07:50.032: INFO: Got endpoints: latency-svc-gf7gp [54.98483ms]
    Mar  2 02:07:50.042: INFO: Created: latency-svc-djx7v
    Mar  2 02:07:50.043: INFO: Got endpoints: latency-svc-ttwxx [66.774183ms]
    Mar  2 02:07:50.077: INFO: Got endpoints: latency-svc-djx7v [99.705672ms]
    Mar  2 02:07:50.078: INFO: Created: latency-svc-jkq54
    Mar  2 02:07:50.084: INFO: Created: latency-svc-g8p7j
    Mar  2 02:07:50.091: INFO: Created: latency-svc-sgst5
    Mar  2 02:07:50.106: INFO: Got endpoints: latency-svc-g8p7j [128.394998ms]
    Mar  2 02:07:50.106: INFO: Got endpoints: latency-svc-jkq54 [129.213107ms]
    Mar  2 02:07:50.111: INFO: Got endpoints: latency-svc-sgst5 [134.325975ms]
    Mar  2 02:07:50.120: INFO: Created: latency-svc-b2z4c
    Mar  2 02:07:50.141: INFO: Got endpoints: latency-svc-b2z4c [163.990999ms]
    Mar  2 02:07:50.150: INFO: Created: latency-svc-lk488
    Mar  2 02:07:50.162: INFO: Created: latency-svc-v8mbw
    Mar  2 02:07:50.166: INFO: Got endpoints: latency-svc-lk488 [189.84487ms]
    Mar  2 02:07:50.178: INFO: Got endpoints: latency-svc-v8mbw [200.832706ms]
    Mar  2 02:07:50.180: INFO: Created: latency-svc-xsll2
    Mar  2 02:07:50.209: INFO: Got endpoints: latency-svc-xsll2 [232.087715ms]
    Mar  2 02:07:50.210: INFO: Created: latency-svc-vpdq9
    Mar  2 02:07:50.211: INFO: Created: latency-svc-bmkhz
    Mar  2 02:07:50.213: INFO: Got endpoints: latency-svc-vpdq9 [235.074399ms]
    Mar  2 02:07:50.225: INFO: Created: latency-svc-xbg7x
    Mar  2 02:07:50.227: INFO: Got endpoints: latency-svc-bmkhz [249.44867ms]
    Mar  2 02:07:50.246: INFO: Got endpoints: latency-svc-xbg7x [268.360741ms]
    Mar  2 02:07:50.247: INFO: Created: latency-svc-q97xn
    Mar  2 02:07:50.265: INFO: Got endpoints: latency-svc-q97xn [287.455176ms]
    Mar  2 02:07:50.266: INFO: Created: latency-svc-sf5kx
    Mar  2 02:07:50.278: INFO: Got endpoints: latency-svc-sf5kx [300.274971ms]
    Mar  2 02:07:50.281: INFO: Created: latency-svc-fsbhx
    Mar  2 02:07:50.293: INFO: Got endpoints: latency-svc-fsbhx [261.63393ms]
    Mar  2 02:07:50.299: INFO: Created: latency-svc-cg6n2
    Mar  2 02:07:50.315: INFO: Got endpoints: latency-svc-cg6n2 [271.157891ms]
    Mar  2 02:07:50.318: INFO: Created: latency-svc-fcpt7
    Mar  2 02:07:50.332: INFO: Created: latency-svc-nfwh7
    Mar  2 02:07:50.338: INFO: Got endpoints: latency-svc-fcpt7 [260.79015ms]
    Mar  2 02:07:50.342: INFO: Got endpoints: latency-svc-nfwh7 [235.328497ms]
    Mar  2 02:07:50.353: INFO: Created: latency-svc-wm5qv
    Mar  2 02:07:50.368: INFO: Got endpoints: latency-svc-wm5qv [261.899294ms]
    Mar  2 02:07:50.382: INFO: Created: latency-svc-ksk6z
    Mar  2 02:07:50.385: INFO: Created: latency-svc-dnrgn
    Mar  2 02:07:50.393: INFO: Got endpoints: latency-svc-ksk6z [281.569689ms]
    Mar  2 02:07:50.401: INFO: Got endpoints: latency-svc-dnrgn [259.6497ms]
    Mar  2 02:07:50.402: INFO: Created: latency-svc-b97ls
    Mar  2 02:07:50.416: INFO: Created: latency-svc-2gjzb
    Mar  2 02:07:50.416: INFO: Got endpoints: latency-svc-b97ls [249.245139ms]
    Mar  2 02:07:50.431: INFO: Got endpoints: latency-svc-2gjzb [252.804234ms]
    Mar  2 02:07:50.436: INFO: Created: latency-svc-k6k2g
    Mar  2 02:07:50.451: INFO: Created: latency-svc-nrp98
    Mar  2 02:07:50.454: INFO: Got endpoints: latency-svc-k6k2g [244.828315ms]
    Mar  2 02:07:50.464: INFO: Got endpoints: latency-svc-nrp98 [251.210622ms]
    Mar  2 02:07:50.471: INFO: Created: latency-svc-sk6lr
    Mar  2 02:07:50.487: INFO: Got endpoints: latency-svc-sk6lr [259.772017ms]
    Mar  2 02:07:50.491: INFO: Created: latency-svc-772sc
    Mar  2 02:07:50.506: INFO: Got endpoints: latency-svc-772sc [260.185821ms]
    Mar  2 02:07:50.527: INFO: Created: latency-svc-28v6h
    Mar  2 02:07:50.549: INFO: Created: latency-svc-xv6lr
    Mar  2 02:07:50.553: INFO: Got endpoints: latency-svc-28v6h [288.120788ms]
    Mar  2 02:07:50.563: INFO: Got endpoints: latency-svc-xv6lr [285.059008ms]
    Mar  2 02:07:50.581: INFO: Created: latency-svc-h7t66
    Mar  2 02:07:50.590: INFO: Got endpoints: latency-svc-h7t66 [296.279987ms]
    Mar  2 02:07:50.590: INFO: Created: latency-svc-cq9ms
    Mar  2 02:07:50.614: INFO: Got endpoints: latency-svc-cq9ms [299.104509ms]
    Mar  2 02:07:50.616: INFO: Created: latency-svc-gst5d
    Mar  2 02:07:50.622: INFO: Got endpoints: latency-svc-gst5d [284.603804ms]
    Mar  2 02:07:50.625: INFO: Created: latency-svc-547s9
    Mar  2 02:07:50.643: INFO: Got endpoints: latency-svc-547s9 [300.98852ms]
    Mar  2 02:07:50.645: INFO: Created: latency-svc-g9wc2
    Mar  2 02:07:50.659: INFO: Created: latency-svc-5qrhg
    Mar  2 02:07:50.662: INFO: Got endpoints: latency-svc-g9wc2 [294.004858ms]
    Mar  2 02:07:50.690: INFO: Got endpoints: latency-svc-5qrhg [297.049814ms]
    Mar  2 02:07:50.706: INFO: Created: latency-svc-54pdh
    Mar  2 02:07:50.706: INFO: Created: latency-svc-blpwr
    Mar  2 02:07:50.719: INFO: Created: latency-svc-hs9qx
    Mar  2 02:07:50.719: INFO: Got endpoints: latency-svc-54pdh [302.8701ms]
    Mar  2 02:07:50.719: INFO: Got endpoints: latency-svc-blpwr [318.49672ms]
    Mar  2 02:07:50.734: INFO: Got endpoints: latency-svc-hs9qx [302.070782ms]
    Mar  2 02:07:50.738: INFO: Created: latency-svc-67ff9
    Mar  2 02:07:50.750: INFO: Got endpoints: latency-svc-67ff9 [295.388924ms]
    Mar  2 02:07:50.754: INFO: Created: latency-svc-kcx9m
    Mar  2 02:07:50.788: INFO: Got endpoints: latency-svc-kcx9m [323.916181ms]
    Mar  2 02:07:50.791: INFO: Created: latency-svc-h2cgb
    Mar  2 02:07:50.805: INFO: Created: latency-svc-sp5q2
    Mar  2 02:07:50.806: INFO: Got endpoints: latency-svc-h2cgb [318.472624ms]
    Mar  2 02:07:50.823: INFO: Got endpoints: latency-svc-sp5q2 [315.953004ms]
    Mar  2 02:07:50.825: INFO: Created: latency-svc-gfjcv
    Mar  2 02:07:50.838: INFO: Got endpoints: latency-svc-gfjcv [284.435854ms]
    Mar  2 02:07:50.842: INFO: Created: latency-svc-smjdb
    Mar  2 02:07:50.867: INFO: Got endpoints: latency-svc-smjdb [304.349919ms]
    Mar  2 02:07:50.871: INFO: Created: latency-svc-cnmpb
    Mar  2 02:07:50.883: INFO: Created: latency-svc-dhp8t
    Mar  2 02:07:50.889: INFO: Got endpoints: latency-svc-cnmpb [299.239178ms]
    Mar  2 02:07:50.897: INFO: Got endpoints: latency-svc-dhp8t [283.261259ms]
    Mar  2 02:07:50.910: INFO: Created: latency-svc-b4xkf
    Mar  2 02:07:50.926: INFO: Created: latency-svc-djbdq
    Mar  2 02:07:50.939: INFO: Got endpoints: latency-svc-b4xkf [316.37035ms]
    Mar  2 02:07:50.968: INFO: Got endpoints: latency-svc-djbdq [324.650637ms]
    Mar  2 02:07:50.968: INFO: Created: latency-svc-nqf7j
    Mar  2 02:07:50.979: INFO: Got endpoints: latency-svc-nqf7j [316.32033ms]
    Mar  2 02:07:50.979: INFO: Created: latency-svc-pqsh2
    Mar  2 02:07:50.992: INFO: Created: latency-svc-flb66
    Mar  2 02:07:51.005: INFO: Got endpoints: latency-svc-pqsh2 [314.693671ms]
    Mar  2 02:07:51.011: INFO: Got endpoints: latency-svc-flb66 [291.312431ms]
    Mar  2 02:07:51.022: INFO: Created: latency-svc-nzqqj
    Mar  2 02:07:51.035: INFO: Got endpoints: latency-svc-nzqqj [315.931431ms]
    Mar  2 02:07:51.042: INFO: Created: latency-svc-tqk88
    Mar  2 02:07:51.059: INFO: Got endpoints: latency-svc-tqk88 [324.618241ms]
    Mar  2 02:07:51.059: INFO: Created: latency-svc-hzs7j
    Mar  2 02:07:51.072: INFO: Got endpoints: latency-svc-hzs7j [321.930053ms]
    Mar  2 02:07:51.073: INFO: Created: latency-svc-7bwfn
    Mar  2 02:07:51.086: INFO: Got endpoints: latency-svc-7bwfn [297.358567ms]
    Mar  2 02:07:51.095: INFO: Created: latency-svc-65plr
    Mar  2 02:07:51.102: INFO: Got endpoints: latency-svc-65plr [295.836428ms]
    Mar  2 02:07:51.104: INFO: Created: latency-svc-v5p67
    Mar  2 02:07:51.118: INFO: Got endpoints: latency-svc-v5p67 [295.165291ms]
    Mar  2 02:07:51.127: INFO: Created: latency-svc-szwlx
    Mar  2 02:07:51.139: INFO: Got endpoints: latency-svc-szwlx [300.753634ms]
    Mar  2 02:07:51.380: INFO: Created: latency-svc-2mhgm
    Mar  2 02:07:51.381: INFO: Created: latency-svc-7gvmb
    Mar  2 02:07:51.381: INFO: Created: latency-svc-ss66l
    Mar  2 02:07:51.381: INFO: Created: latency-svc-j82dx
    Mar  2 02:07:51.385: INFO: Created: latency-svc-n8bdn
    Mar  2 02:07:51.391: INFO: Created: latency-svc-qcdcz
    Mar  2 02:07:51.394: INFO: Created: latency-svc-kdtrf
    Mar  2 02:07:51.394: INFO: Created: latency-svc-6ck7f
    Mar  2 02:07:51.394: INFO: Created: latency-svc-d8d6s
    Mar  2 02:07:51.395: INFO: Created: latency-svc-6qmxz
    Mar  2 02:07:51.395: INFO: Created: latency-svc-5b4xf
    Mar  2 02:07:51.395: INFO: Created: latency-svc-8sxbc
    Mar  2 02:07:51.395: INFO: Created: latency-svc-vm446
    Mar  2 02:07:51.398: INFO: Created: latency-svc-tgnk9
    Mar  2 02:07:51.399: INFO: Created: latency-svc-mfhlz
    Mar  2 02:07:51.405: INFO: Got endpoints: latency-svc-j82dx [507.229951ms]
    Mar  2 02:07:51.407: INFO: Got endpoints: latency-svc-2mhgm [539.071266ms]
    Mar  2 02:07:51.407: INFO: Got endpoints: latency-svc-ss66l [439.125838ms]
    Mar  2 02:07:51.424: INFO: Got endpoints: latency-svc-n8bdn [285.367167ms]
    Mar  2 02:07:51.424: INFO: Got endpoints: latency-svc-7gvmb [365.543132ms]
    Mar  2 02:07:51.428: INFO: Got endpoints: latency-svc-8sxbc [392.873579ms]
    Mar  2 02:07:51.428: INFO: Got endpoints: latency-svc-6qmxz [538.784984ms]
    Mar  2 02:07:51.429: INFO: Got endpoints: latency-svc-d8d6s [489.405217ms]
    Mar  2 02:07:51.447: INFO: Got endpoints: latency-svc-qcdcz [361.127227ms]
    Mar  2 02:07:51.449: INFO: Got endpoints: latency-svc-6ck7f [444.319135ms]
    Mar  2 02:07:51.450: INFO: Got endpoints: latency-svc-mfhlz [331.989864ms]
    Mar  2 02:07:51.450: INFO: Got endpoints: latency-svc-vm446 [439.134496ms]
    Mar  2 02:07:51.450: INFO: Got endpoints: latency-svc-tgnk9 [378.54865ms]
    Mar  2 02:07:51.457: INFO: Created: latency-svc-x55pf
    Mar  2 02:07:51.490: INFO: Got endpoints: latency-svc-x55pf [82.992079ms]
    Mar  2 02:07:51.490: INFO: Got endpoints: latency-svc-5b4xf [388.080021ms]
    Mar  2 02:07:51.490: INFO: Got endpoints: latency-svc-kdtrf [511.025441ms]
    Mar  2 02:07:51.513: INFO: Created: latency-svc-b5f7n
    Mar  2 02:07:51.520: INFO: Created: latency-svc-5tksk
    Mar  2 02:07:51.526: INFO: Got endpoints: latency-svc-b5f7n [118.952404ms]
    Mar  2 02:07:51.541: INFO: Got endpoints: latency-svc-5tksk [136.309145ms]
    Mar  2 02:07:51.545: INFO: Created: latency-svc-7m7z6
    Mar  2 02:07:51.569: INFO: Got endpoints: latency-svc-7m7z6 [144.657826ms]
    Mar  2 02:07:51.578: INFO: Created: latency-svc-d4ws8
    Mar  2 02:07:51.590: INFO: Created: latency-svc-nt8cn
    Mar  2 02:07:51.601: INFO: Got endpoints: latency-svc-d4ws8 [176.785404ms]
    Mar  2 02:07:51.606: INFO: Got endpoints: latency-svc-nt8cn [178.10951ms]
    Mar  2 02:07:51.612: INFO: Created: latency-svc-5r9t8
    Mar  2 02:07:51.625: INFO: Got endpoints: latency-svc-5r9t8 [196.936705ms]
    Mar  2 02:07:51.626: INFO: Created: latency-svc-cwp95
    Mar  2 02:07:51.640: INFO: Got endpoints: latency-svc-cwp95 [211.132204ms]
    Mar  2 02:07:51.641: INFO: Created: latency-svc-lgvc6
    Mar  2 02:07:51.668: INFO: Got endpoints: latency-svc-lgvc6 [220.328474ms]
    Mar  2 02:07:51.669: INFO: Created: latency-svc-vhlxm
    Mar  2 02:07:51.693: INFO: Got endpoints: latency-svc-vhlxm [243.538059ms]
    Mar  2 02:07:51.706: INFO: Created: latency-svc-9td8f
    Mar  2 02:07:51.720: INFO: Got endpoints: latency-svc-9td8f [270.094823ms]
    Mar  2 02:07:51.729: INFO: Created: latency-svc-7vhgr
    Mar  2 02:07:51.745: INFO: Got endpoints: latency-svc-7vhgr [294.634063ms]
    Mar  2 02:07:51.747: INFO: Created: latency-svc-ttpth
    Mar  2 02:07:51.766: INFO: Created: latency-svc-fgzt7
    Mar  2 02:07:51.786: INFO: Got endpoints: latency-svc-fgzt7 [296.14312ms]
    Mar  2 02:07:51.786: INFO: Got endpoints: latency-svc-ttpth [336.110065ms]
    Mar  2 02:07:51.787: INFO: Created: latency-svc-gj4hq
    Mar  2 02:07:51.804: INFO: Got endpoints: latency-svc-gj4hq [314.07165ms]
    Mar  2 02:07:51.815: INFO: Created: latency-svc-j88r5
    Mar  2 02:07:51.824: INFO: Created: latency-svc-sw5dr
    Mar  2 02:07:51.831: INFO: Got endpoints: latency-svc-j88r5 [340.344034ms]
    Mar  2 02:07:51.855: INFO: Got endpoints: latency-svc-sw5dr [329.003022ms]
    Mar  2 02:07:51.857: INFO: Created: latency-svc-vjmqp
    Mar  2 02:07:51.869: INFO: Created: latency-svc-gglpr
    Mar  2 02:07:51.872: INFO: Got endpoints: latency-svc-vjmqp [330.385503ms]
    Mar  2 02:07:51.885: INFO: Got endpoints: latency-svc-gglpr [315.874928ms]
    Mar  2 02:07:51.886: INFO: Created: latency-svc-vwck4
    Mar  2 02:07:51.902: INFO: Got endpoints: latency-svc-vwck4 [300.945156ms]
    Mar  2 02:07:51.916: INFO: Created: latency-svc-jf4zh
    Mar  2 02:07:51.936: INFO: Got endpoints: latency-svc-jf4zh [329.807525ms]
    Mar  2 02:07:51.936: INFO: Created: latency-svc-znqcc
    Mar  2 02:07:51.963: INFO: Got endpoints: latency-svc-znqcc [337.967525ms]
    Mar  2 02:07:51.979: INFO: Created: latency-svc-pfx87
    Mar  2 02:07:52.026: INFO: Created: latency-svc-zj4nr
    Mar  2 02:07:52.026: INFO: Got endpoints: latency-svc-pfx87 [386.450153ms]
    Mar  2 02:07:52.043: INFO: Got endpoints: latency-svc-zj4nr [375.657057ms]
    Mar  2 02:07:52.044: INFO: Created: latency-svc-szvhc
    Mar  2 02:07:52.046: INFO: Created: latency-svc-g7cwr
    Mar  2 02:07:52.046: INFO: Created: latency-svc-pt872
    Mar  2 02:07:52.055: INFO: Created: latency-svc-lxnps
    Mar  2 02:07:52.068: INFO: Got endpoints: latency-svc-pt872 [321.605398ms]
    Mar  2 02:07:52.068: INFO: Got endpoints: latency-svc-g7cwr [375.356766ms]
    Mar  2 02:07:52.069: INFO: Got endpoints: latency-svc-szvhc [348.146236ms]
    Mar  2 02:07:52.082: INFO: Got endpoints: latency-svc-lxnps [296.098299ms]
    Mar  2 02:07:52.093: INFO: Created: latency-svc-xdxft
    Mar  2 02:07:52.099: INFO: Created: latency-svc-ks27z
    Mar  2 02:07:52.108: INFO: Got endpoints: latency-svc-xdxft [322.320831ms]
    Mar  2 02:07:52.118: INFO: Got endpoints: latency-svc-ks27z [314.167036ms]
    Mar  2 02:07:52.126: INFO: Created: latency-svc-rscmt
    Mar  2 02:07:52.132: INFO: Got endpoints: latency-svc-rscmt [300.353758ms]
    Mar  2 02:07:52.142: INFO: Created: latency-svc-c954z
    Mar  2 02:07:52.149: INFO: Created: latency-svc-kg6r8
    Mar  2 02:07:52.161: INFO: Got endpoints: latency-svc-c954z [306.720333ms]
    Mar  2 02:07:52.164: INFO: Got endpoints: latency-svc-kg6r8 [291.917368ms]
    Mar  2 02:07:52.171: INFO: Created: latency-svc-g8zxh
    Mar  2 02:07:52.190: INFO: Created: latency-svc-6n78t
    Mar  2 02:07:52.193: INFO: Got endpoints: latency-svc-g8zxh [308.246402ms]
    Mar  2 02:07:52.211: INFO: Got endpoints: latency-svc-6n78t [309.383361ms]
    Mar  2 02:07:52.212: INFO: Created: latency-svc-vczkj
    Mar  2 02:07:52.226: INFO: Got endpoints: latency-svc-vczkj [289.422786ms]
    Mar  2 02:07:52.229: INFO: Created: latency-svc-p2k5m
    Mar  2 02:07:52.244: INFO: Got endpoints: latency-svc-p2k5m [280.131363ms]
    Mar  2 02:07:52.245: INFO: Created: latency-svc-f9qns
    Mar  2 02:07:52.264: INFO: Got endpoints: latency-svc-f9qns [237.493933ms]
    Mar  2 02:07:52.273: INFO: Created: latency-svc-x5f2z
    Mar  2 02:07:52.288: INFO: Got endpoints: latency-svc-x5f2z [244.262482ms]
    Mar  2 02:07:52.324: INFO: Created: latency-svc-52m6d
    Mar  2 02:07:52.324: INFO: Created: latency-svc-jw8c5
    Mar  2 02:07:52.325: INFO: Created: latency-svc-sb7kn
    Mar  2 02:07:52.326: INFO: Got endpoints: latency-svc-jw8c5 [257.895704ms]
    Mar  2 02:07:52.326: INFO: Got endpoints: latency-svc-52m6d [257.465589ms]
    Mar  2 02:07:52.337: INFO: Got endpoints: latency-svc-sb7kn [268.503657ms]
    Mar  2 02:07:52.338: INFO: Created: latency-svc-25gsc
    Mar  2 02:07:52.356: INFO: Got endpoints: latency-svc-25gsc [273.483432ms]
    Mar  2 02:07:52.356: INFO: Created: latency-svc-rsccn
    Mar  2 02:07:52.372: INFO: Created: latency-svc-4gkkw
    Mar  2 02:07:52.376: INFO: Got endpoints: latency-svc-rsccn [257.484161ms]
    Mar  2 02:07:52.384: INFO: Got endpoints: latency-svc-4gkkw [275.968364ms]
    Mar  2 02:07:52.392: INFO: Created: latency-svc-d6sb8
    Mar  2 02:07:52.407: INFO: Created: latency-svc-btbwn
    Mar  2 02:07:52.408: INFO: Got endpoints: latency-svc-d6sb8 [276.573217ms]
    Mar  2 02:07:52.421: INFO: Got endpoints: latency-svc-btbwn [258.839212ms]
    Mar  2 02:07:52.431: INFO: Created: latency-svc-6rh24
    Mar  2 02:07:52.445: INFO: Created: latency-svc-bb2mr
    Mar  2 02:07:52.453: INFO: Got endpoints: latency-svc-6rh24 [289.311414ms]
    Mar  2 02:07:52.458: INFO: Got endpoints: latency-svc-bb2mr [264.600784ms]
    Mar  2 02:07:52.463: INFO: Created: latency-svc-dqlkb
    Mar  2 02:07:52.472: INFO: Got endpoints: latency-svc-dqlkb [260.649955ms]
    Mar  2 02:07:52.482: INFO: Created: latency-svc-dxnjk
    Mar  2 02:07:52.496: INFO: Created: latency-svc-dl7jk
    Mar  2 02:07:52.497: INFO: Got endpoints: latency-svc-dxnjk [270.813819ms]
    Mar  2 02:07:52.509: INFO: Got endpoints: latency-svc-dl7jk [265.347184ms]
    Mar  2 02:07:52.514: INFO: Created: latency-svc-cqpgb
    Mar  2 02:07:52.539: INFO: Got endpoints: latency-svc-cqpgb [275.066605ms]
    Mar  2 02:07:52.548: INFO: Created: latency-svc-gbvkt
    Mar  2 02:07:52.564: INFO: Created: latency-svc-hlgbq
    Mar  2 02:07:52.573: INFO: Got endpoints: latency-svc-gbvkt [285.217258ms]
    Mar  2 02:07:52.577: INFO: Got endpoints: latency-svc-hlgbq [251.23958ms]
    Mar  2 02:07:52.583: INFO: Created: latency-svc-qhqhh
    Mar  2 02:07:52.621: INFO: Got endpoints: latency-svc-qhqhh [295.004119ms]
    Mar  2 02:07:52.623: INFO: Created: latency-svc-5sxck
    Mar  2 02:07:52.644: INFO: Got endpoints: latency-svc-5sxck [306.494057ms]
    Mar  2 02:07:52.674: INFO: Created: latency-svc-2vqpx
    Mar  2 02:07:52.690: INFO: Got endpoints: latency-svc-2vqpx [334.43465ms]
    Mar  2 02:07:52.700: INFO: Created: latency-svc-cf7td
    Mar  2 02:07:52.712: INFO: Got endpoints: latency-svc-cf7td [335.879891ms]
    Mar  2 02:07:52.713: INFO: Created: latency-svc-tq4ht
    Mar  2 02:07:52.730: INFO: Got endpoints: latency-svc-tq4ht [345.268137ms]
    Mar  2 02:07:52.741: INFO: Created: latency-svc-lllwf
    Mar  2 02:07:52.753: INFO: Created: latency-svc-nmnvr
    Mar  2 02:07:52.759: INFO: Got endpoints: latency-svc-lllwf [351.196957ms]
    Mar  2 02:07:52.768: INFO: Created: latency-svc-rb8mf
    Mar  2 02:07:52.771: INFO: Got endpoints: latency-svc-nmnvr [350.056644ms]
    Mar  2 02:07:52.782: INFO: Got endpoints: latency-svc-rb8mf [328.613641ms]
    Mar  2 02:07:52.786: INFO: Created: latency-svc-ll56l
    Mar  2 02:07:52.805: INFO: Got endpoints: latency-svc-ll56l [346.280538ms]
    Mar  2 02:07:52.805: INFO: Created: latency-svc-m6jnw
    Mar  2 02:07:52.819: INFO: Got endpoints: latency-svc-m6jnw [346.332142ms]
    Mar  2 02:07:52.824: INFO: Created: latency-svc-8g48k
    Mar  2 02:07:52.871: INFO: Created: latency-svc-fwxvz
    Mar  2 02:07:52.871: INFO: Got endpoints: latency-svc-fwxvz [362.365457ms]
    Mar  2 02:07:52.872: INFO: Created: latency-svc-sjspb
    Mar  2 02:07:52.872: INFO: Got endpoints: latency-svc-8g48k [375.477313ms]
    Mar  2 02:07:52.883: INFO: Got endpoints: latency-svc-sjspb [344.325624ms]
    Mar  2 02:07:52.884: INFO: Created: latency-svc-kp2gv
    Mar  2 02:07:52.904: INFO: Got endpoints: latency-svc-kp2gv [331.441376ms]
    Mar  2 02:07:52.911: INFO: Created: latency-svc-q8kfl
    Mar  2 02:07:52.923: INFO: Created: latency-svc-qwtpg
    Mar  2 02:07:52.927: INFO: Got endpoints: latency-svc-q8kfl [349.513557ms]
    Mar  2 02:07:52.941: INFO: Got endpoints: latency-svc-qwtpg [319.957204ms]
    Mar  2 02:07:52.951: INFO: Created: latency-svc-tbnrd
    Mar  2 02:07:52.967: INFO: Created: latency-svc-9qqrv
    Mar  2 02:07:52.968: INFO: Got endpoints: latency-svc-tbnrd [324.357406ms]
    Mar  2 02:07:52.983: INFO: Got endpoints: latency-svc-9qqrv [292.166292ms]
    Mar  2 02:07:52.984: INFO: Created: latency-svc-d82j2
    Mar  2 02:07:53.002: INFO: Created: latency-svc-grjzt
    Mar  2 02:07:53.002: INFO: Got endpoints: latency-svc-d82j2 [290.405586ms]
    Mar  2 02:07:53.024: INFO: Got endpoints: latency-svc-grjzt [294.597841ms]
    Mar  2 02:07:53.026: INFO: Created: latency-svc-twqpx
    Mar  2 02:07:53.039: INFO: Created: latency-svc-km4hg
    Mar  2 02:07:53.040: INFO: Got endpoints: latency-svc-twqpx [280.428961ms]
    Mar  2 02:07:53.049: INFO: Created: latency-svc-5gdcp
    Mar  2 02:07:53.051: INFO: Got endpoints: latency-svc-km4hg [279.668481ms]
    Mar  2 02:07:53.083: INFO: Created: latency-svc-8kf9h
    Mar  2 02:07:53.083: INFO: Got endpoints: latency-svc-5gdcp [300.91068ms]
    Mar  2 02:07:53.124: INFO: Got endpoints: latency-svc-8kf9h [319.078763ms]
    Mar  2 02:07:53.124: INFO: Created: latency-svc-9wngc
    Mar  2 02:07:53.125: INFO: Created: latency-svc-pkmmv
    Mar  2 02:07:53.132: INFO: Created: latency-svc-h9pmr
    Mar  2 02:07:53.140: INFO: Got endpoints: latency-svc-pkmmv [268.907571ms]
    Mar  2 02:07:53.143: INFO: Got endpoints: latency-svc-9wngc [324.153659ms]
    Mar  2 02:07:53.149: INFO: Got endpoints: latency-svc-h9pmr [276.975723ms]
    Mar  2 02:07:53.156: INFO: Created: latency-svc-976ft
    Mar  2 02:07:53.168: INFO: Created: latency-svc-g6skr
    Mar  2 02:07:53.175: INFO: Got endpoints: latency-svc-976ft [292.030324ms]
    Mar  2 02:07:53.185: INFO: Created: latency-svc-6fsdf
    Mar  2 02:07:53.220: INFO: Created: latency-svc-plcfg
    Mar  2 02:07:53.222: INFO: Got endpoints: latency-svc-6fsdf [295.118778ms]
    Mar  2 02:07:53.223: INFO: Got endpoints: latency-svc-g6skr [318.085364ms]
    Mar  2 02:07:53.224: INFO: Got endpoints: latency-svc-plcfg [282.537669ms]
    Mar  2 02:07:53.226: INFO: Created: latency-svc-d7br5
    Mar  2 02:07:53.248: INFO: Got endpoints: latency-svc-d7br5 [278.661017ms]
    Mar  2 02:07:53.248: INFO: Created: latency-svc-2wr95
    Mar  2 02:07:53.286: INFO: Created: latency-svc-gc5sx
    Mar  2 02:07:53.287: INFO: Created: latency-svc-4d969
    Mar  2 02:07:53.287: INFO: Got endpoints: latency-svc-gc5sx [285.113746ms]
    Mar  2 02:07:53.288: INFO: Got endpoints: latency-svc-2wr95 [304.883837ms]
    Mar  2 02:07:53.303: INFO: Created: latency-svc-wn4gg
    Mar  2 02:07:53.303: INFO: Got endpoints: latency-svc-4d969 [278.761805ms]
    Mar  2 02:07:53.314: INFO: Got endpoints: latency-svc-wn4gg [273.821893ms]
    Mar  2 02:07:53.319: INFO: Created: latency-svc-clmql
    Mar  2 02:07:53.335: INFO: Got endpoints: latency-svc-clmql [283.984628ms]
    Mar  2 02:07:53.339: INFO: Created: latency-svc-5kts6
    Mar  2 02:07:53.353: INFO: Got endpoints: latency-svc-5kts6 [270.264881ms]
    Mar  2 02:07:53.358: INFO: Created: latency-svc-bwklw
    Mar  2 02:07:53.379: INFO: Created: latency-svc-s75wx
    Mar  2 02:07:53.398: INFO: Created: latency-svc-ss7xr
    Mar  2 02:07:53.406: INFO: Got endpoints: latency-svc-bwklw [281.631145ms]
    Mar  2 02:07:53.419: INFO: Got endpoints: latency-svc-s75wx [278.060521ms]
    Mar  2 02:07:53.423: INFO: Got endpoints: latency-svc-ss7xr [279.867873ms]
    Mar  2 02:07:53.432: INFO: Created: latency-svc-njnbm
    Mar  2 02:07:53.448: INFO: Got endpoints: latency-svc-njnbm [298.186939ms]
    Mar  2 02:07:53.455: INFO: Created: latency-svc-h68jv
    Mar  2 02:07:53.480: INFO: Created: latency-svc-q5nds
    Mar  2 02:07:53.490: INFO: Got endpoints: latency-svc-h68jv [314.884136ms]
    Mar  2 02:07:53.499: INFO: Got endpoints: latency-svc-q5nds [277.057968ms]
    Mar  2 02:07:53.500: INFO: Created: latency-svc-dkrmp
    Mar  2 02:07:53.515: INFO: Created: latency-svc-r9nh7
    Mar  2 02:07:53.519: INFO: Got endpoints: latency-svc-dkrmp [296.04072ms]
    Mar  2 02:07:53.529: INFO: Got endpoints: latency-svc-r9nh7 [305.677976ms]
    Mar  2 02:07:53.543: INFO: Created: latency-svc-t9dp5
    Mar  2 02:07:53.557: INFO: Created: latency-svc-vljx8
    Mar  2 02:07:53.561: INFO: Got endpoints: latency-svc-t9dp5 [312.859078ms]
    Mar  2 02:07:53.571: INFO: Created: latency-svc-67hh2
    Mar  2 02:07:53.573: INFO: Got endpoints: latency-svc-vljx8 [285.255763ms]
    Mar  2 02:07:53.587: INFO: Got endpoints: latency-svc-67hh2 [300.05506ms]
    Mar  2 02:07:53.589: INFO: Created: latency-svc-fhjsr
    Mar  2 02:07:53.606: INFO: Got endpoints: latency-svc-fhjsr [302.810177ms]
    Mar  2 02:07:53.607: INFO: Created: latency-svc-qq8sz
    Mar  2 02:07:53.622: INFO: Got endpoints: latency-svc-qq8sz [308.399094ms]
    Mar  2 02:07:53.634: INFO: Created: latency-svc-hw294
    Mar  2 02:07:53.647: INFO: Created: latency-svc-nv6d7
    Mar  2 02:07:53.650: INFO: Got endpoints: latency-svc-hw294 [314.447571ms]
    Mar  2 02:07:53.664: INFO: Created: latency-svc-slhdr
    Mar  2 02:07:53.673: INFO: Got endpoints: latency-svc-nv6d7 [319.819311ms]
    Mar  2 02:07:53.678: INFO: Got endpoints: latency-svc-slhdr [272.301143ms]
    Mar  2 02:07:53.681: INFO: Created: latency-svc-lvs7p
    Mar  2 02:07:53.694: INFO: Got endpoints: latency-svc-lvs7p [275.628244ms]
    Mar  2 02:07:53.704: INFO: Created: latency-svc-c6bnh
    Mar  2 02:07:53.720: INFO: Got endpoints: latency-svc-c6bnh [297.302488ms]
    Mar  2 02:07:53.725: INFO: Created: latency-svc-m575x
    Mar  2 02:07:53.740: INFO: Got endpoints: latency-svc-m575x [292.417226ms]
    Mar  2 02:07:53.744: INFO: Created: latency-svc-vtrkc
    Mar  2 02:07:53.759: INFO: Got endpoints: latency-svc-vtrkc [268.201544ms]
    Mar  2 02:07:53.771: INFO: Created: latency-svc-74z7l
    Mar  2 02:07:53.786: INFO: Got endpoints: latency-svc-74z7l [286.21296ms]
    Mar  2 02:07:53.794: INFO: Created: latency-svc-7pjnv
    Mar  2 02:07:53.811: INFO: Created: latency-svc-wzk2d
    Mar  2 02:07:53.816: INFO: Got endpoints: latency-svc-7pjnv [296.204844ms]
    Mar  2 02:07:53.836: INFO: Created: latency-svc-gf459
    Mar  2 02:07:53.842: INFO: Got endpoints: latency-svc-wzk2d [312.219033ms]
    Mar  2 02:07:53.846: INFO: Got endpoints: latency-svc-gf459 [285.011548ms]
    Mar  2 02:07:53.856: INFO: Created: latency-svc-mtxz8
    Mar  2 02:07:53.869: INFO: Created: latency-svc-vfd9m
    Mar  2 02:07:53.871: INFO: Got endpoints: latency-svc-mtxz8 [298.196648ms]
    Mar  2 02:07:53.888: INFO: Got endpoints: latency-svc-vfd9m [300.142586ms]
    Mar  2 02:07:53.888: INFO: Created: latency-svc-d74kw
    Mar  2 02:07:53.906: INFO: Got endpoints: latency-svc-d74kw [299.389025ms]
    Mar  2 02:07:53.907: INFO: Created: latency-svc-clp7f
    Mar  2 02:07:53.920: INFO: Created: latency-svc-6cz6k
    Mar  2 02:07:53.932: INFO: Got endpoints: latency-svc-clp7f [309.606707ms]
    Mar  2 02:07:53.938: INFO: Got endpoints: latency-svc-6cz6k [287.949294ms]
    Mar  2 02:07:53.946: INFO: Created: latency-svc-sv475
    Mar  2 02:07:53.959: INFO: Created: latency-svc-bfz6f
    Mar  2 02:07:53.963: INFO: Got endpoints: latency-svc-sv475 [289.586125ms]
    Mar  2 02:07:53.973: INFO: Created: latency-svc-74gk8
    Mar  2 02:07:53.976: INFO: Got endpoints: latency-svc-bfz6f [297.847558ms]
    Mar  2 02:07:53.989: INFO: Created: latency-svc-x8xcx
    Mar  2 02:07:53.999: INFO: Got endpoints: latency-svc-74gk8 [304.786935ms]
    Mar  2 02:07:54.003: INFO: Got endpoints: latency-svc-x8xcx [282.444716ms]
    Mar  2 02:07:54.003: INFO: Latencies: [54.98483ms 66.774183ms 82.992079ms 99.705672ms 118.952404ms 128.394998ms 129.213107ms 134.325975ms 136.309145ms 144.657826ms 163.990999ms 176.785404ms 178.10951ms 189.84487ms 196.936705ms 200.832706ms 211.132204ms 220.328474ms 232.087715ms 235.074399ms 235.328497ms 237.493933ms 243.538059ms 244.262482ms 244.828315ms 249.245139ms 249.44867ms 251.210622ms 251.23958ms 252.804234ms 257.465589ms 257.484161ms 257.895704ms 258.839212ms 259.6497ms 259.772017ms 260.185821ms 260.649955ms 260.79015ms 261.63393ms 261.899294ms 264.600784ms 265.347184ms 268.201544ms 268.360741ms 268.503657ms 268.907571ms 270.094823ms 270.264881ms 270.813819ms 271.157891ms 272.301143ms 273.483432ms 273.821893ms 275.066605ms 275.628244ms 275.968364ms 276.573217ms 276.975723ms 277.057968ms 278.060521ms 278.661017ms 278.761805ms 279.668481ms 279.867873ms 280.131363ms 280.428961ms 281.569689ms 281.631145ms 282.444716ms 282.537669ms 283.261259ms 283.984628ms 284.435854ms 284.603804ms 285.011548ms 285.059008ms 285.113746ms 285.217258ms 285.255763ms 285.367167ms 286.21296ms 287.455176ms 287.949294ms 288.120788ms 289.311414ms 289.422786ms 289.586125ms 290.405586ms 291.312431ms 291.917368ms 292.030324ms 292.166292ms 292.417226ms 294.004858ms 294.597841ms 294.634063ms 295.004119ms 295.118778ms 295.165291ms 295.388924ms 295.836428ms 296.04072ms 296.098299ms 296.14312ms 296.204844ms 296.279987ms 297.049814ms 297.302488ms 297.358567ms 297.847558ms 298.186939ms 298.196648ms 299.104509ms 299.239178ms 299.389025ms 300.05506ms 300.142586ms 300.274971ms 300.353758ms 300.753634ms 300.91068ms 300.945156ms 300.98852ms 302.070782ms 302.810177ms 302.8701ms 304.349919ms 304.786935ms 304.883837ms 305.677976ms 306.494057ms 306.720333ms 308.246402ms 308.399094ms 309.383361ms 309.606707ms 312.219033ms 312.859078ms 314.07165ms 314.167036ms 314.447571ms 314.693671ms 314.884136ms 315.874928ms 315.931431ms 315.953004ms 316.32033ms 316.37035ms 318.085364ms 318.472624ms 318.49672ms 319.078763ms 319.819311ms 319.957204ms 321.605398ms 321.930053ms 322.320831ms 323.916181ms 324.153659ms 324.357406ms 324.618241ms 324.650637ms 328.613641ms 329.003022ms 329.807525ms 330.385503ms 331.441376ms 331.989864ms 334.43465ms 335.879891ms 336.110065ms 337.967525ms 340.344034ms 344.325624ms 345.268137ms 346.280538ms 346.332142ms 348.146236ms 349.513557ms 350.056644ms 351.196957ms 361.127227ms 362.365457ms 365.543132ms 375.356766ms 375.477313ms 375.657057ms 378.54865ms 386.450153ms 388.080021ms 392.873579ms 439.125838ms 439.134496ms 444.319135ms 489.405217ms 507.229951ms 511.025441ms 538.784984ms 539.071266ms]
    Mar  2 02:07:54.003: INFO: 50 %ile: 295.388924ms
    Mar  2 02:07:54.003: INFO: 90 %ile: 350.056644ms
    Mar  2 02:07:54.003: INFO: 99 %ile: 538.784984ms
    Mar  2 02:07:54.003: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:187
    Mar  2 02:07:54.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svc-latency-5429" for this suite. 03/02/23 02:07:54.028
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:07:54.056
Mar  2 02:07:54.056: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename sched-pred 03/02/23 02:07:54.058
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:07:54.113
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:07:54.125
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Mar  2 02:07:54.163: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  2 02:07:54.204: INFO: Waiting for terminating namespaces to be deleted...
Mar  2 02:07:54.229: INFO: 
Logging pods the apiserver thinks is on node 10.132.92.143 before test
Mar  2 02:07:54.308: INFO: calico-node-99tft from calico-system started at 2023-03-01 22:49:50 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.308: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 02:07:54.308: INFO: calico-typha-b7f8b755-xlnll from calico-system started at 2023-03-01 22:49:57 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.308: INFO: 	Container calico-typha ready: true, restart count 0
Mar  2 02:07:54.308: INFO: ibm-cloud-provider-ip-169-56-11-242-558f6ff69-jtkgc from ibm-system started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.308: INFO: 	Container ibm-cloud-provider-ip-169-56-11-242 ready: true, restart count 0
Mar  2 02:07:54.308: INFO: ibm-keepalived-watcher-dp97c from kube-system started at 2023-03-01 22:48:16 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.309: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar  2 02:07:54.309: INFO: ibm-master-proxy-static-10.132.92.143 from kube-system started at 2023-03-01 22:48:15 +0000 UTC (2 container statuses recorded)
Mar  2 02:07:54.309: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar  2 02:07:54.309: INFO: 	Container pause ready: true, restart count 0
Mar  2 02:07:54.309: INFO: ibmcloud-block-storage-driver-zfc8r from kube-system started at 2023-03-01 22:48:23 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.309: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar  2 02:07:54.309: INFO: vpn-f6c799ddd-kvwzk from kube-system started at 2023-03-01 22:59:08 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.309: INFO: 	Container vpn ready: true, restart count 0
Mar  2 02:07:54.309: INFO: tuned-w95xr from openshift-cluster-node-tuning-operator started at 2023-03-01 22:55:42 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.309: INFO: 	Container tuned ready: true, restart count 0
Mar  2 02:07:54.309: INFO: console-6c8dcd4bdd-wsgwn from openshift-console started at 2023-03-01 22:59:22 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.309: INFO: 	Container console ready: true, restart count 0
Mar  2 02:07:54.309: INFO: dns-default-zr27n from openshift-dns started at 2023-03-01 22:55:41 +0000 UTC (2 container statuses recorded)
Mar  2 02:07:54.309: INFO: 	Container dns ready: true, restart count 0
Mar  2 02:07:54.310: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:07:54.310: INFO: node-resolver-7wklz from openshift-dns started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.310: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 02:07:54.310: INFO: node-ca-q4vcl from openshift-image-registry started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.310: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 02:07:54.310: INFO: registry-pvc-permissions-6wblc from openshift-image-registry started at 2023-03-01 22:58:39 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.310: INFO: 	Container pvc-permissions ready: false, restart count 0
Mar  2 02:07:54.310: INFO: ingress-canary-rwxvb from openshift-ingress-canary started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.310: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Mar  2 02:07:54.310: INFO: router-default-68bc8785b7-zkcbh from openshift-ingress started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.310: INFO: 	Container router ready: true, restart count 0
Mar  2 02:07:54.310: INFO: openshift-kube-proxy-zdts7 from openshift-kube-proxy started at 2023-03-01 22:49:00 +0000 UTC (2 container statuses recorded)
Mar  2 02:07:54.310: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  2 02:07:54.310: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:07:54.311: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-03-01 22:58:06 +0000 UTC (6 container statuses recorded)
Mar  2 02:07:54.311: INFO: 	Container alertmanager ready: true, restart count 1
Mar  2 02:07:54.311: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 02:07:54.311: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 02:07:54.311: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:07:54.311: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Mar  2 02:07:54.311: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 02:07:54.311: INFO: kube-state-metrics-554994774b-mttch from openshift-monitoring started at 2023-03-01 22:56:39 +0000 UTC (3 container statuses recorded)
Mar  2 02:07:54.311: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Mar  2 02:07:54.311: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Mar  2 02:07:54.311: INFO: 	Container kube-state-metrics ready: true, restart count 0
Mar  2 02:07:54.311: INFO: node-exporter-z52nh from openshift-monitoring started at 2023-03-01 22:56:40 +0000 UTC (2 container statuses recorded)
Mar  2 02:07:54.311: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:07:54.311: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 02:07:54.311: INFO: prometheus-adapter-95d69f68c-2rgvr from openshift-monitoring started at 2023-03-01 22:58:02 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.312: INFO: 	Container prometheus-adapter ready: true, restart count 0
Mar  2 02:07:54.312: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-03-01 22:58:12 +0000 UTC (6 container statuses recorded)
Mar  2 02:07:54.312: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 02:07:54.312: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:07:54.312: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Mar  2 02:07:54.312: INFO: 	Container prometheus ready: true, restart count 0
Mar  2 02:07:54.312: INFO: 	Container prometheus-proxy ready: true, restart count 0
Mar  2 02:07:54.312: INFO: 	Container thanos-sidecar ready: true, restart count 0
Mar  2 02:07:54.312: INFO: prometheus-operator-admission-webhook-6d5fbffb86-7xdt5 from openshift-monitoring started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.312: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Mar  2 02:07:54.312: INFO: telemeter-client-769c487d5b-fv8s4 from openshift-monitoring started at 2023-03-01 22:58:07 +0000 UTC (3 container statuses recorded)
Mar  2 02:07:54.312: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:07:54.312: INFO: 	Container reload ready: true, restart count 0
Mar  2 02:07:54.312: INFO: 	Container telemeter-client ready: true, restart count 0
Mar  2 02:07:54.312: INFO: thanos-querier-6cd8656bbb-6ffxj from openshift-monitoring started at 2023-03-01 22:56:48 +0000 UTC (6 container statuses recorded)
Mar  2 02:07:54.312: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:07:54.313: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Mar  2 02:07:54.313: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Mar  2 02:07:54.313: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 02:07:54.313: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 02:07:54.313: INFO: 	Container thanos-query ready: true, restart count 0
Mar  2 02:07:54.313: INFO: multus-48w86 from openshift-multus started at 2023-03-01 22:48:55 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.313: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 02:07:54.313: INFO: multus-additional-cni-plugins-d2kv5 from openshift-multus started at 2023-03-01 22:48:55 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.313: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Mar  2 02:07:54.313: INFO: multus-admission-controller-6f984f76c7-mtmdg from openshift-multus started at 2023-03-01 22:56:08 +0000 UTC (2 container statuses recorded)
Mar  2 02:07:54.313: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:07:54.313: INFO: 	Container multus-admission-controller ready: true, restart count 0
Mar  2 02:07:54.313: INFO: network-metrics-daemon-7jfj4 from openshift-multus started at 2023-03-01 22:48:56 +0000 UTC (2 container statuses recorded)
Mar  2 02:07:54.313: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:07:54.313: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Mar  2 02:07:54.313: INFO: network-check-target-w7x6h from openshift-network-diagnostics started at 2023-03-01 22:49:03 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.314: INFO: 	Container network-check-target-container ready: true, restart count 0
Mar  2 02:07:54.314: INFO: sonobuoy from sonobuoy started at 2023-03-02 01:07:05 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.314: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  2 02:07:54.314: INFO: sonobuoy-systemd-logs-daemon-set-7266eb5e626146f7-4c6tp from sonobuoy started at 2023-03-02 01:07:14 +0000 UTC (2 container statuses recorded)
Mar  2 02:07:54.314: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 02:07:54.314: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 02:07:54.314: INFO: svc-latency-rc-cmwxn from svc-latency-5429 started at 2023-03-02 02:07:47 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.314: INFO: 	Container svc-latency-rc ready: true, restart count 0
Mar  2 02:07:54.314: INFO: 
Logging pods the apiserver thinks is on node 10.132.92.186 before test
Mar  2 02:07:54.375: INFO: calico-kube-controllers-5b6b964466-vlvht from calico-system started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.375: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar  2 02:07:54.375: INFO: calico-node-5vvbw from calico-system started at 2023-03-01 22:49:50 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.375: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 02:07:54.375: INFO: calico-typha-b7f8b755-bdnm9 from calico-system started at 2023-03-01 22:49:50 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.375: INFO: 	Container calico-typha ready: true, restart count 0
Mar  2 02:07:54.375: INFO: managed-storage-validation-webhooks-757b765d54-f72mn from ibm-odf-validation-webhook started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.375: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Mar  2 02:07:54.376: INFO: managed-storage-validation-webhooks-757b765d54-gshct from ibm-odf-validation-webhook started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.376: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Mar  2 02:07:54.376: INFO: managed-storage-validation-webhooks-757b765d54-h4j5p from ibm-odf-validation-webhook started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.376: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Mar  2 02:07:54.376: INFO: ibm-file-plugin-68bfbbdc7-77m77 from kube-system started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.376: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Mar  2 02:07:54.376: INFO: ibm-keepalived-watcher-knrjl from kube-system started at 2023-03-01 22:48:27 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.376: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar  2 02:07:54.376: INFO: ibm-master-proxy-static-10.132.92.186 from kube-system started at 2023-03-01 22:48:22 +0000 UTC (2 container statuses recorded)
Mar  2 02:07:54.376: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar  2 02:07:54.376: INFO: 	Container pause ready: true, restart count 0
Mar  2 02:07:54.376: INFO: ibm-storage-metrics-agent-6c459b7855-cv8d6 from kube-system started at 2023-03-01 22:51:20 +0000 UTC (2 container statuses recorded)
Mar  2 02:07:54.376: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
Mar  2 02:07:54.376: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Mar  2 02:07:54.376: INFO: ibm-storage-watcher-67589fd788-z5w99 from kube-system started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.376: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Mar  2 02:07:54.376: INFO: ibmcloud-block-storage-driver-l498z from kube-system started at 2023-03-01 22:48:36 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.376: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar  2 02:07:54.376: INFO: ibmcloud-block-storage-plugin-86f4cbdf95-klfqp from kube-system started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.377: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Mar  2 02:07:54.377: INFO: cluster-node-tuning-operator-5df67c68d9-gd552 from openshift-cluster-node-tuning-operator started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.377: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Mar  2 02:07:54.377: INFO: tuned-h4jnq from openshift-cluster-node-tuning-operator started at 2023-03-01 22:55:42 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.377: INFO: 	Container tuned ready: true, restart count 0
Mar  2 02:07:54.377: INFO: cluster-samples-operator-5948f44764-88mzn from openshift-cluster-samples-operator started at 2023-03-01 22:51:20 +0000 UTC (2 container statuses recorded)
Mar  2 02:07:54.377: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Mar  2 02:07:54.377: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Mar  2 02:07:54.377: INFO: cluster-storage-operator-54cf568cb7-smstf from openshift-cluster-storage-operator started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.377: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Mar  2 02:07:54.377: INFO: csi-snapshot-controller-85d666c956-5hhrv from openshift-cluster-storage-operator started at 2023-03-01 22:51:57 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.377: INFO: 	Container snapshot-controller ready: true, restart count 0
Mar  2 02:07:54.377: INFO: csi-snapshot-controller-operator-6b76cb8c94-9bvdj from openshift-cluster-storage-operator started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.377: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Mar  2 02:07:54.377: INFO: csi-snapshot-webhook-6cbf55b956-z69cb from openshift-cluster-storage-operator started at 2023-03-01 22:51:57 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.377: INFO: 	Container webhook ready: true, restart count 0
Mar  2 02:07:54.377: INFO: console-operator-6975f688f8-xwx92 from openshift-console-operator started at 2023-03-01 22:51:20 +0000 UTC (2 container statuses recorded)
Mar  2 02:07:54.377: INFO: 	Container console-operator ready: true, restart count 1
Mar  2 02:07:54.377: INFO: 	Container conversion-webhook-server ready: true, restart count 3
Mar  2 02:07:54.377: INFO: console-6c8dcd4bdd-c2dbn from openshift-console started at 2023-03-01 22:59:49 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.377: INFO: 	Container console ready: true, restart count 0
Mar  2 02:07:54.377: INFO: downloads-8b4bf4d5-pjlj4 from openshift-console started at 2023-03-01 22:51:55 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.377: INFO: 	Container download-server ready: true, restart count 0
Mar  2 02:07:54.377: INFO: dns-operator-b8598b55c-87ghp from openshift-dns-operator started at 2023-03-01 22:51:20 +0000 UTC (2 container statuses recorded)
Mar  2 02:07:54.377: INFO: 	Container dns-operator ready: true, restart count 0
Mar  2 02:07:54.377: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:07:54.377: INFO: dns-default-rdd5h from openshift-dns started at 2023-03-01 22:55:41 +0000 UTC (2 container statuses recorded)
Mar  2 02:07:54.377: INFO: 	Container dns ready: true, restart count 0
Mar  2 02:07:54.377: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:07:54.377: INFO: node-resolver-9wrks from openshift-dns started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.377: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 02:07:54.377: INFO: cluster-image-registry-operator-797c5fb5f8-tvjwj from openshift-image-registry started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.377: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Mar  2 02:07:54.377: INFO: node-ca-6ffsq from openshift-image-registry started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.377: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 02:07:54.377: INFO: ingress-canary-zgjd4 from openshift-ingress-canary started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.378: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Mar  2 02:07:54.378: INFO: ingress-operator-7b8c5ddb4b-wr6sb from openshift-ingress-operator started at 2023-03-01 22:51:20 +0000 UTC (2 container statuses recorded)
Mar  2 02:07:54.378: INFO: 	Container ingress-operator ready: true, restart count 0
Mar  2 02:07:54.378: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:07:54.378: INFO: insights-operator-7b97bdcbd8-5zm5p from openshift-insights started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.378: INFO: 	Container insights-operator ready: true, restart count 1
Mar  2 02:07:54.378: INFO: openshift-kube-proxy-xgktp from openshift-kube-proxy started at 2023-03-01 22:49:00 +0000 UTC (2 container statuses recorded)
Mar  2 02:07:54.378: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  2 02:07:54.378: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:07:54.378: INFO: kube-storage-version-migrator-operator-78bc45549c-qmzfr from openshift-kube-storage-version-migrator-operator started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.378: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Mar  2 02:07:54.378: INFO: marketplace-operator-66f5c8895c-l9r8f from openshift-marketplace started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.378: INFO: 	Container marketplace-operator ready: true, restart count 0
Mar  2 02:07:54.378: INFO: cluster-monitoring-operator-7ffc6575dd-kkb8m from openshift-monitoring started at 2023-03-01 22:51:20 +0000 UTC (2 container statuses recorded)
Mar  2 02:07:54.378: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Mar  2 02:07:54.378: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:07:54.378: INFO: node-exporter-95fzh from openshift-monitoring started at 2023-03-01 22:56:40 +0000 UTC (2 container statuses recorded)
Mar  2 02:07:54.378: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:07:54.378: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 02:07:54.378: INFO: multus-89qpj from openshift-multus started at 2023-03-01 22:48:55 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.378: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 02:07:54.378: INFO: multus-additional-cni-plugins-ff7kf from openshift-multus started at 2023-03-01 22:48:55 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.378: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Mar  2 02:07:54.378: INFO: network-metrics-daemon-7m69v from openshift-multus started at 2023-03-01 22:48:56 +0000 UTC (2 container statuses recorded)
Mar  2 02:07:54.378: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:07:54.378: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Mar  2 02:07:54.378: INFO: network-check-source-848f87f9d4-bxlxt from openshift-network-diagnostics started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.378: INFO: 	Container check-endpoints ready: true, restart count 0
Mar  2 02:07:54.378: INFO: network-check-target-jqnqt from openshift-network-diagnostics started at 2023-03-01 22:49:03 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.378: INFO: 	Container network-check-target-container ready: true, restart count 0
Mar  2 02:07:54.378: INFO: catalog-operator-657ff7bb9f-c7tqq from openshift-operator-lifecycle-manager started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.378: INFO: 	Container catalog-operator ready: true, restart count 0
Mar  2 02:07:54.378: INFO: collect-profiles-27962010-nhpf6 from openshift-operator-lifecycle-manager started at 2023-03-02 01:30:00 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.378: INFO: 	Container collect-profiles ready: false, restart count 0
Mar  2 02:07:54.378: INFO: olm-operator-56ccd7794f-4ffrj from openshift-operator-lifecycle-manager started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.378: INFO: 	Container olm-operator ready: true, restart count 0
Mar  2 02:07:54.378: INFO: package-server-manager-599745f678-rmjg4 from openshift-operator-lifecycle-manager started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.378: INFO: 	Container package-server-manager ready: true, restart count 0
Mar  2 02:07:54.378: INFO: packageserver-6c7c56df95-jq89f from openshift-operator-lifecycle-manager started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.378: INFO: 	Container packageserver ready: true, restart count 0
Mar  2 02:07:54.378: INFO: metrics-784dfd67b7-rjkpg from openshift-roks-metrics started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.378: INFO: 	Container metrics ready: true, restart count 3
Mar  2 02:07:54.378: INFO: push-gateway-646cc86d79-cgtct from openshift-roks-metrics started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.378: INFO: 	Container push-gateway ready: true, restart count 0
Mar  2 02:07:54.378: INFO: service-ca-operator-6f8744cbc5-w5srs from openshift-service-ca-operator started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.378: INFO: 	Container service-ca-operator ready: true, restart count 1
Mar  2 02:07:54.378: INFO: sonobuoy-systemd-logs-daemon-set-7266eb5e626146f7-n2dsq from sonobuoy started at 2023-03-02 01:07:14 +0000 UTC (2 container statuses recorded)
Mar  2 02:07:54.378: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 02:07:54.378: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 02:07:54.378: INFO: 
Logging pods the apiserver thinks is on node 10.132.92.188 before test
Mar  2 02:07:54.454: INFO: calico-node-mkvsm from calico-system started at 2023-03-01 22:49:50 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.454: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 02:07:54.454: INFO: ibm-cloud-provider-ip-169-56-11-242-558f6ff69-xvh2v from ibm-system started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.454: INFO: 	Container ibm-cloud-provider-ip-169-56-11-242 ready: true, restart count 0
Mar  2 02:07:54.454: INFO: ibm-keepalived-watcher-zwfb8 from kube-system started at 2023-03-01 22:48:10 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.454: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar  2 02:07:54.454: INFO: ibm-master-proxy-static-10.132.92.188 from kube-system started at 2023-03-01 22:48:07 +0000 UTC (2 container statuses recorded)
Mar  2 02:07:54.454: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar  2 02:07:54.454: INFO: 	Container pause ready: true, restart count 0
Mar  2 02:07:54.454: INFO: ibmcloud-block-storage-driver-qbg9f from kube-system started at 2023-03-01 22:48:16 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.454: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar  2 02:07:54.454: INFO: tuned-5z98r from openshift-cluster-node-tuning-operator started at 2023-03-01 22:55:42 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.454: INFO: 	Container tuned ready: true, restart count 0
Mar  2 02:07:54.454: INFO: csi-snapshot-controller-85d666c956-xsh6c from openshift-cluster-storage-operator started at 2023-03-01 22:51:57 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.454: INFO: 	Container snapshot-controller ready: true, restart count 0
Mar  2 02:07:54.454: INFO: csi-snapshot-webhook-6cbf55b956-wq8bh from openshift-cluster-storage-operator started at 2023-03-01 22:51:57 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.454: INFO: 	Container webhook ready: true, restart count 0
Mar  2 02:07:54.454: INFO: downloads-8b4bf4d5-b9kfz from openshift-console started at 2023-03-01 22:51:55 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.454: INFO: 	Container download-server ready: true, restart count 0
Mar  2 02:07:54.454: INFO: dns-default-nt82k from openshift-dns started at 2023-03-01 22:55:41 +0000 UTC (2 container statuses recorded)
Mar  2 02:07:54.454: INFO: 	Container dns ready: true, restart count 0
Mar  2 02:07:54.454: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:07:54.454: INFO: node-resolver-wcng9 from openshift-dns started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.454: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 02:07:54.454: INFO: image-pruner-27961920-wd4pv from openshift-image-registry started at 2023-03-02 00:00:00 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.454: INFO: 	Container image-pruner ready: false, restart count 0
Mar  2 02:07:54.454: INFO: image-registry-7fb7bd8df6-lb2bt from openshift-image-registry started at 2023-03-01 22:58:31 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.454: INFO: 	Container registry ready: true, restart count 0
Mar  2 02:07:54.454: INFO: node-ca-qlt4w from openshift-image-registry started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.454: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 02:07:54.454: INFO: ingress-canary-qvcz9 from openshift-ingress-canary started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.454: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Mar  2 02:07:54.454: INFO: router-default-68bc8785b7-wz8ct from openshift-ingress started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.454: INFO: 	Container router ready: true, restart count 0
Mar  2 02:07:54.454: INFO: openshift-kube-proxy-mth7h from openshift-kube-proxy started at 2023-03-01 22:49:00 +0000 UTC (2 container statuses recorded)
Mar  2 02:07:54.454: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  2 02:07:54.454: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:07:54.454: INFO: migrator-5687b466dd-glln7 from openshift-kube-storage-version-migrator started at 2023-03-01 22:51:57 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.454: INFO: 	Container migrator ready: true, restart count 0
Mar  2 02:07:54.454: INFO: certified-operators-8n5vw from openshift-marketplace started at 2023-03-02 00:13:35 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.454: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 02:07:54.454: INFO: community-operators-lngtl from openshift-marketplace started at 2023-03-01 22:54:15 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.454: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 02:07:54.454: INFO: redhat-marketplace-ld8wh from openshift-marketplace started at 2023-03-02 00:27:02 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.454: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 02:07:54.454: INFO: redhat-operators-tk548 from openshift-marketplace started at 2023-03-01 22:54:14 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.454: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 02:07:54.454: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-03-01 22:58:39 +0000 UTC (6 container statuses recorded)
Mar  2 02:07:54.454: INFO: 	Container alertmanager ready: true, restart count 1
Mar  2 02:07:54.454: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 02:07:54.454: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 02:07:54.454: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:07:54.454: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Mar  2 02:07:54.454: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 02:07:54.454: INFO: node-exporter-gj7lr from openshift-monitoring started at 2023-03-01 22:56:39 +0000 UTC (2 container statuses recorded)
Mar  2 02:07:54.454: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:07:54.454: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 02:07:54.454: INFO: openshift-state-metrics-56599d9b6d-6sw48 from openshift-monitoring started at 2023-03-01 22:56:39 +0000 UTC (3 container statuses recorded)
Mar  2 02:07:54.454: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Mar  2 02:07:54.454: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Mar  2 02:07:54.454: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Mar  2 02:07:54.454: INFO: prometheus-adapter-95d69f68c-gtx2d from openshift-monitoring started at 2023-03-01 22:58:02 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.454: INFO: 	Container prometheus-adapter ready: true, restart count 0
Mar  2 02:07:54.454: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-03-01 22:58:30 +0000 UTC (6 container statuses recorded)
Mar  2 02:07:54.454: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 02:07:54.454: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:07:54.454: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Mar  2 02:07:54.454: INFO: 	Container prometheus ready: true, restart count 0
Mar  2 02:07:54.454: INFO: 	Container prometheus-proxy ready: true, restart count 0
Mar  2 02:07:54.454: INFO: 	Container thanos-sidecar ready: true, restart count 0
Mar  2 02:07:54.454: INFO: prometheus-operator-9bf584898-89sgf from openshift-monitoring started at 2023-03-01 22:56:19 +0000 UTC (2 container statuses recorded)
Mar  2 02:07:54.454: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:07:54.454: INFO: 	Container prometheus-operator ready: true, restart count 0
Mar  2 02:07:54.454: INFO: prometheus-operator-admission-webhook-6d5fbffb86-hhfbw from openshift-monitoring started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.454: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Mar  2 02:07:54.454: INFO: thanos-querier-6cd8656bbb-8hv4v from openshift-monitoring started at 2023-03-01 22:56:48 +0000 UTC (6 container statuses recorded)
Mar  2 02:07:54.454: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:07:54.454: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Mar  2 02:07:54.454: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Mar  2 02:07:54.454: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 02:07:54.454: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 02:07:54.454: INFO: 	Container thanos-query ready: true, restart count 0
Mar  2 02:07:54.454: INFO: multus-additional-cni-plugins-5pjnd from openshift-multus started at 2023-03-01 22:48:55 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.454: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Mar  2 02:07:54.454: INFO: multus-admission-controller-6f984f76c7-hclf4 from openshift-multus started at 2023-03-01 22:55:41 +0000 UTC (2 container statuses recorded)
Mar  2 02:07:54.454: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:07:54.454: INFO: 	Container multus-admission-controller ready: true, restart count 0
Mar  2 02:07:54.454: INFO: multus-g4k8f from openshift-multus started at 2023-03-01 22:48:55 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.454: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 02:07:54.454: INFO: network-metrics-daemon-95rvd from openshift-multus started at 2023-03-01 22:48:56 +0000 UTC (2 container statuses recorded)
Mar  2 02:07:54.454: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:07:54.454: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Mar  2 02:07:54.454: INFO: network-check-target-4nkgt from openshift-network-diagnostics started at 2023-03-01 22:49:03 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.454: INFO: 	Container network-check-target-container ready: true, restart count 0
Mar  2 02:07:54.454: INFO: network-operator-68bbf8cc76-mg97z from openshift-network-operator started at 2023-03-01 22:48:19 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.454: INFO: 	Container network-operator ready: true, restart count 1
Mar  2 02:07:54.454: INFO: collect-profiles-27962025-rd7rx from openshift-operator-lifecycle-manager started at 2023-03-02 01:45:00 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.454: INFO: 	Container collect-profiles ready: false, restart count 0
Mar  2 02:07:54.454: INFO: collect-profiles-27962040-2rff7 from openshift-operator-lifecycle-manager started at 2023-03-02 02:00:00 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.454: INFO: 	Container collect-profiles ready: false, restart count 0
Mar  2 02:07:54.454: INFO: packageserver-6c7c56df95-7wr7f from openshift-operator-lifecycle-manager started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.454: INFO: 	Container packageserver ready: true, restart count 0
Mar  2 02:07:54.454: INFO: service-ca-578d67f6f8-89gtp from openshift-service-ca started at 2023-03-01 22:52:15 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.454: INFO: 	Container service-ca-controller ready: true, restart count 0
Mar  2 02:07:54.454: INFO: sonobuoy-e2e-job-c6c00a6e72aa4160 from sonobuoy started at 2023-03-02 01:07:14 +0000 UTC (2 container statuses recorded)
Mar  2 02:07:54.454: INFO: 	Container e2e ready: true, restart count 0
Mar  2 02:07:54.454: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 02:07:54.454: INFO: sonobuoy-systemd-logs-daemon-set-7266eb5e626146f7-fgqrz from sonobuoy started at 2023-03-02 01:07:14 +0000 UTC (2 container statuses recorded)
Mar  2 02:07:54.454: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 02:07:54.454: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 02:07:54.454: INFO: tigera-operator-84f4f4565b-5w7s7 from tigera-operator started at 2023-03-01 22:48:19 +0000 UTC (1 container statuses recorded)
Mar  2 02:07:54.454: INFO: 	Container tigera-operator ready: true, restart count 2
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
STEP: Trying to launch a pod without a label to get a node which can launch it. 03/02/23 02:07:54.455
Mar  2 02:07:54.515: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-7816" to be "running"
Mar  2 02:07:54.536: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 21.082823ms
Mar  2 02:07:56.549: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034044232s
Mar  2 02:07:58.553: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.037739715s
Mar  2 02:07:58.553: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 03/02/23 02:07:58.566
STEP: Trying to apply a random label on the found node. 03/02/23 02:07:58.605
STEP: verifying the node has the label kubernetes.io/e2e-ca45fe8c-a939-433e-95a9-93e963e3f576 42 03/02/23 02:07:58.642
STEP: Trying to relaunch the pod, now with labels. 03/02/23 02:07:58.663
Mar  2 02:07:58.719: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-7816" to be "not pending"
Mar  2 02:07:58.733: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 14.159064ms
Mar  2 02:08:00.751: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031831339s
Mar  2 02:08:02.755: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 4.035522328s
Mar  2 02:08:02.755: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-ca45fe8c-a939-433e-95a9-93e963e3f576 off the node 10.132.92.143 03/02/23 02:08:02.792
STEP: verifying the node doesn't have the label kubernetes.io/e2e-ca45fe8c-a939-433e-95a9-93e963e3f576 03/02/23 02:08:02.877
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Mar  2 02:08:02.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7816" for this suite. 03/02/23 02:08:02.953
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","completed":195,"skipped":3636,"failed":0}
------------------------------
• [SLOW TEST] [8.922 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:07:54.056
    Mar  2 02:07:54.056: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename sched-pred 03/02/23 02:07:54.058
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:07:54.113
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:07:54.125
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Mar  2 02:07:54.163: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Mar  2 02:07:54.204: INFO: Waiting for terminating namespaces to be deleted...
    Mar  2 02:07:54.229: INFO: 
    Logging pods the apiserver thinks is on node 10.132.92.143 before test
    Mar  2 02:07:54.308: INFO: calico-node-99tft from calico-system started at 2023-03-01 22:49:50 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.308: INFO: 	Container calico-node ready: true, restart count 0
    Mar  2 02:07:54.308: INFO: calico-typha-b7f8b755-xlnll from calico-system started at 2023-03-01 22:49:57 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.308: INFO: 	Container calico-typha ready: true, restart count 0
    Mar  2 02:07:54.308: INFO: ibm-cloud-provider-ip-169-56-11-242-558f6ff69-jtkgc from ibm-system started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.308: INFO: 	Container ibm-cloud-provider-ip-169-56-11-242 ready: true, restart count 0
    Mar  2 02:07:54.308: INFO: ibm-keepalived-watcher-dp97c from kube-system started at 2023-03-01 22:48:16 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.309: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Mar  2 02:07:54.309: INFO: ibm-master-proxy-static-10.132.92.143 from kube-system started at 2023-03-01 22:48:15 +0000 UTC (2 container statuses recorded)
    Mar  2 02:07:54.309: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Mar  2 02:07:54.309: INFO: 	Container pause ready: true, restart count 0
    Mar  2 02:07:54.309: INFO: ibmcloud-block-storage-driver-zfc8r from kube-system started at 2023-03-01 22:48:23 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.309: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Mar  2 02:07:54.309: INFO: vpn-f6c799ddd-kvwzk from kube-system started at 2023-03-01 22:59:08 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.309: INFO: 	Container vpn ready: true, restart count 0
    Mar  2 02:07:54.309: INFO: tuned-w95xr from openshift-cluster-node-tuning-operator started at 2023-03-01 22:55:42 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.309: INFO: 	Container tuned ready: true, restart count 0
    Mar  2 02:07:54.309: INFO: console-6c8dcd4bdd-wsgwn from openshift-console started at 2023-03-01 22:59:22 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.309: INFO: 	Container console ready: true, restart count 0
    Mar  2 02:07:54.309: INFO: dns-default-zr27n from openshift-dns started at 2023-03-01 22:55:41 +0000 UTC (2 container statuses recorded)
    Mar  2 02:07:54.309: INFO: 	Container dns ready: true, restart count 0
    Mar  2 02:07:54.310: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:07:54.310: INFO: node-resolver-7wklz from openshift-dns started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.310: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Mar  2 02:07:54.310: INFO: node-ca-q4vcl from openshift-image-registry started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.310: INFO: 	Container node-ca ready: true, restart count 0
    Mar  2 02:07:54.310: INFO: registry-pvc-permissions-6wblc from openshift-image-registry started at 2023-03-01 22:58:39 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.310: INFO: 	Container pvc-permissions ready: false, restart count 0
    Mar  2 02:07:54.310: INFO: ingress-canary-rwxvb from openshift-ingress-canary started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.310: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Mar  2 02:07:54.310: INFO: router-default-68bc8785b7-zkcbh from openshift-ingress started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.310: INFO: 	Container router ready: true, restart count 0
    Mar  2 02:07:54.310: INFO: openshift-kube-proxy-zdts7 from openshift-kube-proxy started at 2023-03-01 22:49:00 +0000 UTC (2 container statuses recorded)
    Mar  2 02:07:54.310: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar  2 02:07:54.310: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:07:54.311: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-03-01 22:58:06 +0000 UTC (6 container statuses recorded)
    Mar  2 02:07:54.311: INFO: 	Container alertmanager ready: true, restart count 1
    Mar  2 02:07:54.311: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Mar  2 02:07:54.311: INFO: 	Container config-reloader ready: true, restart count 0
    Mar  2 02:07:54.311: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:07:54.311: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Mar  2 02:07:54.311: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Mar  2 02:07:54.311: INFO: kube-state-metrics-554994774b-mttch from openshift-monitoring started at 2023-03-01 22:56:39 +0000 UTC (3 container statuses recorded)
    Mar  2 02:07:54.311: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Mar  2 02:07:54.311: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Mar  2 02:07:54.311: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Mar  2 02:07:54.311: INFO: node-exporter-z52nh from openshift-monitoring started at 2023-03-01 22:56:40 +0000 UTC (2 container statuses recorded)
    Mar  2 02:07:54.311: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:07:54.311: INFO: 	Container node-exporter ready: true, restart count 0
    Mar  2 02:07:54.311: INFO: prometheus-adapter-95d69f68c-2rgvr from openshift-monitoring started at 2023-03-01 22:58:02 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.312: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Mar  2 02:07:54.312: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-03-01 22:58:12 +0000 UTC (6 container statuses recorded)
    Mar  2 02:07:54.312: INFO: 	Container config-reloader ready: true, restart count 0
    Mar  2 02:07:54.312: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:07:54.312: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Mar  2 02:07:54.312: INFO: 	Container prometheus ready: true, restart count 0
    Mar  2 02:07:54.312: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Mar  2 02:07:54.312: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Mar  2 02:07:54.312: INFO: prometheus-operator-admission-webhook-6d5fbffb86-7xdt5 from openshift-monitoring started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.312: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Mar  2 02:07:54.312: INFO: telemeter-client-769c487d5b-fv8s4 from openshift-monitoring started at 2023-03-01 22:58:07 +0000 UTC (3 container statuses recorded)
    Mar  2 02:07:54.312: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:07:54.312: INFO: 	Container reload ready: true, restart count 0
    Mar  2 02:07:54.312: INFO: 	Container telemeter-client ready: true, restart count 0
    Mar  2 02:07:54.312: INFO: thanos-querier-6cd8656bbb-6ffxj from openshift-monitoring started at 2023-03-01 22:56:48 +0000 UTC (6 container statuses recorded)
    Mar  2 02:07:54.312: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:07:54.313: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Mar  2 02:07:54.313: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Mar  2 02:07:54.313: INFO: 	Container oauth-proxy ready: true, restart count 0
    Mar  2 02:07:54.313: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Mar  2 02:07:54.313: INFO: 	Container thanos-query ready: true, restart count 0
    Mar  2 02:07:54.313: INFO: multus-48w86 from openshift-multus started at 2023-03-01 22:48:55 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.313: INFO: 	Container kube-multus ready: true, restart count 0
    Mar  2 02:07:54.313: INFO: multus-additional-cni-plugins-d2kv5 from openshift-multus started at 2023-03-01 22:48:55 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.313: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Mar  2 02:07:54.313: INFO: multus-admission-controller-6f984f76c7-mtmdg from openshift-multus started at 2023-03-01 22:56:08 +0000 UTC (2 container statuses recorded)
    Mar  2 02:07:54.313: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:07:54.313: INFO: 	Container multus-admission-controller ready: true, restart count 0
    Mar  2 02:07:54.313: INFO: network-metrics-daemon-7jfj4 from openshift-multus started at 2023-03-01 22:48:56 +0000 UTC (2 container statuses recorded)
    Mar  2 02:07:54.313: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:07:54.313: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Mar  2 02:07:54.313: INFO: network-check-target-w7x6h from openshift-network-diagnostics started at 2023-03-01 22:49:03 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.314: INFO: 	Container network-check-target-container ready: true, restart count 0
    Mar  2 02:07:54.314: INFO: sonobuoy from sonobuoy started at 2023-03-02 01:07:05 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.314: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Mar  2 02:07:54.314: INFO: sonobuoy-systemd-logs-daemon-set-7266eb5e626146f7-4c6tp from sonobuoy started at 2023-03-02 01:07:14 +0000 UTC (2 container statuses recorded)
    Mar  2 02:07:54.314: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  2 02:07:54.314: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar  2 02:07:54.314: INFO: svc-latency-rc-cmwxn from svc-latency-5429 started at 2023-03-02 02:07:47 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.314: INFO: 	Container svc-latency-rc ready: true, restart count 0
    Mar  2 02:07:54.314: INFO: 
    Logging pods the apiserver thinks is on node 10.132.92.186 before test
    Mar  2 02:07:54.375: INFO: calico-kube-controllers-5b6b964466-vlvht from calico-system started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.375: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Mar  2 02:07:54.375: INFO: calico-node-5vvbw from calico-system started at 2023-03-01 22:49:50 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.375: INFO: 	Container calico-node ready: true, restart count 0
    Mar  2 02:07:54.375: INFO: calico-typha-b7f8b755-bdnm9 from calico-system started at 2023-03-01 22:49:50 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.375: INFO: 	Container calico-typha ready: true, restart count 0
    Mar  2 02:07:54.375: INFO: managed-storage-validation-webhooks-757b765d54-f72mn from ibm-odf-validation-webhook started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.375: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
    Mar  2 02:07:54.376: INFO: managed-storage-validation-webhooks-757b765d54-gshct from ibm-odf-validation-webhook started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.376: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
    Mar  2 02:07:54.376: INFO: managed-storage-validation-webhooks-757b765d54-h4j5p from ibm-odf-validation-webhook started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.376: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
    Mar  2 02:07:54.376: INFO: ibm-file-plugin-68bfbbdc7-77m77 from kube-system started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.376: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
    Mar  2 02:07:54.376: INFO: ibm-keepalived-watcher-knrjl from kube-system started at 2023-03-01 22:48:27 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.376: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Mar  2 02:07:54.376: INFO: ibm-master-proxy-static-10.132.92.186 from kube-system started at 2023-03-01 22:48:22 +0000 UTC (2 container statuses recorded)
    Mar  2 02:07:54.376: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Mar  2 02:07:54.376: INFO: 	Container pause ready: true, restart count 0
    Mar  2 02:07:54.376: INFO: ibm-storage-metrics-agent-6c459b7855-cv8d6 from kube-system started at 2023-03-01 22:51:20 +0000 UTC (2 container statuses recorded)
    Mar  2 02:07:54.376: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
    Mar  2 02:07:54.376: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
    Mar  2 02:07:54.376: INFO: ibm-storage-watcher-67589fd788-z5w99 from kube-system started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.376: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
    Mar  2 02:07:54.376: INFO: ibmcloud-block-storage-driver-l498z from kube-system started at 2023-03-01 22:48:36 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.376: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Mar  2 02:07:54.376: INFO: ibmcloud-block-storage-plugin-86f4cbdf95-klfqp from kube-system started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.377: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
    Mar  2 02:07:54.377: INFO: cluster-node-tuning-operator-5df67c68d9-gd552 from openshift-cluster-node-tuning-operator started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.377: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
    Mar  2 02:07:54.377: INFO: tuned-h4jnq from openshift-cluster-node-tuning-operator started at 2023-03-01 22:55:42 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.377: INFO: 	Container tuned ready: true, restart count 0
    Mar  2 02:07:54.377: INFO: cluster-samples-operator-5948f44764-88mzn from openshift-cluster-samples-operator started at 2023-03-01 22:51:20 +0000 UTC (2 container statuses recorded)
    Mar  2 02:07:54.377: INFO: 	Container cluster-samples-operator ready: true, restart count 0
    Mar  2 02:07:54.377: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
    Mar  2 02:07:54.377: INFO: cluster-storage-operator-54cf568cb7-smstf from openshift-cluster-storage-operator started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.377: INFO: 	Container cluster-storage-operator ready: true, restart count 1
    Mar  2 02:07:54.377: INFO: csi-snapshot-controller-85d666c956-5hhrv from openshift-cluster-storage-operator started at 2023-03-01 22:51:57 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.377: INFO: 	Container snapshot-controller ready: true, restart count 0
    Mar  2 02:07:54.377: INFO: csi-snapshot-controller-operator-6b76cb8c94-9bvdj from openshift-cluster-storage-operator started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.377: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
    Mar  2 02:07:54.377: INFO: csi-snapshot-webhook-6cbf55b956-z69cb from openshift-cluster-storage-operator started at 2023-03-01 22:51:57 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.377: INFO: 	Container webhook ready: true, restart count 0
    Mar  2 02:07:54.377: INFO: console-operator-6975f688f8-xwx92 from openshift-console-operator started at 2023-03-01 22:51:20 +0000 UTC (2 container statuses recorded)
    Mar  2 02:07:54.377: INFO: 	Container console-operator ready: true, restart count 1
    Mar  2 02:07:54.377: INFO: 	Container conversion-webhook-server ready: true, restart count 3
    Mar  2 02:07:54.377: INFO: console-6c8dcd4bdd-c2dbn from openshift-console started at 2023-03-01 22:59:49 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.377: INFO: 	Container console ready: true, restart count 0
    Mar  2 02:07:54.377: INFO: downloads-8b4bf4d5-pjlj4 from openshift-console started at 2023-03-01 22:51:55 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.377: INFO: 	Container download-server ready: true, restart count 0
    Mar  2 02:07:54.377: INFO: dns-operator-b8598b55c-87ghp from openshift-dns-operator started at 2023-03-01 22:51:20 +0000 UTC (2 container statuses recorded)
    Mar  2 02:07:54.377: INFO: 	Container dns-operator ready: true, restart count 0
    Mar  2 02:07:54.377: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:07:54.377: INFO: dns-default-rdd5h from openshift-dns started at 2023-03-01 22:55:41 +0000 UTC (2 container statuses recorded)
    Mar  2 02:07:54.377: INFO: 	Container dns ready: true, restart count 0
    Mar  2 02:07:54.377: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:07:54.377: INFO: node-resolver-9wrks from openshift-dns started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.377: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Mar  2 02:07:54.377: INFO: cluster-image-registry-operator-797c5fb5f8-tvjwj from openshift-image-registry started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.377: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
    Mar  2 02:07:54.377: INFO: node-ca-6ffsq from openshift-image-registry started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.377: INFO: 	Container node-ca ready: true, restart count 0
    Mar  2 02:07:54.377: INFO: ingress-canary-zgjd4 from openshift-ingress-canary started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.378: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Mar  2 02:07:54.378: INFO: ingress-operator-7b8c5ddb4b-wr6sb from openshift-ingress-operator started at 2023-03-01 22:51:20 +0000 UTC (2 container statuses recorded)
    Mar  2 02:07:54.378: INFO: 	Container ingress-operator ready: true, restart count 0
    Mar  2 02:07:54.378: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:07:54.378: INFO: insights-operator-7b97bdcbd8-5zm5p from openshift-insights started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.378: INFO: 	Container insights-operator ready: true, restart count 1
    Mar  2 02:07:54.378: INFO: openshift-kube-proxy-xgktp from openshift-kube-proxy started at 2023-03-01 22:49:00 +0000 UTC (2 container statuses recorded)
    Mar  2 02:07:54.378: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar  2 02:07:54.378: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:07:54.378: INFO: kube-storage-version-migrator-operator-78bc45549c-qmzfr from openshift-kube-storage-version-migrator-operator started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.378: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
    Mar  2 02:07:54.378: INFO: marketplace-operator-66f5c8895c-l9r8f from openshift-marketplace started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.378: INFO: 	Container marketplace-operator ready: true, restart count 0
    Mar  2 02:07:54.378: INFO: cluster-monitoring-operator-7ffc6575dd-kkb8m from openshift-monitoring started at 2023-03-01 22:51:20 +0000 UTC (2 container statuses recorded)
    Mar  2 02:07:54.378: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
    Mar  2 02:07:54.378: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:07:54.378: INFO: node-exporter-95fzh from openshift-monitoring started at 2023-03-01 22:56:40 +0000 UTC (2 container statuses recorded)
    Mar  2 02:07:54.378: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:07:54.378: INFO: 	Container node-exporter ready: true, restart count 0
    Mar  2 02:07:54.378: INFO: multus-89qpj from openshift-multus started at 2023-03-01 22:48:55 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.378: INFO: 	Container kube-multus ready: true, restart count 0
    Mar  2 02:07:54.378: INFO: multus-additional-cni-plugins-ff7kf from openshift-multus started at 2023-03-01 22:48:55 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.378: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Mar  2 02:07:54.378: INFO: network-metrics-daemon-7m69v from openshift-multus started at 2023-03-01 22:48:56 +0000 UTC (2 container statuses recorded)
    Mar  2 02:07:54.378: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:07:54.378: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Mar  2 02:07:54.378: INFO: network-check-source-848f87f9d4-bxlxt from openshift-network-diagnostics started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.378: INFO: 	Container check-endpoints ready: true, restart count 0
    Mar  2 02:07:54.378: INFO: network-check-target-jqnqt from openshift-network-diagnostics started at 2023-03-01 22:49:03 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.378: INFO: 	Container network-check-target-container ready: true, restart count 0
    Mar  2 02:07:54.378: INFO: catalog-operator-657ff7bb9f-c7tqq from openshift-operator-lifecycle-manager started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.378: INFO: 	Container catalog-operator ready: true, restart count 0
    Mar  2 02:07:54.378: INFO: collect-profiles-27962010-nhpf6 from openshift-operator-lifecycle-manager started at 2023-03-02 01:30:00 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.378: INFO: 	Container collect-profiles ready: false, restart count 0
    Mar  2 02:07:54.378: INFO: olm-operator-56ccd7794f-4ffrj from openshift-operator-lifecycle-manager started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.378: INFO: 	Container olm-operator ready: true, restart count 0
    Mar  2 02:07:54.378: INFO: package-server-manager-599745f678-rmjg4 from openshift-operator-lifecycle-manager started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.378: INFO: 	Container package-server-manager ready: true, restart count 0
    Mar  2 02:07:54.378: INFO: packageserver-6c7c56df95-jq89f from openshift-operator-lifecycle-manager started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.378: INFO: 	Container packageserver ready: true, restart count 0
    Mar  2 02:07:54.378: INFO: metrics-784dfd67b7-rjkpg from openshift-roks-metrics started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.378: INFO: 	Container metrics ready: true, restart count 3
    Mar  2 02:07:54.378: INFO: push-gateway-646cc86d79-cgtct from openshift-roks-metrics started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.378: INFO: 	Container push-gateway ready: true, restart count 0
    Mar  2 02:07:54.378: INFO: service-ca-operator-6f8744cbc5-w5srs from openshift-service-ca-operator started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.378: INFO: 	Container service-ca-operator ready: true, restart count 1
    Mar  2 02:07:54.378: INFO: sonobuoy-systemd-logs-daemon-set-7266eb5e626146f7-n2dsq from sonobuoy started at 2023-03-02 01:07:14 +0000 UTC (2 container statuses recorded)
    Mar  2 02:07:54.378: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  2 02:07:54.378: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar  2 02:07:54.378: INFO: 
    Logging pods the apiserver thinks is on node 10.132.92.188 before test
    Mar  2 02:07:54.454: INFO: calico-node-mkvsm from calico-system started at 2023-03-01 22:49:50 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.454: INFO: 	Container calico-node ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: ibm-cloud-provider-ip-169-56-11-242-558f6ff69-xvh2v from ibm-system started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.454: INFO: 	Container ibm-cloud-provider-ip-169-56-11-242 ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: ibm-keepalived-watcher-zwfb8 from kube-system started at 2023-03-01 22:48:10 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.454: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: ibm-master-proxy-static-10.132.92.188 from kube-system started at 2023-03-01 22:48:07 +0000 UTC (2 container statuses recorded)
    Mar  2 02:07:54.454: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: 	Container pause ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: ibmcloud-block-storage-driver-qbg9f from kube-system started at 2023-03-01 22:48:16 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.454: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: tuned-5z98r from openshift-cluster-node-tuning-operator started at 2023-03-01 22:55:42 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.454: INFO: 	Container tuned ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: csi-snapshot-controller-85d666c956-xsh6c from openshift-cluster-storage-operator started at 2023-03-01 22:51:57 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.454: INFO: 	Container snapshot-controller ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: csi-snapshot-webhook-6cbf55b956-wq8bh from openshift-cluster-storage-operator started at 2023-03-01 22:51:57 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.454: INFO: 	Container webhook ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: downloads-8b4bf4d5-b9kfz from openshift-console started at 2023-03-01 22:51:55 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.454: INFO: 	Container download-server ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: dns-default-nt82k from openshift-dns started at 2023-03-01 22:55:41 +0000 UTC (2 container statuses recorded)
    Mar  2 02:07:54.454: INFO: 	Container dns ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: node-resolver-wcng9 from openshift-dns started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.454: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: image-pruner-27961920-wd4pv from openshift-image-registry started at 2023-03-02 00:00:00 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.454: INFO: 	Container image-pruner ready: false, restart count 0
    Mar  2 02:07:54.454: INFO: image-registry-7fb7bd8df6-lb2bt from openshift-image-registry started at 2023-03-01 22:58:31 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.454: INFO: 	Container registry ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: node-ca-qlt4w from openshift-image-registry started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.454: INFO: 	Container node-ca ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: ingress-canary-qvcz9 from openshift-ingress-canary started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.454: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: router-default-68bc8785b7-wz8ct from openshift-ingress started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.454: INFO: 	Container router ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: openshift-kube-proxy-mth7h from openshift-kube-proxy started at 2023-03-01 22:49:00 +0000 UTC (2 container statuses recorded)
    Mar  2 02:07:54.454: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: migrator-5687b466dd-glln7 from openshift-kube-storage-version-migrator started at 2023-03-01 22:51:57 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.454: INFO: 	Container migrator ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: certified-operators-8n5vw from openshift-marketplace started at 2023-03-02 00:13:35 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.454: INFO: 	Container registry-server ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: community-operators-lngtl from openshift-marketplace started at 2023-03-01 22:54:15 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.454: INFO: 	Container registry-server ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: redhat-marketplace-ld8wh from openshift-marketplace started at 2023-03-02 00:27:02 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.454: INFO: 	Container registry-server ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: redhat-operators-tk548 from openshift-marketplace started at 2023-03-01 22:54:14 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.454: INFO: 	Container registry-server ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-03-01 22:58:39 +0000 UTC (6 container statuses recorded)
    Mar  2 02:07:54.454: INFO: 	Container alertmanager ready: true, restart count 1
    Mar  2 02:07:54.454: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: 	Container config-reloader ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: node-exporter-gj7lr from openshift-monitoring started at 2023-03-01 22:56:39 +0000 UTC (2 container statuses recorded)
    Mar  2 02:07:54.454: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: 	Container node-exporter ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: openshift-state-metrics-56599d9b6d-6sw48 from openshift-monitoring started at 2023-03-01 22:56:39 +0000 UTC (3 container statuses recorded)
    Mar  2 02:07:54.454: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: 	Container openshift-state-metrics ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: prometheus-adapter-95d69f68c-gtx2d from openshift-monitoring started at 2023-03-01 22:58:02 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.454: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-03-01 22:58:30 +0000 UTC (6 container statuses recorded)
    Mar  2 02:07:54.454: INFO: 	Container config-reloader ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: 	Container prometheus ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: prometheus-operator-9bf584898-89sgf from openshift-monitoring started at 2023-03-01 22:56:19 +0000 UTC (2 container statuses recorded)
    Mar  2 02:07:54.454: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: 	Container prometheus-operator ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: prometheus-operator-admission-webhook-6d5fbffb86-hhfbw from openshift-monitoring started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.454: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: thanos-querier-6cd8656bbb-8hv4v from openshift-monitoring started at 2023-03-01 22:56:48 +0000 UTC (6 container statuses recorded)
    Mar  2 02:07:54.454: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: 	Container oauth-proxy ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: 	Container thanos-query ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: multus-additional-cni-plugins-5pjnd from openshift-multus started at 2023-03-01 22:48:55 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.454: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: multus-admission-controller-6f984f76c7-hclf4 from openshift-multus started at 2023-03-01 22:55:41 +0000 UTC (2 container statuses recorded)
    Mar  2 02:07:54.454: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: 	Container multus-admission-controller ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: multus-g4k8f from openshift-multus started at 2023-03-01 22:48:55 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.454: INFO: 	Container kube-multus ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: network-metrics-daemon-95rvd from openshift-multus started at 2023-03-01 22:48:56 +0000 UTC (2 container statuses recorded)
    Mar  2 02:07:54.454: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: network-check-target-4nkgt from openshift-network-diagnostics started at 2023-03-01 22:49:03 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.454: INFO: 	Container network-check-target-container ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: network-operator-68bbf8cc76-mg97z from openshift-network-operator started at 2023-03-01 22:48:19 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.454: INFO: 	Container network-operator ready: true, restart count 1
    Mar  2 02:07:54.454: INFO: collect-profiles-27962025-rd7rx from openshift-operator-lifecycle-manager started at 2023-03-02 01:45:00 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.454: INFO: 	Container collect-profiles ready: false, restart count 0
    Mar  2 02:07:54.454: INFO: collect-profiles-27962040-2rff7 from openshift-operator-lifecycle-manager started at 2023-03-02 02:00:00 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.454: INFO: 	Container collect-profiles ready: false, restart count 0
    Mar  2 02:07:54.454: INFO: packageserver-6c7c56df95-7wr7f from openshift-operator-lifecycle-manager started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.454: INFO: 	Container packageserver ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: service-ca-578d67f6f8-89gtp from openshift-service-ca started at 2023-03-01 22:52:15 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.454: INFO: 	Container service-ca-controller ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: sonobuoy-e2e-job-c6c00a6e72aa4160 from sonobuoy started at 2023-03-02 01:07:14 +0000 UTC (2 container statuses recorded)
    Mar  2 02:07:54.454: INFO: 	Container e2e ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: sonobuoy-systemd-logs-daemon-set-7266eb5e626146f7-fgqrz from sonobuoy started at 2023-03-02 01:07:14 +0000 UTC (2 container statuses recorded)
    Mar  2 02:07:54.454: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar  2 02:07:54.454: INFO: tigera-operator-84f4f4565b-5w7s7 from tigera-operator started at 2023-03-01 22:48:19 +0000 UTC (1 container statuses recorded)
    Mar  2 02:07:54.454: INFO: 	Container tigera-operator ready: true, restart count 2
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:461
    STEP: Trying to launch a pod without a label to get a node which can launch it. 03/02/23 02:07:54.455
    Mar  2 02:07:54.515: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-7816" to be "running"
    Mar  2 02:07:54.536: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 21.082823ms
    Mar  2 02:07:56.549: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034044232s
    Mar  2 02:07:58.553: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.037739715s
    Mar  2 02:07:58.553: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 03/02/23 02:07:58.566
    STEP: Trying to apply a random label on the found node. 03/02/23 02:07:58.605
    STEP: verifying the node has the label kubernetes.io/e2e-ca45fe8c-a939-433e-95a9-93e963e3f576 42 03/02/23 02:07:58.642
    STEP: Trying to relaunch the pod, now with labels. 03/02/23 02:07:58.663
    Mar  2 02:07:58.719: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-7816" to be "not pending"
    Mar  2 02:07:58.733: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 14.159064ms
    Mar  2 02:08:00.751: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031831339s
    Mar  2 02:08:02.755: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 4.035522328s
    Mar  2 02:08:02.755: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-ca45fe8c-a939-433e-95a9-93e963e3f576 off the node 10.132.92.143 03/02/23 02:08:02.792
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-ca45fe8c-a939-433e-95a9-93e963e3f576 03/02/23 02:08:02.877
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Mar  2 02:08:02.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-7816" for this suite. 03/02/23 02:08:02.953
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:08:02.98
Mar  2 02:08:02.980: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename pods 03/02/23 02:08:02.981
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:08:03.055
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:08:03.072
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
STEP: creating a Pod with a static label 03/02/23 02:08:03.143
STEP: watching for Pod to be ready 03/02/23 02:08:03.197
Mar  2 02:08:03.203: INFO: observed Pod pod-test in namespace pods-5799 in phase Pending with labels: map[test-pod-static:true] & conditions []
Mar  2 02:08:03.248: INFO: observed Pod pod-test in namespace pods-5799 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:08:03 +0000 UTC  }]
Mar  2 02:08:03.319: INFO: observed Pod pod-test in namespace pods-5799 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:08:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:08:03 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:08:03 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:08:03 +0000 UTC  }]
Mar  2 02:08:04.860: INFO: observed Pod pod-test in namespace pods-5799 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:08:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:08:03 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:08:03 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:08:03 +0000 UTC  }]
Mar  2 02:08:04.989: INFO: observed Pod pod-test in namespace pods-5799 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:08:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:08:03 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:08:03 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:08:03 +0000 UTC  }]
Mar  2 02:08:06.884: INFO: Found Pod pod-test in namespace pods-5799 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:08:03 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:08:06 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:08:06 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:08:03 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 03/02/23 02:08:06.914
STEP: getting the Pod and ensuring that it's patched 03/02/23 02:08:06.993
STEP: replacing the Pod's status Ready condition to False 03/02/23 02:08:07.059
STEP: check the Pod again to ensure its Ready conditions are False 03/02/23 02:08:07.14
STEP: deleting the Pod via a Collection with a LabelSelector 03/02/23 02:08:07.141
STEP: watching for the Pod to be deleted 03/02/23 02:08:07.232
Mar  2 02:08:07.241: INFO: observed event type MODIFIED
Mar  2 02:08:08.940: INFO: observed event type MODIFIED
Mar  2 02:08:09.507: INFO: observed event type MODIFIED
Mar  2 02:08:10.962: INFO: observed event type MODIFIED
Mar  2 02:08:11.001: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar  2 02:08:11.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5799" for this suite. 03/02/23 02:08:11.062
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","completed":196,"skipped":3658,"failed":0}
------------------------------
• [SLOW TEST] [8.117 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:08:02.98
    Mar  2 02:08:02.980: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename pods 03/02/23 02:08:02.981
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:08:03.055
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:08:03.072
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:895
    STEP: creating a Pod with a static label 03/02/23 02:08:03.143
    STEP: watching for Pod to be ready 03/02/23 02:08:03.197
    Mar  2 02:08:03.203: INFO: observed Pod pod-test in namespace pods-5799 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Mar  2 02:08:03.248: INFO: observed Pod pod-test in namespace pods-5799 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:08:03 +0000 UTC  }]
    Mar  2 02:08:03.319: INFO: observed Pod pod-test in namespace pods-5799 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:08:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:08:03 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:08:03 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:08:03 +0000 UTC  }]
    Mar  2 02:08:04.860: INFO: observed Pod pod-test in namespace pods-5799 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:08:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:08:03 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:08:03 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:08:03 +0000 UTC  }]
    Mar  2 02:08:04.989: INFO: observed Pod pod-test in namespace pods-5799 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:08:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:08:03 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:08:03 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:08:03 +0000 UTC  }]
    Mar  2 02:08:06.884: INFO: Found Pod pod-test in namespace pods-5799 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:08:03 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:08:06 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:08:06 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:08:03 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 03/02/23 02:08:06.914
    STEP: getting the Pod and ensuring that it's patched 03/02/23 02:08:06.993
    STEP: replacing the Pod's status Ready condition to False 03/02/23 02:08:07.059
    STEP: check the Pod again to ensure its Ready conditions are False 03/02/23 02:08:07.14
    STEP: deleting the Pod via a Collection with a LabelSelector 03/02/23 02:08:07.141
    STEP: watching for the Pod to be deleted 03/02/23 02:08:07.232
    Mar  2 02:08:07.241: INFO: observed event type MODIFIED
    Mar  2 02:08:08.940: INFO: observed event type MODIFIED
    Mar  2 02:08:09.507: INFO: observed event type MODIFIED
    Mar  2 02:08:10.962: INFO: observed event type MODIFIED
    Mar  2 02:08:11.001: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar  2 02:08:11.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-5799" for this suite. 03/02/23 02:08:11.062
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:08:11.097
Mar  2 02:08:11.097: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename projected 03/02/23 02:08:11.098
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:08:11.162
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:08:11.193
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
STEP: Creating configMap with name projected-configmap-test-volume-map-e161149c-f70e-46f9-ad75-b5bb18d122e5 03/02/23 02:08:11.209
STEP: Creating a pod to test consume configMaps 03/02/23 02:08:11.231
Mar  2 02:08:11.397: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-26d3b3f3-4b67-47a5-a801-aa4710d29cb4" in namespace "projected-9616" to be "Succeeded or Failed"
Mar  2 02:08:11.462: INFO: Pod "pod-projected-configmaps-26d3b3f3-4b67-47a5-a801-aa4710d29cb4": Phase="Pending", Reason="", readiness=false. Elapsed: 64.338706ms
Mar  2 02:08:13.485: INFO: Pod "pod-projected-configmaps-26d3b3f3-4b67-47a5-a801-aa4710d29cb4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.087531894s
Mar  2 02:08:15.481: INFO: Pod "pod-projected-configmaps-26d3b3f3-4b67-47a5-a801-aa4710d29cb4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.083177424s
Mar  2 02:08:17.478: INFO: Pod "pod-projected-configmaps-26d3b3f3-4b67-47a5-a801-aa4710d29cb4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.080299424s
STEP: Saw pod success 03/02/23 02:08:17.478
Mar  2 02:08:17.479: INFO: Pod "pod-projected-configmaps-26d3b3f3-4b67-47a5-a801-aa4710d29cb4" satisfied condition "Succeeded or Failed"
Mar  2 02:08:17.491: INFO: Trying to get logs from node 10.132.92.143 pod pod-projected-configmaps-26d3b3f3-4b67-47a5-a801-aa4710d29cb4 container agnhost-container: <nil>
STEP: delete the pod 03/02/23 02:08:17.524
Mar  2 02:08:17.605: INFO: Waiting for pod pod-projected-configmaps-26d3b3f3-4b67-47a5-a801-aa4710d29cb4 to disappear
Mar  2 02:08:17.619: INFO: Pod pod-projected-configmaps-26d3b3f3-4b67-47a5-a801-aa4710d29cb4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar  2 02:08:17.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9616" for this suite. 03/02/23 02:08:17.65
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":197,"skipped":3664,"failed":0}
------------------------------
• [SLOW TEST] [6.588 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:08:11.097
    Mar  2 02:08:11.097: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename projected 03/02/23 02:08:11.098
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:08:11.162
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:08:11.193
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:88
    STEP: Creating configMap with name projected-configmap-test-volume-map-e161149c-f70e-46f9-ad75-b5bb18d122e5 03/02/23 02:08:11.209
    STEP: Creating a pod to test consume configMaps 03/02/23 02:08:11.231
    Mar  2 02:08:11.397: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-26d3b3f3-4b67-47a5-a801-aa4710d29cb4" in namespace "projected-9616" to be "Succeeded or Failed"
    Mar  2 02:08:11.462: INFO: Pod "pod-projected-configmaps-26d3b3f3-4b67-47a5-a801-aa4710d29cb4": Phase="Pending", Reason="", readiness=false. Elapsed: 64.338706ms
    Mar  2 02:08:13.485: INFO: Pod "pod-projected-configmaps-26d3b3f3-4b67-47a5-a801-aa4710d29cb4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.087531894s
    Mar  2 02:08:15.481: INFO: Pod "pod-projected-configmaps-26d3b3f3-4b67-47a5-a801-aa4710d29cb4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.083177424s
    Mar  2 02:08:17.478: INFO: Pod "pod-projected-configmaps-26d3b3f3-4b67-47a5-a801-aa4710d29cb4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.080299424s
    STEP: Saw pod success 03/02/23 02:08:17.478
    Mar  2 02:08:17.479: INFO: Pod "pod-projected-configmaps-26d3b3f3-4b67-47a5-a801-aa4710d29cb4" satisfied condition "Succeeded or Failed"
    Mar  2 02:08:17.491: INFO: Trying to get logs from node 10.132.92.143 pod pod-projected-configmaps-26d3b3f3-4b67-47a5-a801-aa4710d29cb4 container agnhost-container: <nil>
    STEP: delete the pod 03/02/23 02:08:17.524
    Mar  2 02:08:17.605: INFO: Waiting for pod pod-projected-configmaps-26d3b3f3-4b67-47a5-a801-aa4710d29cb4 to disappear
    Mar  2 02:08:17.619: INFO: Pod pod-projected-configmaps-26d3b3f3-4b67-47a5-a801-aa4710d29cb4 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar  2 02:08:17.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9616" for this suite. 03/02/23 02:08:17.65
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:08:17.694
Mar  2 02:08:17.694: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename taint-multiple-pods 03/02/23 02:08:17.695
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:08:17.8
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:08:17.836
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:348
Mar  2 02:08:17.850: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  2 02:09:18.116: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
Mar  2 02:09:18.142: INFO: Starting informer...
STEP: Starting pods... 03/02/23 02:09:18.142
Mar  2 02:09:18.437: INFO: Pod1 is running on 10.132.92.143. Tainting Node
Mar  2 02:09:18.699: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-5247" to be "running"
Mar  2 02:09:18.722: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 23.325431ms
Mar  2 02:09:20.736: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.037385121s
Mar  2 02:09:20.736: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Mar  2 02:09:20.736: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-5247" to be "running"
Mar  2 02:09:20.750: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 14.286414ms
Mar  2 02:09:20.751: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Mar  2 02:09:20.751: INFO: Pod2 is running on 10.132.92.143. Tainting Node
STEP: Trying to apply a taint on the Node 03/02/23 02:09:20.751
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/02/23 02:09:20.797
STEP: Waiting for Pod1 and Pod2 to be deleted 03/02/23 02:09:20.816
Mar  2 02:09:28.039: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Mar  2 02:09:47.139: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/02/23 02:09:47.204
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:187
Mar  2 02:09:47.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-5247" for this suite. 03/02/23 02:09:47.246
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","completed":198,"skipped":3708,"failed":0}
------------------------------
• [SLOW TEST] [89.593 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:08:17.694
    Mar  2 02:08:17.694: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename taint-multiple-pods 03/02/23 02:08:17.695
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:08:17.8
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:08:17.836
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:348
    Mar  2 02:08:17.850: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar  2 02:09:18.116: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:420
    Mar  2 02:09:18.142: INFO: Starting informer...
    STEP: Starting pods... 03/02/23 02:09:18.142
    Mar  2 02:09:18.437: INFO: Pod1 is running on 10.132.92.143. Tainting Node
    Mar  2 02:09:18.699: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-5247" to be "running"
    Mar  2 02:09:18.722: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 23.325431ms
    Mar  2 02:09:20.736: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.037385121s
    Mar  2 02:09:20.736: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Mar  2 02:09:20.736: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-5247" to be "running"
    Mar  2 02:09:20.750: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 14.286414ms
    Mar  2 02:09:20.751: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Mar  2 02:09:20.751: INFO: Pod2 is running on 10.132.92.143. Tainting Node
    STEP: Trying to apply a taint on the Node 03/02/23 02:09:20.751
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/02/23 02:09:20.797
    STEP: Waiting for Pod1 and Pod2 to be deleted 03/02/23 02:09:20.816
    Mar  2 02:09:28.039: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Mar  2 02:09:47.139: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/02/23 02:09:47.204
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:187
    Mar  2 02:09:47.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-multiple-pods-5247" for this suite. 03/02/23 02:09:47.246
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:09:47.289
Mar  2 02:09:47.289: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename services 03/02/23 02:09:47.291
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:09:47.397
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:09:47.41
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
STEP: creating service in namespace services-3293 03/02/23 02:09:47.42
STEP: creating service affinity-nodeport in namespace services-3293 03/02/23 02:09:47.421
STEP: creating replication controller affinity-nodeport in namespace services-3293 03/02/23 02:09:47.479
I0302 02:09:47.517373      20 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-3293, replica count: 3
I0302 02:09:50.569823      20 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 02:09:53.570300      20 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 02:09:53.621: INFO: Creating new exec pod
Mar  2 02:09:53.672: INFO: Waiting up to 5m0s for pod "execpod-affinityz9vq5" in namespace "services-3293" to be "running"
Mar  2 02:09:53.686: INFO: Pod "execpod-affinityz9vq5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.039518ms
Mar  2 02:09:55.703: INFO: Pod "execpod-affinityz9vq5": Phase="Running", Reason="", readiness=true. Elapsed: 2.030546918s
Mar  2 02:09:55.703: INFO: Pod "execpod-affinityz9vq5" satisfied condition "running"
Mar  2 02:09:56.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-3293 exec execpod-affinityz9vq5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Mar  2 02:09:57.172: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Mar  2 02:09:57.172: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 02:09:57.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-3293 exec execpod-affinityz9vq5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.34.228 80'
Mar  2 02:09:57.514: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.34.228 80\nConnection to 172.21.34.228 80 port [tcp/http] succeeded!\n"
Mar  2 02:09:57.514: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 02:09:57.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-3293 exec execpod-affinityz9vq5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.132.92.143 31362'
Mar  2 02:09:57.826: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.132.92.143 31362\nConnection to 10.132.92.143 31362 port [tcp/*] succeeded!\n"
Mar  2 02:09:57.826: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 02:09:57.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-3293 exec execpod-affinityz9vq5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.132.92.186 31362'
Mar  2 02:09:58.171: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.132.92.186 31362\nConnection to 10.132.92.186 31362 port [tcp/*] succeeded!\n"
Mar  2 02:09:58.171: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 02:09:58.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-3293 exec execpod-affinityz9vq5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.132.92.143:31362/ ; done'
Mar  2 02:09:58.639: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31362/\n"
Mar  2 02:09:58.639: INFO: stdout: "\naffinity-nodeport-djktk\naffinity-nodeport-djktk\naffinity-nodeport-djktk\naffinity-nodeport-djktk\naffinity-nodeport-djktk\naffinity-nodeport-djktk\naffinity-nodeport-djktk\naffinity-nodeport-djktk\naffinity-nodeport-djktk\naffinity-nodeport-djktk\naffinity-nodeport-djktk\naffinity-nodeport-djktk\naffinity-nodeport-djktk\naffinity-nodeport-djktk\naffinity-nodeport-djktk\naffinity-nodeport-djktk"
Mar  2 02:09:58.639: INFO: Received response from host: affinity-nodeport-djktk
Mar  2 02:09:58.639: INFO: Received response from host: affinity-nodeport-djktk
Mar  2 02:09:58.639: INFO: Received response from host: affinity-nodeport-djktk
Mar  2 02:09:58.639: INFO: Received response from host: affinity-nodeport-djktk
Mar  2 02:09:58.639: INFO: Received response from host: affinity-nodeport-djktk
Mar  2 02:09:58.639: INFO: Received response from host: affinity-nodeport-djktk
Mar  2 02:09:58.639: INFO: Received response from host: affinity-nodeport-djktk
Mar  2 02:09:58.639: INFO: Received response from host: affinity-nodeport-djktk
Mar  2 02:09:58.639: INFO: Received response from host: affinity-nodeport-djktk
Mar  2 02:09:58.639: INFO: Received response from host: affinity-nodeport-djktk
Mar  2 02:09:58.639: INFO: Received response from host: affinity-nodeport-djktk
Mar  2 02:09:58.639: INFO: Received response from host: affinity-nodeport-djktk
Mar  2 02:09:58.639: INFO: Received response from host: affinity-nodeport-djktk
Mar  2 02:09:58.639: INFO: Received response from host: affinity-nodeport-djktk
Mar  2 02:09:58.639: INFO: Received response from host: affinity-nodeport-djktk
Mar  2 02:09:58.639: INFO: Received response from host: affinity-nodeport-djktk
Mar  2 02:09:58.639: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-3293, will wait for the garbage collector to delete the pods 03/02/23 02:09:58.722
Mar  2 02:09:58.816: INFO: Deleting ReplicationController affinity-nodeport took: 27.126668ms
Mar  2 02:09:59.118: INFO: Terminating ReplicationController affinity-nodeport pods took: 302.657684ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  2 02:10:02.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3293" for this suite. 03/02/23 02:10:02.887
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","completed":199,"skipped":3718,"failed":0}
------------------------------
• [SLOW TEST] [15.636 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:09:47.289
    Mar  2 02:09:47.289: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename services 03/02/23 02:09:47.291
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:09:47.397
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:09:47.41
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2204
    STEP: creating service in namespace services-3293 03/02/23 02:09:47.42
    STEP: creating service affinity-nodeport in namespace services-3293 03/02/23 02:09:47.421
    STEP: creating replication controller affinity-nodeport in namespace services-3293 03/02/23 02:09:47.479
    I0302 02:09:47.517373      20 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-3293, replica count: 3
    I0302 02:09:50.569823      20 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0302 02:09:53.570300      20 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar  2 02:09:53.621: INFO: Creating new exec pod
    Mar  2 02:09:53.672: INFO: Waiting up to 5m0s for pod "execpod-affinityz9vq5" in namespace "services-3293" to be "running"
    Mar  2 02:09:53.686: INFO: Pod "execpod-affinityz9vq5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.039518ms
    Mar  2 02:09:55.703: INFO: Pod "execpod-affinityz9vq5": Phase="Running", Reason="", readiness=true. Elapsed: 2.030546918s
    Mar  2 02:09:55.703: INFO: Pod "execpod-affinityz9vq5" satisfied condition "running"
    Mar  2 02:09:56.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-3293 exec execpod-affinityz9vq5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
    Mar  2 02:09:57.172: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Mar  2 02:09:57.172: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  2 02:09:57.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-3293 exec execpod-affinityz9vq5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.34.228 80'
    Mar  2 02:09:57.514: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.34.228 80\nConnection to 172.21.34.228 80 port [tcp/http] succeeded!\n"
    Mar  2 02:09:57.514: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  2 02:09:57.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-3293 exec execpod-affinityz9vq5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.132.92.143 31362'
    Mar  2 02:09:57.826: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.132.92.143 31362\nConnection to 10.132.92.143 31362 port [tcp/*] succeeded!\n"
    Mar  2 02:09:57.826: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  2 02:09:57.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-3293 exec execpod-affinityz9vq5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.132.92.186 31362'
    Mar  2 02:09:58.171: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.132.92.186 31362\nConnection to 10.132.92.186 31362 port [tcp/*] succeeded!\n"
    Mar  2 02:09:58.171: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  2 02:09:58.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-3293 exec execpod-affinityz9vq5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.132.92.143:31362/ ; done'
    Mar  2 02:09:58.639: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:31362/\n"
    Mar  2 02:09:58.639: INFO: stdout: "\naffinity-nodeport-djktk\naffinity-nodeport-djktk\naffinity-nodeport-djktk\naffinity-nodeport-djktk\naffinity-nodeport-djktk\naffinity-nodeport-djktk\naffinity-nodeport-djktk\naffinity-nodeport-djktk\naffinity-nodeport-djktk\naffinity-nodeport-djktk\naffinity-nodeport-djktk\naffinity-nodeport-djktk\naffinity-nodeport-djktk\naffinity-nodeport-djktk\naffinity-nodeport-djktk\naffinity-nodeport-djktk"
    Mar  2 02:09:58.639: INFO: Received response from host: affinity-nodeport-djktk
    Mar  2 02:09:58.639: INFO: Received response from host: affinity-nodeport-djktk
    Mar  2 02:09:58.639: INFO: Received response from host: affinity-nodeport-djktk
    Mar  2 02:09:58.639: INFO: Received response from host: affinity-nodeport-djktk
    Mar  2 02:09:58.639: INFO: Received response from host: affinity-nodeport-djktk
    Mar  2 02:09:58.639: INFO: Received response from host: affinity-nodeport-djktk
    Mar  2 02:09:58.639: INFO: Received response from host: affinity-nodeport-djktk
    Mar  2 02:09:58.639: INFO: Received response from host: affinity-nodeport-djktk
    Mar  2 02:09:58.639: INFO: Received response from host: affinity-nodeport-djktk
    Mar  2 02:09:58.639: INFO: Received response from host: affinity-nodeport-djktk
    Mar  2 02:09:58.639: INFO: Received response from host: affinity-nodeport-djktk
    Mar  2 02:09:58.639: INFO: Received response from host: affinity-nodeport-djktk
    Mar  2 02:09:58.639: INFO: Received response from host: affinity-nodeport-djktk
    Mar  2 02:09:58.639: INFO: Received response from host: affinity-nodeport-djktk
    Mar  2 02:09:58.639: INFO: Received response from host: affinity-nodeport-djktk
    Mar  2 02:09:58.639: INFO: Received response from host: affinity-nodeport-djktk
    Mar  2 02:09:58.639: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-3293, will wait for the garbage collector to delete the pods 03/02/23 02:09:58.722
    Mar  2 02:09:58.816: INFO: Deleting ReplicationController affinity-nodeport took: 27.126668ms
    Mar  2 02:09:59.118: INFO: Terminating ReplicationController affinity-nodeport pods took: 302.657684ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  2 02:10:02.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3293" for this suite. 03/02/23 02:10:02.887
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:10:02.93
Mar  2 02:10:02.930: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename projected 03/02/23 02:10:02.932
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:10:02.994
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:10:03.011
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
STEP: Creating a pod to test downward API volume plugin 03/02/23 02:10:03.023
Mar  2 02:10:03.132: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0da6f6ad-f075-4611-b1ee-8617adb0a47f" in namespace "projected-8258" to be "Succeeded or Failed"
Mar  2 02:10:03.145: INFO: Pod "downwardapi-volume-0da6f6ad-f075-4611-b1ee-8617adb0a47f": Phase="Pending", Reason="", readiness=false. Elapsed: 13.545888ms
Mar  2 02:10:05.160: INFO: Pod "downwardapi-volume-0da6f6ad-f075-4611-b1ee-8617adb0a47f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028365458s
Mar  2 02:10:07.173: INFO: Pod "downwardapi-volume-0da6f6ad-f075-4611-b1ee-8617adb0a47f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041656409s
Mar  2 02:10:09.163: INFO: Pod "downwardapi-volume-0da6f6ad-f075-4611-b1ee-8617adb0a47f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031327889s
STEP: Saw pod success 03/02/23 02:10:09.163
Mar  2 02:10:09.163: INFO: Pod "downwardapi-volume-0da6f6ad-f075-4611-b1ee-8617adb0a47f" satisfied condition "Succeeded or Failed"
Mar  2 02:10:09.176: INFO: Trying to get logs from node 10.132.92.143 pod downwardapi-volume-0da6f6ad-f075-4611-b1ee-8617adb0a47f container client-container: <nil>
STEP: delete the pod 03/02/23 02:10:09.253
Mar  2 02:10:09.291: INFO: Waiting for pod downwardapi-volume-0da6f6ad-f075-4611-b1ee-8617adb0a47f to disappear
Mar  2 02:10:09.338: INFO: Pod downwardapi-volume-0da6f6ad-f075-4611-b1ee-8617adb0a47f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar  2 02:10:09.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8258" for this suite. 03/02/23 02:10:09.364
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":200,"skipped":3759,"failed":0}
------------------------------
• [SLOW TEST] [6.463 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:10:02.93
    Mar  2 02:10:02.930: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename projected 03/02/23 02:10:02.932
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:10:02.994
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:10:03.011
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:260
    STEP: Creating a pod to test downward API volume plugin 03/02/23 02:10:03.023
    Mar  2 02:10:03.132: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0da6f6ad-f075-4611-b1ee-8617adb0a47f" in namespace "projected-8258" to be "Succeeded or Failed"
    Mar  2 02:10:03.145: INFO: Pod "downwardapi-volume-0da6f6ad-f075-4611-b1ee-8617adb0a47f": Phase="Pending", Reason="", readiness=false. Elapsed: 13.545888ms
    Mar  2 02:10:05.160: INFO: Pod "downwardapi-volume-0da6f6ad-f075-4611-b1ee-8617adb0a47f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028365458s
    Mar  2 02:10:07.173: INFO: Pod "downwardapi-volume-0da6f6ad-f075-4611-b1ee-8617adb0a47f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041656409s
    Mar  2 02:10:09.163: INFO: Pod "downwardapi-volume-0da6f6ad-f075-4611-b1ee-8617adb0a47f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031327889s
    STEP: Saw pod success 03/02/23 02:10:09.163
    Mar  2 02:10:09.163: INFO: Pod "downwardapi-volume-0da6f6ad-f075-4611-b1ee-8617adb0a47f" satisfied condition "Succeeded or Failed"
    Mar  2 02:10:09.176: INFO: Trying to get logs from node 10.132.92.143 pod downwardapi-volume-0da6f6ad-f075-4611-b1ee-8617adb0a47f container client-container: <nil>
    STEP: delete the pod 03/02/23 02:10:09.253
    Mar  2 02:10:09.291: INFO: Waiting for pod downwardapi-volume-0da6f6ad-f075-4611-b1ee-8617adb0a47f to disappear
    Mar  2 02:10:09.338: INFO: Pod downwardapi-volume-0da6f6ad-f075-4611-b1ee-8617adb0a47f no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar  2 02:10:09.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8258" for this suite. 03/02/23 02:10:09.364
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:10:09.394
Mar  2 02:10:09.394: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename init-container 03/02/23 02:10:09.395
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:10:09.453
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:10:09.472
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
STEP: creating the pod 03/02/23 02:10:09.49
Mar  2 02:10:09.490: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Mar  2 02:10:13.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-76" for this suite. 03/02/23 02:10:13.545
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","completed":201,"skipped":3776,"failed":0}
------------------------------
• [4.176 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:10:09.394
    Mar  2 02:10:09.394: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename init-container 03/02/23 02:10:09.395
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:10:09.453
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:10:09.472
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:254
    STEP: creating the pod 03/02/23 02:10:09.49
    Mar  2 02:10:09.490: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Mar  2 02:10:13.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-76" for this suite. 03/02/23 02:10:13.545
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:10:13.571
Mar  2 02:10:13.571: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename var-expansion 03/02/23 02:10:13.572
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:10:13.683
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:10:13.703
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
STEP: creating the pod with failed condition 03/02/23 02:10:13.722
Mar  2 02:10:13.798: INFO: Waiting up to 2m0s for pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a" in namespace "var-expansion-121" to be "running"
Mar  2 02:10:13.813: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 14.338855ms
Mar  2 02:10:15.834: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03500117s
Mar  2 02:10:17.827: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028246648s
Mar  2 02:10:19.829: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.030573111s
Mar  2 02:10:21.828: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.029384078s
Mar  2 02:10:23.828: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.029408122s
Mar  2 02:10:25.828: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.02965858s
Mar  2 02:10:27.829: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 14.030186662s
Mar  2 02:10:29.849: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 16.050261787s
Mar  2 02:10:31.831: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 18.032000038s
Mar  2 02:10:33.855: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 20.0568505s
Mar  2 02:10:35.835: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 22.036092016s
Mar  2 02:10:37.827: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 24.028645794s
Mar  2 02:10:39.830: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 26.031246835s
Mar  2 02:10:41.834: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 28.035279657s
Mar  2 02:10:43.828: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 30.029711022s
Mar  2 02:10:45.829: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 32.030899689s
Mar  2 02:10:47.827: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 34.028697037s
Mar  2 02:10:49.827: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 36.028602768s
Mar  2 02:10:51.936: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 38.137285836s
Mar  2 02:10:53.828: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 40.029748226s
Mar  2 02:10:55.832: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 42.033222143s
Mar  2 02:10:57.828: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 44.029265471s
Mar  2 02:10:59.828: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 46.029352511s
Mar  2 02:11:01.830: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 48.031568433s
Mar  2 02:11:03.829: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 50.030662814s
Mar  2 02:11:05.834: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 52.035256373s
Mar  2 02:11:07.827: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 54.028947113s
Mar  2 02:11:09.847: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 56.048165796s
Mar  2 02:11:11.829: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 58.030809178s
Mar  2 02:11:13.829: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.030011754s
Mar  2 02:11:15.829: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.030664692s
Mar  2 02:11:17.827: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.028587238s
Mar  2 02:11:19.827: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.027987025s
Mar  2 02:11:21.828: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.029593243s
Mar  2 02:11:23.828: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.029373398s
Mar  2 02:11:25.828: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.029851328s
Mar  2 02:11:27.934: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.135275512s
Mar  2 02:11:29.827: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.028530075s
Mar  2 02:11:31.831: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.032631967s
Mar  2 02:11:33.830: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.031358428s
Mar  2 02:11:35.828: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.029008546s
Mar  2 02:11:37.826: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.027683071s
Mar  2 02:11:39.829: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.030813974s
Mar  2 02:11:41.832: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.033031789s
Mar  2 02:11:43.842: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.043642669s
Mar  2 02:11:45.830: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.031549902s
Mar  2 02:11:47.829: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.030037028s
Mar  2 02:11:49.834: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.03586053s
Mar  2 02:11:51.860: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.061357161s
Mar  2 02:11:53.831: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.032262897s
Mar  2 02:11:55.828: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.029552104s
Mar  2 02:11:57.833: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.034472504s
Mar  2 02:11:59.835: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.036594104s
Mar  2 02:12:01.845: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.046193545s
Mar  2 02:12:03.837: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.038464539s
Mar  2 02:12:05.831: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.032008037s
Mar  2 02:12:07.830: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.031275665s
Mar  2 02:12:09.844: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.045101039s
Mar  2 02:12:11.833: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.034574261s
Mar  2 02:12:13.857: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.058444711s
Mar  2 02:12:13.874: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.07578031s
STEP: updating the pod 03/02/23 02:12:13.874
Mar  2 02:12:14.463: INFO: Successfully updated pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a"
STEP: waiting for pod running 03/02/23 02:12:14.463
Mar  2 02:12:14.464: INFO: Waiting up to 2m0s for pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a" in namespace "var-expansion-121" to be "running"
Mar  2 02:12:14.485: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 21.723066ms
Mar  2 02:12:16.500: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Running", Reason="", readiness=true. Elapsed: 2.036631394s
Mar  2 02:12:16.500: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a" satisfied condition "running"
STEP: deleting the pod gracefully 03/02/23 02:12:16.5
Mar  2 02:12:16.501: INFO: Deleting pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a" in namespace "var-expansion-121"
Mar  2 02:12:16.530: INFO: Wait up to 5m0s for pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar  2 02:12:48.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-121" for this suite. 03/02/23 02:12:48.611
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","completed":202,"skipped":3776,"failed":0}
------------------------------
• [SLOW TEST] [155.071 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:10:13.571
    Mar  2 02:10:13.571: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename var-expansion 03/02/23 02:10:13.572
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:10:13.683
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:10:13.703
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:224
    STEP: creating the pod with failed condition 03/02/23 02:10:13.722
    Mar  2 02:10:13.798: INFO: Waiting up to 2m0s for pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a" in namespace "var-expansion-121" to be "running"
    Mar  2 02:10:13.813: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 14.338855ms
    Mar  2 02:10:15.834: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03500117s
    Mar  2 02:10:17.827: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028246648s
    Mar  2 02:10:19.829: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.030573111s
    Mar  2 02:10:21.828: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.029384078s
    Mar  2 02:10:23.828: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.029408122s
    Mar  2 02:10:25.828: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.02965858s
    Mar  2 02:10:27.829: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 14.030186662s
    Mar  2 02:10:29.849: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 16.050261787s
    Mar  2 02:10:31.831: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 18.032000038s
    Mar  2 02:10:33.855: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 20.0568505s
    Mar  2 02:10:35.835: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 22.036092016s
    Mar  2 02:10:37.827: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 24.028645794s
    Mar  2 02:10:39.830: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 26.031246835s
    Mar  2 02:10:41.834: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 28.035279657s
    Mar  2 02:10:43.828: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 30.029711022s
    Mar  2 02:10:45.829: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 32.030899689s
    Mar  2 02:10:47.827: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 34.028697037s
    Mar  2 02:10:49.827: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 36.028602768s
    Mar  2 02:10:51.936: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 38.137285836s
    Mar  2 02:10:53.828: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 40.029748226s
    Mar  2 02:10:55.832: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 42.033222143s
    Mar  2 02:10:57.828: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 44.029265471s
    Mar  2 02:10:59.828: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 46.029352511s
    Mar  2 02:11:01.830: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 48.031568433s
    Mar  2 02:11:03.829: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 50.030662814s
    Mar  2 02:11:05.834: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 52.035256373s
    Mar  2 02:11:07.827: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 54.028947113s
    Mar  2 02:11:09.847: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 56.048165796s
    Mar  2 02:11:11.829: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 58.030809178s
    Mar  2 02:11:13.829: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.030011754s
    Mar  2 02:11:15.829: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.030664692s
    Mar  2 02:11:17.827: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.028587238s
    Mar  2 02:11:19.827: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.027987025s
    Mar  2 02:11:21.828: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.029593243s
    Mar  2 02:11:23.828: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.029373398s
    Mar  2 02:11:25.828: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.029851328s
    Mar  2 02:11:27.934: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.135275512s
    Mar  2 02:11:29.827: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.028530075s
    Mar  2 02:11:31.831: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.032631967s
    Mar  2 02:11:33.830: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.031358428s
    Mar  2 02:11:35.828: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.029008546s
    Mar  2 02:11:37.826: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.027683071s
    Mar  2 02:11:39.829: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.030813974s
    Mar  2 02:11:41.832: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.033031789s
    Mar  2 02:11:43.842: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.043642669s
    Mar  2 02:11:45.830: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.031549902s
    Mar  2 02:11:47.829: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.030037028s
    Mar  2 02:11:49.834: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.03586053s
    Mar  2 02:11:51.860: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.061357161s
    Mar  2 02:11:53.831: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.032262897s
    Mar  2 02:11:55.828: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.029552104s
    Mar  2 02:11:57.833: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.034472504s
    Mar  2 02:11:59.835: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.036594104s
    Mar  2 02:12:01.845: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.046193545s
    Mar  2 02:12:03.837: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.038464539s
    Mar  2 02:12:05.831: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.032008037s
    Mar  2 02:12:07.830: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.031275665s
    Mar  2 02:12:09.844: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.045101039s
    Mar  2 02:12:11.833: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.034574261s
    Mar  2 02:12:13.857: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.058444711s
    Mar  2 02:12:13.874: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.07578031s
    STEP: updating the pod 03/02/23 02:12:13.874
    Mar  2 02:12:14.463: INFO: Successfully updated pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a"
    STEP: waiting for pod running 03/02/23 02:12:14.463
    Mar  2 02:12:14.464: INFO: Waiting up to 2m0s for pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a" in namespace "var-expansion-121" to be "running"
    Mar  2 02:12:14.485: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Pending", Reason="", readiness=false. Elapsed: 21.723066ms
    Mar  2 02:12:16.500: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a": Phase="Running", Reason="", readiness=true. Elapsed: 2.036631394s
    Mar  2 02:12:16.500: INFO: Pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a" satisfied condition "running"
    STEP: deleting the pod gracefully 03/02/23 02:12:16.5
    Mar  2 02:12:16.501: INFO: Deleting pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a" in namespace "var-expansion-121"
    Mar  2 02:12:16.530: INFO: Wait up to 5m0s for pod "var-expansion-6710a9a6-12b0-43e1-bb4c-90fbc2d9366a" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar  2 02:12:48.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-121" for this suite. 03/02/23 02:12:48.611
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:12:48.645
Mar  2 02:12:48.645: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename container-runtime 03/02/23 02:12:48.646
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:12:48.724
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:12:48.736
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
STEP: create the container 03/02/23 02:12:48.763
STEP: wait for the container to reach Failed 03/02/23 02:12:48.82
STEP: get the container status 03/02/23 02:12:53.918
STEP: the container should be terminated 03/02/23 02:12:53.932
STEP: the termination message should be set 03/02/23 02:12:53.932
Mar  2 02:12:53.932: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 03/02/23 02:12:53.932
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Mar  2 02:12:54.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9778" for this suite. 03/02/23 02:12:54.103
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":203,"skipped":3811,"failed":0}
------------------------------
• [SLOW TEST] [5.491 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:12:48.645
    Mar  2 02:12:48.645: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename container-runtime 03/02/23 02:12:48.646
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:12:48.724
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:12:48.736
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215
    STEP: create the container 03/02/23 02:12:48.763
    STEP: wait for the container to reach Failed 03/02/23 02:12:48.82
    STEP: get the container status 03/02/23 02:12:53.918
    STEP: the container should be terminated 03/02/23 02:12:53.932
    STEP: the termination message should be set 03/02/23 02:12:53.932
    Mar  2 02:12:53.932: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 03/02/23 02:12:53.932
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Mar  2 02:12:54.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-9778" for this suite. 03/02/23 02:12:54.103
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:12:54.136
Mar  2 02:12:54.137: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename daemonsets 03/02/23 02:12:54.139
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:12:54.239
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:12:54.307
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
Mar  2 02:12:54.560: INFO: Create a RollingUpdate DaemonSet
Mar  2 02:12:54.585: INFO: Check that daemon pods launch on every node of the cluster
Mar  2 02:12:54.630: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 02:12:54.630: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
Mar  2 02:12:55.718: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 02:12:55.718: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
Mar  2 02:12:56.704: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 02:12:56.704: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
Mar  2 02:12:57.669: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar  2 02:12:57.670: INFO: Node 10.132.92.186 is running 0 daemon pod, expected 1
Mar  2 02:12:58.690: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar  2 02:12:58.690: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
Mar  2 02:12:58.690: INFO: Update the DaemonSet to trigger a rollout
Mar  2 02:12:58.925: INFO: Updating DaemonSet daemon-set
Mar  2 02:13:03.077: INFO: Roll back the DaemonSet before rollout is complete
Mar  2 02:13:03.120: INFO: Updating DaemonSet daemon-set
Mar  2 02:13:03.120: INFO: Make sure DaemonSet rollback is complete
Mar  2 02:13:03.142: INFO: Wrong image for pod: daemon-set-pjkrl. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Mar  2 02:13:03.143: INFO: Pod daemon-set-pjkrl is not available
Mar  2 02:13:09.184: INFO: Pod daemon-set-hvhhk is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 03/02/23 02:13:09.237
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6291, will wait for the garbage collector to delete the pods 03/02/23 02:13:09.238
Mar  2 02:13:09.333: INFO: Deleting DaemonSet.extensions daemon-set took: 26.931778ms
Mar  2 02:13:09.434: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.33347ms
Mar  2 02:13:13.349: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 02:13:13.349: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar  2 02:13:13.364: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"117384"},"items":null}

Mar  2 02:13:13.380: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"117384"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Mar  2 02:13:13.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6291" for this suite. 03/02/23 02:13:13.468
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","completed":204,"skipped":3811,"failed":0}
------------------------------
• [SLOW TEST] [19.355 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:12:54.136
    Mar  2 02:12:54.137: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename daemonsets 03/02/23 02:12:54.139
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:12:54.239
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:12:54.307
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:431
    Mar  2 02:12:54.560: INFO: Create a RollingUpdate DaemonSet
    Mar  2 02:12:54.585: INFO: Check that daemon pods launch on every node of the cluster
    Mar  2 02:12:54.630: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 02:12:54.630: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
    Mar  2 02:12:55.718: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 02:12:55.718: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
    Mar  2 02:12:56.704: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 02:12:56.704: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
    Mar  2 02:12:57.669: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar  2 02:12:57.670: INFO: Node 10.132.92.186 is running 0 daemon pod, expected 1
    Mar  2 02:12:58.690: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar  2 02:12:58.690: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    Mar  2 02:12:58.690: INFO: Update the DaemonSet to trigger a rollout
    Mar  2 02:12:58.925: INFO: Updating DaemonSet daemon-set
    Mar  2 02:13:03.077: INFO: Roll back the DaemonSet before rollout is complete
    Mar  2 02:13:03.120: INFO: Updating DaemonSet daemon-set
    Mar  2 02:13:03.120: INFO: Make sure DaemonSet rollback is complete
    Mar  2 02:13:03.142: INFO: Wrong image for pod: daemon-set-pjkrl. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
    Mar  2 02:13:03.143: INFO: Pod daemon-set-pjkrl is not available
    Mar  2 02:13:09.184: INFO: Pod daemon-set-hvhhk is not available
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 03/02/23 02:13:09.237
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6291, will wait for the garbage collector to delete the pods 03/02/23 02:13:09.238
    Mar  2 02:13:09.333: INFO: Deleting DaemonSet.extensions daemon-set took: 26.931778ms
    Mar  2 02:13:09.434: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.33347ms
    Mar  2 02:13:13.349: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 02:13:13.349: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar  2 02:13:13.364: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"117384"},"items":null}

    Mar  2 02:13:13.380: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"117384"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Mar  2 02:13:13.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-6291" for this suite. 03/02/23 02:13:13.468
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:13:13.492
Mar  2 02:13:13.493: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename discovery 03/02/23 02:13:13.494
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:13:13.586
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:13:13.6
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 03/02/23 02:13:13.618
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Mar  2 02:13:14.958: INFO: Checking APIGroup: apiregistration.k8s.io
Mar  2 02:13:14.963: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Mar  2 02:13:14.963: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Mar  2 02:13:14.963: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Mar  2 02:13:14.963: INFO: Checking APIGroup: apps
Mar  2 02:13:14.968: INFO: PreferredVersion.GroupVersion: apps/v1
Mar  2 02:13:14.968: INFO: Versions found [{apps/v1 v1}]
Mar  2 02:13:14.968: INFO: apps/v1 matches apps/v1
Mar  2 02:13:14.968: INFO: Checking APIGroup: events.k8s.io
Mar  2 02:13:14.974: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Mar  2 02:13:14.974: INFO: Versions found [{events.k8s.io/v1 v1}]
Mar  2 02:13:14.974: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Mar  2 02:13:14.974: INFO: Checking APIGroup: authentication.k8s.io
Mar  2 02:13:14.980: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Mar  2 02:13:14.980: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Mar  2 02:13:14.980: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Mar  2 02:13:14.980: INFO: Checking APIGroup: authorization.k8s.io
Mar  2 02:13:14.984: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Mar  2 02:13:14.984: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Mar  2 02:13:14.984: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Mar  2 02:13:14.984: INFO: Checking APIGroup: autoscaling
Mar  2 02:13:14.989: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Mar  2 02:13:14.989: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
Mar  2 02:13:14.989: INFO: autoscaling/v2 matches autoscaling/v2
Mar  2 02:13:14.989: INFO: Checking APIGroup: batch
Mar  2 02:13:14.995: INFO: PreferredVersion.GroupVersion: batch/v1
Mar  2 02:13:14.995: INFO: Versions found [{batch/v1 v1}]
Mar  2 02:13:14.995: INFO: batch/v1 matches batch/v1
Mar  2 02:13:14.995: INFO: Checking APIGroup: certificates.k8s.io
Mar  2 02:13:15.000: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Mar  2 02:13:15.000: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Mar  2 02:13:15.000: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Mar  2 02:13:15.000: INFO: Checking APIGroup: networking.k8s.io
Mar  2 02:13:15.005: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Mar  2 02:13:15.005: INFO: Versions found [{networking.k8s.io/v1 v1}]
Mar  2 02:13:15.005: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Mar  2 02:13:15.005: INFO: Checking APIGroup: policy
Mar  2 02:13:15.011: INFO: PreferredVersion.GroupVersion: policy/v1
Mar  2 02:13:15.011: INFO: Versions found [{policy/v1 v1}]
Mar  2 02:13:15.011: INFO: policy/v1 matches policy/v1
Mar  2 02:13:15.011: INFO: Checking APIGroup: rbac.authorization.k8s.io
Mar  2 02:13:15.016: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Mar  2 02:13:15.016: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Mar  2 02:13:15.016: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Mar  2 02:13:15.016: INFO: Checking APIGroup: storage.k8s.io
Mar  2 02:13:15.023: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Mar  2 02:13:15.023: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Mar  2 02:13:15.023: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Mar  2 02:13:15.023: INFO: Checking APIGroup: admissionregistration.k8s.io
Mar  2 02:13:15.029: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Mar  2 02:13:15.029: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Mar  2 02:13:15.029: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Mar  2 02:13:15.029: INFO: Checking APIGroup: apiextensions.k8s.io
Mar  2 02:13:15.035: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Mar  2 02:13:15.035: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Mar  2 02:13:15.035: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Mar  2 02:13:15.035: INFO: Checking APIGroup: scheduling.k8s.io
Mar  2 02:13:15.041: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Mar  2 02:13:15.041: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Mar  2 02:13:15.041: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Mar  2 02:13:15.041: INFO: Checking APIGroup: coordination.k8s.io
Mar  2 02:13:15.047: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Mar  2 02:13:15.047: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Mar  2 02:13:15.047: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Mar  2 02:13:15.047: INFO: Checking APIGroup: node.k8s.io
Mar  2 02:13:15.056: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Mar  2 02:13:15.056: INFO: Versions found [{node.k8s.io/v1 v1}]
Mar  2 02:13:15.056: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Mar  2 02:13:15.056: INFO: Checking APIGroup: discovery.k8s.io
Mar  2 02:13:15.061: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Mar  2 02:13:15.061: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Mar  2 02:13:15.061: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Mar  2 02:13:15.061: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Mar  2 02:13:15.065: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Mar  2 02:13:15.065: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Mar  2 02:13:15.065: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Mar  2 02:13:15.065: INFO: Checking APIGroup: apps.openshift.io
Mar  2 02:13:15.071: INFO: PreferredVersion.GroupVersion: apps.openshift.io/v1
Mar  2 02:13:15.071: INFO: Versions found [{apps.openshift.io/v1 v1}]
Mar  2 02:13:15.071: INFO: apps.openshift.io/v1 matches apps.openshift.io/v1
Mar  2 02:13:15.071: INFO: Checking APIGroup: authorization.openshift.io
Mar  2 02:13:15.077: INFO: PreferredVersion.GroupVersion: authorization.openshift.io/v1
Mar  2 02:13:15.077: INFO: Versions found [{authorization.openshift.io/v1 v1}]
Mar  2 02:13:15.077: INFO: authorization.openshift.io/v1 matches authorization.openshift.io/v1
Mar  2 02:13:15.077: INFO: Checking APIGroup: build.openshift.io
Mar  2 02:13:15.083: INFO: PreferredVersion.GroupVersion: build.openshift.io/v1
Mar  2 02:13:15.083: INFO: Versions found [{build.openshift.io/v1 v1}]
Mar  2 02:13:15.083: INFO: build.openshift.io/v1 matches build.openshift.io/v1
Mar  2 02:13:15.083: INFO: Checking APIGroup: image.openshift.io
Mar  2 02:13:15.088: INFO: PreferredVersion.GroupVersion: image.openshift.io/v1
Mar  2 02:13:15.088: INFO: Versions found [{image.openshift.io/v1 v1}]
Mar  2 02:13:15.088: INFO: image.openshift.io/v1 matches image.openshift.io/v1
Mar  2 02:13:15.088: INFO: Checking APIGroup: oauth.openshift.io
Mar  2 02:13:15.093: INFO: PreferredVersion.GroupVersion: oauth.openshift.io/v1
Mar  2 02:13:15.093: INFO: Versions found [{oauth.openshift.io/v1 v1}]
Mar  2 02:13:15.093: INFO: oauth.openshift.io/v1 matches oauth.openshift.io/v1
Mar  2 02:13:15.093: INFO: Checking APIGroup: project.openshift.io
Mar  2 02:13:15.099: INFO: PreferredVersion.GroupVersion: project.openshift.io/v1
Mar  2 02:13:15.099: INFO: Versions found [{project.openshift.io/v1 v1}]
Mar  2 02:13:15.099: INFO: project.openshift.io/v1 matches project.openshift.io/v1
Mar  2 02:13:15.099: INFO: Checking APIGroup: quota.openshift.io
Mar  2 02:13:15.105: INFO: PreferredVersion.GroupVersion: quota.openshift.io/v1
Mar  2 02:13:15.105: INFO: Versions found [{quota.openshift.io/v1 v1}]
Mar  2 02:13:15.105: INFO: quota.openshift.io/v1 matches quota.openshift.io/v1
Mar  2 02:13:15.105: INFO: Checking APIGroup: route.openshift.io
Mar  2 02:13:15.111: INFO: PreferredVersion.GroupVersion: route.openshift.io/v1
Mar  2 02:13:15.111: INFO: Versions found [{route.openshift.io/v1 v1}]
Mar  2 02:13:15.111: INFO: route.openshift.io/v1 matches route.openshift.io/v1
Mar  2 02:13:15.111: INFO: Checking APIGroup: security.openshift.io
Mar  2 02:13:15.115: INFO: PreferredVersion.GroupVersion: security.openshift.io/v1
Mar  2 02:13:15.115: INFO: Versions found [{security.openshift.io/v1 v1}]
Mar  2 02:13:15.115: INFO: security.openshift.io/v1 matches security.openshift.io/v1
Mar  2 02:13:15.115: INFO: Checking APIGroup: template.openshift.io
Mar  2 02:13:15.121: INFO: PreferredVersion.GroupVersion: template.openshift.io/v1
Mar  2 02:13:15.121: INFO: Versions found [{template.openshift.io/v1 v1}]
Mar  2 02:13:15.121: INFO: template.openshift.io/v1 matches template.openshift.io/v1
Mar  2 02:13:15.121: INFO: Checking APIGroup: user.openshift.io
Mar  2 02:13:15.127: INFO: PreferredVersion.GroupVersion: user.openshift.io/v1
Mar  2 02:13:15.127: INFO: Versions found [{user.openshift.io/v1 v1}]
Mar  2 02:13:15.127: INFO: user.openshift.io/v1 matches user.openshift.io/v1
Mar  2 02:13:15.127: INFO: Checking APIGroup: packages.operators.coreos.com
Mar  2 02:13:15.133: INFO: PreferredVersion.GroupVersion: packages.operators.coreos.com/v1
Mar  2 02:13:15.133: INFO: Versions found [{packages.operators.coreos.com/v1 v1}]
Mar  2 02:13:15.133: INFO: packages.operators.coreos.com/v1 matches packages.operators.coreos.com/v1
Mar  2 02:13:15.133: INFO: Checking APIGroup: config.openshift.io
Mar  2 02:13:15.138: INFO: PreferredVersion.GroupVersion: config.openshift.io/v1
Mar  2 02:13:15.138: INFO: Versions found [{config.openshift.io/v1 v1}]
Mar  2 02:13:15.139: INFO: config.openshift.io/v1 matches config.openshift.io/v1
Mar  2 02:13:15.139: INFO: Checking APIGroup: operator.openshift.io
Mar  2 02:13:15.143: INFO: PreferredVersion.GroupVersion: operator.openshift.io/v1
Mar  2 02:13:15.143: INFO: Versions found [{operator.openshift.io/v1 v1} {operator.openshift.io/v1alpha1 v1alpha1}]
Mar  2 02:13:15.143: INFO: operator.openshift.io/v1 matches operator.openshift.io/v1
Mar  2 02:13:15.143: INFO: Checking APIGroup: apiserver.openshift.io
Mar  2 02:13:15.149: INFO: PreferredVersion.GroupVersion: apiserver.openshift.io/v1
Mar  2 02:13:15.149: INFO: Versions found [{apiserver.openshift.io/v1 v1}]
Mar  2 02:13:15.149: INFO: apiserver.openshift.io/v1 matches apiserver.openshift.io/v1
Mar  2 02:13:15.149: INFO: Checking APIGroup: cloudcredential.openshift.io
Mar  2 02:13:15.154: INFO: PreferredVersion.GroupVersion: cloudcredential.openshift.io/v1
Mar  2 02:13:15.154: INFO: Versions found [{cloudcredential.openshift.io/v1 v1}]
Mar  2 02:13:15.154: INFO: cloudcredential.openshift.io/v1 matches cloudcredential.openshift.io/v1
Mar  2 02:13:15.154: INFO: Checking APIGroup: console.openshift.io
Mar  2 02:13:15.171: INFO: PreferredVersion.GroupVersion: console.openshift.io/v1
Mar  2 02:13:15.171: INFO: Versions found [{console.openshift.io/v1 v1} {console.openshift.io/v1alpha1 v1alpha1}]
Mar  2 02:13:15.171: INFO: console.openshift.io/v1 matches console.openshift.io/v1
Mar  2 02:13:15.171: INFO: Checking APIGroup: crd.projectcalico.org
Mar  2 02:13:15.177: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Mar  2 02:13:15.177: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Mar  2 02:13:15.177: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Mar  2 02:13:15.177: INFO: Checking APIGroup: imageregistry.operator.openshift.io
Mar  2 02:13:15.183: INFO: PreferredVersion.GroupVersion: imageregistry.operator.openshift.io/v1
Mar  2 02:13:15.183: INFO: Versions found [{imageregistry.operator.openshift.io/v1 v1}]
Mar  2 02:13:15.183: INFO: imageregistry.operator.openshift.io/v1 matches imageregistry.operator.openshift.io/v1
Mar  2 02:13:15.183: INFO: Checking APIGroup: ingress.operator.openshift.io
Mar  2 02:13:15.188: INFO: PreferredVersion.GroupVersion: ingress.operator.openshift.io/v1
Mar  2 02:13:15.188: INFO: Versions found [{ingress.operator.openshift.io/v1 v1}]
Mar  2 02:13:15.188: INFO: ingress.operator.openshift.io/v1 matches ingress.operator.openshift.io/v1
Mar  2 02:13:15.188: INFO: Checking APIGroup: k8s.cni.cncf.io
Mar  2 02:13:15.194: INFO: PreferredVersion.GroupVersion: k8s.cni.cncf.io/v1
Mar  2 02:13:15.194: INFO: Versions found [{k8s.cni.cncf.io/v1 v1}]
Mar  2 02:13:15.194: INFO: k8s.cni.cncf.io/v1 matches k8s.cni.cncf.io/v1
Mar  2 02:13:15.194: INFO: Checking APIGroup: machineconfiguration.openshift.io
Mar  2 02:13:15.202: INFO: PreferredVersion.GroupVersion: machineconfiguration.openshift.io/v1
Mar  2 02:13:15.202: INFO: Versions found [{machineconfiguration.openshift.io/v1 v1}]
Mar  2 02:13:15.202: INFO: machineconfiguration.openshift.io/v1 matches machineconfiguration.openshift.io/v1
Mar  2 02:13:15.202: INFO: Checking APIGroup: monitoring.coreos.com
Mar  2 02:13:15.208: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
Mar  2 02:13:15.209: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1beta1 v1beta1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
Mar  2 02:13:15.209: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
Mar  2 02:13:15.209: INFO: Checking APIGroup: network.operator.openshift.io
Mar  2 02:13:15.218: INFO: PreferredVersion.GroupVersion: network.operator.openshift.io/v1
Mar  2 02:13:15.218: INFO: Versions found [{network.operator.openshift.io/v1 v1}]
Mar  2 02:13:15.219: INFO: network.operator.openshift.io/v1 matches network.operator.openshift.io/v1
Mar  2 02:13:15.219: INFO: Checking APIGroup: operator.tigera.io
Mar  2 02:13:15.247: INFO: PreferredVersion.GroupVersion: operator.tigera.io/v1
Mar  2 02:13:15.247: INFO: Versions found [{operator.tigera.io/v1 v1}]
Mar  2 02:13:15.247: INFO: operator.tigera.io/v1 matches operator.tigera.io/v1
Mar  2 02:13:15.247: INFO: Checking APIGroup: operators.coreos.com
Mar  2 02:13:15.271: INFO: PreferredVersion.GroupVersion: operators.coreos.com/v2
Mar  2 02:13:15.271: INFO: Versions found [{operators.coreos.com/v2 v2} {operators.coreos.com/v1 v1} {operators.coreos.com/v1alpha2 v1alpha2} {operators.coreos.com/v1alpha1 v1alpha1}]
Mar  2 02:13:15.271: INFO: operators.coreos.com/v2 matches operators.coreos.com/v2
Mar  2 02:13:15.271: INFO: Checking APIGroup: performance.openshift.io
Mar  2 02:13:15.277: INFO: PreferredVersion.GroupVersion: performance.openshift.io/v2
Mar  2 02:13:15.277: INFO: Versions found [{performance.openshift.io/v2 v2} {performance.openshift.io/v1 v1} {performance.openshift.io/v1alpha1 v1alpha1}]
Mar  2 02:13:15.277: INFO: performance.openshift.io/v2 matches performance.openshift.io/v2
Mar  2 02:13:15.277: INFO: Checking APIGroup: samples.operator.openshift.io
Mar  2 02:13:15.289: INFO: PreferredVersion.GroupVersion: samples.operator.openshift.io/v1
Mar  2 02:13:15.289: INFO: Versions found [{samples.operator.openshift.io/v1 v1}]
Mar  2 02:13:15.289: INFO: samples.operator.openshift.io/v1 matches samples.operator.openshift.io/v1
Mar  2 02:13:15.289: INFO: Checking APIGroup: security.internal.openshift.io
Mar  2 02:13:15.295: INFO: PreferredVersion.GroupVersion: security.internal.openshift.io/v1
Mar  2 02:13:15.295: INFO: Versions found [{security.internal.openshift.io/v1 v1}]
Mar  2 02:13:15.295: INFO: security.internal.openshift.io/v1 matches security.internal.openshift.io/v1
Mar  2 02:13:15.295: INFO: Checking APIGroup: snapshot.storage.k8s.io
Mar  2 02:13:15.302: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Mar  2 02:13:15.302: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1}]
Mar  2 02:13:15.303: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
Mar  2 02:13:15.303: INFO: Checking APIGroup: tuned.openshift.io
Mar  2 02:13:15.309: INFO: PreferredVersion.GroupVersion: tuned.openshift.io/v1
Mar  2 02:13:15.309: INFO: Versions found [{tuned.openshift.io/v1 v1}]
Mar  2 02:13:15.309: INFO: tuned.openshift.io/v1 matches tuned.openshift.io/v1
Mar  2 02:13:15.309: INFO: Checking APIGroup: controlplane.operator.openshift.io
Mar  2 02:13:15.316: INFO: PreferredVersion.GroupVersion: controlplane.operator.openshift.io/v1alpha1
Mar  2 02:13:15.316: INFO: Versions found [{controlplane.operator.openshift.io/v1alpha1 v1alpha1}]
Mar  2 02:13:15.316: INFO: controlplane.operator.openshift.io/v1alpha1 matches controlplane.operator.openshift.io/v1alpha1
Mar  2 02:13:15.317: INFO: Checking APIGroup: ibm.com
Mar  2 02:13:15.327: INFO: PreferredVersion.GroupVersion: ibm.com/v1alpha1
Mar  2 02:13:15.327: INFO: Versions found [{ibm.com/v1alpha1 v1alpha1}]
Mar  2 02:13:15.327: INFO: ibm.com/v1alpha1 matches ibm.com/v1alpha1
Mar  2 02:13:15.327: INFO: Checking APIGroup: migration.k8s.io
Mar  2 02:13:15.332: INFO: PreferredVersion.GroupVersion: migration.k8s.io/v1alpha1
Mar  2 02:13:15.332: INFO: Versions found [{migration.k8s.io/v1alpha1 v1alpha1}]
Mar  2 02:13:15.332: INFO: migration.k8s.io/v1alpha1 matches migration.k8s.io/v1alpha1
Mar  2 02:13:15.332: INFO: Checking APIGroup: whereabouts.cni.cncf.io
Mar  2 02:13:15.338: INFO: PreferredVersion.GroupVersion: whereabouts.cni.cncf.io/v1alpha1
Mar  2 02:13:15.338: INFO: Versions found [{whereabouts.cni.cncf.io/v1alpha1 v1alpha1}]
Mar  2 02:13:15.338: INFO: whereabouts.cni.cncf.io/v1alpha1 matches whereabouts.cni.cncf.io/v1alpha1
Mar  2 02:13:15.338: INFO: Checking APIGroup: helm.openshift.io
Mar  2 02:13:15.343: INFO: PreferredVersion.GroupVersion: helm.openshift.io/v1beta1
Mar  2 02:13:15.343: INFO: Versions found [{helm.openshift.io/v1beta1 v1beta1}]
Mar  2 02:13:15.343: INFO: helm.openshift.io/v1beta1 matches helm.openshift.io/v1beta1
Mar  2 02:13:15.343: INFO: Checking APIGroup: metrics.k8s.io
Mar  2 02:13:15.349: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Mar  2 02:13:15.349: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Mar  2 02:13:15.349: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:187
Mar  2 02:13:15.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-461" for this suite. 03/02/23 02:13:15.377
{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","completed":205,"skipped":3813,"failed":0}
------------------------------
• [1.943 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:13:13.492
    Mar  2 02:13:13.493: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename discovery 03/02/23 02:13:13.494
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:13:13.586
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:13:13.6
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 03/02/23 02:13:13.618
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Mar  2 02:13:14.958: INFO: Checking APIGroup: apiregistration.k8s.io
    Mar  2 02:13:14.963: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Mar  2 02:13:14.963: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Mar  2 02:13:14.963: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Mar  2 02:13:14.963: INFO: Checking APIGroup: apps
    Mar  2 02:13:14.968: INFO: PreferredVersion.GroupVersion: apps/v1
    Mar  2 02:13:14.968: INFO: Versions found [{apps/v1 v1}]
    Mar  2 02:13:14.968: INFO: apps/v1 matches apps/v1
    Mar  2 02:13:14.968: INFO: Checking APIGroup: events.k8s.io
    Mar  2 02:13:14.974: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Mar  2 02:13:14.974: INFO: Versions found [{events.k8s.io/v1 v1}]
    Mar  2 02:13:14.974: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Mar  2 02:13:14.974: INFO: Checking APIGroup: authentication.k8s.io
    Mar  2 02:13:14.980: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Mar  2 02:13:14.980: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Mar  2 02:13:14.980: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Mar  2 02:13:14.980: INFO: Checking APIGroup: authorization.k8s.io
    Mar  2 02:13:14.984: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Mar  2 02:13:14.984: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Mar  2 02:13:14.984: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Mar  2 02:13:14.984: INFO: Checking APIGroup: autoscaling
    Mar  2 02:13:14.989: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Mar  2 02:13:14.989: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
    Mar  2 02:13:14.989: INFO: autoscaling/v2 matches autoscaling/v2
    Mar  2 02:13:14.989: INFO: Checking APIGroup: batch
    Mar  2 02:13:14.995: INFO: PreferredVersion.GroupVersion: batch/v1
    Mar  2 02:13:14.995: INFO: Versions found [{batch/v1 v1}]
    Mar  2 02:13:14.995: INFO: batch/v1 matches batch/v1
    Mar  2 02:13:14.995: INFO: Checking APIGroup: certificates.k8s.io
    Mar  2 02:13:15.000: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Mar  2 02:13:15.000: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Mar  2 02:13:15.000: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Mar  2 02:13:15.000: INFO: Checking APIGroup: networking.k8s.io
    Mar  2 02:13:15.005: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Mar  2 02:13:15.005: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Mar  2 02:13:15.005: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Mar  2 02:13:15.005: INFO: Checking APIGroup: policy
    Mar  2 02:13:15.011: INFO: PreferredVersion.GroupVersion: policy/v1
    Mar  2 02:13:15.011: INFO: Versions found [{policy/v1 v1}]
    Mar  2 02:13:15.011: INFO: policy/v1 matches policy/v1
    Mar  2 02:13:15.011: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Mar  2 02:13:15.016: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Mar  2 02:13:15.016: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Mar  2 02:13:15.016: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Mar  2 02:13:15.016: INFO: Checking APIGroup: storage.k8s.io
    Mar  2 02:13:15.023: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Mar  2 02:13:15.023: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Mar  2 02:13:15.023: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Mar  2 02:13:15.023: INFO: Checking APIGroup: admissionregistration.k8s.io
    Mar  2 02:13:15.029: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Mar  2 02:13:15.029: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Mar  2 02:13:15.029: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Mar  2 02:13:15.029: INFO: Checking APIGroup: apiextensions.k8s.io
    Mar  2 02:13:15.035: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Mar  2 02:13:15.035: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Mar  2 02:13:15.035: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Mar  2 02:13:15.035: INFO: Checking APIGroup: scheduling.k8s.io
    Mar  2 02:13:15.041: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Mar  2 02:13:15.041: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Mar  2 02:13:15.041: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Mar  2 02:13:15.041: INFO: Checking APIGroup: coordination.k8s.io
    Mar  2 02:13:15.047: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Mar  2 02:13:15.047: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Mar  2 02:13:15.047: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Mar  2 02:13:15.047: INFO: Checking APIGroup: node.k8s.io
    Mar  2 02:13:15.056: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Mar  2 02:13:15.056: INFO: Versions found [{node.k8s.io/v1 v1}]
    Mar  2 02:13:15.056: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Mar  2 02:13:15.056: INFO: Checking APIGroup: discovery.k8s.io
    Mar  2 02:13:15.061: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Mar  2 02:13:15.061: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Mar  2 02:13:15.061: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Mar  2 02:13:15.061: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Mar  2 02:13:15.065: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
    Mar  2 02:13:15.065: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
    Mar  2 02:13:15.065: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
    Mar  2 02:13:15.065: INFO: Checking APIGroup: apps.openshift.io
    Mar  2 02:13:15.071: INFO: PreferredVersion.GroupVersion: apps.openshift.io/v1
    Mar  2 02:13:15.071: INFO: Versions found [{apps.openshift.io/v1 v1}]
    Mar  2 02:13:15.071: INFO: apps.openshift.io/v1 matches apps.openshift.io/v1
    Mar  2 02:13:15.071: INFO: Checking APIGroup: authorization.openshift.io
    Mar  2 02:13:15.077: INFO: PreferredVersion.GroupVersion: authorization.openshift.io/v1
    Mar  2 02:13:15.077: INFO: Versions found [{authorization.openshift.io/v1 v1}]
    Mar  2 02:13:15.077: INFO: authorization.openshift.io/v1 matches authorization.openshift.io/v1
    Mar  2 02:13:15.077: INFO: Checking APIGroup: build.openshift.io
    Mar  2 02:13:15.083: INFO: PreferredVersion.GroupVersion: build.openshift.io/v1
    Mar  2 02:13:15.083: INFO: Versions found [{build.openshift.io/v1 v1}]
    Mar  2 02:13:15.083: INFO: build.openshift.io/v1 matches build.openshift.io/v1
    Mar  2 02:13:15.083: INFO: Checking APIGroup: image.openshift.io
    Mar  2 02:13:15.088: INFO: PreferredVersion.GroupVersion: image.openshift.io/v1
    Mar  2 02:13:15.088: INFO: Versions found [{image.openshift.io/v1 v1}]
    Mar  2 02:13:15.088: INFO: image.openshift.io/v1 matches image.openshift.io/v1
    Mar  2 02:13:15.088: INFO: Checking APIGroup: oauth.openshift.io
    Mar  2 02:13:15.093: INFO: PreferredVersion.GroupVersion: oauth.openshift.io/v1
    Mar  2 02:13:15.093: INFO: Versions found [{oauth.openshift.io/v1 v1}]
    Mar  2 02:13:15.093: INFO: oauth.openshift.io/v1 matches oauth.openshift.io/v1
    Mar  2 02:13:15.093: INFO: Checking APIGroup: project.openshift.io
    Mar  2 02:13:15.099: INFO: PreferredVersion.GroupVersion: project.openshift.io/v1
    Mar  2 02:13:15.099: INFO: Versions found [{project.openshift.io/v1 v1}]
    Mar  2 02:13:15.099: INFO: project.openshift.io/v1 matches project.openshift.io/v1
    Mar  2 02:13:15.099: INFO: Checking APIGroup: quota.openshift.io
    Mar  2 02:13:15.105: INFO: PreferredVersion.GroupVersion: quota.openshift.io/v1
    Mar  2 02:13:15.105: INFO: Versions found [{quota.openshift.io/v1 v1}]
    Mar  2 02:13:15.105: INFO: quota.openshift.io/v1 matches quota.openshift.io/v1
    Mar  2 02:13:15.105: INFO: Checking APIGroup: route.openshift.io
    Mar  2 02:13:15.111: INFO: PreferredVersion.GroupVersion: route.openshift.io/v1
    Mar  2 02:13:15.111: INFO: Versions found [{route.openshift.io/v1 v1}]
    Mar  2 02:13:15.111: INFO: route.openshift.io/v1 matches route.openshift.io/v1
    Mar  2 02:13:15.111: INFO: Checking APIGroup: security.openshift.io
    Mar  2 02:13:15.115: INFO: PreferredVersion.GroupVersion: security.openshift.io/v1
    Mar  2 02:13:15.115: INFO: Versions found [{security.openshift.io/v1 v1}]
    Mar  2 02:13:15.115: INFO: security.openshift.io/v1 matches security.openshift.io/v1
    Mar  2 02:13:15.115: INFO: Checking APIGroup: template.openshift.io
    Mar  2 02:13:15.121: INFO: PreferredVersion.GroupVersion: template.openshift.io/v1
    Mar  2 02:13:15.121: INFO: Versions found [{template.openshift.io/v1 v1}]
    Mar  2 02:13:15.121: INFO: template.openshift.io/v1 matches template.openshift.io/v1
    Mar  2 02:13:15.121: INFO: Checking APIGroup: user.openshift.io
    Mar  2 02:13:15.127: INFO: PreferredVersion.GroupVersion: user.openshift.io/v1
    Mar  2 02:13:15.127: INFO: Versions found [{user.openshift.io/v1 v1}]
    Mar  2 02:13:15.127: INFO: user.openshift.io/v1 matches user.openshift.io/v1
    Mar  2 02:13:15.127: INFO: Checking APIGroup: packages.operators.coreos.com
    Mar  2 02:13:15.133: INFO: PreferredVersion.GroupVersion: packages.operators.coreos.com/v1
    Mar  2 02:13:15.133: INFO: Versions found [{packages.operators.coreos.com/v1 v1}]
    Mar  2 02:13:15.133: INFO: packages.operators.coreos.com/v1 matches packages.operators.coreos.com/v1
    Mar  2 02:13:15.133: INFO: Checking APIGroup: config.openshift.io
    Mar  2 02:13:15.138: INFO: PreferredVersion.GroupVersion: config.openshift.io/v1
    Mar  2 02:13:15.138: INFO: Versions found [{config.openshift.io/v1 v1}]
    Mar  2 02:13:15.139: INFO: config.openshift.io/v1 matches config.openshift.io/v1
    Mar  2 02:13:15.139: INFO: Checking APIGroup: operator.openshift.io
    Mar  2 02:13:15.143: INFO: PreferredVersion.GroupVersion: operator.openshift.io/v1
    Mar  2 02:13:15.143: INFO: Versions found [{operator.openshift.io/v1 v1} {operator.openshift.io/v1alpha1 v1alpha1}]
    Mar  2 02:13:15.143: INFO: operator.openshift.io/v1 matches operator.openshift.io/v1
    Mar  2 02:13:15.143: INFO: Checking APIGroup: apiserver.openshift.io
    Mar  2 02:13:15.149: INFO: PreferredVersion.GroupVersion: apiserver.openshift.io/v1
    Mar  2 02:13:15.149: INFO: Versions found [{apiserver.openshift.io/v1 v1}]
    Mar  2 02:13:15.149: INFO: apiserver.openshift.io/v1 matches apiserver.openshift.io/v1
    Mar  2 02:13:15.149: INFO: Checking APIGroup: cloudcredential.openshift.io
    Mar  2 02:13:15.154: INFO: PreferredVersion.GroupVersion: cloudcredential.openshift.io/v1
    Mar  2 02:13:15.154: INFO: Versions found [{cloudcredential.openshift.io/v1 v1}]
    Mar  2 02:13:15.154: INFO: cloudcredential.openshift.io/v1 matches cloudcredential.openshift.io/v1
    Mar  2 02:13:15.154: INFO: Checking APIGroup: console.openshift.io
    Mar  2 02:13:15.171: INFO: PreferredVersion.GroupVersion: console.openshift.io/v1
    Mar  2 02:13:15.171: INFO: Versions found [{console.openshift.io/v1 v1} {console.openshift.io/v1alpha1 v1alpha1}]
    Mar  2 02:13:15.171: INFO: console.openshift.io/v1 matches console.openshift.io/v1
    Mar  2 02:13:15.171: INFO: Checking APIGroup: crd.projectcalico.org
    Mar  2 02:13:15.177: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    Mar  2 02:13:15.177: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    Mar  2 02:13:15.177: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    Mar  2 02:13:15.177: INFO: Checking APIGroup: imageregistry.operator.openshift.io
    Mar  2 02:13:15.183: INFO: PreferredVersion.GroupVersion: imageregistry.operator.openshift.io/v1
    Mar  2 02:13:15.183: INFO: Versions found [{imageregistry.operator.openshift.io/v1 v1}]
    Mar  2 02:13:15.183: INFO: imageregistry.operator.openshift.io/v1 matches imageregistry.operator.openshift.io/v1
    Mar  2 02:13:15.183: INFO: Checking APIGroup: ingress.operator.openshift.io
    Mar  2 02:13:15.188: INFO: PreferredVersion.GroupVersion: ingress.operator.openshift.io/v1
    Mar  2 02:13:15.188: INFO: Versions found [{ingress.operator.openshift.io/v1 v1}]
    Mar  2 02:13:15.188: INFO: ingress.operator.openshift.io/v1 matches ingress.operator.openshift.io/v1
    Mar  2 02:13:15.188: INFO: Checking APIGroup: k8s.cni.cncf.io
    Mar  2 02:13:15.194: INFO: PreferredVersion.GroupVersion: k8s.cni.cncf.io/v1
    Mar  2 02:13:15.194: INFO: Versions found [{k8s.cni.cncf.io/v1 v1}]
    Mar  2 02:13:15.194: INFO: k8s.cni.cncf.io/v1 matches k8s.cni.cncf.io/v1
    Mar  2 02:13:15.194: INFO: Checking APIGroup: machineconfiguration.openshift.io
    Mar  2 02:13:15.202: INFO: PreferredVersion.GroupVersion: machineconfiguration.openshift.io/v1
    Mar  2 02:13:15.202: INFO: Versions found [{machineconfiguration.openshift.io/v1 v1}]
    Mar  2 02:13:15.202: INFO: machineconfiguration.openshift.io/v1 matches machineconfiguration.openshift.io/v1
    Mar  2 02:13:15.202: INFO: Checking APIGroup: monitoring.coreos.com
    Mar  2 02:13:15.208: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
    Mar  2 02:13:15.209: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1beta1 v1beta1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
    Mar  2 02:13:15.209: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
    Mar  2 02:13:15.209: INFO: Checking APIGroup: network.operator.openshift.io
    Mar  2 02:13:15.218: INFO: PreferredVersion.GroupVersion: network.operator.openshift.io/v1
    Mar  2 02:13:15.218: INFO: Versions found [{network.operator.openshift.io/v1 v1}]
    Mar  2 02:13:15.219: INFO: network.operator.openshift.io/v1 matches network.operator.openshift.io/v1
    Mar  2 02:13:15.219: INFO: Checking APIGroup: operator.tigera.io
    Mar  2 02:13:15.247: INFO: PreferredVersion.GroupVersion: operator.tigera.io/v1
    Mar  2 02:13:15.247: INFO: Versions found [{operator.tigera.io/v1 v1}]
    Mar  2 02:13:15.247: INFO: operator.tigera.io/v1 matches operator.tigera.io/v1
    Mar  2 02:13:15.247: INFO: Checking APIGroup: operators.coreos.com
    Mar  2 02:13:15.271: INFO: PreferredVersion.GroupVersion: operators.coreos.com/v2
    Mar  2 02:13:15.271: INFO: Versions found [{operators.coreos.com/v2 v2} {operators.coreos.com/v1 v1} {operators.coreos.com/v1alpha2 v1alpha2} {operators.coreos.com/v1alpha1 v1alpha1}]
    Mar  2 02:13:15.271: INFO: operators.coreos.com/v2 matches operators.coreos.com/v2
    Mar  2 02:13:15.271: INFO: Checking APIGroup: performance.openshift.io
    Mar  2 02:13:15.277: INFO: PreferredVersion.GroupVersion: performance.openshift.io/v2
    Mar  2 02:13:15.277: INFO: Versions found [{performance.openshift.io/v2 v2} {performance.openshift.io/v1 v1} {performance.openshift.io/v1alpha1 v1alpha1}]
    Mar  2 02:13:15.277: INFO: performance.openshift.io/v2 matches performance.openshift.io/v2
    Mar  2 02:13:15.277: INFO: Checking APIGroup: samples.operator.openshift.io
    Mar  2 02:13:15.289: INFO: PreferredVersion.GroupVersion: samples.operator.openshift.io/v1
    Mar  2 02:13:15.289: INFO: Versions found [{samples.operator.openshift.io/v1 v1}]
    Mar  2 02:13:15.289: INFO: samples.operator.openshift.io/v1 matches samples.operator.openshift.io/v1
    Mar  2 02:13:15.289: INFO: Checking APIGroup: security.internal.openshift.io
    Mar  2 02:13:15.295: INFO: PreferredVersion.GroupVersion: security.internal.openshift.io/v1
    Mar  2 02:13:15.295: INFO: Versions found [{security.internal.openshift.io/v1 v1}]
    Mar  2 02:13:15.295: INFO: security.internal.openshift.io/v1 matches security.internal.openshift.io/v1
    Mar  2 02:13:15.295: INFO: Checking APIGroup: snapshot.storage.k8s.io
    Mar  2 02:13:15.302: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
    Mar  2 02:13:15.302: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1}]
    Mar  2 02:13:15.303: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
    Mar  2 02:13:15.303: INFO: Checking APIGroup: tuned.openshift.io
    Mar  2 02:13:15.309: INFO: PreferredVersion.GroupVersion: tuned.openshift.io/v1
    Mar  2 02:13:15.309: INFO: Versions found [{tuned.openshift.io/v1 v1}]
    Mar  2 02:13:15.309: INFO: tuned.openshift.io/v1 matches tuned.openshift.io/v1
    Mar  2 02:13:15.309: INFO: Checking APIGroup: controlplane.operator.openshift.io
    Mar  2 02:13:15.316: INFO: PreferredVersion.GroupVersion: controlplane.operator.openshift.io/v1alpha1
    Mar  2 02:13:15.316: INFO: Versions found [{controlplane.operator.openshift.io/v1alpha1 v1alpha1}]
    Mar  2 02:13:15.316: INFO: controlplane.operator.openshift.io/v1alpha1 matches controlplane.operator.openshift.io/v1alpha1
    Mar  2 02:13:15.317: INFO: Checking APIGroup: ibm.com
    Mar  2 02:13:15.327: INFO: PreferredVersion.GroupVersion: ibm.com/v1alpha1
    Mar  2 02:13:15.327: INFO: Versions found [{ibm.com/v1alpha1 v1alpha1}]
    Mar  2 02:13:15.327: INFO: ibm.com/v1alpha1 matches ibm.com/v1alpha1
    Mar  2 02:13:15.327: INFO: Checking APIGroup: migration.k8s.io
    Mar  2 02:13:15.332: INFO: PreferredVersion.GroupVersion: migration.k8s.io/v1alpha1
    Mar  2 02:13:15.332: INFO: Versions found [{migration.k8s.io/v1alpha1 v1alpha1}]
    Mar  2 02:13:15.332: INFO: migration.k8s.io/v1alpha1 matches migration.k8s.io/v1alpha1
    Mar  2 02:13:15.332: INFO: Checking APIGroup: whereabouts.cni.cncf.io
    Mar  2 02:13:15.338: INFO: PreferredVersion.GroupVersion: whereabouts.cni.cncf.io/v1alpha1
    Mar  2 02:13:15.338: INFO: Versions found [{whereabouts.cni.cncf.io/v1alpha1 v1alpha1}]
    Mar  2 02:13:15.338: INFO: whereabouts.cni.cncf.io/v1alpha1 matches whereabouts.cni.cncf.io/v1alpha1
    Mar  2 02:13:15.338: INFO: Checking APIGroup: helm.openshift.io
    Mar  2 02:13:15.343: INFO: PreferredVersion.GroupVersion: helm.openshift.io/v1beta1
    Mar  2 02:13:15.343: INFO: Versions found [{helm.openshift.io/v1beta1 v1beta1}]
    Mar  2 02:13:15.343: INFO: helm.openshift.io/v1beta1 matches helm.openshift.io/v1beta1
    Mar  2 02:13:15.343: INFO: Checking APIGroup: metrics.k8s.io
    Mar  2 02:13:15.349: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
    Mar  2 02:13:15.349: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
    Mar  2 02:13:15.349: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:187
    Mar  2 02:13:15.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "discovery-461" for this suite. 03/02/23 02:13:15.377
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:13:15.436
Mar  2 02:13:15.437: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename containers 03/02/23 02:13:15.438
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:13:15.521
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:13:15.542
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
STEP: Creating a pod to test override all 03/02/23 02:13:15.558
Mar  2 02:13:15.679: INFO: Waiting up to 5m0s for pod "client-containers-3173776c-0784-4b4e-91b4-88ac10682708" in namespace "containers-9366" to be "Succeeded or Failed"
Mar  2 02:13:15.739: INFO: Pod "client-containers-3173776c-0784-4b4e-91b4-88ac10682708": Phase="Pending", Reason="", readiness=false. Elapsed: 59.562685ms
Mar  2 02:13:17.765: INFO: Pod "client-containers-3173776c-0784-4b4e-91b4-88ac10682708": Phase="Pending", Reason="", readiness=false. Elapsed: 2.085638227s
Mar  2 02:13:19.757: INFO: Pod "client-containers-3173776c-0784-4b4e-91b4-88ac10682708": Phase="Pending", Reason="", readiness=false. Elapsed: 4.077552367s
Mar  2 02:13:21.758: INFO: Pod "client-containers-3173776c-0784-4b4e-91b4-88ac10682708": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.079147408s
STEP: Saw pod success 03/02/23 02:13:21.758
Mar  2 02:13:21.759: INFO: Pod "client-containers-3173776c-0784-4b4e-91b4-88ac10682708" satisfied condition "Succeeded or Failed"
Mar  2 02:13:21.773: INFO: Trying to get logs from node 10.132.92.143 pod client-containers-3173776c-0784-4b4e-91b4-88ac10682708 container agnhost-container: <nil>
STEP: delete the pod 03/02/23 02:13:21.843
Mar  2 02:13:21.940: INFO: Waiting for pod client-containers-3173776c-0784-4b4e-91b4-88ac10682708 to disappear
Mar  2 02:13:21.960: INFO: Pod client-containers-3173776c-0784-4b4e-91b4-88ac10682708 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Mar  2 02:13:21.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9366" for this suite. 03/02/23 02:13:21.989
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","completed":206,"skipped":3824,"failed":0}
------------------------------
• [SLOW TEST] [6.626 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:13:15.436
    Mar  2 02:13:15.437: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename containers 03/02/23 02:13:15.438
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:13:15.521
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:13:15.542
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:86
    STEP: Creating a pod to test override all 03/02/23 02:13:15.558
    Mar  2 02:13:15.679: INFO: Waiting up to 5m0s for pod "client-containers-3173776c-0784-4b4e-91b4-88ac10682708" in namespace "containers-9366" to be "Succeeded or Failed"
    Mar  2 02:13:15.739: INFO: Pod "client-containers-3173776c-0784-4b4e-91b4-88ac10682708": Phase="Pending", Reason="", readiness=false. Elapsed: 59.562685ms
    Mar  2 02:13:17.765: INFO: Pod "client-containers-3173776c-0784-4b4e-91b4-88ac10682708": Phase="Pending", Reason="", readiness=false. Elapsed: 2.085638227s
    Mar  2 02:13:19.757: INFO: Pod "client-containers-3173776c-0784-4b4e-91b4-88ac10682708": Phase="Pending", Reason="", readiness=false. Elapsed: 4.077552367s
    Mar  2 02:13:21.758: INFO: Pod "client-containers-3173776c-0784-4b4e-91b4-88ac10682708": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.079147408s
    STEP: Saw pod success 03/02/23 02:13:21.758
    Mar  2 02:13:21.759: INFO: Pod "client-containers-3173776c-0784-4b4e-91b4-88ac10682708" satisfied condition "Succeeded or Failed"
    Mar  2 02:13:21.773: INFO: Trying to get logs from node 10.132.92.143 pod client-containers-3173776c-0784-4b4e-91b4-88ac10682708 container agnhost-container: <nil>
    STEP: delete the pod 03/02/23 02:13:21.843
    Mar  2 02:13:21.940: INFO: Waiting for pod client-containers-3173776c-0784-4b4e-91b4-88ac10682708 to disappear
    Mar  2 02:13:21.960: INFO: Pod client-containers-3173776c-0784-4b4e-91b4-88ac10682708 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Mar  2 02:13:21.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-9366" for this suite. 03/02/23 02:13:21.989
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:13:22.064
Mar  2 02:13:22.064: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename projected 03/02/23 02:13:22.065
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:13:22.158
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:13:22.175
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
STEP: Creating a pod to test downward API volume plugin 03/02/23 02:13:22.196
Mar  2 02:13:22.298: INFO: Waiting up to 5m0s for pod "downwardapi-volume-79f3c6f5-17a8-421a-8366-6cf9d81b2165" in namespace "projected-1851" to be "Succeeded or Failed"
Mar  2 02:13:22.319: INFO: Pod "downwardapi-volume-79f3c6f5-17a8-421a-8366-6cf9d81b2165": Phase="Pending", Reason="", readiness=false. Elapsed: 20.633832ms
Mar  2 02:13:24.364: INFO: Pod "downwardapi-volume-79f3c6f5-17a8-421a-8366-6cf9d81b2165": Phase="Pending", Reason="", readiness=false. Elapsed: 2.065895148s
Mar  2 02:13:26.344: INFO: Pod "downwardapi-volume-79f3c6f5-17a8-421a-8366-6cf9d81b2165": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04601254s
STEP: Saw pod success 03/02/23 02:13:26.345
Mar  2 02:13:26.345: INFO: Pod "downwardapi-volume-79f3c6f5-17a8-421a-8366-6cf9d81b2165" satisfied condition "Succeeded or Failed"
Mar  2 02:13:26.367: INFO: Trying to get logs from node 10.132.92.143 pod downwardapi-volume-79f3c6f5-17a8-421a-8366-6cf9d81b2165 container client-container: <nil>
STEP: delete the pod 03/02/23 02:13:26.447
Mar  2 02:13:26.521: INFO: Waiting for pod downwardapi-volume-79f3c6f5-17a8-421a-8366-6cf9d81b2165 to disappear
Mar  2 02:13:26.536: INFO: Pod downwardapi-volume-79f3c6f5-17a8-421a-8366-6cf9d81b2165 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar  2 02:13:26.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1851" for this suite. 03/02/23 02:13:26.561
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","completed":207,"skipped":3846,"failed":0}
------------------------------
• [4.522 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:13:22.064
    Mar  2 02:13:22.064: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename projected 03/02/23 02:13:22.065
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:13:22.158
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:13:22.175
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:234
    STEP: Creating a pod to test downward API volume plugin 03/02/23 02:13:22.196
    Mar  2 02:13:22.298: INFO: Waiting up to 5m0s for pod "downwardapi-volume-79f3c6f5-17a8-421a-8366-6cf9d81b2165" in namespace "projected-1851" to be "Succeeded or Failed"
    Mar  2 02:13:22.319: INFO: Pod "downwardapi-volume-79f3c6f5-17a8-421a-8366-6cf9d81b2165": Phase="Pending", Reason="", readiness=false. Elapsed: 20.633832ms
    Mar  2 02:13:24.364: INFO: Pod "downwardapi-volume-79f3c6f5-17a8-421a-8366-6cf9d81b2165": Phase="Pending", Reason="", readiness=false. Elapsed: 2.065895148s
    Mar  2 02:13:26.344: INFO: Pod "downwardapi-volume-79f3c6f5-17a8-421a-8366-6cf9d81b2165": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04601254s
    STEP: Saw pod success 03/02/23 02:13:26.345
    Mar  2 02:13:26.345: INFO: Pod "downwardapi-volume-79f3c6f5-17a8-421a-8366-6cf9d81b2165" satisfied condition "Succeeded or Failed"
    Mar  2 02:13:26.367: INFO: Trying to get logs from node 10.132.92.143 pod downwardapi-volume-79f3c6f5-17a8-421a-8366-6cf9d81b2165 container client-container: <nil>
    STEP: delete the pod 03/02/23 02:13:26.447
    Mar  2 02:13:26.521: INFO: Waiting for pod downwardapi-volume-79f3c6f5-17a8-421a-8366-6cf9d81b2165 to disappear
    Mar  2 02:13:26.536: INFO: Pod downwardapi-volume-79f3c6f5-17a8-421a-8366-6cf9d81b2165 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar  2 02:13:26.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1851" for this suite. 03/02/23 02:13:26.561
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:13:26.588
Mar  2 02:13:26.589: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename disruption 03/02/23 02:13:26.591
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:13:26.728
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:13:26.742
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
STEP: Waiting for the pdb to be processed 03/02/23 02:13:26.771
STEP: Waiting for all pods to be running 03/02/23 02:13:28.978
Mar  2 02:13:29.016: INFO: running pods: 0 < 3
Mar  2 02:13:31.035: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Mar  2 02:13:33.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-3122" for this suite. 03/02/23 02:13:33.121
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","completed":208,"skipped":3861,"failed":0}
------------------------------
• [SLOW TEST] [6.599 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:13:26.588
    Mar  2 02:13:26.589: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename disruption 03/02/23 02:13:26.591
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:13:26.728
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:13:26.742
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:140
    STEP: Waiting for the pdb to be processed 03/02/23 02:13:26.771
    STEP: Waiting for all pods to be running 03/02/23 02:13:28.978
    Mar  2 02:13:29.016: INFO: running pods: 0 < 3
    Mar  2 02:13:31.035: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Mar  2 02:13:33.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-3122" for this suite. 03/02/23 02:13:33.121
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:13:33.193
Mar  2 02:13:33.193: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename sched-pred 03/02/23 02:13:33.195
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:13:33.317
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:13:33.337
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Mar  2 02:13:33.350: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  2 02:13:33.417: INFO: Waiting for terminating namespaces to be deleted...
Mar  2 02:13:33.462: INFO: 
Logging pods the apiserver thinks is on node 10.132.92.143 before test
Mar  2 02:13:33.583: INFO: calico-node-99tft from calico-system started at 2023-03-01 22:49:50 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.583: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 02:13:33.583: INFO: calico-typha-b7f8b755-xlnll from calico-system started at 2023-03-01 22:49:57 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.583: INFO: 	Container calico-typha ready: true, restart count 0
Mar  2 02:13:33.583: INFO: pod-0 from disruption-3122 started at 2023-03-02 02:13:28 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.583: INFO: 	Container donothing ready: true, restart count 0
Mar  2 02:13:33.583: INFO: pod-1 from disruption-3122 started at 2023-03-02 02:13:28 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.583: INFO: 	Container donothing ready: true, restart count 0
Mar  2 02:13:33.583: INFO: pod-2 from disruption-3122 started at 2023-03-02 02:13:29 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.583: INFO: 	Container donothing ready: true, restart count 0
Mar  2 02:13:33.583: INFO: ibm-keepalived-watcher-dp97c from kube-system started at 2023-03-01 22:48:16 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.583: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar  2 02:13:33.584: INFO: ibm-master-proxy-static-10.132.92.143 from kube-system started at 2023-03-01 22:48:15 +0000 UTC (2 container statuses recorded)
Mar  2 02:13:33.584: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar  2 02:13:33.584: INFO: 	Container pause ready: true, restart count 0
Mar  2 02:13:33.584: INFO: ibmcloud-block-storage-driver-zfc8r from kube-system started at 2023-03-01 22:48:23 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.584: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar  2 02:13:33.584: INFO: tuned-w95xr from openshift-cluster-node-tuning-operator started at 2023-03-01 22:55:42 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.584: INFO: 	Container tuned ready: true, restart count 0
Mar  2 02:13:33.584: INFO: dns-default-tx6ch from openshift-dns started at 2023-03-02 02:09:47 +0000 UTC (2 container statuses recorded)
Mar  2 02:13:33.584: INFO: 	Container dns ready: true, restart count 0
Mar  2 02:13:33.584: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:13:33.584: INFO: node-resolver-7wklz from openshift-dns started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.584: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 02:13:33.584: INFO: node-ca-q4vcl from openshift-image-registry started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.584: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 02:13:33.584: INFO: registry-pvc-permissions-6wblc from openshift-image-registry started at 2023-03-01 22:58:39 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.584: INFO: 	Container pvc-permissions ready: false, restart count 0
Mar  2 02:13:33.584: INFO: ingress-canary-5xj2t from openshift-ingress-canary started at 2023-03-02 02:09:47 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.584: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Mar  2 02:13:33.584: INFO: openshift-kube-proxy-zdts7 from openshift-kube-proxy started at 2023-03-01 22:49:00 +0000 UTC (2 container statuses recorded)
Mar  2 02:13:33.584: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  2 02:13:33.584: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:13:33.584: INFO: node-exporter-z52nh from openshift-monitoring started at 2023-03-01 22:56:40 +0000 UTC (2 container statuses recorded)
Mar  2 02:13:33.584: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:13:33.584: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 02:13:33.584: INFO: multus-48w86 from openshift-multus started at 2023-03-01 22:48:55 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.584: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 02:13:33.584: INFO: multus-additional-cni-plugins-d2kv5 from openshift-multus started at 2023-03-01 22:48:55 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.584: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Mar  2 02:13:33.584: INFO: network-metrics-daemon-7jfj4 from openshift-multus started at 2023-03-01 22:48:56 +0000 UTC (2 container statuses recorded)
Mar  2 02:13:33.584: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:13:33.584: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Mar  2 02:13:33.584: INFO: network-check-target-w7x6h from openshift-network-diagnostics started at 2023-03-01 22:49:03 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.584: INFO: 	Container network-check-target-container ready: true, restart count 0
Mar  2 02:13:33.584: INFO: sonobuoy from sonobuoy started at 2023-03-02 01:07:05 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.584: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  2 02:13:33.584: INFO: sonobuoy-systemd-logs-daemon-set-7266eb5e626146f7-4c6tp from sonobuoy started at 2023-03-02 01:07:14 +0000 UTC (2 container statuses recorded)
Mar  2 02:13:33.584: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 02:13:33.584: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 02:13:33.584: INFO: 
Logging pods the apiserver thinks is on node 10.132.92.186 before test
Mar  2 02:13:33.660: INFO: calico-kube-controllers-5b6b964466-vlvht from calico-system started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.660: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar  2 02:13:33.660: INFO: calico-node-5vvbw from calico-system started at 2023-03-01 22:49:50 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.660: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 02:13:33.660: INFO: calico-typha-b7f8b755-bdnm9 from calico-system started at 2023-03-01 22:49:50 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.660: INFO: 	Container calico-typha ready: true, restart count 0
Mar  2 02:13:33.660: INFO: managed-storage-validation-webhooks-757b765d54-f72mn from ibm-odf-validation-webhook started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.660: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Mar  2 02:13:33.660: INFO: managed-storage-validation-webhooks-757b765d54-gshct from ibm-odf-validation-webhook started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.660: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Mar  2 02:13:33.660: INFO: managed-storage-validation-webhooks-757b765d54-h4j5p from ibm-odf-validation-webhook started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.660: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Mar  2 02:13:33.660: INFO: ibm-cloud-provider-ip-169-56-11-242-558f6ff69-s4dz5 from ibm-system started at 2023-03-02 02:09:21 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.661: INFO: 	Container ibm-cloud-provider-ip-169-56-11-242 ready: true, restart count 0
Mar  2 02:13:33.661: INFO: ibm-file-plugin-68bfbbdc7-77m77 from kube-system started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.661: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Mar  2 02:13:33.661: INFO: ibm-keepalived-watcher-knrjl from kube-system started at 2023-03-01 22:48:27 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.661: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar  2 02:13:33.661: INFO: ibm-master-proxy-static-10.132.92.186 from kube-system started at 2023-03-01 22:48:22 +0000 UTC (2 container statuses recorded)
Mar  2 02:13:33.661: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar  2 02:13:33.661: INFO: 	Container pause ready: true, restart count 0
Mar  2 02:13:33.661: INFO: ibm-storage-metrics-agent-6c459b7855-cv8d6 from kube-system started at 2023-03-01 22:51:20 +0000 UTC (2 container statuses recorded)
Mar  2 02:13:33.661: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
Mar  2 02:13:33.661: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Mar  2 02:13:33.661: INFO: ibm-storage-watcher-67589fd788-z5w99 from kube-system started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.661: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Mar  2 02:13:33.661: INFO: ibmcloud-block-storage-driver-l498z from kube-system started at 2023-03-01 22:48:36 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.661: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar  2 02:13:33.661: INFO: ibmcloud-block-storage-plugin-86f4cbdf95-klfqp from kube-system started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.661: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Mar  2 02:13:33.661: INFO: cluster-node-tuning-operator-5df67c68d9-gd552 from openshift-cluster-node-tuning-operator started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.661: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Mar  2 02:13:33.661: INFO: tuned-h4jnq from openshift-cluster-node-tuning-operator started at 2023-03-01 22:55:42 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.661: INFO: 	Container tuned ready: true, restart count 0
Mar  2 02:13:33.661: INFO: cluster-samples-operator-5948f44764-88mzn from openshift-cluster-samples-operator started at 2023-03-01 22:51:20 +0000 UTC (2 container statuses recorded)
Mar  2 02:13:33.661: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Mar  2 02:13:33.661: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Mar  2 02:13:33.661: INFO: cluster-storage-operator-54cf568cb7-smstf from openshift-cluster-storage-operator started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.661: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Mar  2 02:13:33.661: INFO: csi-snapshot-controller-85d666c956-5hhrv from openshift-cluster-storage-operator started at 2023-03-01 22:51:57 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.661: INFO: 	Container snapshot-controller ready: true, restart count 0
Mar  2 02:13:33.661: INFO: csi-snapshot-controller-operator-6b76cb8c94-9bvdj from openshift-cluster-storage-operator started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.661: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Mar  2 02:13:33.661: INFO: csi-snapshot-webhook-6cbf55b956-z69cb from openshift-cluster-storage-operator started at 2023-03-01 22:51:57 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.661: INFO: 	Container webhook ready: true, restart count 0
Mar  2 02:13:33.661: INFO: console-operator-6975f688f8-xwx92 from openshift-console-operator started at 2023-03-01 22:51:20 +0000 UTC (2 container statuses recorded)
Mar  2 02:13:33.661: INFO: 	Container console-operator ready: true, restart count 1
Mar  2 02:13:33.661: INFO: 	Container conversion-webhook-server ready: true, restart count 3
Mar  2 02:13:33.661: INFO: console-6c8dcd4bdd-c2dbn from openshift-console started at 2023-03-01 22:59:49 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.661: INFO: 	Container console ready: true, restart count 0
Mar  2 02:13:33.662: INFO: downloads-8b4bf4d5-pjlj4 from openshift-console started at 2023-03-01 22:51:55 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.662: INFO: 	Container download-server ready: true, restart count 0
Mar  2 02:13:33.662: INFO: dns-operator-b8598b55c-87ghp from openshift-dns-operator started at 2023-03-01 22:51:20 +0000 UTC (2 container statuses recorded)
Mar  2 02:13:33.662: INFO: 	Container dns-operator ready: true, restart count 0
Mar  2 02:13:33.662: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:13:33.662: INFO: dns-default-rdd5h from openshift-dns started at 2023-03-01 22:55:41 +0000 UTC (2 container statuses recorded)
Mar  2 02:13:33.662: INFO: 	Container dns ready: true, restart count 0
Mar  2 02:13:33.662: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:13:33.662: INFO: node-resolver-9wrks from openshift-dns started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.662: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 02:13:33.662: INFO: cluster-image-registry-operator-797c5fb5f8-tvjwj from openshift-image-registry started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.662: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Mar  2 02:13:33.662: INFO: node-ca-6ffsq from openshift-image-registry started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.662: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 02:13:33.662: INFO: ingress-canary-zgjd4 from openshift-ingress-canary started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.662: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Mar  2 02:13:33.662: INFO: ingress-operator-7b8c5ddb4b-wr6sb from openshift-ingress-operator started at 2023-03-01 22:51:20 +0000 UTC (2 container statuses recorded)
Mar  2 02:13:33.662: INFO: 	Container ingress-operator ready: true, restart count 0
Mar  2 02:13:33.662: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:13:33.662: INFO: router-default-68bc8785b7-vl6bd from openshift-ingress started at 2023-03-02 02:09:21 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.662: INFO: 	Container router ready: true, restart count 0
Mar  2 02:13:33.662: INFO: insights-operator-7b97bdcbd8-5zm5p from openshift-insights started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.662: INFO: 	Container insights-operator ready: true, restart count 1
Mar  2 02:13:33.662: INFO: openshift-kube-proxy-xgktp from openshift-kube-proxy started at 2023-03-01 22:49:00 +0000 UTC (2 container statuses recorded)
Mar  2 02:13:33.662: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  2 02:13:33.662: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:13:33.662: INFO: kube-storage-version-migrator-operator-78bc45549c-qmzfr from openshift-kube-storage-version-migrator-operator started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.662: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Mar  2 02:13:33.662: INFO: marketplace-operator-66f5c8895c-l9r8f from openshift-marketplace started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.662: INFO: 	Container marketplace-operator ready: true, restart count 0
Mar  2 02:13:33.662: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-03-02 02:09:26 +0000 UTC (6 container statuses recorded)
Mar  2 02:13:33.662: INFO: 	Container alertmanager ready: true, restart count 1
Mar  2 02:13:33.662: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 02:13:33.662: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 02:13:33.662: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:13:33.662: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Mar  2 02:13:33.662: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 02:13:33.662: INFO: cluster-monitoring-operator-7ffc6575dd-kkb8m from openshift-monitoring started at 2023-03-01 22:51:20 +0000 UTC (2 container statuses recorded)
Mar  2 02:13:33.662: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Mar  2 02:13:33.662: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:13:33.662: INFO: node-exporter-95fzh from openshift-monitoring started at 2023-03-01 22:56:40 +0000 UTC (2 container statuses recorded)
Mar  2 02:13:33.662: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:13:33.662: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 02:13:33.662: INFO: prometheus-adapter-95d69f68c-vtxdk from openshift-monitoring started at 2023-03-02 02:09:21 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.662: INFO: 	Container prometheus-adapter ready: true, restart count 0
Mar  2 02:13:33.663: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-03-02 02:09:26 +0000 UTC (6 container statuses recorded)
Mar  2 02:13:33.663: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 02:13:33.663: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:13:33.663: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Mar  2 02:13:33.663: INFO: 	Container prometheus ready: true, restart count 0
Mar  2 02:13:33.663: INFO: 	Container prometheus-proxy ready: true, restart count 0
Mar  2 02:13:33.663: INFO: 	Container thanos-sidecar ready: true, restart count 0
Mar  2 02:13:33.663: INFO: prometheus-operator-admission-webhook-6d5fbffb86-sm9lf from openshift-monitoring started at 2023-03-02 02:09:21 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.663: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Mar  2 02:13:33.663: INFO: thanos-querier-6cd8656bbb-j85bx from openshift-monitoring started at 2023-03-02 02:09:21 +0000 UTC (6 container statuses recorded)
Mar  2 02:13:33.663: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:13:33.663: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Mar  2 02:13:33.663: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Mar  2 02:13:33.663: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 02:13:33.663: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 02:13:33.663: INFO: 	Container thanos-query ready: true, restart count 0
Mar  2 02:13:33.663: INFO: multus-89qpj from openshift-multus started at 2023-03-01 22:48:55 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.663: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 02:13:33.663: INFO: multus-additional-cni-plugins-ff7kf from openshift-multus started at 2023-03-01 22:48:55 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.663: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Mar  2 02:13:33.663: INFO: multus-admission-controller-6f984f76c7-xsk9f from openshift-multus started at 2023-03-02 02:09:21 +0000 UTC (2 container statuses recorded)
Mar  2 02:13:33.663: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:13:33.663: INFO: 	Container multus-admission-controller ready: true, restart count 0
Mar  2 02:13:33.663: INFO: network-metrics-daemon-7m69v from openshift-multus started at 2023-03-01 22:48:56 +0000 UTC (2 container statuses recorded)
Mar  2 02:13:33.663: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:13:33.663: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Mar  2 02:13:33.663: INFO: network-check-source-848f87f9d4-bxlxt from openshift-network-diagnostics started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.663: INFO: 	Container check-endpoints ready: true, restart count 0
Mar  2 02:13:33.663: INFO: network-check-target-jqnqt from openshift-network-diagnostics started at 2023-03-01 22:49:03 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.663: INFO: 	Container network-check-target-container ready: true, restart count 0
Mar  2 02:13:33.663: INFO: catalog-operator-657ff7bb9f-c7tqq from openshift-operator-lifecycle-manager started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.663: INFO: 	Container catalog-operator ready: true, restart count 0
Mar  2 02:13:33.663: INFO: collect-profiles-27962010-nhpf6 from openshift-operator-lifecycle-manager started at 2023-03-02 01:30:00 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.663: INFO: 	Container collect-profiles ready: false, restart count 0
Mar  2 02:13:33.663: INFO: olm-operator-56ccd7794f-4ffrj from openshift-operator-lifecycle-manager started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.663: INFO: 	Container olm-operator ready: true, restart count 0
Mar  2 02:13:33.663: INFO: package-server-manager-599745f678-rmjg4 from openshift-operator-lifecycle-manager started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.663: INFO: 	Container package-server-manager ready: true, restart count 0
Mar  2 02:13:33.663: INFO: packageserver-6c7c56df95-jq89f from openshift-operator-lifecycle-manager started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.663: INFO: 	Container packageserver ready: true, restart count 0
Mar  2 02:13:33.663: INFO: metrics-784dfd67b7-rjkpg from openshift-roks-metrics started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.663: INFO: 	Container metrics ready: true, restart count 3
Mar  2 02:13:33.663: INFO: push-gateway-646cc86d79-cgtct from openshift-roks-metrics started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.663: INFO: 	Container push-gateway ready: true, restart count 0
Mar  2 02:13:33.663: INFO: service-ca-operator-6f8744cbc5-w5srs from openshift-service-ca-operator started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.663: INFO: 	Container service-ca-operator ready: true, restart count 1
Mar  2 02:13:33.663: INFO: sonobuoy-systemd-logs-daemon-set-7266eb5e626146f7-n2dsq from sonobuoy started at 2023-03-02 01:07:14 +0000 UTC (2 container statuses recorded)
Mar  2 02:13:33.663: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 02:13:33.663: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 02:13:33.663: INFO: 
Logging pods the apiserver thinks is on node 10.132.92.188 before test
Mar  2 02:13:33.745: INFO: calico-node-mkvsm from calico-system started at 2023-03-01 22:49:50 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.745: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 02:13:33.745: INFO: ibm-cloud-provider-ip-169-56-11-242-558f6ff69-xvh2v from ibm-system started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.745: INFO: 	Container ibm-cloud-provider-ip-169-56-11-242 ready: true, restart count 0
Mar  2 02:13:33.745: INFO: ibm-keepalived-watcher-zwfb8 from kube-system started at 2023-03-01 22:48:10 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.745: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar  2 02:13:33.745: INFO: ibm-master-proxy-static-10.132.92.188 from kube-system started at 2023-03-01 22:48:07 +0000 UTC (2 container statuses recorded)
Mar  2 02:13:33.745: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar  2 02:13:33.745: INFO: 	Container pause ready: true, restart count 0
Mar  2 02:13:33.745: INFO: ibmcloud-block-storage-driver-qbg9f from kube-system started at 2023-03-01 22:48:16 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.745: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar  2 02:13:33.745: INFO: vpn-f6c799ddd-xls7d from kube-system started at 2023-03-02 02:09:21 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.745: INFO: 	Container vpn ready: true, restart count 0
Mar  2 02:13:33.745: INFO: tuned-5z98r from openshift-cluster-node-tuning-operator started at 2023-03-01 22:55:42 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.745: INFO: 	Container tuned ready: true, restart count 0
Mar  2 02:13:33.745: INFO: csi-snapshot-controller-85d666c956-xsh6c from openshift-cluster-storage-operator started at 2023-03-01 22:51:57 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.745: INFO: 	Container snapshot-controller ready: true, restart count 0
Mar  2 02:13:33.745: INFO: csi-snapshot-webhook-6cbf55b956-wq8bh from openshift-cluster-storage-operator started at 2023-03-01 22:51:57 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.745: INFO: 	Container webhook ready: true, restart count 0
Mar  2 02:13:33.745: INFO: console-6c8dcd4bdd-ddzwf from openshift-console started at 2023-03-02 02:09:21 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.745: INFO: 	Container console ready: true, restart count 0
Mar  2 02:13:33.745: INFO: downloads-8b4bf4d5-b9kfz from openshift-console started at 2023-03-01 22:51:55 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.745: INFO: 	Container download-server ready: true, restart count 0
Mar  2 02:13:33.745: INFO: dns-default-nt82k from openshift-dns started at 2023-03-01 22:55:41 +0000 UTC (2 container statuses recorded)
Mar  2 02:13:33.745: INFO: 	Container dns ready: true, restart count 0
Mar  2 02:13:33.745: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:13:33.745: INFO: node-resolver-wcng9 from openshift-dns started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.745: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 02:13:33.745: INFO: image-pruner-27961920-wd4pv from openshift-image-registry started at 2023-03-02 00:00:00 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.745: INFO: 	Container image-pruner ready: false, restart count 0
Mar  2 02:13:33.745: INFO: image-registry-7fb7bd8df6-lb2bt from openshift-image-registry started at 2023-03-01 22:58:31 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.745: INFO: 	Container registry ready: true, restart count 0
Mar  2 02:13:33.745: INFO: node-ca-qlt4w from openshift-image-registry started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.745: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 02:13:33.745: INFO: ingress-canary-qvcz9 from openshift-ingress-canary started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.745: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Mar  2 02:13:33.745: INFO: router-default-68bc8785b7-wz8ct from openshift-ingress started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.745: INFO: 	Container router ready: true, restart count 0
Mar  2 02:13:33.745: INFO: openshift-kube-proxy-mth7h from openshift-kube-proxy started at 2023-03-01 22:49:00 +0000 UTC (2 container statuses recorded)
Mar  2 02:13:33.745: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  2 02:13:33.745: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:13:33.745: INFO: migrator-5687b466dd-glln7 from openshift-kube-storage-version-migrator started at 2023-03-01 22:51:57 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.745: INFO: 	Container migrator ready: true, restart count 0
Mar  2 02:13:33.745: INFO: certified-operators-8n5vw from openshift-marketplace started at 2023-03-02 00:13:35 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.745: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 02:13:33.745: INFO: community-operators-lngtl from openshift-marketplace started at 2023-03-01 22:54:15 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.745: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 02:13:33.745: INFO: redhat-marketplace-ld8wh from openshift-marketplace started at 2023-03-02 00:27:02 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.745: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 02:13:33.745: INFO: redhat-operators-tk548 from openshift-marketplace started at 2023-03-01 22:54:14 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.745: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 02:13:33.745: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-03-01 22:58:39 +0000 UTC (6 container statuses recorded)
Mar  2 02:13:33.745: INFO: 	Container alertmanager ready: true, restart count 1
Mar  2 02:13:33.745: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 02:13:33.745: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 02:13:33.745: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:13:33.745: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Mar  2 02:13:33.745: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 02:13:33.745: INFO: kube-state-metrics-554994774b-4xpf2 from openshift-monitoring started at 2023-03-02 02:09:21 +0000 UTC (3 container statuses recorded)
Mar  2 02:13:33.745: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Mar  2 02:13:33.745: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Mar  2 02:13:33.745: INFO: 	Container kube-state-metrics ready: true, restart count 0
Mar  2 02:13:33.745: INFO: node-exporter-gj7lr from openshift-monitoring started at 2023-03-01 22:56:39 +0000 UTC (2 container statuses recorded)
Mar  2 02:13:33.745: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:13:33.745: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 02:13:33.745: INFO: openshift-state-metrics-56599d9b6d-6sw48 from openshift-monitoring started at 2023-03-01 22:56:39 +0000 UTC (3 container statuses recorded)
Mar  2 02:13:33.745: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Mar  2 02:13:33.745: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Mar  2 02:13:33.745: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Mar  2 02:13:33.745: INFO: prometheus-adapter-95d69f68c-gtx2d from openshift-monitoring started at 2023-03-01 22:58:02 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.745: INFO: 	Container prometheus-adapter ready: true, restart count 0
Mar  2 02:13:33.745: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-03-01 22:58:30 +0000 UTC (6 container statuses recorded)
Mar  2 02:13:33.745: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 02:13:33.745: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:13:33.745: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Mar  2 02:13:33.745: INFO: 	Container prometheus ready: true, restart count 0
Mar  2 02:13:33.745: INFO: 	Container prometheus-proxy ready: true, restart count 0
Mar  2 02:13:33.745: INFO: 	Container thanos-sidecar ready: true, restart count 0
Mar  2 02:13:33.745: INFO: prometheus-operator-9bf584898-89sgf from openshift-monitoring started at 2023-03-01 22:56:19 +0000 UTC (2 container statuses recorded)
Mar  2 02:13:33.745: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:13:33.745: INFO: 	Container prometheus-operator ready: true, restart count 0
Mar  2 02:13:33.745: INFO: prometheus-operator-admission-webhook-6d5fbffb86-hhfbw from openshift-monitoring started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.745: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Mar  2 02:13:33.745: INFO: telemeter-client-769c487d5b-l5fxr from openshift-monitoring started at 2023-03-02 02:09:21 +0000 UTC (3 container statuses recorded)
Mar  2 02:13:33.745: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:13:33.745: INFO: 	Container reload ready: true, restart count 0
Mar  2 02:13:33.745: INFO: 	Container telemeter-client ready: true, restart count 0
Mar  2 02:13:33.745: INFO: thanos-querier-6cd8656bbb-8hv4v from openshift-monitoring started at 2023-03-01 22:56:48 +0000 UTC (6 container statuses recorded)
Mar  2 02:13:33.745: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:13:33.745: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Mar  2 02:13:33.745: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Mar  2 02:13:33.745: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 02:13:33.745: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 02:13:33.745: INFO: 	Container thanos-query ready: true, restart count 0
Mar  2 02:13:33.745: INFO: multus-additional-cni-plugins-5pjnd from openshift-multus started at 2023-03-01 22:48:55 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.745: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Mar  2 02:13:33.745: INFO: multus-admission-controller-6f984f76c7-hclf4 from openshift-multus started at 2023-03-01 22:55:41 +0000 UTC (2 container statuses recorded)
Mar  2 02:13:33.745: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:13:33.745: INFO: 	Container multus-admission-controller ready: true, restart count 0
Mar  2 02:13:33.745: INFO: multus-g4k8f from openshift-multus started at 2023-03-01 22:48:55 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.745: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 02:13:33.745: INFO: network-metrics-daemon-95rvd from openshift-multus started at 2023-03-01 22:48:56 +0000 UTC (2 container statuses recorded)
Mar  2 02:13:33.746: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:13:33.746: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Mar  2 02:13:33.746: INFO: network-check-target-4nkgt from openshift-network-diagnostics started at 2023-03-01 22:49:03 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.746: INFO: 	Container network-check-target-container ready: true, restart count 0
Mar  2 02:13:33.746: INFO: network-operator-68bbf8cc76-mg97z from openshift-network-operator started at 2023-03-01 22:48:19 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.746: INFO: 	Container network-operator ready: true, restart count 1
Mar  2 02:13:33.746: INFO: collect-profiles-27962025-rd7rx from openshift-operator-lifecycle-manager started at 2023-03-02 01:45:00 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.746: INFO: 	Container collect-profiles ready: false, restart count 0
Mar  2 02:13:33.746: INFO: collect-profiles-27962040-2rff7 from openshift-operator-lifecycle-manager started at 2023-03-02 02:00:00 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.746: INFO: 	Container collect-profiles ready: false, restart count 0
Mar  2 02:13:33.746: INFO: packageserver-6c7c56df95-7wr7f from openshift-operator-lifecycle-manager started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.746: INFO: 	Container packageserver ready: true, restart count 0
Mar  2 02:13:33.746: INFO: service-ca-578d67f6f8-89gtp from openshift-service-ca started at 2023-03-01 22:52:15 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.746: INFO: 	Container service-ca-controller ready: true, restart count 0
Mar  2 02:13:33.746: INFO: sonobuoy-e2e-job-c6c00a6e72aa4160 from sonobuoy started at 2023-03-02 01:07:14 +0000 UTC (2 container statuses recorded)
Mar  2 02:13:33.746: INFO: 	Container e2e ready: true, restart count 0
Mar  2 02:13:33.746: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 02:13:33.746: INFO: sonobuoy-systemd-logs-daemon-set-7266eb5e626146f7-fgqrz from sonobuoy started at 2023-03-02 01:07:14 +0000 UTC (2 container statuses recorded)
Mar  2 02:13:33.746: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 02:13:33.746: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 02:13:33.746: INFO: tigera-operator-84f4f4565b-5w7s7 from tigera-operator started at 2023-03-01 22:48:19 +0000 UTC (1 container statuses recorded)
Mar  2 02:13:33.746: INFO: 	Container tigera-operator ready: true, restart count 2
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it. 03/02/23 02:13:33.746
Mar  2 02:13:33.862: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-631" to be "running"
Mar  2 02:13:33.877: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 14.634674ms
Mar  2 02:13:35.893: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030184875s
Mar  2 02:13:37.894: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.031332788s
Mar  2 02:13:37.894: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 03/02/23 02:13:37.907
STEP: Trying to apply a random label on the found node. 03/02/23 02:13:37.97
STEP: verifying the node has the label kubernetes.io/e2e-e3796432-c892-4100-a35d-1d6d2d7561be 95 03/02/23 02:13:38.01
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 03/02/23 02:13:38.034
Mar  2 02:13:38.083: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-631" to be "not pending"
Mar  2 02:13:38.096: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.384159ms
Mar  2 02:13:40.115: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031874858s
Mar  2 02:13:42.114: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 4.030301809s
Mar  2 02:13:42.114: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.132.92.143 on the node which pod4 resides and expect not scheduled 03/02/23 02:13:42.114
Mar  2 02:13:42.167: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-631" to be "not pending"
Mar  2 02:13:42.184: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.445081ms
Mar  2 02:13:44.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03195654s
Mar  2 02:13:46.201: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033210573s
Mar  2 02:13:48.201: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.033140763s
Mar  2 02:13:50.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.032069685s
Mar  2 02:13:52.202: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.03436837s
Mar  2 02:13:54.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.031563206s
Mar  2 02:13:56.210: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.042257349s
Mar  2 02:13:58.226: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.058991757s
Mar  2 02:14:00.211: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.043125275s
Mar  2 02:14:02.204: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.036221604s
Mar  2 02:14:04.201: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.033322659s
Mar  2 02:14:06.198: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.030864544s
Mar  2 02:14:08.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.031050657s
Mar  2 02:14:10.202: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.034872831s
Mar  2 02:14:12.198: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.030650435s
Mar  2 02:14:14.223: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.055720678s
Mar  2 02:14:16.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.03279343s
Mar  2 02:14:18.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.032400333s
Mar  2 02:14:20.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.032148567s
Mar  2 02:14:22.217: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.049170386s
Mar  2 02:14:24.202: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.034017313s
Mar  2 02:14:26.203: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.035082584s
Mar  2 02:14:28.210: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.042521146s
Mar  2 02:14:30.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.031319798s
Mar  2 02:14:32.202: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.034275163s
Mar  2 02:14:34.201: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.03365694s
Mar  2 02:14:36.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.032629533s
Mar  2 02:14:38.203: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.035334229s
Mar  2 02:14:40.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.031918829s
Mar  2 02:14:42.211: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.042963213s
Mar  2 02:14:44.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.032034914s
Mar  2 02:14:46.201: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.033577999s
Mar  2 02:14:48.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.031715013s
Mar  2 02:14:50.232: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.06459281s
Mar  2 02:14:52.202: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.034623168s
Mar  2 02:14:54.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.032397437s
Mar  2 02:14:56.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.031305476s
Mar  2 02:14:58.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.031397107s
Mar  2 02:15:00.207: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.039417605s
Mar  2 02:15:02.234: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.066651911s
Mar  2 02:15:04.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.032097155s
Mar  2 02:15:06.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.061390653s
Mar  2 02:15:08.206: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.038924885s
Mar  2 02:15:10.240: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.07293474s
Mar  2 02:15:12.206: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.038258703s
Mar  2 02:15:14.203: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.035910916s
Mar  2 02:15:16.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.031602987s
Mar  2 02:15:18.198: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.030995188s
Mar  2 02:15:20.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.031275485s
Mar  2 02:15:22.208: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.04088086s
Mar  2 02:15:24.220: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.052742687s
Mar  2 02:15:26.213: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.045828096s
Mar  2 02:15:28.205: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.037836227s
Mar  2 02:15:30.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.032018394s
Mar  2 02:15:32.205: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.03749332s
Mar  2 02:15:34.208: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.040030183s
Mar  2 02:15:36.203: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.035945136s
Mar  2 02:15:38.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.031919634s
Mar  2 02:15:40.204: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.036388457s
Mar  2 02:15:42.219: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.051071918s
Mar  2 02:15:44.236: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.068712195s
Mar  2 02:15:46.206: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.038185479s
Mar  2 02:15:48.201: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.033484325s
Mar  2 02:15:50.215: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.047016904s
Mar  2 02:15:52.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.090706579s
Mar  2 02:15:54.203: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.035685965s
Mar  2 02:15:56.203: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.035514791s
Mar  2 02:15:58.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.031537284s
Mar  2 02:16:00.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.032153561s
Mar  2 02:16:02.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.031714211s
Mar  2 02:16:04.201: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.033031575s
Mar  2 02:16:06.203: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.035109755s
Mar  2 02:16:08.219: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.05117434s
Mar  2 02:16:10.214: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.046801301s
Mar  2 02:16:12.201: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.033602341s
Mar  2 02:16:14.231: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.063563454s
Mar  2 02:16:16.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.03255009s
Mar  2 02:16:18.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.031385832s
Mar  2 02:16:20.202: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.034008958s
Mar  2 02:16:22.205: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.037196712s
Mar  2 02:16:24.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.031295267s
Mar  2 02:16:26.198: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.03021945s
Mar  2 02:16:28.207: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.039978799s
Mar  2 02:16:30.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.031224414s
Mar  2 02:16:32.202: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.034908244s
Mar  2 02:16:34.202: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.034328562s
Mar  2 02:16:36.216: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.048060969s
Mar  2 02:16:38.201: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.03310325s
Mar  2 02:16:40.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.031193749s
Mar  2 02:16:42.201: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.033987601s
Mar  2 02:16:44.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.070160625s
Mar  2 02:16:46.203: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.035833867s
Mar  2 02:16:48.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.031781315s
Mar  2 02:16:50.228: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.060542882s
Mar  2 02:16:52.201: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.033062666s
Mar  2 02:16:54.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.032011619s
Mar  2 02:16:56.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.031281882s
Mar  2 02:16:58.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.031896744s
Mar  2 02:17:00.201: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.033351335s
Mar  2 02:17:02.205: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.037882188s
Mar  2 02:17:04.201: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.033502098s
Mar  2 02:17:06.204: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.036625102s
Mar  2 02:17:08.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.031353641s
Mar  2 02:17:10.201: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.03304833s
Mar  2 02:17:12.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.032190702s
Mar  2 02:17:14.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.031492682s
Mar  2 02:17:16.214: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.046077662s
Mar  2 02:17:18.204: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.036447936s
Mar  2 02:17:20.198: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.030047593s
Mar  2 02:17:22.204: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.036009183s
Mar  2 02:17:24.206: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.038213299s
Mar  2 02:17:26.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.031553535s
Mar  2 02:17:28.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.032541825s
Mar  2 02:17:30.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.032254602s
Mar  2 02:17:32.201: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.033542033s
Mar  2 02:17:34.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.032665534s
Mar  2 02:17:36.216: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.048735727s
Mar  2 02:17:38.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.032662869s
Mar  2 02:17:40.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.031371759s
Mar  2 02:17:42.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.03298087s
Mar  2 02:17:44.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.032929284s
Mar  2 02:17:46.207: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.039102676s
Mar  2 02:17:48.204: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.036469556s
Mar  2 02:17:50.206: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.038409757s
Mar  2 02:17:52.202: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.034247338s
Mar  2 02:17:54.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.032882515s
Mar  2 02:17:56.197: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.029562587s
Mar  2 02:17:58.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.031295537s
Mar  2 02:18:00.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.03226431s
Mar  2 02:18:02.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.032037857s
Mar  2 02:18:04.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.032768634s
Mar  2 02:18:06.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.032554577s
Mar  2 02:18:08.198: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.030494194s
Mar  2 02:18:10.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.032431746s
Mar  2 02:18:12.201: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.033374316s
Mar  2 02:18:14.205: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.037733931s
Mar  2 02:18:16.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.032439481s
Mar  2 02:18:18.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.03274069s
Mar  2 02:18:20.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.031562772s
Mar  2 02:18:22.201: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.033336777s
Mar  2 02:18:24.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.031399578s
Mar  2 02:18:26.252: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.084036443s
Mar  2 02:18:28.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.03166657s
Mar  2 02:18:30.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.032159796s
Mar  2 02:18:32.202: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.034374066s
Mar  2 02:18:34.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.031836165s
Mar  2 02:18:36.209: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.041990127s
Mar  2 02:18:38.203: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.035491704s
Mar  2 02:18:40.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.032745478s
Mar  2 02:18:42.202: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.03490912s
Mar  2 02:18:42.219: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.051925502s
STEP: removing the label kubernetes.io/e2e-e3796432-c892-4100-a35d-1d6d2d7561be off the node 10.132.92.143 03/02/23 02:18:42.219
STEP: verifying the node doesn't have the label kubernetes.io/e2e-e3796432-c892-4100-a35d-1d6d2d7561be 03/02/23 02:18:42.292
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Mar  2 02:18:42.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-631" for this suite. 03/02/23 02:18:42.34
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","completed":209,"skipped":3888,"failed":0}
------------------------------
• [SLOW TEST] [309.188 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:13:33.193
    Mar  2 02:13:33.193: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename sched-pred 03/02/23 02:13:33.195
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:13:33.317
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:13:33.337
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Mar  2 02:13:33.350: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Mar  2 02:13:33.417: INFO: Waiting for terminating namespaces to be deleted...
    Mar  2 02:13:33.462: INFO: 
    Logging pods the apiserver thinks is on node 10.132.92.143 before test
    Mar  2 02:13:33.583: INFO: calico-node-99tft from calico-system started at 2023-03-01 22:49:50 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.583: INFO: 	Container calico-node ready: true, restart count 0
    Mar  2 02:13:33.583: INFO: calico-typha-b7f8b755-xlnll from calico-system started at 2023-03-01 22:49:57 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.583: INFO: 	Container calico-typha ready: true, restart count 0
    Mar  2 02:13:33.583: INFO: pod-0 from disruption-3122 started at 2023-03-02 02:13:28 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.583: INFO: 	Container donothing ready: true, restart count 0
    Mar  2 02:13:33.583: INFO: pod-1 from disruption-3122 started at 2023-03-02 02:13:28 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.583: INFO: 	Container donothing ready: true, restart count 0
    Mar  2 02:13:33.583: INFO: pod-2 from disruption-3122 started at 2023-03-02 02:13:29 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.583: INFO: 	Container donothing ready: true, restart count 0
    Mar  2 02:13:33.583: INFO: ibm-keepalived-watcher-dp97c from kube-system started at 2023-03-01 22:48:16 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.583: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Mar  2 02:13:33.584: INFO: ibm-master-proxy-static-10.132.92.143 from kube-system started at 2023-03-01 22:48:15 +0000 UTC (2 container statuses recorded)
    Mar  2 02:13:33.584: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Mar  2 02:13:33.584: INFO: 	Container pause ready: true, restart count 0
    Mar  2 02:13:33.584: INFO: ibmcloud-block-storage-driver-zfc8r from kube-system started at 2023-03-01 22:48:23 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.584: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Mar  2 02:13:33.584: INFO: tuned-w95xr from openshift-cluster-node-tuning-operator started at 2023-03-01 22:55:42 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.584: INFO: 	Container tuned ready: true, restart count 0
    Mar  2 02:13:33.584: INFO: dns-default-tx6ch from openshift-dns started at 2023-03-02 02:09:47 +0000 UTC (2 container statuses recorded)
    Mar  2 02:13:33.584: INFO: 	Container dns ready: true, restart count 0
    Mar  2 02:13:33.584: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:13:33.584: INFO: node-resolver-7wklz from openshift-dns started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.584: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Mar  2 02:13:33.584: INFO: node-ca-q4vcl from openshift-image-registry started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.584: INFO: 	Container node-ca ready: true, restart count 0
    Mar  2 02:13:33.584: INFO: registry-pvc-permissions-6wblc from openshift-image-registry started at 2023-03-01 22:58:39 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.584: INFO: 	Container pvc-permissions ready: false, restart count 0
    Mar  2 02:13:33.584: INFO: ingress-canary-5xj2t from openshift-ingress-canary started at 2023-03-02 02:09:47 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.584: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Mar  2 02:13:33.584: INFO: openshift-kube-proxy-zdts7 from openshift-kube-proxy started at 2023-03-01 22:49:00 +0000 UTC (2 container statuses recorded)
    Mar  2 02:13:33.584: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar  2 02:13:33.584: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:13:33.584: INFO: node-exporter-z52nh from openshift-monitoring started at 2023-03-01 22:56:40 +0000 UTC (2 container statuses recorded)
    Mar  2 02:13:33.584: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:13:33.584: INFO: 	Container node-exporter ready: true, restart count 0
    Mar  2 02:13:33.584: INFO: multus-48w86 from openshift-multus started at 2023-03-01 22:48:55 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.584: INFO: 	Container kube-multus ready: true, restart count 0
    Mar  2 02:13:33.584: INFO: multus-additional-cni-plugins-d2kv5 from openshift-multus started at 2023-03-01 22:48:55 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.584: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Mar  2 02:13:33.584: INFO: network-metrics-daemon-7jfj4 from openshift-multus started at 2023-03-01 22:48:56 +0000 UTC (2 container statuses recorded)
    Mar  2 02:13:33.584: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:13:33.584: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Mar  2 02:13:33.584: INFO: network-check-target-w7x6h from openshift-network-diagnostics started at 2023-03-01 22:49:03 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.584: INFO: 	Container network-check-target-container ready: true, restart count 0
    Mar  2 02:13:33.584: INFO: sonobuoy from sonobuoy started at 2023-03-02 01:07:05 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.584: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Mar  2 02:13:33.584: INFO: sonobuoy-systemd-logs-daemon-set-7266eb5e626146f7-4c6tp from sonobuoy started at 2023-03-02 01:07:14 +0000 UTC (2 container statuses recorded)
    Mar  2 02:13:33.584: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  2 02:13:33.584: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar  2 02:13:33.584: INFO: 
    Logging pods the apiserver thinks is on node 10.132.92.186 before test
    Mar  2 02:13:33.660: INFO: calico-kube-controllers-5b6b964466-vlvht from calico-system started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.660: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Mar  2 02:13:33.660: INFO: calico-node-5vvbw from calico-system started at 2023-03-01 22:49:50 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.660: INFO: 	Container calico-node ready: true, restart count 0
    Mar  2 02:13:33.660: INFO: calico-typha-b7f8b755-bdnm9 from calico-system started at 2023-03-01 22:49:50 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.660: INFO: 	Container calico-typha ready: true, restart count 0
    Mar  2 02:13:33.660: INFO: managed-storage-validation-webhooks-757b765d54-f72mn from ibm-odf-validation-webhook started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.660: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
    Mar  2 02:13:33.660: INFO: managed-storage-validation-webhooks-757b765d54-gshct from ibm-odf-validation-webhook started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.660: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
    Mar  2 02:13:33.660: INFO: managed-storage-validation-webhooks-757b765d54-h4j5p from ibm-odf-validation-webhook started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.660: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
    Mar  2 02:13:33.660: INFO: ibm-cloud-provider-ip-169-56-11-242-558f6ff69-s4dz5 from ibm-system started at 2023-03-02 02:09:21 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.661: INFO: 	Container ibm-cloud-provider-ip-169-56-11-242 ready: true, restart count 0
    Mar  2 02:13:33.661: INFO: ibm-file-plugin-68bfbbdc7-77m77 from kube-system started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.661: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
    Mar  2 02:13:33.661: INFO: ibm-keepalived-watcher-knrjl from kube-system started at 2023-03-01 22:48:27 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.661: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Mar  2 02:13:33.661: INFO: ibm-master-proxy-static-10.132.92.186 from kube-system started at 2023-03-01 22:48:22 +0000 UTC (2 container statuses recorded)
    Mar  2 02:13:33.661: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Mar  2 02:13:33.661: INFO: 	Container pause ready: true, restart count 0
    Mar  2 02:13:33.661: INFO: ibm-storage-metrics-agent-6c459b7855-cv8d6 from kube-system started at 2023-03-01 22:51:20 +0000 UTC (2 container statuses recorded)
    Mar  2 02:13:33.661: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
    Mar  2 02:13:33.661: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
    Mar  2 02:13:33.661: INFO: ibm-storage-watcher-67589fd788-z5w99 from kube-system started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.661: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
    Mar  2 02:13:33.661: INFO: ibmcloud-block-storage-driver-l498z from kube-system started at 2023-03-01 22:48:36 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.661: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Mar  2 02:13:33.661: INFO: ibmcloud-block-storage-plugin-86f4cbdf95-klfqp from kube-system started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.661: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
    Mar  2 02:13:33.661: INFO: cluster-node-tuning-operator-5df67c68d9-gd552 from openshift-cluster-node-tuning-operator started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.661: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
    Mar  2 02:13:33.661: INFO: tuned-h4jnq from openshift-cluster-node-tuning-operator started at 2023-03-01 22:55:42 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.661: INFO: 	Container tuned ready: true, restart count 0
    Mar  2 02:13:33.661: INFO: cluster-samples-operator-5948f44764-88mzn from openshift-cluster-samples-operator started at 2023-03-01 22:51:20 +0000 UTC (2 container statuses recorded)
    Mar  2 02:13:33.661: INFO: 	Container cluster-samples-operator ready: true, restart count 0
    Mar  2 02:13:33.661: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
    Mar  2 02:13:33.661: INFO: cluster-storage-operator-54cf568cb7-smstf from openshift-cluster-storage-operator started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.661: INFO: 	Container cluster-storage-operator ready: true, restart count 1
    Mar  2 02:13:33.661: INFO: csi-snapshot-controller-85d666c956-5hhrv from openshift-cluster-storage-operator started at 2023-03-01 22:51:57 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.661: INFO: 	Container snapshot-controller ready: true, restart count 0
    Mar  2 02:13:33.661: INFO: csi-snapshot-controller-operator-6b76cb8c94-9bvdj from openshift-cluster-storage-operator started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.661: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
    Mar  2 02:13:33.661: INFO: csi-snapshot-webhook-6cbf55b956-z69cb from openshift-cluster-storage-operator started at 2023-03-01 22:51:57 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.661: INFO: 	Container webhook ready: true, restart count 0
    Mar  2 02:13:33.661: INFO: console-operator-6975f688f8-xwx92 from openshift-console-operator started at 2023-03-01 22:51:20 +0000 UTC (2 container statuses recorded)
    Mar  2 02:13:33.661: INFO: 	Container console-operator ready: true, restart count 1
    Mar  2 02:13:33.661: INFO: 	Container conversion-webhook-server ready: true, restart count 3
    Mar  2 02:13:33.661: INFO: console-6c8dcd4bdd-c2dbn from openshift-console started at 2023-03-01 22:59:49 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.661: INFO: 	Container console ready: true, restart count 0
    Mar  2 02:13:33.662: INFO: downloads-8b4bf4d5-pjlj4 from openshift-console started at 2023-03-01 22:51:55 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.662: INFO: 	Container download-server ready: true, restart count 0
    Mar  2 02:13:33.662: INFO: dns-operator-b8598b55c-87ghp from openshift-dns-operator started at 2023-03-01 22:51:20 +0000 UTC (2 container statuses recorded)
    Mar  2 02:13:33.662: INFO: 	Container dns-operator ready: true, restart count 0
    Mar  2 02:13:33.662: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:13:33.662: INFO: dns-default-rdd5h from openshift-dns started at 2023-03-01 22:55:41 +0000 UTC (2 container statuses recorded)
    Mar  2 02:13:33.662: INFO: 	Container dns ready: true, restart count 0
    Mar  2 02:13:33.662: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:13:33.662: INFO: node-resolver-9wrks from openshift-dns started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.662: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Mar  2 02:13:33.662: INFO: cluster-image-registry-operator-797c5fb5f8-tvjwj from openshift-image-registry started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.662: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
    Mar  2 02:13:33.662: INFO: node-ca-6ffsq from openshift-image-registry started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.662: INFO: 	Container node-ca ready: true, restart count 0
    Mar  2 02:13:33.662: INFO: ingress-canary-zgjd4 from openshift-ingress-canary started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.662: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Mar  2 02:13:33.662: INFO: ingress-operator-7b8c5ddb4b-wr6sb from openshift-ingress-operator started at 2023-03-01 22:51:20 +0000 UTC (2 container statuses recorded)
    Mar  2 02:13:33.662: INFO: 	Container ingress-operator ready: true, restart count 0
    Mar  2 02:13:33.662: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:13:33.662: INFO: router-default-68bc8785b7-vl6bd from openshift-ingress started at 2023-03-02 02:09:21 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.662: INFO: 	Container router ready: true, restart count 0
    Mar  2 02:13:33.662: INFO: insights-operator-7b97bdcbd8-5zm5p from openshift-insights started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.662: INFO: 	Container insights-operator ready: true, restart count 1
    Mar  2 02:13:33.662: INFO: openshift-kube-proxy-xgktp from openshift-kube-proxy started at 2023-03-01 22:49:00 +0000 UTC (2 container statuses recorded)
    Mar  2 02:13:33.662: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar  2 02:13:33.662: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:13:33.662: INFO: kube-storage-version-migrator-operator-78bc45549c-qmzfr from openshift-kube-storage-version-migrator-operator started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.662: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
    Mar  2 02:13:33.662: INFO: marketplace-operator-66f5c8895c-l9r8f from openshift-marketplace started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.662: INFO: 	Container marketplace-operator ready: true, restart count 0
    Mar  2 02:13:33.662: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-03-02 02:09:26 +0000 UTC (6 container statuses recorded)
    Mar  2 02:13:33.662: INFO: 	Container alertmanager ready: true, restart count 1
    Mar  2 02:13:33.662: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Mar  2 02:13:33.662: INFO: 	Container config-reloader ready: true, restart count 0
    Mar  2 02:13:33.662: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:13:33.662: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Mar  2 02:13:33.662: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Mar  2 02:13:33.662: INFO: cluster-monitoring-operator-7ffc6575dd-kkb8m from openshift-monitoring started at 2023-03-01 22:51:20 +0000 UTC (2 container statuses recorded)
    Mar  2 02:13:33.662: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
    Mar  2 02:13:33.662: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:13:33.662: INFO: node-exporter-95fzh from openshift-monitoring started at 2023-03-01 22:56:40 +0000 UTC (2 container statuses recorded)
    Mar  2 02:13:33.662: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:13:33.662: INFO: 	Container node-exporter ready: true, restart count 0
    Mar  2 02:13:33.662: INFO: prometheus-adapter-95d69f68c-vtxdk from openshift-monitoring started at 2023-03-02 02:09:21 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.662: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Mar  2 02:13:33.663: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-03-02 02:09:26 +0000 UTC (6 container statuses recorded)
    Mar  2 02:13:33.663: INFO: 	Container config-reloader ready: true, restart count 0
    Mar  2 02:13:33.663: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:13:33.663: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Mar  2 02:13:33.663: INFO: 	Container prometheus ready: true, restart count 0
    Mar  2 02:13:33.663: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Mar  2 02:13:33.663: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Mar  2 02:13:33.663: INFO: prometheus-operator-admission-webhook-6d5fbffb86-sm9lf from openshift-monitoring started at 2023-03-02 02:09:21 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.663: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Mar  2 02:13:33.663: INFO: thanos-querier-6cd8656bbb-j85bx from openshift-monitoring started at 2023-03-02 02:09:21 +0000 UTC (6 container statuses recorded)
    Mar  2 02:13:33.663: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:13:33.663: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Mar  2 02:13:33.663: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Mar  2 02:13:33.663: INFO: 	Container oauth-proxy ready: true, restart count 0
    Mar  2 02:13:33.663: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Mar  2 02:13:33.663: INFO: 	Container thanos-query ready: true, restart count 0
    Mar  2 02:13:33.663: INFO: multus-89qpj from openshift-multus started at 2023-03-01 22:48:55 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.663: INFO: 	Container kube-multus ready: true, restart count 0
    Mar  2 02:13:33.663: INFO: multus-additional-cni-plugins-ff7kf from openshift-multus started at 2023-03-01 22:48:55 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.663: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Mar  2 02:13:33.663: INFO: multus-admission-controller-6f984f76c7-xsk9f from openshift-multus started at 2023-03-02 02:09:21 +0000 UTC (2 container statuses recorded)
    Mar  2 02:13:33.663: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:13:33.663: INFO: 	Container multus-admission-controller ready: true, restart count 0
    Mar  2 02:13:33.663: INFO: network-metrics-daemon-7m69v from openshift-multus started at 2023-03-01 22:48:56 +0000 UTC (2 container statuses recorded)
    Mar  2 02:13:33.663: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:13:33.663: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Mar  2 02:13:33.663: INFO: network-check-source-848f87f9d4-bxlxt from openshift-network-diagnostics started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.663: INFO: 	Container check-endpoints ready: true, restart count 0
    Mar  2 02:13:33.663: INFO: network-check-target-jqnqt from openshift-network-diagnostics started at 2023-03-01 22:49:03 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.663: INFO: 	Container network-check-target-container ready: true, restart count 0
    Mar  2 02:13:33.663: INFO: catalog-operator-657ff7bb9f-c7tqq from openshift-operator-lifecycle-manager started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.663: INFO: 	Container catalog-operator ready: true, restart count 0
    Mar  2 02:13:33.663: INFO: collect-profiles-27962010-nhpf6 from openshift-operator-lifecycle-manager started at 2023-03-02 01:30:00 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.663: INFO: 	Container collect-profiles ready: false, restart count 0
    Mar  2 02:13:33.663: INFO: olm-operator-56ccd7794f-4ffrj from openshift-operator-lifecycle-manager started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.663: INFO: 	Container olm-operator ready: true, restart count 0
    Mar  2 02:13:33.663: INFO: package-server-manager-599745f678-rmjg4 from openshift-operator-lifecycle-manager started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.663: INFO: 	Container package-server-manager ready: true, restart count 0
    Mar  2 02:13:33.663: INFO: packageserver-6c7c56df95-jq89f from openshift-operator-lifecycle-manager started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.663: INFO: 	Container packageserver ready: true, restart count 0
    Mar  2 02:13:33.663: INFO: metrics-784dfd67b7-rjkpg from openshift-roks-metrics started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.663: INFO: 	Container metrics ready: true, restart count 3
    Mar  2 02:13:33.663: INFO: push-gateway-646cc86d79-cgtct from openshift-roks-metrics started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.663: INFO: 	Container push-gateway ready: true, restart count 0
    Mar  2 02:13:33.663: INFO: service-ca-operator-6f8744cbc5-w5srs from openshift-service-ca-operator started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.663: INFO: 	Container service-ca-operator ready: true, restart count 1
    Mar  2 02:13:33.663: INFO: sonobuoy-systemd-logs-daemon-set-7266eb5e626146f7-n2dsq from sonobuoy started at 2023-03-02 01:07:14 +0000 UTC (2 container statuses recorded)
    Mar  2 02:13:33.663: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  2 02:13:33.663: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar  2 02:13:33.663: INFO: 
    Logging pods the apiserver thinks is on node 10.132.92.188 before test
    Mar  2 02:13:33.745: INFO: calico-node-mkvsm from calico-system started at 2023-03-01 22:49:50 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.745: INFO: 	Container calico-node ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: ibm-cloud-provider-ip-169-56-11-242-558f6ff69-xvh2v from ibm-system started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.745: INFO: 	Container ibm-cloud-provider-ip-169-56-11-242 ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: ibm-keepalived-watcher-zwfb8 from kube-system started at 2023-03-01 22:48:10 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.745: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: ibm-master-proxy-static-10.132.92.188 from kube-system started at 2023-03-01 22:48:07 +0000 UTC (2 container statuses recorded)
    Mar  2 02:13:33.745: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: 	Container pause ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: ibmcloud-block-storage-driver-qbg9f from kube-system started at 2023-03-01 22:48:16 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.745: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: vpn-f6c799ddd-xls7d from kube-system started at 2023-03-02 02:09:21 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.745: INFO: 	Container vpn ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: tuned-5z98r from openshift-cluster-node-tuning-operator started at 2023-03-01 22:55:42 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.745: INFO: 	Container tuned ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: csi-snapshot-controller-85d666c956-xsh6c from openshift-cluster-storage-operator started at 2023-03-01 22:51:57 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.745: INFO: 	Container snapshot-controller ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: csi-snapshot-webhook-6cbf55b956-wq8bh from openshift-cluster-storage-operator started at 2023-03-01 22:51:57 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.745: INFO: 	Container webhook ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: console-6c8dcd4bdd-ddzwf from openshift-console started at 2023-03-02 02:09:21 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.745: INFO: 	Container console ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: downloads-8b4bf4d5-b9kfz from openshift-console started at 2023-03-01 22:51:55 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.745: INFO: 	Container download-server ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: dns-default-nt82k from openshift-dns started at 2023-03-01 22:55:41 +0000 UTC (2 container statuses recorded)
    Mar  2 02:13:33.745: INFO: 	Container dns ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: node-resolver-wcng9 from openshift-dns started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.745: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: image-pruner-27961920-wd4pv from openshift-image-registry started at 2023-03-02 00:00:00 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.745: INFO: 	Container image-pruner ready: false, restart count 0
    Mar  2 02:13:33.745: INFO: image-registry-7fb7bd8df6-lb2bt from openshift-image-registry started at 2023-03-01 22:58:31 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.745: INFO: 	Container registry ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: node-ca-qlt4w from openshift-image-registry started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.745: INFO: 	Container node-ca ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: ingress-canary-qvcz9 from openshift-ingress-canary started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.745: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: router-default-68bc8785b7-wz8ct from openshift-ingress started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.745: INFO: 	Container router ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: openshift-kube-proxy-mth7h from openshift-kube-proxy started at 2023-03-01 22:49:00 +0000 UTC (2 container statuses recorded)
    Mar  2 02:13:33.745: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: migrator-5687b466dd-glln7 from openshift-kube-storage-version-migrator started at 2023-03-01 22:51:57 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.745: INFO: 	Container migrator ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: certified-operators-8n5vw from openshift-marketplace started at 2023-03-02 00:13:35 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.745: INFO: 	Container registry-server ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: community-operators-lngtl from openshift-marketplace started at 2023-03-01 22:54:15 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.745: INFO: 	Container registry-server ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: redhat-marketplace-ld8wh from openshift-marketplace started at 2023-03-02 00:27:02 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.745: INFO: 	Container registry-server ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: redhat-operators-tk548 from openshift-marketplace started at 2023-03-01 22:54:14 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.745: INFO: 	Container registry-server ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-03-01 22:58:39 +0000 UTC (6 container statuses recorded)
    Mar  2 02:13:33.745: INFO: 	Container alertmanager ready: true, restart count 1
    Mar  2 02:13:33.745: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: 	Container config-reloader ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: kube-state-metrics-554994774b-4xpf2 from openshift-monitoring started at 2023-03-02 02:09:21 +0000 UTC (3 container statuses recorded)
    Mar  2 02:13:33.745: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: node-exporter-gj7lr from openshift-monitoring started at 2023-03-01 22:56:39 +0000 UTC (2 container statuses recorded)
    Mar  2 02:13:33.745: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: 	Container node-exporter ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: openshift-state-metrics-56599d9b6d-6sw48 from openshift-monitoring started at 2023-03-01 22:56:39 +0000 UTC (3 container statuses recorded)
    Mar  2 02:13:33.745: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: 	Container openshift-state-metrics ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: prometheus-adapter-95d69f68c-gtx2d from openshift-monitoring started at 2023-03-01 22:58:02 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.745: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-03-01 22:58:30 +0000 UTC (6 container statuses recorded)
    Mar  2 02:13:33.745: INFO: 	Container config-reloader ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: 	Container prometheus ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: prometheus-operator-9bf584898-89sgf from openshift-monitoring started at 2023-03-01 22:56:19 +0000 UTC (2 container statuses recorded)
    Mar  2 02:13:33.745: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: 	Container prometheus-operator ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: prometheus-operator-admission-webhook-6d5fbffb86-hhfbw from openshift-monitoring started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.745: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: telemeter-client-769c487d5b-l5fxr from openshift-monitoring started at 2023-03-02 02:09:21 +0000 UTC (3 container statuses recorded)
    Mar  2 02:13:33.745: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: 	Container reload ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: 	Container telemeter-client ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: thanos-querier-6cd8656bbb-8hv4v from openshift-monitoring started at 2023-03-01 22:56:48 +0000 UTC (6 container statuses recorded)
    Mar  2 02:13:33.745: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: 	Container oauth-proxy ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: 	Container thanos-query ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: multus-additional-cni-plugins-5pjnd from openshift-multus started at 2023-03-01 22:48:55 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.745: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: multus-admission-controller-6f984f76c7-hclf4 from openshift-multus started at 2023-03-01 22:55:41 +0000 UTC (2 container statuses recorded)
    Mar  2 02:13:33.745: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: 	Container multus-admission-controller ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: multus-g4k8f from openshift-multus started at 2023-03-01 22:48:55 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.745: INFO: 	Container kube-multus ready: true, restart count 0
    Mar  2 02:13:33.745: INFO: network-metrics-daemon-95rvd from openshift-multus started at 2023-03-01 22:48:56 +0000 UTC (2 container statuses recorded)
    Mar  2 02:13:33.746: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:13:33.746: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Mar  2 02:13:33.746: INFO: network-check-target-4nkgt from openshift-network-diagnostics started at 2023-03-01 22:49:03 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.746: INFO: 	Container network-check-target-container ready: true, restart count 0
    Mar  2 02:13:33.746: INFO: network-operator-68bbf8cc76-mg97z from openshift-network-operator started at 2023-03-01 22:48:19 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.746: INFO: 	Container network-operator ready: true, restart count 1
    Mar  2 02:13:33.746: INFO: collect-profiles-27962025-rd7rx from openshift-operator-lifecycle-manager started at 2023-03-02 01:45:00 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.746: INFO: 	Container collect-profiles ready: false, restart count 0
    Mar  2 02:13:33.746: INFO: collect-profiles-27962040-2rff7 from openshift-operator-lifecycle-manager started at 2023-03-02 02:00:00 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.746: INFO: 	Container collect-profiles ready: false, restart count 0
    Mar  2 02:13:33.746: INFO: packageserver-6c7c56df95-7wr7f from openshift-operator-lifecycle-manager started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.746: INFO: 	Container packageserver ready: true, restart count 0
    Mar  2 02:13:33.746: INFO: service-ca-578d67f6f8-89gtp from openshift-service-ca started at 2023-03-01 22:52:15 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.746: INFO: 	Container service-ca-controller ready: true, restart count 0
    Mar  2 02:13:33.746: INFO: sonobuoy-e2e-job-c6c00a6e72aa4160 from sonobuoy started at 2023-03-02 01:07:14 +0000 UTC (2 container statuses recorded)
    Mar  2 02:13:33.746: INFO: 	Container e2e ready: true, restart count 0
    Mar  2 02:13:33.746: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  2 02:13:33.746: INFO: sonobuoy-systemd-logs-daemon-set-7266eb5e626146f7-fgqrz from sonobuoy started at 2023-03-02 01:07:14 +0000 UTC (2 container statuses recorded)
    Mar  2 02:13:33.746: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  2 02:13:33.746: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar  2 02:13:33.746: INFO: tigera-operator-84f4f4565b-5w7s7 from tigera-operator started at 2023-03-01 22:48:19 +0000 UTC (1 container statuses recorded)
    Mar  2 02:13:33.746: INFO: 	Container tigera-operator ready: true, restart count 2
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:699
    STEP: Trying to launch a pod without a label to get a node which can launch it. 03/02/23 02:13:33.746
    Mar  2 02:13:33.862: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-631" to be "running"
    Mar  2 02:13:33.877: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 14.634674ms
    Mar  2 02:13:35.893: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030184875s
    Mar  2 02:13:37.894: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.031332788s
    Mar  2 02:13:37.894: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 03/02/23 02:13:37.907
    STEP: Trying to apply a random label on the found node. 03/02/23 02:13:37.97
    STEP: verifying the node has the label kubernetes.io/e2e-e3796432-c892-4100-a35d-1d6d2d7561be 95 03/02/23 02:13:38.01
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 03/02/23 02:13:38.034
    Mar  2 02:13:38.083: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-631" to be "not pending"
    Mar  2 02:13:38.096: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.384159ms
    Mar  2 02:13:40.115: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031874858s
    Mar  2 02:13:42.114: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 4.030301809s
    Mar  2 02:13:42.114: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.132.92.143 on the node which pod4 resides and expect not scheduled 03/02/23 02:13:42.114
    Mar  2 02:13:42.167: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-631" to be "not pending"
    Mar  2 02:13:42.184: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.445081ms
    Mar  2 02:13:44.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03195654s
    Mar  2 02:13:46.201: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033210573s
    Mar  2 02:13:48.201: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.033140763s
    Mar  2 02:13:50.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.032069685s
    Mar  2 02:13:52.202: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.03436837s
    Mar  2 02:13:54.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.031563206s
    Mar  2 02:13:56.210: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.042257349s
    Mar  2 02:13:58.226: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.058991757s
    Mar  2 02:14:00.211: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.043125275s
    Mar  2 02:14:02.204: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.036221604s
    Mar  2 02:14:04.201: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.033322659s
    Mar  2 02:14:06.198: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.030864544s
    Mar  2 02:14:08.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.031050657s
    Mar  2 02:14:10.202: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.034872831s
    Mar  2 02:14:12.198: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.030650435s
    Mar  2 02:14:14.223: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.055720678s
    Mar  2 02:14:16.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.03279343s
    Mar  2 02:14:18.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.032400333s
    Mar  2 02:14:20.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.032148567s
    Mar  2 02:14:22.217: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.049170386s
    Mar  2 02:14:24.202: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.034017313s
    Mar  2 02:14:26.203: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.035082584s
    Mar  2 02:14:28.210: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.042521146s
    Mar  2 02:14:30.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.031319798s
    Mar  2 02:14:32.202: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.034275163s
    Mar  2 02:14:34.201: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.03365694s
    Mar  2 02:14:36.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.032629533s
    Mar  2 02:14:38.203: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.035334229s
    Mar  2 02:14:40.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.031918829s
    Mar  2 02:14:42.211: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.042963213s
    Mar  2 02:14:44.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.032034914s
    Mar  2 02:14:46.201: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.033577999s
    Mar  2 02:14:48.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.031715013s
    Mar  2 02:14:50.232: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.06459281s
    Mar  2 02:14:52.202: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.034623168s
    Mar  2 02:14:54.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.032397437s
    Mar  2 02:14:56.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.031305476s
    Mar  2 02:14:58.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.031397107s
    Mar  2 02:15:00.207: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.039417605s
    Mar  2 02:15:02.234: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.066651911s
    Mar  2 02:15:04.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.032097155s
    Mar  2 02:15:06.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.061390653s
    Mar  2 02:15:08.206: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.038924885s
    Mar  2 02:15:10.240: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.07293474s
    Mar  2 02:15:12.206: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.038258703s
    Mar  2 02:15:14.203: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.035910916s
    Mar  2 02:15:16.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.031602987s
    Mar  2 02:15:18.198: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.030995188s
    Mar  2 02:15:20.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.031275485s
    Mar  2 02:15:22.208: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.04088086s
    Mar  2 02:15:24.220: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.052742687s
    Mar  2 02:15:26.213: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.045828096s
    Mar  2 02:15:28.205: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.037836227s
    Mar  2 02:15:30.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.032018394s
    Mar  2 02:15:32.205: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.03749332s
    Mar  2 02:15:34.208: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.040030183s
    Mar  2 02:15:36.203: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.035945136s
    Mar  2 02:15:38.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.031919634s
    Mar  2 02:15:40.204: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.036388457s
    Mar  2 02:15:42.219: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.051071918s
    Mar  2 02:15:44.236: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.068712195s
    Mar  2 02:15:46.206: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.038185479s
    Mar  2 02:15:48.201: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.033484325s
    Mar  2 02:15:50.215: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.047016904s
    Mar  2 02:15:52.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.090706579s
    Mar  2 02:15:54.203: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.035685965s
    Mar  2 02:15:56.203: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.035514791s
    Mar  2 02:15:58.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.031537284s
    Mar  2 02:16:00.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.032153561s
    Mar  2 02:16:02.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.031714211s
    Mar  2 02:16:04.201: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.033031575s
    Mar  2 02:16:06.203: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.035109755s
    Mar  2 02:16:08.219: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.05117434s
    Mar  2 02:16:10.214: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.046801301s
    Mar  2 02:16:12.201: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.033602341s
    Mar  2 02:16:14.231: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.063563454s
    Mar  2 02:16:16.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.03255009s
    Mar  2 02:16:18.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.031385832s
    Mar  2 02:16:20.202: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.034008958s
    Mar  2 02:16:22.205: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.037196712s
    Mar  2 02:16:24.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.031295267s
    Mar  2 02:16:26.198: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.03021945s
    Mar  2 02:16:28.207: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.039978799s
    Mar  2 02:16:30.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.031224414s
    Mar  2 02:16:32.202: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.034908244s
    Mar  2 02:16:34.202: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.034328562s
    Mar  2 02:16:36.216: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.048060969s
    Mar  2 02:16:38.201: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.03310325s
    Mar  2 02:16:40.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.031193749s
    Mar  2 02:16:42.201: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.033987601s
    Mar  2 02:16:44.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.070160625s
    Mar  2 02:16:46.203: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.035833867s
    Mar  2 02:16:48.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.031781315s
    Mar  2 02:16:50.228: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.060542882s
    Mar  2 02:16:52.201: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.033062666s
    Mar  2 02:16:54.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.032011619s
    Mar  2 02:16:56.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.031281882s
    Mar  2 02:16:58.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.031896744s
    Mar  2 02:17:00.201: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.033351335s
    Mar  2 02:17:02.205: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.037882188s
    Mar  2 02:17:04.201: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.033502098s
    Mar  2 02:17:06.204: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.036625102s
    Mar  2 02:17:08.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.031353641s
    Mar  2 02:17:10.201: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.03304833s
    Mar  2 02:17:12.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.032190702s
    Mar  2 02:17:14.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.031492682s
    Mar  2 02:17:16.214: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.046077662s
    Mar  2 02:17:18.204: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.036447936s
    Mar  2 02:17:20.198: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.030047593s
    Mar  2 02:17:22.204: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.036009183s
    Mar  2 02:17:24.206: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.038213299s
    Mar  2 02:17:26.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.031553535s
    Mar  2 02:17:28.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.032541825s
    Mar  2 02:17:30.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.032254602s
    Mar  2 02:17:32.201: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.033542033s
    Mar  2 02:17:34.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.032665534s
    Mar  2 02:17:36.216: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.048735727s
    Mar  2 02:17:38.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.032662869s
    Mar  2 02:17:40.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.031371759s
    Mar  2 02:17:42.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.03298087s
    Mar  2 02:17:44.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.032929284s
    Mar  2 02:17:46.207: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.039102676s
    Mar  2 02:17:48.204: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.036469556s
    Mar  2 02:17:50.206: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.038409757s
    Mar  2 02:17:52.202: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.034247338s
    Mar  2 02:17:54.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.032882515s
    Mar  2 02:17:56.197: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.029562587s
    Mar  2 02:17:58.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.031295537s
    Mar  2 02:18:00.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.03226431s
    Mar  2 02:18:02.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.032037857s
    Mar  2 02:18:04.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.032768634s
    Mar  2 02:18:06.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.032554577s
    Mar  2 02:18:08.198: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.030494194s
    Mar  2 02:18:10.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.032431746s
    Mar  2 02:18:12.201: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.033374316s
    Mar  2 02:18:14.205: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.037733931s
    Mar  2 02:18:16.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.032439481s
    Mar  2 02:18:18.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.03274069s
    Mar  2 02:18:20.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.031562772s
    Mar  2 02:18:22.201: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.033336777s
    Mar  2 02:18:24.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.031399578s
    Mar  2 02:18:26.252: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.084036443s
    Mar  2 02:18:28.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.03166657s
    Mar  2 02:18:30.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.032159796s
    Mar  2 02:18:32.202: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.034374066s
    Mar  2 02:18:34.199: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.031836165s
    Mar  2 02:18:36.209: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.041990127s
    Mar  2 02:18:38.203: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.035491704s
    Mar  2 02:18:40.200: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.032745478s
    Mar  2 02:18:42.202: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.03490912s
    Mar  2 02:18:42.219: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.051925502s
    STEP: removing the label kubernetes.io/e2e-e3796432-c892-4100-a35d-1d6d2d7561be off the node 10.132.92.143 03/02/23 02:18:42.219
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-e3796432-c892-4100-a35d-1d6d2d7561be 03/02/23 02:18:42.292
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Mar  2 02:18:42.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-631" for this suite. 03/02/23 02:18:42.34
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:18:42.388
Mar  2 02:18:42.388: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename emptydir 03/02/23 02:18:42.389
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:18:42.46
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:18:42.507
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
STEP: Creating a pod to test emptydir 0777 on node default medium 03/02/23 02:18:42.525
Mar  2 02:18:42.746: INFO: Waiting up to 5m0s for pod "pod-a3b270d9-a0ba-48b7-b90b-b5b5e03215e6" in namespace "emptydir-8182" to be "Succeeded or Failed"
Mar  2 02:18:42.814: INFO: Pod "pod-a3b270d9-a0ba-48b7-b90b-b5b5e03215e6": Phase="Pending", Reason="", readiness=false. Elapsed: 68.209898ms
Mar  2 02:18:44.830: INFO: Pod "pod-a3b270d9-a0ba-48b7-b90b-b5b5e03215e6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.084182468s
Mar  2 02:18:46.834: INFO: Pod "pod-a3b270d9-a0ba-48b7-b90b-b5b5e03215e6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.088829528s
Mar  2 02:18:48.830: INFO: Pod "pod-a3b270d9-a0ba-48b7-b90b-b5b5e03215e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.084919868s
STEP: Saw pod success 03/02/23 02:18:48.831
Mar  2 02:18:48.831: INFO: Pod "pod-a3b270d9-a0ba-48b7-b90b-b5b5e03215e6" satisfied condition "Succeeded or Failed"
Mar  2 02:18:48.849: INFO: Trying to get logs from node 10.132.92.143 pod pod-a3b270d9-a0ba-48b7-b90b-b5b5e03215e6 container test-container: <nil>
STEP: delete the pod 03/02/23 02:18:48.934
Mar  2 02:18:48.983: INFO: Waiting for pod pod-a3b270d9-a0ba-48b7-b90b-b5b5e03215e6 to disappear
Mar  2 02:18:48.998: INFO: Pod pod-a3b270d9-a0ba-48b7-b90b-b5b5e03215e6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  2 02:18:48.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8182" for this suite. 03/02/23 02:18:49.021
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":210,"skipped":3895,"failed":0}
------------------------------
• [SLOW TEST] [6.658 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:18:42.388
    Mar  2 02:18:42.388: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename emptydir 03/02/23 02:18:42.389
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:18:42.46
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:18:42.507
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:216
    STEP: Creating a pod to test emptydir 0777 on node default medium 03/02/23 02:18:42.525
    Mar  2 02:18:42.746: INFO: Waiting up to 5m0s for pod "pod-a3b270d9-a0ba-48b7-b90b-b5b5e03215e6" in namespace "emptydir-8182" to be "Succeeded or Failed"
    Mar  2 02:18:42.814: INFO: Pod "pod-a3b270d9-a0ba-48b7-b90b-b5b5e03215e6": Phase="Pending", Reason="", readiness=false. Elapsed: 68.209898ms
    Mar  2 02:18:44.830: INFO: Pod "pod-a3b270d9-a0ba-48b7-b90b-b5b5e03215e6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.084182468s
    Mar  2 02:18:46.834: INFO: Pod "pod-a3b270d9-a0ba-48b7-b90b-b5b5e03215e6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.088829528s
    Mar  2 02:18:48.830: INFO: Pod "pod-a3b270d9-a0ba-48b7-b90b-b5b5e03215e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.084919868s
    STEP: Saw pod success 03/02/23 02:18:48.831
    Mar  2 02:18:48.831: INFO: Pod "pod-a3b270d9-a0ba-48b7-b90b-b5b5e03215e6" satisfied condition "Succeeded or Failed"
    Mar  2 02:18:48.849: INFO: Trying to get logs from node 10.132.92.143 pod pod-a3b270d9-a0ba-48b7-b90b-b5b5e03215e6 container test-container: <nil>
    STEP: delete the pod 03/02/23 02:18:48.934
    Mar  2 02:18:48.983: INFO: Waiting for pod pod-a3b270d9-a0ba-48b7-b90b-b5b5e03215e6 to disappear
    Mar  2 02:18:48.998: INFO: Pod pod-a3b270d9-a0ba-48b7-b90b-b5b5e03215e6 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  2 02:18:48.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8182" for this suite. 03/02/23 02:18:49.021
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:18:49.048
Mar  2 02:18:49.048: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename services 03/02/23 02:18:49.049
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:18:49.11
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:18:49.122
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
STEP: creating service endpoint-test2 in namespace services-4953 03/02/23 02:18:49.15
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4953 to expose endpoints map[] 03/02/23 02:18:49.2
Mar  2 02:18:49.221: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Mar  2 02:18:50.257: INFO: successfully validated that service endpoint-test2 in namespace services-4953 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-4953 03/02/23 02:18:50.257
Mar  2 02:18:50.326: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-4953" to be "running and ready"
Mar  2 02:18:50.344: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 17.766273ms
Mar  2 02:18:50.344: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 02:18:52.360: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033117519s
Mar  2 02:18:52.360: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 02:18:54.363: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.035874663s
Mar  2 02:18:54.363: INFO: The phase of Pod pod1 is Running (Ready = true)
Mar  2 02:18:54.363: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4953 to expose endpoints map[pod1:[80]] 03/02/23 02:18:54.378
Mar  2 02:18:54.429: INFO: successfully validated that service endpoint-test2 in namespace services-4953 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 03/02/23 02:18:54.43
Mar  2 02:18:54.430: INFO: Creating new exec pod
Mar  2 02:18:54.510: INFO: Waiting up to 5m0s for pod "execpodp9vlk" in namespace "services-4953" to be "running"
Mar  2 02:18:54.552: INFO: Pod "execpodp9vlk": Phase="Pending", Reason="", readiness=false. Elapsed: 41.985356ms
Mar  2 02:18:56.568: INFO: Pod "execpodp9vlk": Phase="Running", Reason="", readiness=true. Elapsed: 2.057702118s
Mar  2 02:18:56.568: INFO: Pod "execpodp9vlk" satisfied condition "running"
Mar  2 02:18:57.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-4953 exec execpodp9vlk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Mar  2 02:18:57.929: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Mar  2 02:18:57.929: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 02:18:57.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-4953 exec execpodp9vlk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.185.56 80'
Mar  2 02:18:58.302: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.185.56 80\nConnection to 172.21.185.56 80 port [tcp/http] succeeded!\n"
Mar  2 02:18:58.302: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-4953 03/02/23 02:18:58.302
Mar  2 02:18:58.351: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-4953" to be "running and ready"
Mar  2 02:18:58.409: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 58.15275ms
Mar  2 02:18:58.409: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 02:19:00.425: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.074102529s
Mar  2 02:19:00.425: INFO: The phase of Pod pod2 is Running (Ready = true)
Mar  2 02:19:00.425: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4953 to expose endpoints map[pod1:[80] pod2:[80]] 03/02/23 02:19:00.441
Mar  2 02:19:00.504: INFO: successfully validated that service endpoint-test2 in namespace services-4953 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 03/02/23 02:19:00.504
Mar  2 02:19:01.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-4953 exec execpodp9vlk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Mar  2 02:19:01.845: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Mar  2 02:19:01.845: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 02:19:01.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-4953 exec execpodp9vlk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.185.56 80'
Mar  2 02:19:02.266: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.185.56 80\nConnection to 172.21.185.56 80 port [tcp/http] succeeded!\n"
Mar  2 02:19:02.267: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-4953 03/02/23 02:19:02.267
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4953 to expose endpoints map[pod2:[80]] 03/02/23 02:19:02.332
Mar  2 02:19:02.424: INFO: successfully validated that service endpoint-test2 in namespace services-4953 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 03/02/23 02:19:02.424
Mar  2 02:19:03.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-4953 exec execpodp9vlk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Mar  2 02:19:03.860: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Mar  2 02:19:03.860: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 02:19:03.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-4953 exec execpodp9vlk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.185.56 80'
Mar  2 02:19:04.188: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.185.56 80\nConnection to 172.21.185.56 80 port [tcp/http] succeeded!\n"
Mar  2 02:19:04.188: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-4953 03/02/23 02:19:04.188
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4953 to expose endpoints map[] 03/02/23 02:19:04.248
Mar  2 02:19:04.314: INFO: successfully validated that service endpoint-test2 in namespace services-4953 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  2 02:19:04.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4953" for this suite. 03/02/23 02:19:04.444
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","completed":211,"skipped":3905,"failed":0}
------------------------------
• [SLOW TEST] [15.423 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:18:49.048
    Mar  2 02:18:49.048: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename services 03/02/23 02:18:49.049
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:18:49.11
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:18:49.122
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:791
    STEP: creating service endpoint-test2 in namespace services-4953 03/02/23 02:18:49.15
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4953 to expose endpoints map[] 03/02/23 02:18:49.2
    Mar  2 02:18:49.221: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
    Mar  2 02:18:50.257: INFO: successfully validated that service endpoint-test2 in namespace services-4953 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-4953 03/02/23 02:18:50.257
    Mar  2 02:18:50.326: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-4953" to be "running and ready"
    Mar  2 02:18:50.344: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 17.766273ms
    Mar  2 02:18:50.344: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 02:18:52.360: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033117519s
    Mar  2 02:18:52.360: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 02:18:54.363: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.035874663s
    Mar  2 02:18:54.363: INFO: The phase of Pod pod1 is Running (Ready = true)
    Mar  2 02:18:54.363: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4953 to expose endpoints map[pod1:[80]] 03/02/23 02:18:54.378
    Mar  2 02:18:54.429: INFO: successfully validated that service endpoint-test2 in namespace services-4953 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 03/02/23 02:18:54.43
    Mar  2 02:18:54.430: INFO: Creating new exec pod
    Mar  2 02:18:54.510: INFO: Waiting up to 5m0s for pod "execpodp9vlk" in namespace "services-4953" to be "running"
    Mar  2 02:18:54.552: INFO: Pod "execpodp9vlk": Phase="Pending", Reason="", readiness=false. Elapsed: 41.985356ms
    Mar  2 02:18:56.568: INFO: Pod "execpodp9vlk": Phase="Running", Reason="", readiness=true. Elapsed: 2.057702118s
    Mar  2 02:18:56.568: INFO: Pod "execpodp9vlk" satisfied condition "running"
    Mar  2 02:18:57.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-4953 exec execpodp9vlk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Mar  2 02:18:57.929: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Mar  2 02:18:57.929: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  2 02:18:57.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-4953 exec execpodp9vlk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.185.56 80'
    Mar  2 02:18:58.302: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.185.56 80\nConnection to 172.21.185.56 80 port [tcp/http] succeeded!\n"
    Mar  2 02:18:58.302: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Creating pod pod2 in namespace services-4953 03/02/23 02:18:58.302
    Mar  2 02:18:58.351: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-4953" to be "running and ready"
    Mar  2 02:18:58.409: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 58.15275ms
    Mar  2 02:18:58.409: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 02:19:00.425: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.074102529s
    Mar  2 02:19:00.425: INFO: The phase of Pod pod2 is Running (Ready = true)
    Mar  2 02:19:00.425: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4953 to expose endpoints map[pod1:[80] pod2:[80]] 03/02/23 02:19:00.441
    Mar  2 02:19:00.504: INFO: successfully validated that service endpoint-test2 in namespace services-4953 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 03/02/23 02:19:00.504
    Mar  2 02:19:01.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-4953 exec execpodp9vlk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Mar  2 02:19:01.845: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Mar  2 02:19:01.845: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  2 02:19:01.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-4953 exec execpodp9vlk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.185.56 80'
    Mar  2 02:19:02.266: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.185.56 80\nConnection to 172.21.185.56 80 port [tcp/http] succeeded!\n"
    Mar  2 02:19:02.267: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-4953 03/02/23 02:19:02.267
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4953 to expose endpoints map[pod2:[80]] 03/02/23 02:19:02.332
    Mar  2 02:19:02.424: INFO: successfully validated that service endpoint-test2 in namespace services-4953 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 03/02/23 02:19:02.424
    Mar  2 02:19:03.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-4953 exec execpodp9vlk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Mar  2 02:19:03.860: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Mar  2 02:19:03.860: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  2 02:19:03.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-4953 exec execpodp9vlk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.185.56 80'
    Mar  2 02:19:04.188: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.185.56 80\nConnection to 172.21.185.56 80 port [tcp/http] succeeded!\n"
    Mar  2 02:19:04.188: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod2 in namespace services-4953 03/02/23 02:19:04.188
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4953 to expose endpoints map[] 03/02/23 02:19:04.248
    Mar  2 02:19:04.314: INFO: successfully validated that service endpoint-test2 in namespace services-4953 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  2 02:19:04.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4953" for this suite. 03/02/23 02:19:04.444
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:19:04.473
Mar  2 02:19:04.473: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename conformance-tests 03/02/23 02:19:04.477
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:19:04.539
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:19:04.591
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 03/02/23 02:19:04.606
Mar  2 02:19:04.607: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:187
Mar  2 02:19:04.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "conformance-tests-1511" for this suite. 03/02/23 02:19:04.684
{"msg":"PASSED [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]","completed":212,"skipped":3909,"failed":0}
------------------------------
• [0.245 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:19:04.473
    Mar  2 02:19:04.473: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename conformance-tests 03/02/23 02:19:04.477
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:19:04.539
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:19:04.591
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 03/02/23 02:19:04.606
    Mar  2 02:19:04.607: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:187
    Mar  2 02:19:04.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "conformance-tests-1511" for this suite. 03/02/23 02:19:04.684
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:19:04.718
Mar  2 02:19:04.719: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename projected 03/02/23 02:19:04.732
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:19:04.794
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:19:04.806
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
STEP: Creating configMap with name projected-configmap-test-volume-62d79fcc-e020-4892-86d8-71937ff26dd8 03/02/23 02:19:04.818
STEP: Creating a pod to test consume configMaps 03/02/23 02:19:04.84
Mar  2 02:19:04.924: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0ff688f9-d2b1-4df6-a67a-5745f1fd118b" in namespace "projected-4758" to be "Succeeded or Failed"
Mar  2 02:19:04.945: INFO: Pod "pod-projected-configmaps-0ff688f9-d2b1-4df6-a67a-5745f1fd118b": Phase="Pending", Reason="", readiness=false. Elapsed: 21.316396ms
Mar  2 02:19:07.002: INFO: Pod "pod-projected-configmaps-0ff688f9-d2b1-4df6-a67a-5745f1fd118b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.078742142s
Mar  2 02:19:08.988: INFO: Pod "pod-projected-configmaps-0ff688f9-d2b1-4df6-a67a-5745f1fd118b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.064723748s
STEP: Saw pod success 03/02/23 02:19:08.989
Mar  2 02:19:08.989: INFO: Pod "pod-projected-configmaps-0ff688f9-d2b1-4df6-a67a-5745f1fd118b" satisfied condition "Succeeded or Failed"
Mar  2 02:19:09.004: INFO: Trying to get logs from node 10.132.92.143 pod pod-projected-configmaps-0ff688f9-d2b1-4df6-a67a-5745f1fd118b container agnhost-container: <nil>
STEP: delete the pod 03/02/23 02:19:09.048
Mar  2 02:19:09.089: INFO: Waiting for pod pod-projected-configmaps-0ff688f9-d2b1-4df6-a67a-5745f1fd118b to disappear
Mar  2 02:19:09.103: INFO: Pod pod-projected-configmaps-0ff688f9-d2b1-4df6-a67a-5745f1fd118b no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar  2 02:19:09.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4758" for this suite. 03/02/23 02:19:09.137
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":213,"skipped":3915,"failed":0}
------------------------------
• [4.446 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:19:04.718
    Mar  2 02:19:04.719: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename projected 03/02/23 02:19:04.732
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:19:04.794
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:19:04.806
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:73
    STEP: Creating configMap with name projected-configmap-test-volume-62d79fcc-e020-4892-86d8-71937ff26dd8 03/02/23 02:19:04.818
    STEP: Creating a pod to test consume configMaps 03/02/23 02:19:04.84
    Mar  2 02:19:04.924: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0ff688f9-d2b1-4df6-a67a-5745f1fd118b" in namespace "projected-4758" to be "Succeeded or Failed"
    Mar  2 02:19:04.945: INFO: Pod "pod-projected-configmaps-0ff688f9-d2b1-4df6-a67a-5745f1fd118b": Phase="Pending", Reason="", readiness=false. Elapsed: 21.316396ms
    Mar  2 02:19:07.002: INFO: Pod "pod-projected-configmaps-0ff688f9-d2b1-4df6-a67a-5745f1fd118b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.078742142s
    Mar  2 02:19:08.988: INFO: Pod "pod-projected-configmaps-0ff688f9-d2b1-4df6-a67a-5745f1fd118b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.064723748s
    STEP: Saw pod success 03/02/23 02:19:08.989
    Mar  2 02:19:08.989: INFO: Pod "pod-projected-configmaps-0ff688f9-d2b1-4df6-a67a-5745f1fd118b" satisfied condition "Succeeded or Failed"
    Mar  2 02:19:09.004: INFO: Trying to get logs from node 10.132.92.143 pod pod-projected-configmaps-0ff688f9-d2b1-4df6-a67a-5745f1fd118b container agnhost-container: <nil>
    STEP: delete the pod 03/02/23 02:19:09.048
    Mar  2 02:19:09.089: INFO: Waiting for pod pod-projected-configmaps-0ff688f9-d2b1-4df6-a67a-5745f1fd118b to disappear
    Mar  2 02:19:09.103: INFO: Pod pod-projected-configmaps-0ff688f9-d2b1-4df6-a67a-5745f1fd118b no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar  2 02:19:09.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4758" for this suite. 03/02/23 02:19:09.137
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:19:09.167
Mar  2 02:19:09.167: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename replicaset 03/02/23 02:19:09.169
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:19:09.229
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:19:09.243
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 03/02/23 02:19:09.286
W0302 02:19:09.337604      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Verify that the required pods have come up. 03/02/23 02:19:09.337
Mar  2 02:19:09.366: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar  2 02:19:14.388: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/02/23 02:19:14.388
STEP: Getting /status 03/02/23 02:19:14.388
Mar  2 02:19:14.413: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 03/02/23 02:19:14.413
Mar  2 02:19:14.508: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 03/02/23 02:19:14.508
Mar  2 02:19:14.515: INFO: Observed &ReplicaSet event: ADDED
Mar  2 02:19:14.516: INFO: Observed &ReplicaSet event: MODIFIED
Mar  2 02:19:14.516: INFO: Observed &ReplicaSet event: MODIFIED
Mar  2 02:19:14.525: INFO: Observed &ReplicaSet event: MODIFIED
Mar  2 02:19:14.525: INFO: Found replicaset test-rs in namespace replicaset-1948 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar  2 02:19:14.525: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 03/02/23 02:19:14.525
Mar  2 02:19:14.525: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Mar  2 02:19:14.571: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 03/02/23 02:19:14.571
Mar  2 02:19:14.577: INFO: Observed &ReplicaSet event: ADDED
Mar  2 02:19:14.577: INFO: Observed &ReplicaSet event: MODIFIED
Mar  2 02:19:14.578: INFO: Observed &ReplicaSet event: MODIFIED
Mar  2 02:19:14.578: INFO: Observed &ReplicaSet event: MODIFIED
Mar  2 02:19:14.578: INFO: Observed replicaset test-rs in namespace replicaset-1948 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar  2 02:19:14.578: INFO: Observed &ReplicaSet event: MODIFIED
Mar  2 02:19:14.578: INFO: Found replicaset test-rs in namespace replicaset-1948 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Mar  2 02:19:14.578: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Mar  2 02:19:14.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1948" for this suite. 03/02/23 02:19:14.599
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","completed":214,"skipped":3936,"failed":0}
------------------------------
• [SLOW TEST] [5.516 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:19:09.167
    Mar  2 02:19:09.167: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename replicaset 03/02/23 02:19:09.169
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:19:09.229
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:19:09.243
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 03/02/23 02:19:09.286
    W0302 02:19:09.337604      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Verify that the required pods have come up. 03/02/23 02:19:09.337
    Mar  2 02:19:09.366: INFO: Pod name sample-pod: Found 0 pods out of 1
    Mar  2 02:19:14.388: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/02/23 02:19:14.388
    STEP: Getting /status 03/02/23 02:19:14.388
    Mar  2 02:19:14.413: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 03/02/23 02:19:14.413
    Mar  2 02:19:14.508: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 03/02/23 02:19:14.508
    Mar  2 02:19:14.515: INFO: Observed &ReplicaSet event: ADDED
    Mar  2 02:19:14.516: INFO: Observed &ReplicaSet event: MODIFIED
    Mar  2 02:19:14.516: INFO: Observed &ReplicaSet event: MODIFIED
    Mar  2 02:19:14.525: INFO: Observed &ReplicaSet event: MODIFIED
    Mar  2 02:19:14.525: INFO: Found replicaset test-rs in namespace replicaset-1948 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Mar  2 02:19:14.525: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 03/02/23 02:19:14.525
    Mar  2 02:19:14.525: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Mar  2 02:19:14.571: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 03/02/23 02:19:14.571
    Mar  2 02:19:14.577: INFO: Observed &ReplicaSet event: ADDED
    Mar  2 02:19:14.577: INFO: Observed &ReplicaSet event: MODIFIED
    Mar  2 02:19:14.578: INFO: Observed &ReplicaSet event: MODIFIED
    Mar  2 02:19:14.578: INFO: Observed &ReplicaSet event: MODIFIED
    Mar  2 02:19:14.578: INFO: Observed replicaset test-rs in namespace replicaset-1948 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar  2 02:19:14.578: INFO: Observed &ReplicaSet event: MODIFIED
    Mar  2 02:19:14.578: INFO: Found replicaset test-rs in namespace replicaset-1948 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Mar  2 02:19:14.578: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Mar  2 02:19:14.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-1948" for this suite. 03/02/23 02:19:14.599
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:19:14.689
Mar  2 02:19:14.690: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename replication-controller 03/02/23 02:19:14.691
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:19:14.753
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:19:14.764
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
STEP: Given a ReplicationController is created 03/02/23 02:19:14.775
W0302 02:19:14.796891      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod-release" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-release" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-release" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-release" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: When the matched label of one of its pods change 03/02/23 02:19:14.797
Mar  2 02:19:14.826: INFO: Pod name pod-release: Found 0 pods out of 1
Mar  2 02:19:19.857: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 03/02/23 02:19:19.931
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Mar  2 02:19:20.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8307" for this suite. 03/02/23 02:19:21.031
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","completed":215,"skipped":3960,"failed":0}
------------------------------
• [SLOW TEST] [6.392 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:19:14.689
    Mar  2 02:19:14.690: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename replication-controller 03/02/23 02:19:14.691
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:19:14.753
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:19:14.764
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:100
    STEP: Given a ReplicationController is created 03/02/23 02:19:14.775
    W0302 02:19:14.796891      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod-release" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-release" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-release" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-release" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: When the matched label of one of its pods change 03/02/23 02:19:14.797
    Mar  2 02:19:14.826: INFO: Pod name pod-release: Found 0 pods out of 1
    Mar  2 02:19:19.857: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 03/02/23 02:19:19.931
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Mar  2 02:19:20.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-8307" for this suite. 03/02/23 02:19:21.031
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:19:21.083
Mar  2 02:19:21.083: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename container-probe 03/02/23 02:19:21.084
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:19:21.191
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:19:21.204
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
STEP: Creating pod liveness-ed6577f7-17fb-43c4-ad3e-0241edf65286 in namespace container-probe-4873 03/02/23 02:19:21.217
Mar  2 02:19:21.341: INFO: Waiting up to 5m0s for pod "liveness-ed6577f7-17fb-43c4-ad3e-0241edf65286" in namespace "container-probe-4873" to be "not pending"
Mar  2 02:19:21.403: INFO: Pod "liveness-ed6577f7-17fb-43c4-ad3e-0241edf65286": Phase="Pending", Reason="", readiness=false. Elapsed: 62.071244ms
Mar  2 02:19:23.418: INFO: Pod "liveness-ed6577f7-17fb-43c4-ad3e-0241edf65286": Phase="Pending", Reason="", readiness=false. Elapsed: 2.077098635s
Mar  2 02:19:25.450: INFO: Pod "liveness-ed6577f7-17fb-43c4-ad3e-0241edf65286": Phase="Running", Reason="", readiness=true. Elapsed: 4.108100067s
Mar  2 02:19:25.450: INFO: Pod "liveness-ed6577f7-17fb-43c4-ad3e-0241edf65286" satisfied condition "not pending"
Mar  2 02:19:25.450: INFO: Started pod liveness-ed6577f7-17fb-43c4-ad3e-0241edf65286 in namespace container-probe-4873
STEP: checking the pod's current state and verifying that restartCount is present 03/02/23 02:19:25.45
Mar  2 02:19:25.470: INFO: Initial restart count of pod liveness-ed6577f7-17fb-43c4-ad3e-0241edf65286 is 0
Mar  2 02:19:43.704: INFO: Restart count of pod container-probe-4873/liveness-ed6577f7-17fb-43c4-ad3e-0241edf65286 is now 1 (18.233888674s elapsed)
Mar  2 02:20:03.909: INFO: Restart count of pod container-probe-4873/liveness-ed6577f7-17fb-43c4-ad3e-0241edf65286 is now 2 (38.438843643s elapsed)
Mar  2 02:20:24.090: INFO: Restart count of pod container-probe-4873/liveness-ed6577f7-17fb-43c4-ad3e-0241edf65286 is now 3 (58.619963389s elapsed)
Mar  2 02:20:44.295: INFO: Restart count of pod container-probe-4873/liveness-ed6577f7-17fb-43c4-ad3e-0241edf65286 is now 4 (1m18.824421166s elapsed)
Mar  2 02:21:46.964: INFO: Restart count of pod container-probe-4873/liveness-ed6577f7-17fb-43c4-ad3e-0241edf65286 is now 5 (2m21.493980995s elapsed)
STEP: deleting the pod 03/02/23 02:21:46.965
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar  2 02:21:47.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4873" for this suite. 03/02/23 02:21:47.08
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","completed":216,"skipped":3974,"failed":0}
------------------------------
• [SLOW TEST] [146.031 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:19:21.083
    Mar  2 02:19:21.083: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename container-probe 03/02/23 02:19:21.084
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:19:21.191
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:19:21.204
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:195
    STEP: Creating pod liveness-ed6577f7-17fb-43c4-ad3e-0241edf65286 in namespace container-probe-4873 03/02/23 02:19:21.217
    Mar  2 02:19:21.341: INFO: Waiting up to 5m0s for pod "liveness-ed6577f7-17fb-43c4-ad3e-0241edf65286" in namespace "container-probe-4873" to be "not pending"
    Mar  2 02:19:21.403: INFO: Pod "liveness-ed6577f7-17fb-43c4-ad3e-0241edf65286": Phase="Pending", Reason="", readiness=false. Elapsed: 62.071244ms
    Mar  2 02:19:23.418: INFO: Pod "liveness-ed6577f7-17fb-43c4-ad3e-0241edf65286": Phase="Pending", Reason="", readiness=false. Elapsed: 2.077098635s
    Mar  2 02:19:25.450: INFO: Pod "liveness-ed6577f7-17fb-43c4-ad3e-0241edf65286": Phase="Running", Reason="", readiness=true. Elapsed: 4.108100067s
    Mar  2 02:19:25.450: INFO: Pod "liveness-ed6577f7-17fb-43c4-ad3e-0241edf65286" satisfied condition "not pending"
    Mar  2 02:19:25.450: INFO: Started pod liveness-ed6577f7-17fb-43c4-ad3e-0241edf65286 in namespace container-probe-4873
    STEP: checking the pod's current state and verifying that restartCount is present 03/02/23 02:19:25.45
    Mar  2 02:19:25.470: INFO: Initial restart count of pod liveness-ed6577f7-17fb-43c4-ad3e-0241edf65286 is 0
    Mar  2 02:19:43.704: INFO: Restart count of pod container-probe-4873/liveness-ed6577f7-17fb-43c4-ad3e-0241edf65286 is now 1 (18.233888674s elapsed)
    Mar  2 02:20:03.909: INFO: Restart count of pod container-probe-4873/liveness-ed6577f7-17fb-43c4-ad3e-0241edf65286 is now 2 (38.438843643s elapsed)
    Mar  2 02:20:24.090: INFO: Restart count of pod container-probe-4873/liveness-ed6577f7-17fb-43c4-ad3e-0241edf65286 is now 3 (58.619963389s elapsed)
    Mar  2 02:20:44.295: INFO: Restart count of pod container-probe-4873/liveness-ed6577f7-17fb-43c4-ad3e-0241edf65286 is now 4 (1m18.824421166s elapsed)
    Mar  2 02:21:46.964: INFO: Restart count of pod container-probe-4873/liveness-ed6577f7-17fb-43c4-ad3e-0241edf65286 is now 5 (2m21.493980995s elapsed)
    STEP: deleting the pod 03/02/23 02:21:46.965
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar  2 02:21:47.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-4873" for this suite. 03/02/23 02:21:47.08
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:21:47.115
Mar  2 02:21:47.115: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename downward-api 03/02/23 02:21:47.116
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:21:47.193
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:21:47.218
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
STEP: Creating the pod 03/02/23 02:21:47.242
Mar  2 02:21:47.398: INFO: Waiting up to 5m0s for pod "annotationupdate6392b54f-7a49-4793-a094-e94bd5c88107" in namespace "downward-api-2020" to be "running and ready"
Mar  2 02:21:47.430: INFO: Pod "annotationupdate6392b54f-7a49-4793-a094-e94bd5c88107": Phase="Pending", Reason="", readiness=false. Elapsed: 32.418302ms
Mar  2 02:21:47.431: INFO: The phase of Pod annotationupdate6392b54f-7a49-4793-a094-e94bd5c88107 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 02:21:49.446: INFO: Pod "annotationupdate6392b54f-7a49-4793-a094-e94bd5c88107": Phase="Running", Reason="", readiness=true. Elapsed: 2.04745919s
Mar  2 02:21:49.446: INFO: The phase of Pod annotationupdate6392b54f-7a49-4793-a094-e94bd5c88107 is Running (Ready = true)
Mar  2 02:21:49.446: INFO: Pod "annotationupdate6392b54f-7a49-4793-a094-e94bd5c88107" satisfied condition "running and ready"
Mar  2 02:21:50.132: INFO: Successfully updated pod "annotationupdate6392b54f-7a49-4793-a094-e94bd5c88107"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar  2 02:21:52.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2020" for this suite. 03/02/23 02:21:52.232
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","completed":217,"skipped":3982,"failed":0}
------------------------------
• [SLOW TEST] [5.145 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:21:47.115
    Mar  2 02:21:47.115: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename downward-api 03/02/23 02:21:47.116
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:21:47.193
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:21:47.218
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:161
    STEP: Creating the pod 03/02/23 02:21:47.242
    Mar  2 02:21:47.398: INFO: Waiting up to 5m0s for pod "annotationupdate6392b54f-7a49-4793-a094-e94bd5c88107" in namespace "downward-api-2020" to be "running and ready"
    Mar  2 02:21:47.430: INFO: Pod "annotationupdate6392b54f-7a49-4793-a094-e94bd5c88107": Phase="Pending", Reason="", readiness=false. Elapsed: 32.418302ms
    Mar  2 02:21:47.431: INFO: The phase of Pod annotationupdate6392b54f-7a49-4793-a094-e94bd5c88107 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 02:21:49.446: INFO: Pod "annotationupdate6392b54f-7a49-4793-a094-e94bd5c88107": Phase="Running", Reason="", readiness=true. Elapsed: 2.04745919s
    Mar  2 02:21:49.446: INFO: The phase of Pod annotationupdate6392b54f-7a49-4793-a094-e94bd5c88107 is Running (Ready = true)
    Mar  2 02:21:49.446: INFO: Pod "annotationupdate6392b54f-7a49-4793-a094-e94bd5c88107" satisfied condition "running and ready"
    Mar  2 02:21:50.132: INFO: Successfully updated pod "annotationupdate6392b54f-7a49-4793-a094-e94bd5c88107"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar  2 02:21:52.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2020" for this suite. 03/02/23 02:21:52.232
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:21:52.263
Mar  2 02:21:52.264: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename daemonsets 03/02/23 02:21:52.265
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:21:52.368
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:21:52.382
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
STEP: Creating a simple DaemonSet "daemon-set" 03/02/23 02:21:52.497
STEP: Check that daemon pods launch on every node of the cluster. 03/02/23 02:21:52.524
Mar  2 02:21:52.560: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 02:21:52.560: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
Mar  2 02:21:53.610: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 02:21:53.610: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
Mar  2 02:21:54.599: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 02:21:54.599: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
Mar  2 02:21:55.599: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar  2 02:21:55.599: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 03/02/23 02:21:55.621
Mar  2 02:21:55.762: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar  2 02:21:55.763: INFO: Node 10.132.92.188 is running 0 daemon pod, expected 1
Mar  2 02:21:56.831: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar  2 02:21:56.831: INFO: Node 10.132.92.188 is running 0 daemon pod, expected 1
Mar  2 02:21:57.814: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar  2 02:21:57.814: INFO: Node 10.132.92.188 is running 0 daemon pod, expected 1
Mar  2 02:21:58.815: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar  2 02:21:58.815: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 03/02/23 02:21:58.815
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 03/02/23 02:21:58.845
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9477, will wait for the garbage collector to delete the pods 03/02/23 02:21:58.846
Mar  2 02:21:59.012: INFO: Deleting DaemonSet.extensions daemon-set took: 65.240323ms
Mar  2 02:21:59.113: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.279127ms
Mar  2 02:22:02.567: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 02:22:02.567: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar  2 02:22:02.602: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"121574"},"items":null}

Mar  2 02:22:02.647: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"121574"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Mar  2 02:22:02.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9477" for this suite. 03/02/23 02:22:02.775
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","completed":218,"skipped":4035,"failed":0}
------------------------------
• [SLOW TEST] [10.555 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:21:52.263
    Mar  2 02:21:52.264: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename daemonsets 03/02/23 02:21:52.265
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:21:52.368
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:21:52.382
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:293
    STEP: Creating a simple DaemonSet "daemon-set" 03/02/23 02:21:52.497
    STEP: Check that daemon pods launch on every node of the cluster. 03/02/23 02:21:52.524
    Mar  2 02:21:52.560: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 02:21:52.560: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
    Mar  2 02:21:53.610: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 02:21:53.610: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
    Mar  2 02:21:54.599: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 02:21:54.599: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
    Mar  2 02:21:55.599: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar  2 02:21:55.599: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 03/02/23 02:21:55.621
    Mar  2 02:21:55.762: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar  2 02:21:55.763: INFO: Node 10.132.92.188 is running 0 daemon pod, expected 1
    Mar  2 02:21:56.831: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar  2 02:21:56.831: INFO: Node 10.132.92.188 is running 0 daemon pod, expected 1
    Mar  2 02:21:57.814: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar  2 02:21:57.814: INFO: Node 10.132.92.188 is running 0 daemon pod, expected 1
    Mar  2 02:21:58.815: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar  2 02:21:58.815: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 03/02/23 02:21:58.815
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 03/02/23 02:21:58.845
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9477, will wait for the garbage collector to delete the pods 03/02/23 02:21:58.846
    Mar  2 02:21:59.012: INFO: Deleting DaemonSet.extensions daemon-set took: 65.240323ms
    Mar  2 02:21:59.113: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.279127ms
    Mar  2 02:22:02.567: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 02:22:02.567: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar  2 02:22:02.602: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"121574"},"items":null}

    Mar  2 02:22:02.647: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"121574"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Mar  2 02:22:02.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-9477" for this suite. 03/02/23 02:22:02.775
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:22:02.84
Mar  2 02:22:02.840: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename webhook 03/02/23 02:22:02.843
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:22:02.976
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:22:02.997
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/02/23 02:22:03.115
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 02:22:03.65
STEP: Deploying the webhook pod 03/02/23 02:22:03.692
STEP: Wait for the deployment to be ready 03/02/23 02:22:03.762
Mar  2 02:22:03.827: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 02:22:05.890: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 2, 22, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 22, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 2, 22, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 22, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/02/23 02:22:07.907
STEP: Verifying the service has paired with the endpoint 03/02/23 02:22:07.941
Mar  2 02:22:08.941: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
Mar  2 02:22:08.954: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Registering the custom resource webhook via the AdmissionRegistration API 03/02/23 02:22:09.498
STEP: Creating a custom resource that should be denied by the webhook 03/02/23 02:22:09.556
STEP: Creating a custom resource whose deletion would be denied by the webhook 03/02/23 02:22:11.696
STEP: Updating the custom resource with disallowed data should be denied 03/02/23 02:22:11.764
STEP: Deleting the custom resource should be denied 03/02/23 02:22:11.822
STEP: Remove the offending key and value from the custom resource data 03/02/23 02:22:11.856
STEP: Deleting the updated custom resource should be successful 03/02/23 02:22:11.925
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 02:22:12.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6865" for this suite. 03/02/23 02:22:12.623
STEP: Destroying namespace "webhook-6865-markers" for this suite. 03/02/23 02:22:12.65
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","completed":219,"skipped":4054,"failed":0}
------------------------------
• [SLOW TEST] [10.034 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:22:02.84
    Mar  2 02:22:02.840: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename webhook 03/02/23 02:22:02.843
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:22:02.976
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:22:02.997
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/02/23 02:22:03.115
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 02:22:03.65
    STEP: Deploying the webhook pod 03/02/23 02:22:03.692
    STEP: Wait for the deployment to be ready 03/02/23 02:22:03.762
    Mar  2 02:22:03.827: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Mar  2 02:22:05.890: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 2, 22, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 22, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 2, 22, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 22, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/02/23 02:22:07.907
    STEP: Verifying the service has paired with the endpoint 03/02/23 02:22:07.941
    Mar  2 02:22:08.941: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:220
    Mar  2 02:22:08.954: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 03/02/23 02:22:09.498
    STEP: Creating a custom resource that should be denied by the webhook 03/02/23 02:22:09.556
    STEP: Creating a custom resource whose deletion would be denied by the webhook 03/02/23 02:22:11.696
    STEP: Updating the custom resource with disallowed data should be denied 03/02/23 02:22:11.764
    STEP: Deleting the custom resource should be denied 03/02/23 02:22:11.822
    STEP: Remove the offending key and value from the custom resource data 03/02/23 02:22:11.856
    STEP: Deleting the updated custom resource should be successful 03/02/23 02:22:11.925
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 02:22:12.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6865" for this suite. 03/02/23 02:22:12.623
    STEP: Destroying namespace "webhook-6865-markers" for this suite. 03/02/23 02:22:12.65
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:22:12.88
Mar  2 02:22:12.880: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename services 03/02/23 02:22:12.882
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:22:12.997
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:22:13.009
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
STEP: creating service in namespace services-182 03/02/23 02:22:13.027
STEP: creating service affinity-clusterip-transition in namespace services-182 03/02/23 02:22:13.027
STEP: creating replication controller affinity-clusterip-transition in namespace services-182 03/02/23 02:22:13.131
I0302 02:22:13.225293      20 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-182, replica count: 3
I0302 02:22:16.276788      20 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 02:22:19.277041      20 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 02:22:19.339: INFO: Creating new exec pod
Mar  2 02:22:19.420: INFO: Waiting up to 5m0s for pod "execpod-affinity6mqss" in namespace "services-182" to be "running"
Mar  2 02:22:19.440: INFO: Pod "execpod-affinity6mqss": Phase="Pending", Reason="", readiness=false. Elapsed: 19.254359ms
Mar  2 02:22:21.479: INFO: Pod "execpod-affinity6mqss": Phase="Running", Reason="", readiness=true. Elapsed: 2.058523778s
Mar  2 02:22:21.479: INFO: Pod "execpod-affinity6mqss" satisfied condition "running"
Mar  2 02:22:22.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-182 exec execpod-affinity6mqss -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Mar  2 02:22:23.000: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Mar  2 02:22:23.000: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 02:22:23.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-182 exec execpod-affinity6mqss -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.57.138 80'
Mar  2 02:22:23.313: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.57.138 80\nConnection to 172.21.57.138 80 port [tcp/http] succeeded!\n"
Mar  2 02:22:23.313: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 02:22:23.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-182 exec execpod-affinity6mqss -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.57.138:80/ ; done'
Mar  2 02:22:23.822: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n"
Mar  2 02:22:23.822: INFO: stdout: "\naffinity-clusterip-transition-m4m6m\naffinity-clusterip-transition-frksx\naffinity-clusterip-transition-8ksc7\naffinity-clusterip-transition-m4m6m\naffinity-clusterip-transition-frksx\naffinity-clusterip-transition-8ksc7\naffinity-clusterip-transition-8ksc7\naffinity-clusterip-transition-m4m6m\naffinity-clusterip-transition-m4m6m\naffinity-clusterip-transition-8ksc7\naffinity-clusterip-transition-m4m6m\naffinity-clusterip-transition-frksx\naffinity-clusterip-transition-frksx\naffinity-clusterip-transition-frksx\naffinity-clusterip-transition-m4m6m\naffinity-clusterip-transition-m4m6m"
Mar  2 02:22:23.822: INFO: Received response from host: affinity-clusterip-transition-m4m6m
Mar  2 02:22:23.822: INFO: Received response from host: affinity-clusterip-transition-frksx
Mar  2 02:22:23.823: INFO: Received response from host: affinity-clusterip-transition-8ksc7
Mar  2 02:22:23.823: INFO: Received response from host: affinity-clusterip-transition-m4m6m
Mar  2 02:22:23.823: INFO: Received response from host: affinity-clusterip-transition-frksx
Mar  2 02:22:23.823: INFO: Received response from host: affinity-clusterip-transition-8ksc7
Mar  2 02:22:23.823: INFO: Received response from host: affinity-clusterip-transition-8ksc7
Mar  2 02:22:23.823: INFO: Received response from host: affinity-clusterip-transition-m4m6m
Mar  2 02:22:23.823: INFO: Received response from host: affinity-clusterip-transition-m4m6m
Mar  2 02:22:23.823: INFO: Received response from host: affinity-clusterip-transition-8ksc7
Mar  2 02:22:23.823: INFO: Received response from host: affinity-clusterip-transition-m4m6m
Mar  2 02:22:23.823: INFO: Received response from host: affinity-clusterip-transition-frksx
Mar  2 02:22:23.823: INFO: Received response from host: affinity-clusterip-transition-frksx
Mar  2 02:22:23.823: INFO: Received response from host: affinity-clusterip-transition-frksx
Mar  2 02:22:23.823: INFO: Received response from host: affinity-clusterip-transition-m4m6m
Mar  2 02:22:23.823: INFO: Received response from host: affinity-clusterip-transition-m4m6m
Mar  2 02:22:23.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-182 exec execpod-affinity6mqss -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.57.138:80/ ; done'
Mar  2 02:22:24.313: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n"
Mar  2 02:22:24.313: INFO: stdout: "\naffinity-clusterip-transition-frksx\naffinity-clusterip-transition-frksx\naffinity-clusterip-transition-frksx\naffinity-clusterip-transition-frksx\naffinity-clusterip-transition-frksx\naffinity-clusterip-transition-frksx\naffinity-clusterip-transition-frksx\naffinity-clusterip-transition-frksx\naffinity-clusterip-transition-frksx\naffinity-clusterip-transition-frksx\naffinity-clusterip-transition-frksx\naffinity-clusterip-transition-frksx\naffinity-clusterip-transition-frksx\naffinity-clusterip-transition-frksx\naffinity-clusterip-transition-frksx\naffinity-clusterip-transition-frksx"
Mar  2 02:22:24.313: INFO: Received response from host: affinity-clusterip-transition-frksx
Mar  2 02:22:24.313: INFO: Received response from host: affinity-clusterip-transition-frksx
Mar  2 02:22:24.313: INFO: Received response from host: affinity-clusterip-transition-frksx
Mar  2 02:22:24.313: INFO: Received response from host: affinity-clusterip-transition-frksx
Mar  2 02:22:24.313: INFO: Received response from host: affinity-clusterip-transition-frksx
Mar  2 02:22:24.313: INFO: Received response from host: affinity-clusterip-transition-frksx
Mar  2 02:22:24.313: INFO: Received response from host: affinity-clusterip-transition-frksx
Mar  2 02:22:24.313: INFO: Received response from host: affinity-clusterip-transition-frksx
Mar  2 02:22:24.313: INFO: Received response from host: affinity-clusterip-transition-frksx
Mar  2 02:22:24.313: INFO: Received response from host: affinity-clusterip-transition-frksx
Mar  2 02:22:24.313: INFO: Received response from host: affinity-clusterip-transition-frksx
Mar  2 02:22:24.313: INFO: Received response from host: affinity-clusterip-transition-frksx
Mar  2 02:22:24.313: INFO: Received response from host: affinity-clusterip-transition-frksx
Mar  2 02:22:24.313: INFO: Received response from host: affinity-clusterip-transition-frksx
Mar  2 02:22:24.313: INFO: Received response from host: affinity-clusterip-transition-frksx
Mar  2 02:22:24.313: INFO: Received response from host: affinity-clusterip-transition-frksx
Mar  2 02:22:24.313: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-182, will wait for the garbage collector to delete the pods 03/02/23 02:22:24.351
Mar  2 02:22:24.456: INFO: Deleting ReplicationController affinity-clusterip-transition took: 38.858144ms
Mar  2 02:22:24.556: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.202135ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  2 02:22:27.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-182" for this suite. 03/02/23 02:22:27.966
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","completed":220,"skipped":4073,"failed":0}
------------------------------
• [SLOW TEST] [15.124 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:22:12.88
    Mar  2 02:22:12.880: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename services 03/02/23 02:22:12.882
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:22:12.997
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:22:13.009
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2189
    STEP: creating service in namespace services-182 03/02/23 02:22:13.027
    STEP: creating service affinity-clusterip-transition in namespace services-182 03/02/23 02:22:13.027
    STEP: creating replication controller affinity-clusterip-transition in namespace services-182 03/02/23 02:22:13.131
    I0302 02:22:13.225293      20 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-182, replica count: 3
    I0302 02:22:16.276788      20 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0302 02:22:19.277041      20 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar  2 02:22:19.339: INFO: Creating new exec pod
    Mar  2 02:22:19.420: INFO: Waiting up to 5m0s for pod "execpod-affinity6mqss" in namespace "services-182" to be "running"
    Mar  2 02:22:19.440: INFO: Pod "execpod-affinity6mqss": Phase="Pending", Reason="", readiness=false. Elapsed: 19.254359ms
    Mar  2 02:22:21.479: INFO: Pod "execpod-affinity6mqss": Phase="Running", Reason="", readiness=true. Elapsed: 2.058523778s
    Mar  2 02:22:21.479: INFO: Pod "execpod-affinity6mqss" satisfied condition "running"
    Mar  2 02:22:22.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-182 exec execpod-affinity6mqss -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
    Mar  2 02:22:23.000: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Mar  2 02:22:23.000: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  2 02:22:23.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-182 exec execpod-affinity6mqss -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.57.138 80'
    Mar  2 02:22:23.313: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.57.138 80\nConnection to 172.21.57.138 80 port [tcp/http] succeeded!\n"
    Mar  2 02:22:23.313: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  2 02:22:23.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-182 exec execpod-affinity6mqss -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.57.138:80/ ; done'
    Mar  2 02:22:23.822: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n"
    Mar  2 02:22:23.822: INFO: stdout: "\naffinity-clusterip-transition-m4m6m\naffinity-clusterip-transition-frksx\naffinity-clusterip-transition-8ksc7\naffinity-clusterip-transition-m4m6m\naffinity-clusterip-transition-frksx\naffinity-clusterip-transition-8ksc7\naffinity-clusterip-transition-8ksc7\naffinity-clusterip-transition-m4m6m\naffinity-clusterip-transition-m4m6m\naffinity-clusterip-transition-8ksc7\naffinity-clusterip-transition-m4m6m\naffinity-clusterip-transition-frksx\naffinity-clusterip-transition-frksx\naffinity-clusterip-transition-frksx\naffinity-clusterip-transition-m4m6m\naffinity-clusterip-transition-m4m6m"
    Mar  2 02:22:23.822: INFO: Received response from host: affinity-clusterip-transition-m4m6m
    Mar  2 02:22:23.822: INFO: Received response from host: affinity-clusterip-transition-frksx
    Mar  2 02:22:23.823: INFO: Received response from host: affinity-clusterip-transition-8ksc7
    Mar  2 02:22:23.823: INFO: Received response from host: affinity-clusterip-transition-m4m6m
    Mar  2 02:22:23.823: INFO: Received response from host: affinity-clusterip-transition-frksx
    Mar  2 02:22:23.823: INFO: Received response from host: affinity-clusterip-transition-8ksc7
    Mar  2 02:22:23.823: INFO: Received response from host: affinity-clusterip-transition-8ksc7
    Mar  2 02:22:23.823: INFO: Received response from host: affinity-clusterip-transition-m4m6m
    Mar  2 02:22:23.823: INFO: Received response from host: affinity-clusterip-transition-m4m6m
    Mar  2 02:22:23.823: INFO: Received response from host: affinity-clusterip-transition-8ksc7
    Mar  2 02:22:23.823: INFO: Received response from host: affinity-clusterip-transition-m4m6m
    Mar  2 02:22:23.823: INFO: Received response from host: affinity-clusterip-transition-frksx
    Mar  2 02:22:23.823: INFO: Received response from host: affinity-clusterip-transition-frksx
    Mar  2 02:22:23.823: INFO: Received response from host: affinity-clusterip-transition-frksx
    Mar  2 02:22:23.823: INFO: Received response from host: affinity-clusterip-transition-m4m6m
    Mar  2 02:22:23.823: INFO: Received response from host: affinity-clusterip-transition-m4m6m
    Mar  2 02:22:23.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-182 exec execpod-affinity6mqss -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.57.138:80/ ; done'
    Mar  2 02:22:24.313: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.57.138:80/\n"
    Mar  2 02:22:24.313: INFO: stdout: "\naffinity-clusterip-transition-frksx\naffinity-clusterip-transition-frksx\naffinity-clusterip-transition-frksx\naffinity-clusterip-transition-frksx\naffinity-clusterip-transition-frksx\naffinity-clusterip-transition-frksx\naffinity-clusterip-transition-frksx\naffinity-clusterip-transition-frksx\naffinity-clusterip-transition-frksx\naffinity-clusterip-transition-frksx\naffinity-clusterip-transition-frksx\naffinity-clusterip-transition-frksx\naffinity-clusterip-transition-frksx\naffinity-clusterip-transition-frksx\naffinity-clusterip-transition-frksx\naffinity-clusterip-transition-frksx"
    Mar  2 02:22:24.313: INFO: Received response from host: affinity-clusterip-transition-frksx
    Mar  2 02:22:24.313: INFO: Received response from host: affinity-clusterip-transition-frksx
    Mar  2 02:22:24.313: INFO: Received response from host: affinity-clusterip-transition-frksx
    Mar  2 02:22:24.313: INFO: Received response from host: affinity-clusterip-transition-frksx
    Mar  2 02:22:24.313: INFO: Received response from host: affinity-clusterip-transition-frksx
    Mar  2 02:22:24.313: INFO: Received response from host: affinity-clusterip-transition-frksx
    Mar  2 02:22:24.313: INFO: Received response from host: affinity-clusterip-transition-frksx
    Mar  2 02:22:24.313: INFO: Received response from host: affinity-clusterip-transition-frksx
    Mar  2 02:22:24.313: INFO: Received response from host: affinity-clusterip-transition-frksx
    Mar  2 02:22:24.313: INFO: Received response from host: affinity-clusterip-transition-frksx
    Mar  2 02:22:24.313: INFO: Received response from host: affinity-clusterip-transition-frksx
    Mar  2 02:22:24.313: INFO: Received response from host: affinity-clusterip-transition-frksx
    Mar  2 02:22:24.313: INFO: Received response from host: affinity-clusterip-transition-frksx
    Mar  2 02:22:24.313: INFO: Received response from host: affinity-clusterip-transition-frksx
    Mar  2 02:22:24.313: INFO: Received response from host: affinity-clusterip-transition-frksx
    Mar  2 02:22:24.313: INFO: Received response from host: affinity-clusterip-transition-frksx
    Mar  2 02:22:24.313: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-182, will wait for the garbage collector to delete the pods 03/02/23 02:22:24.351
    Mar  2 02:22:24.456: INFO: Deleting ReplicationController affinity-clusterip-transition took: 38.858144ms
    Mar  2 02:22:24.556: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.202135ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  2 02:22:27.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-182" for this suite. 03/02/23 02:22:27.966
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:22:28.01
Mar  2 02:22:28.010: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename csistoragecapacity 03/02/23 02:22:28.011
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:22:28.166
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:22:28.181
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 03/02/23 02:22:28.193
STEP: getting /apis/storage.k8s.io 03/02/23 02:22:28.205
STEP: getting /apis/storage.k8s.io/v1 03/02/23 02:22:28.211
STEP: creating 03/02/23 02:22:28.217
STEP: watching 03/02/23 02:22:28.301
Mar  2 02:22:28.301: INFO: starting watch
STEP: getting 03/02/23 02:22:28.396
STEP: listing in namespace 03/02/23 02:22:28.417
STEP: listing across namespaces 03/02/23 02:22:28.448
STEP: patching 03/02/23 02:22:28.462
STEP: updating 03/02/23 02:22:28.508
Mar  2 02:22:28.552: INFO: waiting for watch events with expected annotations in namespace
Mar  2 02:22:28.552: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 03/02/23 02:22:28.552
STEP: deleting a collection 03/02/23 02:22:28.602
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:187
Mar  2 02:22:28.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "csistoragecapacity-5857" for this suite. 03/02/23 02:22:28.684
{"msg":"PASSED [sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]","completed":221,"skipped":4096,"failed":0}
------------------------------
• [0.712 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:22:28.01
    Mar  2 02:22:28.010: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename csistoragecapacity 03/02/23 02:22:28.011
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:22:28.166
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:22:28.181
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 03/02/23 02:22:28.193
    STEP: getting /apis/storage.k8s.io 03/02/23 02:22:28.205
    STEP: getting /apis/storage.k8s.io/v1 03/02/23 02:22:28.211
    STEP: creating 03/02/23 02:22:28.217
    STEP: watching 03/02/23 02:22:28.301
    Mar  2 02:22:28.301: INFO: starting watch
    STEP: getting 03/02/23 02:22:28.396
    STEP: listing in namespace 03/02/23 02:22:28.417
    STEP: listing across namespaces 03/02/23 02:22:28.448
    STEP: patching 03/02/23 02:22:28.462
    STEP: updating 03/02/23 02:22:28.508
    Mar  2 02:22:28.552: INFO: waiting for watch events with expected annotations in namespace
    Mar  2 02:22:28.552: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 03/02/23 02:22:28.552
    STEP: deleting a collection 03/02/23 02:22:28.602
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:187
    Mar  2 02:22:28.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "csistoragecapacity-5857" for this suite. 03/02/23 02:22:28.684
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:22:28.722
Mar  2 02:22:28.722: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename var-expansion 03/02/23 02:22:28.724
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:22:28.81
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:22:28.827
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
STEP: creating the pod 03/02/23 02:22:28.87
STEP: waiting for pod running 03/02/23 02:22:29.071
Mar  2 02:22:29.071: INFO: Waiting up to 2m0s for pod "var-expansion-2d5d5324-5a6f-4b5f-a3c3-05f2920cce9c" in namespace "var-expansion-7512" to be "running"
Mar  2 02:22:29.105: INFO: Pod "var-expansion-2d5d5324-5a6f-4b5f-a3c3-05f2920cce9c": Phase="Pending", Reason="", readiness=false. Elapsed: 33.755189ms
Mar  2 02:22:31.121: INFO: Pod "var-expansion-2d5d5324-5a6f-4b5f-a3c3-05f2920cce9c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050059823s
Mar  2 02:22:33.123: INFO: Pod "var-expansion-2d5d5324-5a6f-4b5f-a3c3-05f2920cce9c": Phase="Running", Reason="", readiness=true. Elapsed: 4.052462365s
Mar  2 02:22:33.124: INFO: Pod "var-expansion-2d5d5324-5a6f-4b5f-a3c3-05f2920cce9c" satisfied condition "running"
STEP: creating a file in subpath 03/02/23 02:22:33.124
Mar  2 02:22:33.141: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-7512 PodName:var-expansion-2d5d5324-5a6f-4b5f-a3c3-05f2920cce9c ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 02:22:33.141: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
Mar  2 02:22:33.142: INFO: ExecWithOptions: Clientset creation
Mar  2 02:22:33.142: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/var-expansion-7512/pods/var-expansion-2d5d5324-5a6f-4b5f-a3c3-05f2920cce9c/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 03/02/23 02:22:33.337
Mar  2 02:22:33.351: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-7512 PodName:var-expansion-2d5d5324-5a6f-4b5f-a3c3-05f2920cce9c ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 02:22:33.351: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
Mar  2 02:22:33.352: INFO: ExecWithOptions: Clientset creation
Mar  2 02:22:33.352: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/var-expansion-7512/pods/var-expansion-2d5d5324-5a6f-4b5f-a3c3-05f2920cce9c/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 03/02/23 02:22:33.571
Mar  2 02:22:34.201: INFO: Successfully updated pod "var-expansion-2d5d5324-5a6f-4b5f-a3c3-05f2920cce9c"
STEP: waiting for annotated pod running 03/02/23 02:22:34.201
Mar  2 02:22:34.201: INFO: Waiting up to 2m0s for pod "var-expansion-2d5d5324-5a6f-4b5f-a3c3-05f2920cce9c" in namespace "var-expansion-7512" to be "running"
Mar  2 02:22:34.271: INFO: Pod "var-expansion-2d5d5324-5a6f-4b5f-a3c3-05f2920cce9c": Phase="Running", Reason="", readiness=true. Elapsed: 69.666533ms
Mar  2 02:22:34.271: INFO: Pod "var-expansion-2d5d5324-5a6f-4b5f-a3c3-05f2920cce9c" satisfied condition "running"
STEP: deleting the pod gracefully 03/02/23 02:22:34.271
Mar  2 02:22:34.271: INFO: Deleting pod "var-expansion-2d5d5324-5a6f-4b5f-a3c3-05f2920cce9c" in namespace "var-expansion-7512"
Mar  2 02:22:34.300: INFO: Wait up to 5m0s for pod "var-expansion-2d5d5324-5a6f-4b5f-a3c3-05f2920cce9c" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar  2 02:23:06.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7512" for this suite. 03/02/23 02:23:06.403
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","completed":222,"skipped":4100,"failed":0}
------------------------------
• [SLOW TEST] [37.717 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:22:28.722
    Mar  2 02:22:28.722: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename var-expansion 03/02/23 02:22:28.724
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:22:28.81
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:22:28.827
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:296
    STEP: creating the pod 03/02/23 02:22:28.87
    STEP: waiting for pod running 03/02/23 02:22:29.071
    Mar  2 02:22:29.071: INFO: Waiting up to 2m0s for pod "var-expansion-2d5d5324-5a6f-4b5f-a3c3-05f2920cce9c" in namespace "var-expansion-7512" to be "running"
    Mar  2 02:22:29.105: INFO: Pod "var-expansion-2d5d5324-5a6f-4b5f-a3c3-05f2920cce9c": Phase="Pending", Reason="", readiness=false. Elapsed: 33.755189ms
    Mar  2 02:22:31.121: INFO: Pod "var-expansion-2d5d5324-5a6f-4b5f-a3c3-05f2920cce9c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050059823s
    Mar  2 02:22:33.123: INFO: Pod "var-expansion-2d5d5324-5a6f-4b5f-a3c3-05f2920cce9c": Phase="Running", Reason="", readiness=true. Elapsed: 4.052462365s
    Mar  2 02:22:33.124: INFO: Pod "var-expansion-2d5d5324-5a6f-4b5f-a3c3-05f2920cce9c" satisfied condition "running"
    STEP: creating a file in subpath 03/02/23 02:22:33.124
    Mar  2 02:22:33.141: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-7512 PodName:var-expansion-2d5d5324-5a6f-4b5f-a3c3-05f2920cce9c ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 02:22:33.141: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    Mar  2 02:22:33.142: INFO: ExecWithOptions: Clientset creation
    Mar  2 02:22:33.142: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/var-expansion-7512/pods/var-expansion-2d5d5324-5a6f-4b5f-a3c3-05f2920cce9c/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 03/02/23 02:22:33.337
    Mar  2 02:22:33.351: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-7512 PodName:var-expansion-2d5d5324-5a6f-4b5f-a3c3-05f2920cce9c ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 02:22:33.351: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    Mar  2 02:22:33.352: INFO: ExecWithOptions: Clientset creation
    Mar  2 02:22:33.352: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/var-expansion-7512/pods/var-expansion-2d5d5324-5a6f-4b5f-a3c3-05f2920cce9c/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 03/02/23 02:22:33.571
    Mar  2 02:22:34.201: INFO: Successfully updated pod "var-expansion-2d5d5324-5a6f-4b5f-a3c3-05f2920cce9c"
    STEP: waiting for annotated pod running 03/02/23 02:22:34.201
    Mar  2 02:22:34.201: INFO: Waiting up to 2m0s for pod "var-expansion-2d5d5324-5a6f-4b5f-a3c3-05f2920cce9c" in namespace "var-expansion-7512" to be "running"
    Mar  2 02:22:34.271: INFO: Pod "var-expansion-2d5d5324-5a6f-4b5f-a3c3-05f2920cce9c": Phase="Running", Reason="", readiness=true. Elapsed: 69.666533ms
    Mar  2 02:22:34.271: INFO: Pod "var-expansion-2d5d5324-5a6f-4b5f-a3c3-05f2920cce9c" satisfied condition "running"
    STEP: deleting the pod gracefully 03/02/23 02:22:34.271
    Mar  2 02:22:34.271: INFO: Deleting pod "var-expansion-2d5d5324-5a6f-4b5f-a3c3-05f2920cce9c" in namespace "var-expansion-7512"
    Mar  2 02:22:34.300: INFO: Wait up to 5m0s for pod "var-expansion-2d5d5324-5a6f-4b5f-a3c3-05f2920cce9c" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar  2 02:23:06.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-7512" for this suite. 03/02/23 02:23:06.403
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:23:06.526
Mar  2 02:23:06.526: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename watch 03/02/23 02:23:06.528
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:23:06.591
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:23:06.606
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 03/02/23 02:23:06.65
STEP: creating a new configmap 03/02/23 02:23:06.663
STEP: modifying the configmap once 03/02/23 02:23:06.688
STEP: closing the watch once it receives two notifications 03/02/23 02:23:06.735
Mar  2 02:23:06.737: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9084  41c64390-b2ee-44db-bdbb-d5554ea0bcae 122415 0 2023-03-02 02:23:06 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-02 02:23:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 02:23:06.737: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9084  41c64390-b2ee-44db-bdbb-d5554ea0bcae 122419 0 2023-03-02 02:23:06 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-02 02:23:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 03/02/23 02:23:06.737
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 03/02/23 02:23:06.795
STEP: deleting the configmap 03/02/23 02:23:06.801
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 03/02/23 02:23:06.83
Mar  2 02:23:06.831: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9084  41c64390-b2ee-44db-bdbb-d5554ea0bcae 122425 0 2023-03-02 02:23:06 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-02 02:23:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 02:23:06.832: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9084  41c64390-b2ee-44db-bdbb-d5554ea0bcae 122429 0 2023-03-02 02:23:06 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-02 02:23:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Mar  2 02:23:06.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9084" for this suite. 03/02/23 02:23:06.857
{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","completed":223,"skipped":4147,"failed":0}
------------------------------
• [0.368 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:23:06.526
    Mar  2 02:23:06.526: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename watch 03/02/23 02:23:06.528
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:23:06.591
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:23:06.606
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 03/02/23 02:23:06.65
    STEP: creating a new configmap 03/02/23 02:23:06.663
    STEP: modifying the configmap once 03/02/23 02:23:06.688
    STEP: closing the watch once it receives two notifications 03/02/23 02:23:06.735
    Mar  2 02:23:06.737: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9084  41c64390-b2ee-44db-bdbb-d5554ea0bcae 122415 0 2023-03-02 02:23:06 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-02 02:23:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar  2 02:23:06.737: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9084  41c64390-b2ee-44db-bdbb-d5554ea0bcae 122419 0 2023-03-02 02:23:06 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-02 02:23:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 03/02/23 02:23:06.737
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 03/02/23 02:23:06.795
    STEP: deleting the configmap 03/02/23 02:23:06.801
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 03/02/23 02:23:06.83
    Mar  2 02:23:06.831: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9084  41c64390-b2ee-44db-bdbb-d5554ea0bcae 122425 0 2023-03-02 02:23:06 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-02 02:23:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar  2 02:23:06.832: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9084  41c64390-b2ee-44db-bdbb-d5554ea0bcae 122429 0 2023-03-02 02:23:06 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-02 02:23:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Mar  2 02:23:06.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-9084" for this suite. 03/02/23 02:23:06.857
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:23:06.896
Mar  2 02:23:06.896: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename projected 03/02/23 02:23:06.898
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:23:07.05
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:23:07.066
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
STEP: Creating configMap with name configmap-projected-all-test-volume-d39b8520-43a9-42b6-92e2-b4d05c565d9f 03/02/23 02:23:07.08
STEP: Creating secret with name secret-projected-all-test-volume-7a117b05-83b6-4def-9cf8-14b731f9b058 03/02/23 02:23:07.104
STEP: Creating a pod to test Check all projections for projected volume plugin 03/02/23 02:23:07.13
Mar  2 02:23:07.285: INFO: Waiting up to 5m0s for pod "projected-volume-1ddb79e8-bd55-402a-ba53-4dfb81a3d358" in namespace "projected-2102" to be "Succeeded or Failed"
Mar  2 02:23:07.319: INFO: Pod "projected-volume-1ddb79e8-bd55-402a-ba53-4dfb81a3d358": Phase="Pending", Reason="", readiness=false. Elapsed: 33.942409ms
Mar  2 02:23:09.333: INFO: Pod "projected-volume-1ddb79e8-bd55-402a-ba53-4dfb81a3d358": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047691419s
Mar  2 02:23:11.340: INFO: Pod "projected-volume-1ddb79e8-bd55-402a-ba53-4dfb81a3d358": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054378584s
Mar  2 02:23:13.335: INFO: Pod "projected-volume-1ddb79e8-bd55-402a-ba53-4dfb81a3d358": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.049875696s
STEP: Saw pod success 03/02/23 02:23:13.336
Mar  2 02:23:13.336: INFO: Pod "projected-volume-1ddb79e8-bd55-402a-ba53-4dfb81a3d358" satisfied condition "Succeeded or Failed"
Mar  2 02:23:13.350: INFO: Trying to get logs from node 10.132.92.143 pod projected-volume-1ddb79e8-bd55-402a-ba53-4dfb81a3d358 container projected-all-volume-test: <nil>
STEP: delete the pod 03/02/23 02:23:13.38
Mar  2 02:23:13.431: INFO: Waiting for pod projected-volume-1ddb79e8-bd55-402a-ba53-4dfb81a3d358 to disappear
Mar  2 02:23:13.447: INFO: Pod projected-volume-1ddb79e8-bd55-402a-ba53-4dfb81a3d358 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:187
Mar  2 02:23:13.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2102" for this suite. 03/02/23 02:23:13.49
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","completed":224,"skipped":4148,"failed":0}
------------------------------
• [SLOW TEST] [6.623 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:23:06.896
    Mar  2 02:23:06.896: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename projected 03/02/23 02:23:06.898
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:23:07.05
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:23:07.066
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:43
    STEP: Creating configMap with name configmap-projected-all-test-volume-d39b8520-43a9-42b6-92e2-b4d05c565d9f 03/02/23 02:23:07.08
    STEP: Creating secret with name secret-projected-all-test-volume-7a117b05-83b6-4def-9cf8-14b731f9b058 03/02/23 02:23:07.104
    STEP: Creating a pod to test Check all projections for projected volume plugin 03/02/23 02:23:07.13
    Mar  2 02:23:07.285: INFO: Waiting up to 5m0s for pod "projected-volume-1ddb79e8-bd55-402a-ba53-4dfb81a3d358" in namespace "projected-2102" to be "Succeeded or Failed"
    Mar  2 02:23:07.319: INFO: Pod "projected-volume-1ddb79e8-bd55-402a-ba53-4dfb81a3d358": Phase="Pending", Reason="", readiness=false. Elapsed: 33.942409ms
    Mar  2 02:23:09.333: INFO: Pod "projected-volume-1ddb79e8-bd55-402a-ba53-4dfb81a3d358": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047691419s
    Mar  2 02:23:11.340: INFO: Pod "projected-volume-1ddb79e8-bd55-402a-ba53-4dfb81a3d358": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054378584s
    Mar  2 02:23:13.335: INFO: Pod "projected-volume-1ddb79e8-bd55-402a-ba53-4dfb81a3d358": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.049875696s
    STEP: Saw pod success 03/02/23 02:23:13.336
    Mar  2 02:23:13.336: INFO: Pod "projected-volume-1ddb79e8-bd55-402a-ba53-4dfb81a3d358" satisfied condition "Succeeded or Failed"
    Mar  2 02:23:13.350: INFO: Trying to get logs from node 10.132.92.143 pod projected-volume-1ddb79e8-bd55-402a-ba53-4dfb81a3d358 container projected-all-volume-test: <nil>
    STEP: delete the pod 03/02/23 02:23:13.38
    Mar  2 02:23:13.431: INFO: Waiting for pod projected-volume-1ddb79e8-bd55-402a-ba53-4dfb81a3d358 to disappear
    Mar  2 02:23:13.447: INFO: Pod projected-volume-1ddb79e8-bd55-402a-ba53-4dfb81a3d358 no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:187
    Mar  2 02:23:13.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2102" for this suite. 03/02/23 02:23:13.49
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:23:13.52
Mar  2 02:23:13.521: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename webhook 03/02/23 02:23:13.522
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:23:13.586
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:23:13.604
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/02/23 02:23:13.711
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 02:23:14.1
STEP: Deploying the webhook pod 03/02/23 02:23:14.139
STEP: Wait for the deployment to be ready 03/02/23 02:23:14.205
Mar  2 02:23:14.256: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 02:23:16.300: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 2, 23, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 23, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 2, 23, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 23, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/02/23 02:23:18.318
STEP: Verifying the service has paired with the endpoint 03/02/23 02:23:18.357
Mar  2 02:23:19.357: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
STEP: Listing all of the created validation webhooks 03/02/23 02:23:19.627
STEP: Creating a configMap that does not comply to the validation webhook rules 03/02/23 02:23:19.905
STEP: Deleting the collection of validation webhooks 03/02/23 02:23:19.978
STEP: Creating a configMap that does not comply to the validation webhook rules 03/02/23 02:23:20.185
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 02:23:20.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5783" for this suite. 03/02/23 02:23:20.253
STEP: Destroying namespace "webhook-5783-markers" for this suite. 03/02/23 02:23:20.278
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","completed":225,"skipped":4166,"failed":0}
------------------------------
• [SLOW TEST] [6.964 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:23:13.52
    Mar  2 02:23:13.521: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename webhook 03/02/23 02:23:13.522
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:23:13.586
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:23:13.604
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/02/23 02:23:13.711
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 02:23:14.1
    STEP: Deploying the webhook pod 03/02/23 02:23:14.139
    STEP: Wait for the deployment to be ready 03/02/23 02:23:14.205
    Mar  2 02:23:14.256: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Mar  2 02:23:16.300: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 2, 23, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 23, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 2, 23, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 23, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/02/23 02:23:18.318
    STEP: Verifying the service has paired with the endpoint 03/02/23 02:23:18.357
    Mar  2 02:23:19.357: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:581
    STEP: Listing all of the created validation webhooks 03/02/23 02:23:19.627
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/02/23 02:23:19.905
    STEP: Deleting the collection of validation webhooks 03/02/23 02:23:19.978
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/02/23 02:23:20.185
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 02:23:20.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5783" for this suite. 03/02/23 02:23:20.253
    STEP: Destroying namespace "webhook-5783-markers" for this suite. 03/02/23 02:23:20.278
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:23:20.487
Mar  2 02:23:20.488: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename downward-api 03/02/23 02:23:20.489
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:23:20.56
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:23:20.573
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
STEP: Creating a pod to test downward API volume plugin 03/02/23 02:23:20.592
Mar  2 02:23:20.669: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4db03053-91ea-45f8-9903-64b1bffbd6f9" in namespace "downward-api-9556" to be "Succeeded or Failed"
Mar  2 02:23:20.698: INFO: Pod "downwardapi-volume-4db03053-91ea-45f8-9903-64b1bffbd6f9": Phase="Pending", Reason="", readiness=false. Elapsed: 28.619847ms
Mar  2 02:23:22.717: INFO: Pod "downwardapi-volume-4db03053-91ea-45f8-9903-64b1bffbd6f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047553975s
Mar  2 02:23:24.714: INFO: Pod "downwardapi-volume-4db03053-91ea-45f8-9903-64b1bffbd6f9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044621551s
Mar  2 02:23:26.724: INFO: Pod "downwardapi-volume-4db03053-91ea-45f8-9903-64b1bffbd6f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.054207362s
STEP: Saw pod success 03/02/23 02:23:26.724
Mar  2 02:23:26.724: INFO: Pod "downwardapi-volume-4db03053-91ea-45f8-9903-64b1bffbd6f9" satisfied condition "Succeeded or Failed"
Mar  2 02:23:26.743: INFO: Trying to get logs from node 10.132.92.143 pod downwardapi-volume-4db03053-91ea-45f8-9903-64b1bffbd6f9 container client-container: <nil>
STEP: delete the pod 03/02/23 02:23:26.78
Mar  2 02:23:26.841: INFO: Waiting for pod downwardapi-volume-4db03053-91ea-45f8-9903-64b1bffbd6f9 to disappear
Mar  2 02:23:26.860: INFO: Pod downwardapi-volume-4db03053-91ea-45f8-9903-64b1bffbd6f9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar  2 02:23:26.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9556" for this suite. 03/02/23 02:23:26.897
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","completed":226,"skipped":4198,"failed":0}
------------------------------
• [SLOW TEST] [6.440 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:23:20.487
    Mar  2 02:23:20.488: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename downward-api 03/02/23 02:23:20.489
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:23:20.56
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:23:20.573
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:52
    STEP: Creating a pod to test downward API volume plugin 03/02/23 02:23:20.592
    Mar  2 02:23:20.669: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4db03053-91ea-45f8-9903-64b1bffbd6f9" in namespace "downward-api-9556" to be "Succeeded or Failed"
    Mar  2 02:23:20.698: INFO: Pod "downwardapi-volume-4db03053-91ea-45f8-9903-64b1bffbd6f9": Phase="Pending", Reason="", readiness=false. Elapsed: 28.619847ms
    Mar  2 02:23:22.717: INFO: Pod "downwardapi-volume-4db03053-91ea-45f8-9903-64b1bffbd6f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047553975s
    Mar  2 02:23:24.714: INFO: Pod "downwardapi-volume-4db03053-91ea-45f8-9903-64b1bffbd6f9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044621551s
    Mar  2 02:23:26.724: INFO: Pod "downwardapi-volume-4db03053-91ea-45f8-9903-64b1bffbd6f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.054207362s
    STEP: Saw pod success 03/02/23 02:23:26.724
    Mar  2 02:23:26.724: INFO: Pod "downwardapi-volume-4db03053-91ea-45f8-9903-64b1bffbd6f9" satisfied condition "Succeeded or Failed"
    Mar  2 02:23:26.743: INFO: Trying to get logs from node 10.132.92.143 pod downwardapi-volume-4db03053-91ea-45f8-9903-64b1bffbd6f9 container client-container: <nil>
    STEP: delete the pod 03/02/23 02:23:26.78
    Mar  2 02:23:26.841: INFO: Waiting for pod downwardapi-volume-4db03053-91ea-45f8-9903-64b1bffbd6f9 to disappear
    Mar  2 02:23:26.860: INFO: Pod downwardapi-volume-4db03053-91ea-45f8-9903-64b1bffbd6f9 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar  2 02:23:26.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9556" for this suite. 03/02/23 02:23:26.897
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:23:26.929
Mar  2 02:23:26.929: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename limitrange 03/02/23 02:23:26.93
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:23:27.014
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:23:27.028
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
STEP: Creating a LimitRange 03/02/23 02:23:27.048
STEP: Setting up watch 03/02/23 02:23:27.049
STEP: Submitting a LimitRange 03/02/23 02:23:27.179
STEP: Verifying LimitRange creation was observed 03/02/23 02:23:27.198
STEP: Fetching the LimitRange to ensure it has proper values 03/02/23 02:23:27.198
Mar  2 02:23:27.214: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar  2 02:23:27.214: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 03/02/23 02:23:27.214
STEP: Ensuring Pod has resource requirements applied from LimitRange 03/02/23 02:23:27.258
Mar  2 02:23:27.279: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar  2 02:23:27.279: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 03/02/23 02:23:27.279
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 03/02/23 02:23:27.331
Mar  2 02:23:27.351: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Mar  2 02:23:27.351: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 03/02/23 02:23:27.351
STEP: Failing to create a Pod with more than max resources 03/02/23 02:23:27.371
STEP: Updating a LimitRange 03/02/23 02:23:27.391
STEP: Verifying LimitRange updating is effective 03/02/23 02:23:27.411
STEP: Creating a Pod with less than former min resources 03/02/23 02:23:29.428
STEP: Failing to create a Pod with more than max resources 03/02/23 02:23:29.473
STEP: Deleting a LimitRange 03/02/23 02:23:29.495
STEP: Verifying the LimitRange was deleted 03/02/23 02:23:29.519
Mar  2 02:23:34.533: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 03/02/23 02:23:34.533
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:187
Mar  2 02:23:34.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-2273" for this suite. 03/02/23 02:23:34.634
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","completed":227,"skipped":4211,"failed":0}
------------------------------
• [SLOW TEST] [7.735 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:23:26.929
    Mar  2 02:23:26.929: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename limitrange 03/02/23 02:23:26.93
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:23:27.014
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:23:27.028
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:57
    STEP: Creating a LimitRange 03/02/23 02:23:27.048
    STEP: Setting up watch 03/02/23 02:23:27.049
    STEP: Submitting a LimitRange 03/02/23 02:23:27.179
    STEP: Verifying LimitRange creation was observed 03/02/23 02:23:27.198
    STEP: Fetching the LimitRange to ensure it has proper values 03/02/23 02:23:27.198
    Mar  2 02:23:27.214: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Mar  2 02:23:27.214: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 03/02/23 02:23:27.214
    STEP: Ensuring Pod has resource requirements applied from LimitRange 03/02/23 02:23:27.258
    Mar  2 02:23:27.279: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Mar  2 02:23:27.279: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 03/02/23 02:23:27.279
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 03/02/23 02:23:27.331
    Mar  2 02:23:27.351: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Mar  2 02:23:27.351: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 03/02/23 02:23:27.351
    STEP: Failing to create a Pod with more than max resources 03/02/23 02:23:27.371
    STEP: Updating a LimitRange 03/02/23 02:23:27.391
    STEP: Verifying LimitRange updating is effective 03/02/23 02:23:27.411
    STEP: Creating a Pod with less than former min resources 03/02/23 02:23:29.428
    STEP: Failing to create a Pod with more than max resources 03/02/23 02:23:29.473
    STEP: Deleting a LimitRange 03/02/23 02:23:29.495
    STEP: Verifying the LimitRange was deleted 03/02/23 02:23:29.519
    Mar  2 02:23:34.533: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 03/02/23 02:23:34.533
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:187
    Mar  2 02:23:34.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "limitrange-2273" for this suite. 03/02/23 02:23:34.634
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:23:34.664
Mar  2 02:23:34.664: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename emptydir 03/02/23 02:23:34.666
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:23:34.793
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:23:34.807
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
STEP: Creating a pod to test emptydir 0666 on tmpfs 03/02/23 02:23:34.82
Mar  2 02:23:34.950: INFO: Waiting up to 5m0s for pod "pod-9dcc6c47-2938-4664-a84c-04e084484872" in namespace "emptydir-7822" to be "Succeeded or Failed"
Mar  2 02:23:34.974: INFO: Pod "pod-9dcc6c47-2938-4664-a84c-04e084484872": Phase="Pending", Reason="", readiness=false. Elapsed: 23.898667ms
Mar  2 02:23:36.988: INFO: Pod "pod-9dcc6c47-2938-4664-a84c-04e084484872": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038403305s
Mar  2 02:23:38.989: INFO: Pod "pod-9dcc6c47-2938-4664-a84c-04e084484872": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038896305s
STEP: Saw pod success 03/02/23 02:23:38.989
Mar  2 02:23:38.989: INFO: Pod "pod-9dcc6c47-2938-4664-a84c-04e084484872" satisfied condition "Succeeded or Failed"
Mar  2 02:23:39.003: INFO: Trying to get logs from node 10.132.92.143 pod pod-9dcc6c47-2938-4664-a84c-04e084484872 container test-container: <nil>
STEP: delete the pod 03/02/23 02:23:39.036
Mar  2 02:23:39.071: INFO: Waiting for pod pod-9dcc6c47-2938-4664-a84c-04e084484872 to disappear
Mar  2 02:23:39.085: INFO: Pod pod-9dcc6c47-2938-4664-a84c-04e084484872 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  2 02:23:39.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7822" for this suite. 03/02/23 02:23:39.109
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":228,"skipped":4217,"failed":0}
------------------------------
• [4.474 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:23:34.664
    Mar  2 02:23:34.664: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename emptydir 03/02/23 02:23:34.666
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:23:34.793
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:23:34.807
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:106
    STEP: Creating a pod to test emptydir 0666 on tmpfs 03/02/23 02:23:34.82
    Mar  2 02:23:34.950: INFO: Waiting up to 5m0s for pod "pod-9dcc6c47-2938-4664-a84c-04e084484872" in namespace "emptydir-7822" to be "Succeeded or Failed"
    Mar  2 02:23:34.974: INFO: Pod "pod-9dcc6c47-2938-4664-a84c-04e084484872": Phase="Pending", Reason="", readiness=false. Elapsed: 23.898667ms
    Mar  2 02:23:36.988: INFO: Pod "pod-9dcc6c47-2938-4664-a84c-04e084484872": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038403305s
    Mar  2 02:23:38.989: INFO: Pod "pod-9dcc6c47-2938-4664-a84c-04e084484872": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038896305s
    STEP: Saw pod success 03/02/23 02:23:38.989
    Mar  2 02:23:38.989: INFO: Pod "pod-9dcc6c47-2938-4664-a84c-04e084484872" satisfied condition "Succeeded or Failed"
    Mar  2 02:23:39.003: INFO: Trying to get logs from node 10.132.92.143 pod pod-9dcc6c47-2938-4664-a84c-04e084484872 container test-container: <nil>
    STEP: delete the pod 03/02/23 02:23:39.036
    Mar  2 02:23:39.071: INFO: Waiting for pod pod-9dcc6c47-2938-4664-a84c-04e084484872 to disappear
    Mar  2 02:23:39.085: INFO: Pod pod-9dcc6c47-2938-4664-a84c-04e084484872 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  2 02:23:39.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-7822" for this suite. 03/02/23 02:23:39.109
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:23:39.14
Mar  2 02:23:39.140: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename resourcequota 03/02/23 02:23:39.142
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:23:39.195
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:23:39.211
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
STEP: Counting existing ResourceQuota 03/02/23 02:23:56.243
STEP: Creating a ResourceQuota 03/02/23 02:24:01.261
STEP: Ensuring resource quota status is calculated 03/02/23 02:24:01.304
STEP: Creating a ConfigMap 03/02/23 02:24:03.319
STEP: Ensuring resource quota status captures configMap creation 03/02/23 02:24:03.363
STEP: Deleting a ConfigMap 03/02/23 02:24:05.391
STEP: Ensuring resource quota status released usage 03/02/23 02:24:05.418
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar  2 02:24:07.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2956" for this suite. 03/02/23 02:24:07.454
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","completed":229,"skipped":4217,"failed":0}
------------------------------
• [SLOW TEST] [28.338 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:23:39.14
    Mar  2 02:23:39.140: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename resourcequota 03/02/23 02:23:39.142
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:23:39.195
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:23:39.211
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:316
    STEP: Counting existing ResourceQuota 03/02/23 02:23:56.243
    STEP: Creating a ResourceQuota 03/02/23 02:24:01.261
    STEP: Ensuring resource quota status is calculated 03/02/23 02:24:01.304
    STEP: Creating a ConfigMap 03/02/23 02:24:03.319
    STEP: Ensuring resource quota status captures configMap creation 03/02/23 02:24:03.363
    STEP: Deleting a ConfigMap 03/02/23 02:24:05.391
    STEP: Ensuring resource quota status released usage 03/02/23 02:24:05.418
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar  2 02:24:07.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-2956" for this suite. 03/02/23 02:24:07.454
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:24:07.485
Mar  2 02:24:07.485: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename kubectl 03/02/23 02:24:07.487
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:24:07.559
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:24:07.573
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
STEP: creating a replication controller 03/02/23 02:24:07.586
Mar  2 02:24:07.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1818 create -f -'
Mar  2 02:24:13.643: INFO: stderr: ""
Mar  2 02:24:13.643: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 03/02/23 02:24:13.643
Mar  2 02:24:13.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1818 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 02:24:13.782: INFO: stderr: ""
Mar  2 02:24:13.782: INFO: stdout: "update-demo-nautilus-29p7z update-demo-nautilus-6226v "
Mar  2 02:24:13.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1818 get pods update-demo-nautilus-29p7z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 02:24:13.943: INFO: stderr: ""
Mar  2 02:24:13.943: INFO: stdout: ""
Mar  2 02:24:13.943: INFO: update-demo-nautilus-29p7z is created but not running
Mar  2 02:24:18.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1818 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 02:24:19.095: INFO: stderr: ""
Mar  2 02:24:19.095: INFO: stdout: "update-demo-nautilus-29p7z update-demo-nautilus-6226v "
Mar  2 02:24:19.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1818 get pods update-demo-nautilus-29p7z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 02:24:19.228: INFO: stderr: ""
Mar  2 02:24:19.228: INFO: stdout: "true"
Mar  2 02:24:19.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1818 get pods update-demo-nautilus-29p7z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  2 02:24:19.439: INFO: stderr: ""
Mar  2 02:24:19.439: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar  2 02:24:19.439: INFO: validating pod update-demo-nautilus-29p7z
Mar  2 02:24:19.467: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 02:24:19.467: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 02:24:19.467: INFO: update-demo-nautilus-29p7z is verified up and running
Mar  2 02:24:19.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1818 get pods update-demo-nautilus-6226v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 02:24:19.647: INFO: stderr: ""
Mar  2 02:24:19.647: INFO: stdout: "true"
Mar  2 02:24:19.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1818 get pods update-demo-nautilus-6226v -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  2 02:24:19.784: INFO: stderr: ""
Mar  2 02:24:19.784: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar  2 02:24:19.784: INFO: validating pod update-demo-nautilus-6226v
Mar  2 02:24:19.811: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 02:24:19.811: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 02:24:19.811: INFO: update-demo-nautilus-6226v is verified up and running
STEP: scaling down the replication controller 03/02/23 02:24:19.811
Mar  2 02:24:19.816: INFO: scanned /root for discovery docs: <nil>
Mar  2 02:24:19.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1818 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Mar  2 02:24:21.015: INFO: stderr: ""
Mar  2 02:24:21.015: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 03/02/23 02:24:21.015
Mar  2 02:24:21.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1818 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 02:24:21.149: INFO: stderr: ""
Mar  2 02:24:21.149: INFO: stdout: "update-demo-nautilus-29p7z update-demo-nautilus-6226v "
STEP: Replicas for name=update-demo: expected=1 actual=2 03/02/23 02:24:21.149
Mar  2 02:24:26.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1818 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 02:24:26.306: INFO: stderr: ""
Mar  2 02:24:26.306: INFO: stdout: "update-demo-nautilus-6226v "
Mar  2 02:24:26.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1818 get pods update-demo-nautilus-6226v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 02:24:26.571: INFO: stderr: ""
Mar  2 02:24:26.571: INFO: stdout: "true"
Mar  2 02:24:26.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1818 get pods update-demo-nautilus-6226v -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  2 02:24:26.729: INFO: stderr: ""
Mar  2 02:24:26.729: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar  2 02:24:26.729: INFO: validating pod update-demo-nautilus-6226v
Mar  2 02:24:26.754: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 02:24:26.754: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 02:24:26.754: INFO: update-demo-nautilus-6226v is verified up and running
STEP: scaling up the replication controller 03/02/23 02:24:26.754
Mar  2 02:24:26.761: INFO: scanned /root for discovery docs: <nil>
Mar  2 02:24:26.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1818 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Mar  2 02:24:28.003: INFO: stderr: ""
Mar  2 02:24:28.003: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 03/02/23 02:24:28.003
Mar  2 02:24:28.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1818 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 02:24:28.153: INFO: stderr: ""
Mar  2 02:24:28.153: INFO: stdout: "update-demo-nautilus-6226v update-demo-nautilus-zpnhz "
Mar  2 02:24:28.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1818 get pods update-demo-nautilus-6226v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 02:24:28.279: INFO: stderr: ""
Mar  2 02:24:28.279: INFO: stdout: "true"
Mar  2 02:24:28.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1818 get pods update-demo-nautilus-6226v -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  2 02:24:28.408: INFO: stderr: ""
Mar  2 02:24:28.408: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar  2 02:24:28.408: INFO: validating pod update-demo-nautilus-6226v
Mar  2 02:24:28.427: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 02:24:28.427: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 02:24:28.427: INFO: update-demo-nautilus-6226v is verified up and running
Mar  2 02:24:28.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1818 get pods update-demo-nautilus-zpnhz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 02:24:28.538: INFO: stderr: ""
Mar  2 02:24:28.538: INFO: stdout: ""
Mar  2 02:24:28.538: INFO: update-demo-nautilus-zpnhz is created but not running
Mar  2 02:24:33.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1818 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 02:24:33.667: INFO: stderr: ""
Mar  2 02:24:33.668: INFO: stdout: "update-demo-nautilus-6226v update-demo-nautilus-zpnhz "
Mar  2 02:24:33.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1818 get pods update-demo-nautilus-6226v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 02:24:33.776: INFO: stderr: ""
Mar  2 02:24:33.776: INFO: stdout: "true"
Mar  2 02:24:33.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1818 get pods update-demo-nautilus-6226v -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  2 02:24:33.911: INFO: stderr: ""
Mar  2 02:24:33.911: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar  2 02:24:33.911: INFO: validating pod update-demo-nautilus-6226v
Mar  2 02:24:33.936: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 02:24:33.936: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 02:24:33.936: INFO: update-demo-nautilus-6226v is verified up and running
Mar  2 02:24:33.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1818 get pods update-demo-nautilus-zpnhz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 02:24:34.084: INFO: stderr: ""
Mar  2 02:24:34.084: INFO: stdout: "true"
Mar  2 02:24:34.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1818 get pods update-demo-nautilus-zpnhz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  2 02:24:34.236: INFO: stderr: ""
Mar  2 02:24:34.236: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar  2 02:24:34.236: INFO: validating pod update-demo-nautilus-zpnhz
Mar  2 02:24:34.264: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 02:24:34.264: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 02:24:34.264: INFO: update-demo-nautilus-zpnhz is verified up and running
STEP: using delete to clean up resources 03/02/23 02:24:34.264
Mar  2 02:24:34.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1818 delete --grace-period=0 --force -f -'
Mar  2 02:24:34.403: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 02:24:34.403: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar  2 02:24:34.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1818 get rc,svc -l name=update-demo --no-headers'
Mar  2 02:24:34.576: INFO: stderr: "No resources found in kubectl-1818 namespace.\n"
Mar  2 02:24:34.576: INFO: stdout: ""
Mar  2 02:24:34.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1818 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  2 02:24:34.737: INFO: stderr: ""
Mar  2 02:24:34.737: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  2 02:24:34.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1818" for this suite. 03/02/23 02:24:34.759
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","completed":230,"skipped":4253,"failed":0}
------------------------------
• [SLOW TEST] [27.304 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:350

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:24:07.485
    Mar  2 02:24:07.485: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename kubectl 03/02/23 02:24:07.487
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:24:07.559
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:24:07.573
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:350
    STEP: creating a replication controller 03/02/23 02:24:07.586
    Mar  2 02:24:07.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1818 create -f -'
    Mar  2 02:24:13.643: INFO: stderr: ""
    Mar  2 02:24:13.643: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 03/02/23 02:24:13.643
    Mar  2 02:24:13.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1818 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar  2 02:24:13.782: INFO: stderr: ""
    Mar  2 02:24:13.782: INFO: stdout: "update-demo-nautilus-29p7z update-demo-nautilus-6226v "
    Mar  2 02:24:13.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1818 get pods update-demo-nautilus-29p7z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar  2 02:24:13.943: INFO: stderr: ""
    Mar  2 02:24:13.943: INFO: stdout: ""
    Mar  2 02:24:13.943: INFO: update-demo-nautilus-29p7z is created but not running
    Mar  2 02:24:18.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1818 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar  2 02:24:19.095: INFO: stderr: ""
    Mar  2 02:24:19.095: INFO: stdout: "update-demo-nautilus-29p7z update-demo-nautilus-6226v "
    Mar  2 02:24:19.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1818 get pods update-demo-nautilus-29p7z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar  2 02:24:19.228: INFO: stderr: ""
    Mar  2 02:24:19.228: INFO: stdout: "true"
    Mar  2 02:24:19.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1818 get pods update-demo-nautilus-29p7z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar  2 02:24:19.439: INFO: stderr: ""
    Mar  2 02:24:19.439: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar  2 02:24:19.439: INFO: validating pod update-demo-nautilus-29p7z
    Mar  2 02:24:19.467: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar  2 02:24:19.467: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar  2 02:24:19.467: INFO: update-demo-nautilus-29p7z is verified up and running
    Mar  2 02:24:19.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1818 get pods update-demo-nautilus-6226v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar  2 02:24:19.647: INFO: stderr: ""
    Mar  2 02:24:19.647: INFO: stdout: "true"
    Mar  2 02:24:19.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1818 get pods update-demo-nautilus-6226v -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar  2 02:24:19.784: INFO: stderr: ""
    Mar  2 02:24:19.784: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar  2 02:24:19.784: INFO: validating pod update-demo-nautilus-6226v
    Mar  2 02:24:19.811: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar  2 02:24:19.811: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar  2 02:24:19.811: INFO: update-demo-nautilus-6226v is verified up and running
    STEP: scaling down the replication controller 03/02/23 02:24:19.811
    Mar  2 02:24:19.816: INFO: scanned /root for discovery docs: <nil>
    Mar  2 02:24:19.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1818 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Mar  2 02:24:21.015: INFO: stderr: ""
    Mar  2 02:24:21.015: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 03/02/23 02:24:21.015
    Mar  2 02:24:21.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1818 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar  2 02:24:21.149: INFO: stderr: ""
    Mar  2 02:24:21.149: INFO: stdout: "update-demo-nautilus-29p7z update-demo-nautilus-6226v "
    STEP: Replicas for name=update-demo: expected=1 actual=2 03/02/23 02:24:21.149
    Mar  2 02:24:26.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1818 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar  2 02:24:26.306: INFO: stderr: ""
    Mar  2 02:24:26.306: INFO: stdout: "update-demo-nautilus-6226v "
    Mar  2 02:24:26.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1818 get pods update-demo-nautilus-6226v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar  2 02:24:26.571: INFO: stderr: ""
    Mar  2 02:24:26.571: INFO: stdout: "true"
    Mar  2 02:24:26.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1818 get pods update-demo-nautilus-6226v -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar  2 02:24:26.729: INFO: stderr: ""
    Mar  2 02:24:26.729: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar  2 02:24:26.729: INFO: validating pod update-demo-nautilus-6226v
    Mar  2 02:24:26.754: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar  2 02:24:26.754: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar  2 02:24:26.754: INFO: update-demo-nautilus-6226v is verified up and running
    STEP: scaling up the replication controller 03/02/23 02:24:26.754
    Mar  2 02:24:26.761: INFO: scanned /root for discovery docs: <nil>
    Mar  2 02:24:26.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1818 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Mar  2 02:24:28.003: INFO: stderr: ""
    Mar  2 02:24:28.003: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 03/02/23 02:24:28.003
    Mar  2 02:24:28.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1818 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar  2 02:24:28.153: INFO: stderr: ""
    Mar  2 02:24:28.153: INFO: stdout: "update-demo-nautilus-6226v update-demo-nautilus-zpnhz "
    Mar  2 02:24:28.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1818 get pods update-demo-nautilus-6226v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar  2 02:24:28.279: INFO: stderr: ""
    Mar  2 02:24:28.279: INFO: stdout: "true"
    Mar  2 02:24:28.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1818 get pods update-demo-nautilus-6226v -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar  2 02:24:28.408: INFO: stderr: ""
    Mar  2 02:24:28.408: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar  2 02:24:28.408: INFO: validating pod update-demo-nautilus-6226v
    Mar  2 02:24:28.427: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar  2 02:24:28.427: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar  2 02:24:28.427: INFO: update-demo-nautilus-6226v is verified up and running
    Mar  2 02:24:28.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1818 get pods update-demo-nautilus-zpnhz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar  2 02:24:28.538: INFO: stderr: ""
    Mar  2 02:24:28.538: INFO: stdout: ""
    Mar  2 02:24:28.538: INFO: update-demo-nautilus-zpnhz is created but not running
    Mar  2 02:24:33.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1818 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar  2 02:24:33.667: INFO: stderr: ""
    Mar  2 02:24:33.668: INFO: stdout: "update-demo-nautilus-6226v update-demo-nautilus-zpnhz "
    Mar  2 02:24:33.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1818 get pods update-demo-nautilus-6226v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar  2 02:24:33.776: INFO: stderr: ""
    Mar  2 02:24:33.776: INFO: stdout: "true"
    Mar  2 02:24:33.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1818 get pods update-demo-nautilus-6226v -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar  2 02:24:33.911: INFO: stderr: ""
    Mar  2 02:24:33.911: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar  2 02:24:33.911: INFO: validating pod update-demo-nautilus-6226v
    Mar  2 02:24:33.936: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar  2 02:24:33.936: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar  2 02:24:33.936: INFO: update-demo-nautilus-6226v is verified up and running
    Mar  2 02:24:33.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1818 get pods update-demo-nautilus-zpnhz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar  2 02:24:34.084: INFO: stderr: ""
    Mar  2 02:24:34.084: INFO: stdout: "true"
    Mar  2 02:24:34.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1818 get pods update-demo-nautilus-zpnhz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar  2 02:24:34.236: INFO: stderr: ""
    Mar  2 02:24:34.236: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar  2 02:24:34.236: INFO: validating pod update-demo-nautilus-zpnhz
    Mar  2 02:24:34.264: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar  2 02:24:34.264: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar  2 02:24:34.264: INFO: update-demo-nautilus-zpnhz is verified up and running
    STEP: using delete to clean up resources 03/02/23 02:24:34.264
    Mar  2 02:24:34.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1818 delete --grace-period=0 --force -f -'
    Mar  2 02:24:34.403: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar  2 02:24:34.403: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Mar  2 02:24:34.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1818 get rc,svc -l name=update-demo --no-headers'
    Mar  2 02:24:34.576: INFO: stderr: "No resources found in kubectl-1818 namespace.\n"
    Mar  2 02:24:34.576: INFO: stdout: ""
    Mar  2 02:24:34.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1818 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Mar  2 02:24:34.737: INFO: stderr: ""
    Mar  2 02:24:34.737: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  2 02:24:34.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1818" for this suite. 03/02/23 02:24:34.759
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:24:34.791
Mar  2 02:24:34.791: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename webhook 03/02/23 02:24:34.792
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:24:34.855
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:24:34.869
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/02/23 02:24:34.99
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 02:24:35.62
STEP: Deploying the webhook pod 03/02/23 02:24:35.672
STEP: Wait for the deployment to be ready 03/02/23 02:24:35.74
Mar  2 02:24:35.860: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 2, 24, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 24, 35, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-5d85dd8cdb\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 2, 24, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 24, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Mar  2 02:24:37.876: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 2, 24, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 24, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 2, 24, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 24, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/02/23 02:24:39.876
STEP: Verifying the service has paired with the endpoint 03/02/23 02:24:39.916
Mar  2 02:24:40.917: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
STEP: Creating a validating webhook configuration 03/02/23 02:24:40.932
STEP: Creating a configMap that does not comply to the validation webhook rules 03/02/23 02:24:41.169
STEP: Updating a validating webhook configuration's rules to not include the create operation 03/02/23 02:24:41.369
STEP: Creating a configMap that does not comply to the validation webhook rules 03/02/23 02:24:41.408
STEP: Patching a validating webhook configuration's rules to include the create operation 03/02/23 02:24:41.465
STEP: Creating a configMap that does not comply to the validation webhook rules 03/02/23 02:24:41.492
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 02:24:41.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5010" for this suite. 03/02/23 02:24:41.601
STEP: Destroying namespace "webhook-5010-markers" for this suite. 03/02/23 02:24:41.631
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","completed":231,"skipped":4265,"failed":0}
------------------------------
• [SLOW TEST] [7.072 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:24:34.791
    Mar  2 02:24:34.791: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename webhook 03/02/23 02:24:34.792
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:24:34.855
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:24:34.869
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/02/23 02:24:34.99
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 02:24:35.62
    STEP: Deploying the webhook pod 03/02/23 02:24:35.672
    STEP: Wait for the deployment to be ready 03/02/23 02:24:35.74
    Mar  2 02:24:35.860: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 2, 24, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 24, 35, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-5d85dd8cdb\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 2, 24, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 24, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
    Mar  2 02:24:37.876: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 2, 24, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 24, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 2, 24, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 24, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/02/23 02:24:39.876
    STEP: Verifying the service has paired with the endpoint 03/02/23 02:24:39.916
    Mar  2 02:24:40.917: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:412
    STEP: Creating a validating webhook configuration 03/02/23 02:24:40.932
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/02/23 02:24:41.169
    STEP: Updating a validating webhook configuration's rules to not include the create operation 03/02/23 02:24:41.369
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/02/23 02:24:41.408
    STEP: Patching a validating webhook configuration's rules to include the create operation 03/02/23 02:24:41.465
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/02/23 02:24:41.492
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 02:24:41.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5010" for this suite. 03/02/23 02:24:41.601
    STEP: Destroying namespace "webhook-5010-markers" for this suite. 03/02/23 02:24:41.631
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:24:41.866
Mar  2 02:24:41.866: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename downward-api 03/02/23 02:24:41.867
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:24:41.949
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:24:41.975
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
STEP: Creating a pod to test downward API volume plugin 03/02/23 02:24:42.001
Mar  2 02:24:42.104: INFO: Waiting up to 5m0s for pod "downwardapi-volume-83991791-9648-4e51-b7eb-ee10bd40d44e" in namespace "downward-api-9398" to be "Succeeded or Failed"
Mar  2 02:24:42.122: INFO: Pod "downwardapi-volume-83991791-9648-4e51-b7eb-ee10bd40d44e": Phase="Pending", Reason="", readiness=false. Elapsed: 17.667403ms
Mar  2 02:24:44.139: INFO: Pod "downwardapi-volume-83991791-9648-4e51-b7eb-ee10bd40d44e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034725912s
Mar  2 02:24:46.141: INFO: Pod "downwardapi-volume-83991791-9648-4e51-b7eb-ee10bd40d44e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036958021s
Mar  2 02:24:48.139: INFO: Pod "downwardapi-volume-83991791-9648-4e51-b7eb-ee10bd40d44e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.035369935s
STEP: Saw pod success 03/02/23 02:24:48.139
Mar  2 02:24:48.140: INFO: Pod "downwardapi-volume-83991791-9648-4e51-b7eb-ee10bd40d44e" satisfied condition "Succeeded or Failed"
Mar  2 02:24:48.153: INFO: Trying to get logs from node 10.132.92.143 pod downwardapi-volume-83991791-9648-4e51-b7eb-ee10bd40d44e container client-container: <nil>
STEP: delete the pod 03/02/23 02:24:48.203
Mar  2 02:24:48.252: INFO: Waiting for pod downwardapi-volume-83991791-9648-4e51-b7eb-ee10bd40d44e to disappear
Mar  2 02:24:48.264: INFO: Pod downwardapi-volume-83991791-9648-4e51-b7eb-ee10bd40d44e no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar  2 02:24:48.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9398" for this suite. 03/02/23 02:24:48.288
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":232,"skipped":4295,"failed":0}
------------------------------
• [SLOW TEST] [6.449 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:24:41.866
    Mar  2 02:24:41.866: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename downward-api 03/02/23 02:24:41.867
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:24:41.949
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:24:41.975
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:248
    STEP: Creating a pod to test downward API volume plugin 03/02/23 02:24:42.001
    Mar  2 02:24:42.104: INFO: Waiting up to 5m0s for pod "downwardapi-volume-83991791-9648-4e51-b7eb-ee10bd40d44e" in namespace "downward-api-9398" to be "Succeeded or Failed"
    Mar  2 02:24:42.122: INFO: Pod "downwardapi-volume-83991791-9648-4e51-b7eb-ee10bd40d44e": Phase="Pending", Reason="", readiness=false. Elapsed: 17.667403ms
    Mar  2 02:24:44.139: INFO: Pod "downwardapi-volume-83991791-9648-4e51-b7eb-ee10bd40d44e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034725912s
    Mar  2 02:24:46.141: INFO: Pod "downwardapi-volume-83991791-9648-4e51-b7eb-ee10bd40d44e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036958021s
    Mar  2 02:24:48.139: INFO: Pod "downwardapi-volume-83991791-9648-4e51-b7eb-ee10bd40d44e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.035369935s
    STEP: Saw pod success 03/02/23 02:24:48.139
    Mar  2 02:24:48.140: INFO: Pod "downwardapi-volume-83991791-9648-4e51-b7eb-ee10bd40d44e" satisfied condition "Succeeded or Failed"
    Mar  2 02:24:48.153: INFO: Trying to get logs from node 10.132.92.143 pod downwardapi-volume-83991791-9648-4e51-b7eb-ee10bd40d44e container client-container: <nil>
    STEP: delete the pod 03/02/23 02:24:48.203
    Mar  2 02:24:48.252: INFO: Waiting for pod downwardapi-volume-83991791-9648-4e51-b7eb-ee10bd40d44e to disappear
    Mar  2 02:24:48.264: INFO: Pod downwardapi-volume-83991791-9648-4e51-b7eb-ee10bd40d44e no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar  2 02:24:48.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9398" for this suite. 03/02/23 02:24:48.288
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:24:48.315
Mar  2 02:24:48.315: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename hostport 03/02/23 02:24:48.317
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:24:48.391
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:24:48.409
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 03/02/23 02:24:48.475
Mar  2 02:24:48.598: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-5698" to be "running and ready"
Mar  2 02:24:48.681: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 82.972297ms
Mar  2 02:24:48.681: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 02:24:50.696: INFO: Pod "pod1": Phase="Running", Reason="", readiness=false. Elapsed: 2.097394486s
Mar  2 02:24:50.696: INFO: The phase of Pod pod1 is Running (Ready = false)
Mar  2 02:24:52.697: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.098447852s
Mar  2 02:24:52.697: INFO: The phase of Pod pod1 is Running (Ready = true)
Mar  2 02:24:52.697: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.132.92.188 on the node which pod1 resides and expect scheduled 03/02/23 02:24:52.697
Mar  2 02:24:52.746: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-5698" to be "running and ready"
Mar  2 02:24:52.760: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 14.459135ms
Mar  2 02:24:52.760: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 02:24:54.789: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.043565332s
Mar  2 02:24:54.789: INFO: The phase of Pod pod2 is Running (Ready = false)
Mar  2 02:24:56.778: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.032024942s
Mar  2 02:24:56.778: INFO: The phase of Pod pod2 is Running (Ready = true)
Mar  2 02:24:56.778: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.132.92.188 but use UDP protocol on the node which pod2 resides 03/02/23 02:24:56.778
Mar  2 02:24:56.832: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-5698" to be "running and ready"
Mar  2 02:24:56.846: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 13.319839ms
Mar  2 02:24:56.846: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 02:24:58.861: INFO: Pod "pod3": Phase="Running", Reason="", readiness=false. Elapsed: 2.029218565s
Mar  2 02:24:58.861: INFO: The phase of Pod pod3 is Running (Ready = false)
Mar  2 02:25:00.867: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.034915945s
Mar  2 02:25:00.867: INFO: The phase of Pod pod3 is Running (Ready = true)
Mar  2 02:25:00.867: INFO: Pod "pod3" satisfied condition "running and ready"
Mar  2 02:25:00.907: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-5698" to be "running and ready"
Mar  2 02:25:00.918: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 11.369896ms
Mar  2 02:25:00.919: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Mar  2 02:25:02.958: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.051199598s
Mar  2 02:25:02.958: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Mar  2 02:25:02.958: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 03/02/23 02:25:02.971
Mar  2 02:25:02.972: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.132.92.188 http://127.0.0.1:54323/hostname] Namespace:hostport-5698 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 02:25:02.972: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
Mar  2 02:25:02.973: INFO: ExecWithOptions: Clientset creation
Mar  2 02:25:02.973: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-5698/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.132.92.188+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.132.92.188, port: 54323 03/02/23 02:25:03.353
Mar  2 02:25:03.354: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.132.92.188:54323/hostname] Namespace:hostport-5698 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 02:25:03.354: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
Mar  2 02:25:03.355: INFO: ExecWithOptions: Clientset creation
Mar  2 02:25:03.355: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-5698/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.132.92.188%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.132.92.188, port: 54323 UDP 03/02/23 02:25:03.673
Mar  2 02:25:03.673: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.132.92.188 54323] Namespace:hostport-5698 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 02:25:03.673: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
Mar  2 02:25:03.675: INFO: ExecWithOptions: Clientset creation
Mar  2 02:25:03.675: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-5698/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.132.92.188+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/framework.go:187
Mar  2 02:25:08.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-5698" for this suite. 03/02/23 02:25:08.997
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","completed":233,"skipped":4305,"failed":0}
------------------------------
• [SLOW TEST] [20.704 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:24:48.315
    Mar  2 02:24:48.315: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename hostport 03/02/23 02:24:48.317
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:24:48.391
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:24:48.409
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 03/02/23 02:24:48.475
    Mar  2 02:24:48.598: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-5698" to be "running and ready"
    Mar  2 02:24:48.681: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 82.972297ms
    Mar  2 02:24:48.681: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 02:24:50.696: INFO: Pod "pod1": Phase="Running", Reason="", readiness=false. Elapsed: 2.097394486s
    Mar  2 02:24:50.696: INFO: The phase of Pod pod1 is Running (Ready = false)
    Mar  2 02:24:52.697: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.098447852s
    Mar  2 02:24:52.697: INFO: The phase of Pod pod1 is Running (Ready = true)
    Mar  2 02:24:52.697: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.132.92.188 on the node which pod1 resides and expect scheduled 03/02/23 02:24:52.697
    Mar  2 02:24:52.746: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-5698" to be "running and ready"
    Mar  2 02:24:52.760: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 14.459135ms
    Mar  2 02:24:52.760: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 02:24:54.789: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.043565332s
    Mar  2 02:24:54.789: INFO: The phase of Pod pod2 is Running (Ready = false)
    Mar  2 02:24:56.778: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.032024942s
    Mar  2 02:24:56.778: INFO: The phase of Pod pod2 is Running (Ready = true)
    Mar  2 02:24:56.778: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.132.92.188 but use UDP protocol on the node which pod2 resides 03/02/23 02:24:56.778
    Mar  2 02:24:56.832: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-5698" to be "running and ready"
    Mar  2 02:24:56.846: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 13.319839ms
    Mar  2 02:24:56.846: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 02:24:58.861: INFO: Pod "pod3": Phase="Running", Reason="", readiness=false. Elapsed: 2.029218565s
    Mar  2 02:24:58.861: INFO: The phase of Pod pod3 is Running (Ready = false)
    Mar  2 02:25:00.867: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.034915945s
    Mar  2 02:25:00.867: INFO: The phase of Pod pod3 is Running (Ready = true)
    Mar  2 02:25:00.867: INFO: Pod "pod3" satisfied condition "running and ready"
    Mar  2 02:25:00.907: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-5698" to be "running and ready"
    Mar  2 02:25:00.918: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 11.369896ms
    Mar  2 02:25:00.919: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 02:25:02.958: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.051199598s
    Mar  2 02:25:02.958: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Mar  2 02:25:02.958: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 03/02/23 02:25:02.971
    Mar  2 02:25:02.972: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.132.92.188 http://127.0.0.1:54323/hostname] Namespace:hostport-5698 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 02:25:02.972: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    Mar  2 02:25:02.973: INFO: ExecWithOptions: Clientset creation
    Mar  2 02:25:02.973: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-5698/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.132.92.188+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.132.92.188, port: 54323 03/02/23 02:25:03.353
    Mar  2 02:25:03.354: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.132.92.188:54323/hostname] Namespace:hostport-5698 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 02:25:03.354: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    Mar  2 02:25:03.355: INFO: ExecWithOptions: Clientset creation
    Mar  2 02:25:03.355: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-5698/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.132.92.188%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.132.92.188, port: 54323 UDP 03/02/23 02:25:03.673
    Mar  2 02:25:03.673: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.132.92.188 54323] Namespace:hostport-5698 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 02:25:03.673: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    Mar  2 02:25:03.675: INFO: ExecWithOptions: Clientset creation
    Mar  2 02:25:03.675: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-5698/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.132.92.188+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/framework.go:187
    Mar  2 02:25:08.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "hostport-5698" for this suite. 03/02/23 02:25:08.997
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:25:09.02
Mar  2 02:25:09.020: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename webhook 03/02/23 02:25:09.021
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:25:09.085
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:25:09.097
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/02/23 02:25:09.178
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 02:25:10.104
STEP: Deploying the webhook pod 03/02/23 02:25:10.146
STEP: Wait for the deployment to be ready 03/02/23 02:25:10.209
Mar  2 02:25:10.303: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar  2 02:25:12.416: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 2, 25, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 25, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 2, 25, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 25, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/02/23 02:25:14.43
STEP: Verifying the service has paired with the endpoint 03/02/23 02:25:14.474
Mar  2 02:25:15.474: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
STEP: Creating a mutating webhook configuration 03/02/23 02:25:15.488
STEP: Updating a mutating webhook configuration's rules to not include the create operation 03/02/23 02:25:15.616
STEP: Creating a configMap that should not be mutated 03/02/23 02:25:15.64
STEP: Patching a mutating webhook configuration's rules to include the create operation 03/02/23 02:25:15.694
STEP: Creating a configMap that should be mutated 03/02/23 02:25:15.735
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 02:25:15.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7587" for this suite. 03/02/23 02:25:15.89
STEP: Destroying namespace "webhook-7587-markers" for this suite. 03/02/23 02:25:15.915
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","completed":234,"skipped":4309,"failed":0}
------------------------------
• [SLOW TEST] [7.131 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:25:09.02
    Mar  2 02:25:09.020: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename webhook 03/02/23 02:25:09.021
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:25:09.085
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:25:09.097
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/02/23 02:25:09.178
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 02:25:10.104
    STEP: Deploying the webhook pod 03/02/23 02:25:10.146
    STEP: Wait for the deployment to be ready 03/02/23 02:25:10.209
    Mar  2 02:25:10.303: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Mar  2 02:25:12.416: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 2, 25, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 25, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 2, 25, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 25, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/02/23 02:25:14.43
    STEP: Verifying the service has paired with the endpoint 03/02/23 02:25:14.474
    Mar  2 02:25:15.474: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:507
    STEP: Creating a mutating webhook configuration 03/02/23 02:25:15.488
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 03/02/23 02:25:15.616
    STEP: Creating a configMap that should not be mutated 03/02/23 02:25:15.64
    STEP: Patching a mutating webhook configuration's rules to include the create operation 03/02/23 02:25:15.694
    STEP: Creating a configMap that should be mutated 03/02/23 02:25:15.735
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 02:25:15.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7587" for this suite. 03/02/23 02:25:15.89
    STEP: Destroying namespace "webhook-7587-markers" for this suite. 03/02/23 02:25:15.915
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:25:16.151
Mar  2 02:25:16.151: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename downward-api 03/02/23 02:25:16.152
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:25:16.271
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:25:16.292
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
STEP: Creating a pod to test downward api env vars 03/02/23 02:25:16.317
Mar  2 02:25:16.404: INFO: Waiting up to 5m0s for pod "downward-api-02d45673-b674-448c-b41d-2256c0f9c3d5" in namespace "downward-api-1807" to be "Succeeded or Failed"
Mar  2 02:25:16.458: INFO: Pod "downward-api-02d45673-b674-448c-b41d-2256c0f9c3d5": Phase="Pending", Reason="", readiness=false. Elapsed: 53.632742ms
Mar  2 02:25:18.492: INFO: Pod "downward-api-02d45673-b674-448c-b41d-2256c0f9c3d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.087757042s
Mar  2 02:25:20.479: INFO: Pod "downward-api-02d45673-b674-448c-b41d-2256c0f9c3d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.074867509s
STEP: Saw pod success 03/02/23 02:25:20.48
Mar  2 02:25:20.480: INFO: Pod "downward-api-02d45673-b674-448c-b41d-2256c0f9c3d5" satisfied condition "Succeeded or Failed"
Mar  2 02:25:20.493: INFO: Trying to get logs from node 10.132.92.143 pod downward-api-02d45673-b674-448c-b41d-2256c0f9c3d5 container dapi-container: <nil>
STEP: delete the pod 03/02/23 02:25:20.523
Mar  2 02:25:20.564: INFO: Waiting for pod downward-api-02d45673-b674-448c-b41d-2256c0f9c3d5 to disappear
Mar  2 02:25:20.577: INFO: Pod downward-api-02d45673-b674-448c-b41d-2256c0f9c3d5 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Mar  2 02:25:20.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1807" for this suite. 03/02/23 02:25:20.601
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","completed":235,"skipped":4316,"failed":0}
------------------------------
• [4.507 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:25:16.151
    Mar  2 02:25:16.151: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename downward-api 03/02/23 02:25:16.152
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:25:16.271
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:25:16.292
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:165
    STEP: Creating a pod to test downward api env vars 03/02/23 02:25:16.317
    Mar  2 02:25:16.404: INFO: Waiting up to 5m0s for pod "downward-api-02d45673-b674-448c-b41d-2256c0f9c3d5" in namespace "downward-api-1807" to be "Succeeded or Failed"
    Mar  2 02:25:16.458: INFO: Pod "downward-api-02d45673-b674-448c-b41d-2256c0f9c3d5": Phase="Pending", Reason="", readiness=false. Elapsed: 53.632742ms
    Mar  2 02:25:18.492: INFO: Pod "downward-api-02d45673-b674-448c-b41d-2256c0f9c3d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.087757042s
    Mar  2 02:25:20.479: INFO: Pod "downward-api-02d45673-b674-448c-b41d-2256c0f9c3d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.074867509s
    STEP: Saw pod success 03/02/23 02:25:20.48
    Mar  2 02:25:20.480: INFO: Pod "downward-api-02d45673-b674-448c-b41d-2256c0f9c3d5" satisfied condition "Succeeded or Failed"
    Mar  2 02:25:20.493: INFO: Trying to get logs from node 10.132.92.143 pod downward-api-02d45673-b674-448c-b41d-2256c0f9c3d5 container dapi-container: <nil>
    STEP: delete the pod 03/02/23 02:25:20.523
    Mar  2 02:25:20.564: INFO: Waiting for pod downward-api-02d45673-b674-448c-b41d-2256c0f9c3d5 to disappear
    Mar  2 02:25:20.577: INFO: Pod downward-api-02d45673-b674-448c-b41d-2256c0f9c3d5 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Mar  2 02:25:20.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1807" for this suite. 03/02/23 02:25:20.601
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:25:20.66
Mar  2 02:25:20.660: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename custom-resource-definition 03/02/23 02:25:20.662
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:25:20.764
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:25:20.777
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Mar  2 02:25:20.823: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 02:25:21.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2233" for this suite. 03/02/23 02:25:22.001
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","completed":236,"skipped":4324,"failed":0}
------------------------------
• [1.377 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:25:20.66
    Mar  2 02:25:20.660: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename custom-resource-definition 03/02/23 02:25:20.662
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:25:20.764
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:25:20.777
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Mar  2 02:25:20.823: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 02:25:21.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-2233" for this suite. 03/02/23 02:25:22.001
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:309
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:25:22.041
Mar  2 02:25:22.041: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename job 03/02/23 02:25:22.043
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:25:22.099
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:25:22.112
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:309
STEP: Creating a job 03/02/23 02:25:22.126
W0302 02:25:22.156501      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring active pods == parallelism 03/02/23 02:25:22.156
STEP: delete a job 03/02/23 02:25:26.193
STEP: deleting Job.batch foo in namespace job-7115, will wait for the garbage collector to delete the pods 03/02/23 02:25:26.194
Mar  2 02:25:26.321: INFO: Deleting Job.batch foo took: 38.054697ms
Mar  2 02:25:26.422: INFO: Terminating Job.batch foo pods took: 101.01544ms
STEP: Ensuring job was deleted 03/02/23 02:25:58.723
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Mar  2 02:25:58.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7115" for this suite. 03/02/23 02:25:58.772
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","completed":237,"skipped":4345,"failed":0}
------------------------------
• [SLOW TEST] [36.758 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:25:22.041
    Mar  2 02:25:22.041: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename job 03/02/23 02:25:22.043
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:25:22.099
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:25:22.112
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:309
    STEP: Creating a job 03/02/23 02:25:22.126
    W0302 02:25:22.156501      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring active pods == parallelism 03/02/23 02:25:22.156
    STEP: delete a job 03/02/23 02:25:26.193
    STEP: deleting Job.batch foo in namespace job-7115, will wait for the garbage collector to delete the pods 03/02/23 02:25:26.194
    Mar  2 02:25:26.321: INFO: Deleting Job.batch foo took: 38.054697ms
    Mar  2 02:25:26.422: INFO: Terminating Job.batch foo pods took: 101.01544ms
    STEP: Ensuring job was deleted 03/02/23 02:25:58.723
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Mar  2 02:25:58.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-7115" for this suite. 03/02/23 02:25:58.772
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:25:58.8
Mar  2 02:25:58.800: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename resourcequota 03/02/23 02:25:58.801
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:25:58.862
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:25:58.876
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
STEP: Counting existing ResourceQuota 03/02/23 02:25:58.892
STEP: Creating a ResourceQuota 03/02/23 02:26:03.908
STEP: Ensuring resource quota status is calculated 03/02/23 02:26:03.927
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar  2 02:26:05.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4962" for this suite. 03/02/23 02:26:05.965
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","completed":238,"skipped":4374,"failed":0}
------------------------------
• [SLOW TEST] [7.191 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:25:58.8
    Mar  2 02:25:58.800: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename resourcequota 03/02/23 02:25:58.801
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:25:58.862
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:25:58.876
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:65
    STEP: Counting existing ResourceQuota 03/02/23 02:25:58.892
    STEP: Creating a ResourceQuota 03/02/23 02:26:03.908
    STEP: Ensuring resource quota status is calculated 03/02/23 02:26:03.927
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar  2 02:26:05.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-4962" for this suite. 03/02/23 02:26:05.965
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:26:05.992
Mar  2 02:26:05.992: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename pods 03/02/23 02:26:05.994
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:26:06.057
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:26:06.068
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
Mar  2 02:26:06.082: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: creating the pod 03/02/23 02:26:06.084
STEP: submitting the pod to kubernetes 03/02/23 02:26:06.084
Mar  2 02:26:06.184: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-a9330551-6e5b-49bf-8077-1d002bbbb6e4" in namespace "pods-3242" to be "running and ready"
Mar  2 02:26:06.204: INFO: Pod "pod-logs-websocket-a9330551-6e5b-49bf-8077-1d002bbbb6e4": Phase="Pending", Reason="", readiness=false. Elapsed: 19.413982ms
Mar  2 02:26:06.204: INFO: The phase of Pod pod-logs-websocket-a9330551-6e5b-49bf-8077-1d002bbbb6e4 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 02:26:08.219: INFO: Pod "pod-logs-websocket-a9330551-6e5b-49bf-8077-1d002bbbb6e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03492808s
Mar  2 02:26:08.219: INFO: The phase of Pod pod-logs-websocket-a9330551-6e5b-49bf-8077-1d002bbbb6e4 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 02:26:10.220: INFO: Pod "pod-logs-websocket-a9330551-6e5b-49bf-8077-1d002bbbb6e4": Phase="Running", Reason="", readiness=true. Elapsed: 4.036178525s
Mar  2 02:26:10.220: INFO: The phase of Pod pod-logs-websocket-a9330551-6e5b-49bf-8077-1d002bbbb6e4 is Running (Ready = true)
Mar  2 02:26:10.220: INFO: Pod "pod-logs-websocket-a9330551-6e5b-49bf-8077-1d002bbbb6e4" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar  2 02:26:10.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3242" for this suite. 03/02/23 02:26:10.348
{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","completed":239,"skipped":4382,"failed":0}
------------------------------
• [4.417 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:26:05.992
    Mar  2 02:26:05.992: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename pods 03/02/23 02:26:05.994
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:26:06.057
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:26:06.068
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:617
    Mar  2 02:26:06.082: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: creating the pod 03/02/23 02:26:06.084
    STEP: submitting the pod to kubernetes 03/02/23 02:26:06.084
    Mar  2 02:26:06.184: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-a9330551-6e5b-49bf-8077-1d002bbbb6e4" in namespace "pods-3242" to be "running and ready"
    Mar  2 02:26:06.204: INFO: Pod "pod-logs-websocket-a9330551-6e5b-49bf-8077-1d002bbbb6e4": Phase="Pending", Reason="", readiness=false. Elapsed: 19.413982ms
    Mar  2 02:26:06.204: INFO: The phase of Pod pod-logs-websocket-a9330551-6e5b-49bf-8077-1d002bbbb6e4 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 02:26:08.219: INFO: Pod "pod-logs-websocket-a9330551-6e5b-49bf-8077-1d002bbbb6e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03492808s
    Mar  2 02:26:08.219: INFO: The phase of Pod pod-logs-websocket-a9330551-6e5b-49bf-8077-1d002bbbb6e4 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 02:26:10.220: INFO: Pod "pod-logs-websocket-a9330551-6e5b-49bf-8077-1d002bbbb6e4": Phase="Running", Reason="", readiness=true. Elapsed: 4.036178525s
    Mar  2 02:26:10.220: INFO: The phase of Pod pod-logs-websocket-a9330551-6e5b-49bf-8077-1d002bbbb6e4 is Running (Ready = true)
    Mar  2 02:26:10.220: INFO: Pod "pod-logs-websocket-a9330551-6e5b-49bf-8077-1d002bbbb6e4" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar  2 02:26:10.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-3242" for this suite. 03/02/23 02:26:10.348
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:26:10.416
Mar  2 02:26:10.416: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename dns 03/02/23 02:26:10.42
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:26:10.499
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:26:10.538
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 03/02/23 02:26:10.55
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1420 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1420;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1420 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1420;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1420.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1420.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1420.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1420.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1420.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1420.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1420.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1420.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1420.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1420.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1420.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1420.svc;check="$$(dig +notcp +noall +answer +search 153.29.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.29.153_udp@PTR;check="$$(dig +tcp +noall +answer +search 153.29.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.29.153_tcp@PTR;sleep 1; done
 03/02/23 02:26:10.685
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1420 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1420;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1420 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1420;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1420.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1420.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1420.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1420.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1420.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1420.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1420.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1420.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1420.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1420.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1420.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1420.svc;check="$$(dig +notcp +noall +answer +search 153.29.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.29.153_udp@PTR;check="$$(dig +tcp +noall +answer +search 153.29.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.29.153_tcp@PTR;sleep 1; done
 03/02/23 02:26:10.685
STEP: creating a pod to probe DNS 03/02/23 02:26:10.685
STEP: submitting the pod to kubernetes 03/02/23 02:26:10.685
Mar  2 02:26:10.814: INFO: Waiting up to 15m0s for pod "dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670" in namespace "dns-1420" to be "running"
Mar  2 02:26:10.844: INFO: Pod "dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670": Phase="Pending", Reason="", readiness=false. Elapsed: 27.397618ms
Mar  2 02:26:12.885: INFO: Pod "dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670": Phase="Pending", Reason="", readiness=false. Elapsed: 2.068654649s
Mar  2 02:26:14.860: INFO: Pod "dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670": Phase="Running", Reason="", readiness=true. Elapsed: 4.043602358s
Mar  2 02:26:14.860: INFO: Pod "dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670" satisfied condition "running"
STEP: retrieving the pod 03/02/23 02:26:14.86
STEP: looking for the results for each expected name from probers 03/02/23 02:26:14.875
Mar  2 02:26:14.906: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1420/dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670: the server could not find the requested resource (get pods dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670)
Mar  2 02:26:14.925: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1420/dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670: the server could not find the requested resource (get pods dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670)
Mar  2 02:26:14.945: INFO: Unable to read wheezy_udp@dns-test-service.dns-1420 from pod dns-1420/dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670: the server could not find the requested resource (get pods dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670)
Mar  2 02:26:14.967: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1420 from pod dns-1420/dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670: the server could not find the requested resource (get pods dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670)
Mar  2 02:26:14.989: INFO: Unable to read wheezy_udp@dns-test-service.dns-1420.svc from pod dns-1420/dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670: the server could not find the requested resource (get pods dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670)
Mar  2 02:26:15.012: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1420.svc from pod dns-1420/dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670: the server could not find the requested resource (get pods dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670)
Mar  2 02:26:15.033: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1420.svc from pod dns-1420/dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670: the server could not find the requested resource (get pods dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670)
Mar  2 02:26:15.058: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1420.svc from pod dns-1420/dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670: the server could not find the requested resource (get pods dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670)
Mar  2 02:26:15.210: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1420/dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670: the server could not find the requested resource (get pods dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670)
Mar  2 02:26:15.292: INFO: Unable to read jessie_tcp@dns-test-service.dns-1420 from pod dns-1420/dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670: the server could not find the requested resource (get pods dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670)
Mar  2 02:26:15.343: INFO: Unable to read jessie_tcp@dns-test-service.dns-1420.svc from pod dns-1420/dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670: the server could not find the requested resource (get pods dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670)
Mar  2 02:26:15.546: INFO: Lookups using dns-1420/dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1420 wheezy_tcp@dns-test-service.dns-1420 wheezy_udp@dns-test-service.dns-1420.svc wheezy_tcp@dns-test-service.dns-1420.svc wheezy_udp@_http._tcp.dns-test-service.dns-1420.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1420.svc jessie_tcp@dns-test-service jessie_tcp@dns-test-service.dns-1420 jessie_tcp@dns-test-service.dns-1420.svc]

Mar  2 02:26:21.096: INFO: DNS probes using dns-1420/dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670 succeeded

STEP: deleting the pod 03/02/23 02:26:21.097
STEP: deleting the test service 03/02/23 02:26:21.134
STEP: deleting the test headless service 03/02/23 02:26:21.231
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar  2 02:26:21.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1420" for this suite. 03/02/23 02:26:21.303
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","completed":240,"skipped":4404,"failed":0}
------------------------------
• [SLOW TEST] [10.915 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:26:10.416
    Mar  2 02:26:10.416: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename dns 03/02/23 02:26:10.42
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:26:10.499
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:26:10.538
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 03/02/23 02:26:10.55
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1420 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1420;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1420 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1420;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1420.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1420.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1420.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1420.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1420.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1420.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1420.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1420.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1420.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1420.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1420.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1420.svc;check="$$(dig +notcp +noall +answer +search 153.29.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.29.153_udp@PTR;check="$$(dig +tcp +noall +answer +search 153.29.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.29.153_tcp@PTR;sleep 1; done
     03/02/23 02:26:10.685
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1420 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1420;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1420 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1420;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1420.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1420.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1420.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1420.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1420.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1420.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1420.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1420.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1420.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1420.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1420.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1420.svc;check="$$(dig +notcp +noall +answer +search 153.29.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.29.153_udp@PTR;check="$$(dig +tcp +noall +answer +search 153.29.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.29.153_tcp@PTR;sleep 1; done
     03/02/23 02:26:10.685
    STEP: creating a pod to probe DNS 03/02/23 02:26:10.685
    STEP: submitting the pod to kubernetes 03/02/23 02:26:10.685
    Mar  2 02:26:10.814: INFO: Waiting up to 15m0s for pod "dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670" in namespace "dns-1420" to be "running"
    Mar  2 02:26:10.844: INFO: Pod "dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670": Phase="Pending", Reason="", readiness=false. Elapsed: 27.397618ms
    Mar  2 02:26:12.885: INFO: Pod "dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670": Phase="Pending", Reason="", readiness=false. Elapsed: 2.068654649s
    Mar  2 02:26:14.860: INFO: Pod "dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670": Phase="Running", Reason="", readiness=true. Elapsed: 4.043602358s
    Mar  2 02:26:14.860: INFO: Pod "dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670" satisfied condition "running"
    STEP: retrieving the pod 03/02/23 02:26:14.86
    STEP: looking for the results for each expected name from probers 03/02/23 02:26:14.875
    Mar  2 02:26:14.906: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1420/dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670: the server could not find the requested resource (get pods dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670)
    Mar  2 02:26:14.925: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1420/dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670: the server could not find the requested resource (get pods dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670)
    Mar  2 02:26:14.945: INFO: Unable to read wheezy_udp@dns-test-service.dns-1420 from pod dns-1420/dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670: the server could not find the requested resource (get pods dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670)
    Mar  2 02:26:14.967: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1420 from pod dns-1420/dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670: the server could not find the requested resource (get pods dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670)
    Mar  2 02:26:14.989: INFO: Unable to read wheezy_udp@dns-test-service.dns-1420.svc from pod dns-1420/dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670: the server could not find the requested resource (get pods dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670)
    Mar  2 02:26:15.012: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1420.svc from pod dns-1420/dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670: the server could not find the requested resource (get pods dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670)
    Mar  2 02:26:15.033: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1420.svc from pod dns-1420/dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670: the server could not find the requested resource (get pods dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670)
    Mar  2 02:26:15.058: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1420.svc from pod dns-1420/dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670: the server could not find the requested resource (get pods dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670)
    Mar  2 02:26:15.210: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1420/dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670: the server could not find the requested resource (get pods dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670)
    Mar  2 02:26:15.292: INFO: Unable to read jessie_tcp@dns-test-service.dns-1420 from pod dns-1420/dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670: the server could not find the requested resource (get pods dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670)
    Mar  2 02:26:15.343: INFO: Unable to read jessie_tcp@dns-test-service.dns-1420.svc from pod dns-1420/dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670: the server could not find the requested resource (get pods dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670)
    Mar  2 02:26:15.546: INFO: Lookups using dns-1420/dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1420 wheezy_tcp@dns-test-service.dns-1420 wheezy_udp@dns-test-service.dns-1420.svc wheezy_tcp@dns-test-service.dns-1420.svc wheezy_udp@_http._tcp.dns-test-service.dns-1420.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1420.svc jessie_tcp@dns-test-service jessie_tcp@dns-test-service.dns-1420 jessie_tcp@dns-test-service.dns-1420.svc]

    Mar  2 02:26:21.096: INFO: DNS probes using dns-1420/dns-test-ba90de8c-fac2-4b26-a931-ed3589a2f670 succeeded

    STEP: deleting the pod 03/02/23 02:26:21.097
    STEP: deleting the test service 03/02/23 02:26:21.134
    STEP: deleting the test headless service 03/02/23 02:26:21.231
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar  2 02:26:21.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-1420" for this suite. 03/02/23 02:26:21.303
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:26:21.333
Mar  2 02:26:21.333: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename pods 03/02/23 02:26:21.341
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:26:21.407
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:26:21.42
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
STEP: creating pod 03/02/23 02:26:21.438
Mar  2 02:26:21.503: INFO: Waiting up to 5m0s for pod "pod-hostip-e951798b-8498-4517-bf87-ad4f725b8f4c" in namespace "pods-8890" to be "running and ready"
Mar  2 02:26:21.518: INFO: Pod "pod-hostip-e951798b-8498-4517-bf87-ad4f725b8f4c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.95364ms
Mar  2 02:26:21.518: INFO: The phase of Pod pod-hostip-e951798b-8498-4517-bf87-ad4f725b8f4c is Pending, waiting for it to be Running (with Ready = true)
Mar  2 02:26:23.532: INFO: Pod "pod-hostip-e951798b-8498-4517-bf87-ad4f725b8f4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029235473s
Mar  2 02:26:23.532: INFO: The phase of Pod pod-hostip-e951798b-8498-4517-bf87-ad4f725b8f4c is Pending, waiting for it to be Running (with Ready = true)
Mar  2 02:26:25.534: INFO: Pod "pod-hostip-e951798b-8498-4517-bf87-ad4f725b8f4c": Phase="Running", Reason="", readiness=true. Elapsed: 4.030768815s
Mar  2 02:26:25.534: INFO: The phase of Pod pod-hostip-e951798b-8498-4517-bf87-ad4f725b8f4c is Running (Ready = true)
Mar  2 02:26:25.534: INFO: Pod "pod-hostip-e951798b-8498-4517-bf87-ad4f725b8f4c" satisfied condition "running and ready"
Mar  2 02:26:25.561: INFO: Pod pod-hostip-e951798b-8498-4517-bf87-ad4f725b8f4c has hostIP: 10.132.92.143
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar  2 02:26:25.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8890" for this suite. 03/02/23 02:26:25.582
{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","completed":241,"skipped":4428,"failed":0}
------------------------------
• [4.275 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:26:21.333
    Mar  2 02:26:21.333: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename pods 03/02/23 02:26:21.341
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:26:21.407
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:26:21.42
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:203
    STEP: creating pod 03/02/23 02:26:21.438
    Mar  2 02:26:21.503: INFO: Waiting up to 5m0s for pod "pod-hostip-e951798b-8498-4517-bf87-ad4f725b8f4c" in namespace "pods-8890" to be "running and ready"
    Mar  2 02:26:21.518: INFO: Pod "pod-hostip-e951798b-8498-4517-bf87-ad4f725b8f4c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.95364ms
    Mar  2 02:26:21.518: INFO: The phase of Pod pod-hostip-e951798b-8498-4517-bf87-ad4f725b8f4c is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 02:26:23.532: INFO: Pod "pod-hostip-e951798b-8498-4517-bf87-ad4f725b8f4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029235473s
    Mar  2 02:26:23.532: INFO: The phase of Pod pod-hostip-e951798b-8498-4517-bf87-ad4f725b8f4c is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 02:26:25.534: INFO: Pod "pod-hostip-e951798b-8498-4517-bf87-ad4f725b8f4c": Phase="Running", Reason="", readiness=true. Elapsed: 4.030768815s
    Mar  2 02:26:25.534: INFO: The phase of Pod pod-hostip-e951798b-8498-4517-bf87-ad4f725b8f4c is Running (Ready = true)
    Mar  2 02:26:25.534: INFO: Pod "pod-hostip-e951798b-8498-4517-bf87-ad4f725b8f4c" satisfied condition "running and ready"
    Mar  2 02:26:25.561: INFO: Pod pod-hostip-e951798b-8498-4517-bf87-ad4f725b8f4c has hostIP: 10.132.92.143
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar  2 02:26:25.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-8890" for this suite. 03/02/23 02:26:25.582
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:26:25.61
Mar  2 02:26:25.610: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename pods 03/02/23 02:26:25.611
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:26:25.671
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:26:25.685
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
Mar  2 02:26:25.848: INFO: Waiting up to 5m0s for pod "server-envvars-64a16eef-2bc8-4dc4-b33e-549e706703b4" in namespace "pods-4007" to be "running and ready"
Mar  2 02:26:25.866: INFO: Pod "server-envvars-64a16eef-2bc8-4dc4-b33e-549e706703b4": Phase="Pending", Reason="", readiness=false. Elapsed: 18.737235ms
Mar  2 02:26:25.867: INFO: The phase of Pod server-envvars-64a16eef-2bc8-4dc4-b33e-549e706703b4 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 02:26:27.897: INFO: Pod "server-envvars-64a16eef-2bc8-4dc4-b33e-549e706703b4": Phase="Running", Reason="", readiness=true. Elapsed: 2.049432827s
Mar  2 02:26:27.897: INFO: The phase of Pod server-envvars-64a16eef-2bc8-4dc4-b33e-549e706703b4 is Running (Ready = true)
Mar  2 02:26:27.897: INFO: Pod "server-envvars-64a16eef-2bc8-4dc4-b33e-549e706703b4" satisfied condition "running and ready"
Mar  2 02:26:28.040: INFO: Waiting up to 5m0s for pod "client-envvars-48cd641e-a63b-4975-bc55-c9df0c6d896f" in namespace "pods-4007" to be "Succeeded or Failed"
Mar  2 02:26:28.084: INFO: Pod "client-envvars-48cd641e-a63b-4975-bc55-c9df0c6d896f": Phase="Pending", Reason="", readiness=false. Elapsed: 43.736075ms
Mar  2 02:26:30.151: INFO: Pod "client-envvars-48cd641e-a63b-4975-bc55-c9df0c6d896f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.110030687s
Mar  2 02:26:32.153: INFO: Pod "client-envvars-48cd641e-a63b-4975-bc55-c9df0c6d896f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.112141181s
STEP: Saw pod success 03/02/23 02:26:32.153
Mar  2 02:26:32.153: INFO: Pod "client-envvars-48cd641e-a63b-4975-bc55-c9df0c6d896f" satisfied condition "Succeeded or Failed"
Mar  2 02:26:32.167: INFO: Trying to get logs from node 10.132.92.143 pod client-envvars-48cd641e-a63b-4975-bc55-c9df0c6d896f container env3cont: <nil>
STEP: delete the pod 03/02/23 02:26:32.224
Mar  2 02:26:32.287: INFO: Waiting for pod client-envvars-48cd641e-a63b-4975-bc55-c9df0c6d896f to disappear
Mar  2 02:26:32.302: INFO: Pod client-envvars-48cd641e-a63b-4975-bc55-c9df0c6d896f no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar  2 02:26:32.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4007" for this suite. 03/02/23 02:26:32.323
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","completed":242,"skipped":4450,"failed":0}
------------------------------
• [SLOW TEST] [6.737 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:26:25.61
    Mar  2 02:26:25.610: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename pods 03/02/23 02:26:25.611
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:26:25.671
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:26:25.685
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:443
    Mar  2 02:26:25.848: INFO: Waiting up to 5m0s for pod "server-envvars-64a16eef-2bc8-4dc4-b33e-549e706703b4" in namespace "pods-4007" to be "running and ready"
    Mar  2 02:26:25.866: INFO: Pod "server-envvars-64a16eef-2bc8-4dc4-b33e-549e706703b4": Phase="Pending", Reason="", readiness=false. Elapsed: 18.737235ms
    Mar  2 02:26:25.867: INFO: The phase of Pod server-envvars-64a16eef-2bc8-4dc4-b33e-549e706703b4 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 02:26:27.897: INFO: Pod "server-envvars-64a16eef-2bc8-4dc4-b33e-549e706703b4": Phase="Running", Reason="", readiness=true. Elapsed: 2.049432827s
    Mar  2 02:26:27.897: INFO: The phase of Pod server-envvars-64a16eef-2bc8-4dc4-b33e-549e706703b4 is Running (Ready = true)
    Mar  2 02:26:27.897: INFO: Pod "server-envvars-64a16eef-2bc8-4dc4-b33e-549e706703b4" satisfied condition "running and ready"
    Mar  2 02:26:28.040: INFO: Waiting up to 5m0s for pod "client-envvars-48cd641e-a63b-4975-bc55-c9df0c6d896f" in namespace "pods-4007" to be "Succeeded or Failed"
    Mar  2 02:26:28.084: INFO: Pod "client-envvars-48cd641e-a63b-4975-bc55-c9df0c6d896f": Phase="Pending", Reason="", readiness=false. Elapsed: 43.736075ms
    Mar  2 02:26:30.151: INFO: Pod "client-envvars-48cd641e-a63b-4975-bc55-c9df0c6d896f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.110030687s
    Mar  2 02:26:32.153: INFO: Pod "client-envvars-48cd641e-a63b-4975-bc55-c9df0c6d896f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.112141181s
    STEP: Saw pod success 03/02/23 02:26:32.153
    Mar  2 02:26:32.153: INFO: Pod "client-envvars-48cd641e-a63b-4975-bc55-c9df0c6d896f" satisfied condition "Succeeded or Failed"
    Mar  2 02:26:32.167: INFO: Trying to get logs from node 10.132.92.143 pod client-envvars-48cd641e-a63b-4975-bc55-c9df0c6d896f container env3cont: <nil>
    STEP: delete the pod 03/02/23 02:26:32.224
    Mar  2 02:26:32.287: INFO: Waiting for pod client-envvars-48cd641e-a63b-4975-bc55-c9df0c6d896f to disappear
    Mar  2 02:26:32.302: INFO: Pod client-envvars-48cd641e-a63b-4975-bc55-c9df0c6d896f no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar  2 02:26:32.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4007" for this suite. 03/02/23 02:26:32.323
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:26:32.354
Mar  2 02:26:32.355: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename kubectl 03/02/23 02:26:32.356
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:26:32.423
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:26:32.444
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
STEP: create deployment with httpd image 03/02/23 02:26:32.459
Mar  2 02:26:32.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-4248 create -f -'
Mar  2 02:26:38.239: INFO: stderr: ""
Mar  2 02:26:38.240: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 03/02/23 02:26:38.24
Mar  2 02:26:38.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-4248 diff -f -'
Mar  2 02:26:41.283: INFO: rc: 1
Mar  2 02:26:41.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-4248 delete -f -'
Mar  2 02:26:41.478: INFO: stderr: ""
Mar  2 02:26:41.478: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  2 02:26:41.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4248" for this suite. 03/02/23 02:26:41.506
{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","completed":243,"skipped":4464,"failed":0}
------------------------------
• [SLOW TEST] [9.185 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:923
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:929

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:26:32.354
    Mar  2 02:26:32.355: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename kubectl 03/02/23 02:26:32.356
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:26:32.423
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:26:32.444
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:929
    STEP: create deployment with httpd image 03/02/23 02:26:32.459
    Mar  2 02:26:32.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-4248 create -f -'
    Mar  2 02:26:38.239: INFO: stderr: ""
    Mar  2 02:26:38.240: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 03/02/23 02:26:38.24
    Mar  2 02:26:38.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-4248 diff -f -'
    Mar  2 02:26:41.283: INFO: rc: 1
    Mar  2 02:26:41.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-4248 delete -f -'
    Mar  2 02:26:41.478: INFO: stderr: ""
    Mar  2 02:26:41.478: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  2 02:26:41.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4248" for this suite. 03/02/23 02:26:41.506
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:26:41.541
Mar  2 02:26:41.541: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename webhook 03/02/23 02:26:41.542
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:26:41.597
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:26:41.609
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/02/23 02:26:41.747
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 02:26:42.164
STEP: Deploying the webhook pod 03/02/23 02:26:42.198
STEP: Wait for the deployment to be ready 03/02/23 02:26:42.241
Mar  2 02:26:42.348: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 02:26:44.393: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 2, 26, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 26, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 2, 26, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 26, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/02/23 02:26:46.443
STEP: Verifying the service has paired with the endpoint 03/02/23 02:26:46.494
Mar  2 02:26:47.495: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
STEP: Registering the crd webhook via the AdmissionRegistration API 03/02/23 02:26:47.516
STEP: Creating a custom resource definition that should be denied by the webhook 03/02/23 02:26:47.693
Mar  2 02:26:47.693: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 02:26:47.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8078" for this suite. 03/02/23 02:26:47.96
STEP: Destroying namespace "webhook-8078-markers" for this suite. 03/02/23 02:26:47.986
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","completed":244,"skipped":4489,"failed":0}
------------------------------
• [SLOW TEST] [6.643 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:26:41.541
    Mar  2 02:26:41.541: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename webhook 03/02/23 02:26:41.542
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:26:41.597
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:26:41.609
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/02/23 02:26:41.747
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 02:26:42.164
    STEP: Deploying the webhook pod 03/02/23 02:26:42.198
    STEP: Wait for the deployment to be ready 03/02/23 02:26:42.241
    Mar  2 02:26:42.348: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Mar  2 02:26:44.393: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 2, 26, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 26, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 2, 26, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 26, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/02/23 02:26:46.443
    STEP: Verifying the service has paired with the endpoint 03/02/23 02:26:46.494
    Mar  2 02:26:47.495: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:307
    STEP: Registering the crd webhook via the AdmissionRegistration API 03/02/23 02:26:47.516
    STEP: Creating a custom resource definition that should be denied by the webhook 03/02/23 02:26:47.693
    Mar  2 02:26:47.693: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 02:26:47.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8078" for this suite. 03/02/23 02:26:47.96
    STEP: Destroying namespace "webhook-8078-markers" for this suite. 03/02/23 02:26:47.986
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:26:48.197
Mar  2 02:26:48.197: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename ephemeral-containers-test 03/02/23 02:26:48.198
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:26:48.267
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:26:48.284
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 03/02/23 02:26:48.297
Mar  2 02:26:48.388: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-7117" to be "running and ready"
Mar  2 02:26:48.435: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 47.606104ms
Mar  2 02:26:48.436: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Mar  2 02:26:50.452: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.063919847s
Mar  2 02:26:50.452: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Mar  2 02:26:52.452: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.064563327s
Mar  2 02:26:52.452: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Mar  2 02:26:52.452: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 03/02/23 02:26:52.466
Mar  2 02:26:52.491: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-7117" to be "container debugger running"
Mar  2 02:26:52.504: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 13.005141ms
Mar  2 02:26:54.519: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.027425976s
Mar  2 02:26:54.519: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 03/02/23 02:26:54.519
Mar  2 02:26:54.519: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-7117 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 02:26:54.519: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
Mar  2 02:26:54.520: INFO: ExecWithOptions: Clientset creation
Mar  2 02:26:54.520: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/ephemeral-containers-test-7117/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Mar  2 02:26:54.991: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:187
Mar  2 02:26:55.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ephemeral-containers-test-7117" for this suite. 03/02/23 02:26:55.075
{"msg":"PASSED [sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]","completed":245,"skipped":4554,"failed":0}
------------------------------
• [SLOW TEST] [6.907 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:26:48.197
    Mar  2 02:26:48.197: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename ephemeral-containers-test 03/02/23 02:26:48.198
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:26:48.267
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:26:48.284
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 03/02/23 02:26:48.297
    Mar  2 02:26:48.388: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-7117" to be "running and ready"
    Mar  2 02:26:48.435: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 47.606104ms
    Mar  2 02:26:48.436: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 02:26:50.452: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.063919847s
    Mar  2 02:26:50.452: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 02:26:52.452: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.064563327s
    Mar  2 02:26:52.452: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Mar  2 02:26:52.452: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 03/02/23 02:26:52.466
    Mar  2 02:26:52.491: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-7117" to be "container debugger running"
    Mar  2 02:26:52.504: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 13.005141ms
    Mar  2 02:26:54.519: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.027425976s
    Mar  2 02:26:54.519: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 03/02/23 02:26:54.519
    Mar  2 02:26:54.519: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-7117 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 02:26:54.519: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    Mar  2 02:26:54.520: INFO: ExecWithOptions: Clientset creation
    Mar  2 02:26:54.520: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/ephemeral-containers-test-7117/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Mar  2 02:26:54.991: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:187
    Mar  2 02:26:55.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ephemeral-containers-test-7117" for this suite. 03/02/23 02:26:55.075
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:26:55.105
Mar  2 02:26:55.106: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename lease-test 03/02/23 02:26:55.107
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:26:55.169
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:26:55.186
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/framework.go:187
Mar  2 02:26:55.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-1670" for this suite. 03/02/23 02:26:55.593
{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","completed":246,"skipped":4582,"failed":0}
------------------------------
• [0.512 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:26:55.105
    Mar  2 02:26:55.106: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename lease-test 03/02/23 02:26:55.107
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:26:55.169
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:26:55.186
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/framework.go:187
    Mar  2 02:26:55.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "lease-test-1670" for this suite. 03/02/23 02:26:55.593
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:26:55.622
Mar  2 02:26:55.623: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename projected 03/02/23 02:26:55.625
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:26:55.711
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:26:55.725
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
STEP: Creating projection with secret that has name projected-secret-test-map-1e4c4191-a813-4d4d-b7de-a485f56e6d38 03/02/23 02:26:55.74
STEP: Creating a pod to test consume secrets 03/02/23 02:26:55.813
Mar  2 02:26:55.893: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2118d012-95b3-416c-b4dc-871b726b16f3" in namespace "projected-9358" to be "Succeeded or Failed"
Mar  2 02:26:55.907: INFO: Pod "pod-projected-secrets-2118d012-95b3-416c-b4dc-871b726b16f3": Phase="Pending", Reason="", readiness=false. Elapsed: 13.788701ms
Mar  2 02:26:57.954: INFO: Pod "pod-projected-secrets-2118d012-95b3-416c-b4dc-871b726b16f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.061283716s
Mar  2 02:26:59.921: INFO: Pod "pod-projected-secrets-2118d012-95b3-416c-b4dc-871b726b16f3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028316406s
Mar  2 02:27:01.925: INFO: Pod "pod-projected-secrets-2118d012-95b3-416c-b4dc-871b726b16f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032176017s
STEP: Saw pod success 03/02/23 02:27:01.925
Mar  2 02:27:01.926: INFO: Pod "pod-projected-secrets-2118d012-95b3-416c-b4dc-871b726b16f3" satisfied condition "Succeeded or Failed"
Mar  2 02:27:01.945: INFO: Trying to get logs from node 10.132.92.143 pod pod-projected-secrets-2118d012-95b3-416c-b4dc-871b726b16f3 container projected-secret-volume-test: <nil>
STEP: delete the pod 03/02/23 02:27:02
Mar  2 02:27:02.085: INFO: Waiting for pod pod-projected-secrets-2118d012-95b3-416c-b4dc-871b726b16f3 to disappear
Mar  2 02:27:02.101: INFO: Pod pod-projected-secrets-2118d012-95b3-416c-b4dc-871b726b16f3 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Mar  2 02:27:02.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9358" for this suite. 03/02/23 02:27:02.141
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":247,"skipped":4667,"failed":0}
------------------------------
• [SLOW TEST] [6.562 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:26:55.622
    Mar  2 02:26:55.623: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename projected 03/02/23 02:26:55.625
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:26:55.711
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:26:55.725
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:87
    STEP: Creating projection with secret that has name projected-secret-test-map-1e4c4191-a813-4d4d-b7de-a485f56e6d38 03/02/23 02:26:55.74
    STEP: Creating a pod to test consume secrets 03/02/23 02:26:55.813
    Mar  2 02:26:55.893: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2118d012-95b3-416c-b4dc-871b726b16f3" in namespace "projected-9358" to be "Succeeded or Failed"
    Mar  2 02:26:55.907: INFO: Pod "pod-projected-secrets-2118d012-95b3-416c-b4dc-871b726b16f3": Phase="Pending", Reason="", readiness=false. Elapsed: 13.788701ms
    Mar  2 02:26:57.954: INFO: Pod "pod-projected-secrets-2118d012-95b3-416c-b4dc-871b726b16f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.061283716s
    Mar  2 02:26:59.921: INFO: Pod "pod-projected-secrets-2118d012-95b3-416c-b4dc-871b726b16f3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028316406s
    Mar  2 02:27:01.925: INFO: Pod "pod-projected-secrets-2118d012-95b3-416c-b4dc-871b726b16f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032176017s
    STEP: Saw pod success 03/02/23 02:27:01.925
    Mar  2 02:27:01.926: INFO: Pod "pod-projected-secrets-2118d012-95b3-416c-b4dc-871b726b16f3" satisfied condition "Succeeded or Failed"
    Mar  2 02:27:01.945: INFO: Trying to get logs from node 10.132.92.143 pod pod-projected-secrets-2118d012-95b3-416c-b4dc-871b726b16f3 container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/02/23 02:27:02
    Mar  2 02:27:02.085: INFO: Waiting for pod pod-projected-secrets-2118d012-95b3-416c-b4dc-871b726b16f3 to disappear
    Mar  2 02:27:02.101: INFO: Pod pod-projected-secrets-2118d012-95b3-416c-b4dc-871b726b16f3 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Mar  2 02:27:02.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9358" for this suite. 03/02/23 02:27:02.141
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:27:02.185
Mar  2 02:27:02.186: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename secrets 03/02/23 02:27:02.187
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:27:02.29
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:27:02.305
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
STEP: Creating projection with secret that has name secret-emptykey-test-9053216b-fac0-4d72-abcf-631ec70d1ee3 03/02/23 02:27:02.32
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Mar  2 02:27:02.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4812" for this suite. 03/02/23 02:27:02.366
{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","completed":248,"skipped":4668,"failed":0}
------------------------------
• [0.249 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:27:02.185
    Mar  2 02:27:02.186: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename secrets 03/02/23 02:27:02.187
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:27:02.29
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:27:02.305
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:139
    STEP: Creating projection with secret that has name secret-emptykey-test-9053216b-fac0-4d72-abcf-631ec70d1ee3 03/02/23 02:27:02.32
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Mar  2 02:27:02.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-4812" for this suite. 03/02/23 02:27:02.366
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:27:02.435
Mar  2 02:27:02.435: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename containers 03/02/23 02:27:02.437
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:27:02.528
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:27:02.541
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
STEP: Creating a pod to test override command 03/02/23 02:27:02.556
Mar  2 02:27:02.664: INFO: Waiting up to 5m0s for pod "client-containers-4e6e3335-1b49-4284-9719-992779758aae" in namespace "containers-5536" to be "Succeeded or Failed"
Mar  2 02:27:02.696: INFO: Pod "client-containers-4e6e3335-1b49-4284-9719-992779758aae": Phase="Pending", Reason="", readiness=false. Elapsed: 31.445211ms
Mar  2 02:27:04.723: INFO: Pod "client-containers-4e6e3335-1b49-4284-9719-992779758aae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05820057s
Mar  2 02:27:06.716: INFO: Pod "client-containers-4e6e3335-1b49-4284-9719-992779758aae": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051078015s
Mar  2 02:27:08.711: INFO: Pod "client-containers-4e6e3335-1b49-4284-9719-992779758aae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.046461234s
STEP: Saw pod success 03/02/23 02:27:08.711
Mar  2 02:27:08.711: INFO: Pod "client-containers-4e6e3335-1b49-4284-9719-992779758aae" satisfied condition "Succeeded or Failed"
Mar  2 02:27:08.724: INFO: Trying to get logs from node 10.132.92.143 pod client-containers-4e6e3335-1b49-4284-9719-992779758aae container agnhost-container: <nil>
STEP: delete the pod 03/02/23 02:27:08.76
Mar  2 02:27:08.814: INFO: Waiting for pod client-containers-4e6e3335-1b49-4284-9719-992779758aae to disappear
Mar  2 02:27:08.828: INFO: Pod client-containers-4e6e3335-1b49-4284-9719-992779758aae no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Mar  2 02:27:08.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5536" for this suite. 03/02/23 02:27:08.851
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]","completed":249,"skipped":4670,"failed":0}
------------------------------
• [SLOW TEST] [6.443 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:27:02.435
    Mar  2 02:27:02.435: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename containers 03/02/23 02:27:02.437
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:27:02.528
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:27:02.541
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:72
    STEP: Creating a pod to test override command 03/02/23 02:27:02.556
    Mar  2 02:27:02.664: INFO: Waiting up to 5m0s for pod "client-containers-4e6e3335-1b49-4284-9719-992779758aae" in namespace "containers-5536" to be "Succeeded or Failed"
    Mar  2 02:27:02.696: INFO: Pod "client-containers-4e6e3335-1b49-4284-9719-992779758aae": Phase="Pending", Reason="", readiness=false. Elapsed: 31.445211ms
    Mar  2 02:27:04.723: INFO: Pod "client-containers-4e6e3335-1b49-4284-9719-992779758aae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05820057s
    Mar  2 02:27:06.716: INFO: Pod "client-containers-4e6e3335-1b49-4284-9719-992779758aae": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051078015s
    Mar  2 02:27:08.711: INFO: Pod "client-containers-4e6e3335-1b49-4284-9719-992779758aae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.046461234s
    STEP: Saw pod success 03/02/23 02:27:08.711
    Mar  2 02:27:08.711: INFO: Pod "client-containers-4e6e3335-1b49-4284-9719-992779758aae" satisfied condition "Succeeded or Failed"
    Mar  2 02:27:08.724: INFO: Trying to get logs from node 10.132.92.143 pod client-containers-4e6e3335-1b49-4284-9719-992779758aae container agnhost-container: <nil>
    STEP: delete the pod 03/02/23 02:27:08.76
    Mar  2 02:27:08.814: INFO: Waiting for pod client-containers-4e6e3335-1b49-4284-9719-992779758aae to disappear
    Mar  2 02:27:08.828: INFO: Pod client-containers-4e6e3335-1b49-4284-9719-992779758aae no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Mar  2 02:27:08.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-5536" for this suite. 03/02/23 02:27:08.851
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:27:08.881
Mar  2 02:27:08.882: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename kubelet-test 03/02/23 02:27:08.884
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:27:08.961
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:27:08.975
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 03/02/23 02:27:09.124
Mar  2 02:27:09.124: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases43f4ee8a-8671-40ac-8fbc-cf26aa99fc18" in namespace "kubelet-test-278" to be "completed"
Mar  2 02:27:09.148: INFO: Pod "agnhost-host-aliases43f4ee8a-8671-40ac-8fbc-cf26aa99fc18": Phase="Pending", Reason="", readiness=false. Elapsed: 23.776446ms
Mar  2 02:27:11.162: INFO: Pod "agnhost-host-aliases43f4ee8a-8671-40ac-8fbc-cf26aa99fc18": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03789032s
Mar  2 02:27:13.163: INFO: Pod "agnhost-host-aliases43f4ee8a-8671-40ac-8fbc-cf26aa99fc18": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039005951s
Mar  2 02:27:13.163: INFO: Pod "agnhost-host-aliases43f4ee8a-8671-40ac-8fbc-cf26aa99fc18" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Mar  2 02:27:13.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-278" for this suite. 03/02/23 02:27:13.221
{"msg":"PASSED [sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]","completed":250,"skipped":4676,"failed":0}
------------------------------
• [4.364 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:27:08.881
    Mar  2 02:27:08.882: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename kubelet-test 03/02/23 02:27:08.884
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:27:08.961
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:27:08.975
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 03/02/23 02:27:09.124
    Mar  2 02:27:09.124: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases43f4ee8a-8671-40ac-8fbc-cf26aa99fc18" in namespace "kubelet-test-278" to be "completed"
    Mar  2 02:27:09.148: INFO: Pod "agnhost-host-aliases43f4ee8a-8671-40ac-8fbc-cf26aa99fc18": Phase="Pending", Reason="", readiness=false. Elapsed: 23.776446ms
    Mar  2 02:27:11.162: INFO: Pod "agnhost-host-aliases43f4ee8a-8671-40ac-8fbc-cf26aa99fc18": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03789032s
    Mar  2 02:27:13.163: INFO: Pod "agnhost-host-aliases43f4ee8a-8671-40ac-8fbc-cf26aa99fc18": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039005951s
    Mar  2 02:27:13.163: INFO: Pod "agnhost-host-aliases43f4ee8a-8671-40ac-8fbc-cf26aa99fc18" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Mar  2 02:27:13.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-278" for this suite. 03/02/23 02:27:13.221
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:27:13.249
Mar  2 02:27:13.249: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename resourcequota 03/02/23 02:27:13.25
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:27:13.308
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:27:13.322
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
STEP: Counting existing ResourceQuota 03/02/23 02:27:13.341
STEP: Creating a ResourceQuota 03/02/23 02:27:18.361
STEP: Ensuring resource quota status is calculated 03/02/23 02:27:18.388
STEP: Creating a ReplicaSet 03/02/23 02:27:20.403
STEP: Ensuring resource quota status captures replicaset creation 03/02/23 02:27:20.561
STEP: Deleting a ReplicaSet 03/02/23 02:27:22.58
STEP: Ensuring resource quota status released usage 03/02/23 02:27:22.604
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar  2 02:27:24.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2651" for this suite. 03/02/23 02:27:24.664
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","completed":251,"skipped":4684,"failed":0}
------------------------------
• [SLOW TEST] [11.445 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:27:13.249
    Mar  2 02:27:13.249: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename resourcequota 03/02/23 02:27:13.25
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:27:13.308
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:27:13.322
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:438
    STEP: Counting existing ResourceQuota 03/02/23 02:27:13.341
    STEP: Creating a ResourceQuota 03/02/23 02:27:18.361
    STEP: Ensuring resource quota status is calculated 03/02/23 02:27:18.388
    STEP: Creating a ReplicaSet 03/02/23 02:27:20.403
    STEP: Ensuring resource quota status captures replicaset creation 03/02/23 02:27:20.561
    STEP: Deleting a ReplicaSet 03/02/23 02:27:22.58
    STEP: Ensuring resource quota status released usage 03/02/23 02:27:22.604
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar  2 02:27:24.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-2651" for this suite. 03/02/23 02:27:24.664
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:27:24.718
Mar  2 02:27:24.718: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename endpointslice 03/02/23 02:27:24.72
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:27:24.786
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:27:24.8
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Mar  2 02:27:27.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-2310" for this suite. 03/02/23 02:27:27.118
{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","completed":252,"skipped":4734,"failed":0}
------------------------------
• [2.424 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:27:24.718
    Mar  2 02:27:24.718: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename endpointslice 03/02/23 02:27:24.72
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:27:24.786
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:27:24.8
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:101
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Mar  2 02:27:27.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-2310" for this suite. 03/02/23 02:27:27.118
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:27:27.144
Mar  2 02:27:27.144: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename secrets 03/02/23 02:27:27.146
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:27:27.238
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:27:27.263
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
STEP: Creating secret with name secret-test-068ed12c-5454-46c9-b4a6-8d2e9f2fde02 03/02/23 02:27:27.283
STEP: Creating a pod to test consume secrets 03/02/23 02:27:27.309
Mar  2 02:27:27.377: INFO: Waiting up to 5m0s for pod "pod-secrets-39ae1787-786b-492e-af8d-fcd6f6323274" in namespace "secrets-1814" to be "Succeeded or Failed"
Mar  2 02:27:27.392: INFO: Pod "pod-secrets-39ae1787-786b-492e-af8d-fcd6f6323274": Phase="Pending", Reason="", readiness=false. Elapsed: 14.390572ms
Mar  2 02:27:29.409: INFO: Pod "pod-secrets-39ae1787-786b-492e-af8d-fcd6f6323274": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03122253s
Mar  2 02:27:31.406: INFO: Pod "pod-secrets-39ae1787-786b-492e-af8d-fcd6f6323274": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028902359s
STEP: Saw pod success 03/02/23 02:27:31.406
Mar  2 02:27:31.407: INFO: Pod "pod-secrets-39ae1787-786b-492e-af8d-fcd6f6323274" satisfied condition "Succeeded or Failed"
Mar  2 02:27:31.419: INFO: Trying to get logs from node 10.132.92.143 pod pod-secrets-39ae1787-786b-492e-af8d-fcd6f6323274 container secret-volume-test: <nil>
STEP: delete the pod 03/02/23 02:27:31.467
Mar  2 02:27:31.510: INFO: Waiting for pod pod-secrets-39ae1787-786b-492e-af8d-fcd6f6323274 to disappear
Mar  2 02:27:31.535: INFO: Pod pod-secrets-39ae1787-786b-492e-af8d-fcd6f6323274 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar  2 02:27:31.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1814" for this suite. 03/02/23 02:27:31.558
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":253,"skipped":4743,"failed":0}
------------------------------
• [4.438 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:27:27.144
    Mar  2 02:27:27.144: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename secrets 03/02/23 02:27:27.146
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:27:27.238
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:27:27.263
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:67
    STEP: Creating secret with name secret-test-068ed12c-5454-46c9-b4a6-8d2e9f2fde02 03/02/23 02:27:27.283
    STEP: Creating a pod to test consume secrets 03/02/23 02:27:27.309
    Mar  2 02:27:27.377: INFO: Waiting up to 5m0s for pod "pod-secrets-39ae1787-786b-492e-af8d-fcd6f6323274" in namespace "secrets-1814" to be "Succeeded or Failed"
    Mar  2 02:27:27.392: INFO: Pod "pod-secrets-39ae1787-786b-492e-af8d-fcd6f6323274": Phase="Pending", Reason="", readiness=false. Elapsed: 14.390572ms
    Mar  2 02:27:29.409: INFO: Pod "pod-secrets-39ae1787-786b-492e-af8d-fcd6f6323274": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03122253s
    Mar  2 02:27:31.406: INFO: Pod "pod-secrets-39ae1787-786b-492e-af8d-fcd6f6323274": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028902359s
    STEP: Saw pod success 03/02/23 02:27:31.406
    Mar  2 02:27:31.407: INFO: Pod "pod-secrets-39ae1787-786b-492e-af8d-fcd6f6323274" satisfied condition "Succeeded or Failed"
    Mar  2 02:27:31.419: INFO: Trying to get logs from node 10.132.92.143 pod pod-secrets-39ae1787-786b-492e-af8d-fcd6f6323274 container secret-volume-test: <nil>
    STEP: delete the pod 03/02/23 02:27:31.467
    Mar  2 02:27:31.510: INFO: Waiting for pod pod-secrets-39ae1787-786b-492e-af8d-fcd6f6323274 to disappear
    Mar  2 02:27:31.535: INFO: Pod pod-secrets-39ae1787-786b-492e-af8d-fcd6f6323274 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar  2 02:27:31.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-1814" for this suite. 03/02/23 02:27:31.558
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:27:31.583
Mar  2 02:27:31.583: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename security-context 03/02/23 02:27:31.585
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:27:31.695
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:27:31.709
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 03/02/23 02:27:31.725
Mar  2 02:27:31.806: INFO: Waiting up to 5m0s for pod "security-context-66d39685-99c7-4160-8751-2e03ed5ce231" in namespace "security-context-9437" to be "Succeeded or Failed"
Mar  2 02:27:31.834: INFO: Pod "security-context-66d39685-99c7-4160-8751-2e03ed5ce231": Phase="Pending", Reason="", readiness=false. Elapsed: 28.586339ms
Mar  2 02:27:33.857: INFO: Pod "security-context-66d39685-99c7-4160-8751-2e03ed5ce231": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05141868s
Mar  2 02:27:35.851: INFO: Pod "security-context-66d39685-99c7-4160-8751-2e03ed5ce231": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04552427s
Mar  2 02:27:37.849: INFO: Pod "security-context-66d39685-99c7-4160-8751-2e03ed5ce231": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043292129s
STEP: Saw pod success 03/02/23 02:27:37.849
Mar  2 02:27:37.850: INFO: Pod "security-context-66d39685-99c7-4160-8751-2e03ed5ce231" satisfied condition "Succeeded or Failed"
Mar  2 02:27:37.864: INFO: Trying to get logs from node 10.132.92.143 pod security-context-66d39685-99c7-4160-8751-2e03ed5ce231 container test-container: <nil>
STEP: delete the pod 03/02/23 02:27:37.904
Mar  2 02:27:37.937: INFO: Waiting for pod security-context-66d39685-99c7-4160-8751-2e03ed5ce231 to disappear
Mar  2 02:27:37.950: INFO: Pod security-context-66d39685-99c7-4160-8751-2e03ed5ce231 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Mar  2 02:27:37.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-9437" for this suite. 03/02/23 02:27:37.975
{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":254,"skipped":4751,"failed":0}
------------------------------
• [SLOW TEST] [6.419 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:27:31.583
    Mar  2 02:27:31.583: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename security-context 03/02/23 02:27:31.585
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:27:31.695
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:27:31.709
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:97
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 03/02/23 02:27:31.725
    Mar  2 02:27:31.806: INFO: Waiting up to 5m0s for pod "security-context-66d39685-99c7-4160-8751-2e03ed5ce231" in namespace "security-context-9437" to be "Succeeded or Failed"
    Mar  2 02:27:31.834: INFO: Pod "security-context-66d39685-99c7-4160-8751-2e03ed5ce231": Phase="Pending", Reason="", readiness=false. Elapsed: 28.586339ms
    Mar  2 02:27:33.857: INFO: Pod "security-context-66d39685-99c7-4160-8751-2e03ed5ce231": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05141868s
    Mar  2 02:27:35.851: INFO: Pod "security-context-66d39685-99c7-4160-8751-2e03ed5ce231": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04552427s
    Mar  2 02:27:37.849: INFO: Pod "security-context-66d39685-99c7-4160-8751-2e03ed5ce231": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043292129s
    STEP: Saw pod success 03/02/23 02:27:37.849
    Mar  2 02:27:37.850: INFO: Pod "security-context-66d39685-99c7-4160-8751-2e03ed5ce231" satisfied condition "Succeeded or Failed"
    Mar  2 02:27:37.864: INFO: Trying to get logs from node 10.132.92.143 pod security-context-66d39685-99c7-4160-8751-2e03ed5ce231 container test-container: <nil>
    STEP: delete the pod 03/02/23 02:27:37.904
    Mar  2 02:27:37.937: INFO: Waiting for pod security-context-66d39685-99c7-4160-8751-2e03ed5ce231 to disappear
    Mar  2 02:27:37.950: INFO: Pod security-context-66d39685-99c7-4160-8751-2e03ed5ce231 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Mar  2 02:27:37.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-9437" for this suite. 03/02/23 02:27:37.975
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:27:38.002
Mar  2 02:27:38.002: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename resourcequota 03/02/23 02:27:38.003
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:27:38.071
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:27:38.083
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
STEP: Creating a ResourceQuota with terminating scope 03/02/23 02:27:38.096
STEP: Ensuring ResourceQuota status is calculated 03/02/23 02:27:38.115
STEP: Creating a ResourceQuota with not terminating scope 03/02/23 02:27:40.13
STEP: Ensuring ResourceQuota status is calculated 03/02/23 02:27:40.149
STEP: Creating a long running pod 03/02/23 02:27:42.163
STEP: Ensuring resource quota with not terminating scope captures the pod usage 03/02/23 02:27:42.226
STEP: Ensuring resource quota with terminating scope ignored the pod usage 03/02/23 02:27:44.239
STEP: Deleting the pod 03/02/23 02:27:46.255
STEP: Ensuring resource quota status released the pod usage 03/02/23 02:27:46.301
STEP: Creating a terminating pod 03/02/23 02:27:48.317
STEP: Ensuring resource quota with terminating scope captures the pod usage 03/02/23 02:27:48.374
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 03/02/23 02:27:50.388
STEP: Deleting the pod 03/02/23 02:27:52.406
STEP: Ensuring resource quota status released the pod usage 03/02/23 02:27:52.452
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar  2 02:27:54.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1380" for this suite. 03/02/23 02:27:54.499
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","completed":255,"skipped":4756,"failed":0}
------------------------------
• [SLOW TEST] [16.524 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:27:38.002
    Mar  2 02:27:38.002: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename resourcequota 03/02/23 02:27:38.003
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:27:38.071
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:27:38.083
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:680
    STEP: Creating a ResourceQuota with terminating scope 03/02/23 02:27:38.096
    STEP: Ensuring ResourceQuota status is calculated 03/02/23 02:27:38.115
    STEP: Creating a ResourceQuota with not terminating scope 03/02/23 02:27:40.13
    STEP: Ensuring ResourceQuota status is calculated 03/02/23 02:27:40.149
    STEP: Creating a long running pod 03/02/23 02:27:42.163
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 03/02/23 02:27:42.226
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 03/02/23 02:27:44.239
    STEP: Deleting the pod 03/02/23 02:27:46.255
    STEP: Ensuring resource quota status released the pod usage 03/02/23 02:27:46.301
    STEP: Creating a terminating pod 03/02/23 02:27:48.317
    STEP: Ensuring resource quota with terminating scope captures the pod usage 03/02/23 02:27:48.374
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 03/02/23 02:27:50.388
    STEP: Deleting the pod 03/02/23 02:27:52.406
    STEP: Ensuring resource quota status released the pod usage 03/02/23 02:27:52.452
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar  2 02:27:54.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-1380" for this suite. 03/02/23 02:27:54.499
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:27:54.527
Mar  2 02:27:54.527: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename projected 03/02/23 02:27:54.529
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:27:54.589
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:27:54.6
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
STEP: Creating secret with name projected-secret-test-9aa82e9c-fc38-4830-bece-47a032bd8bac 03/02/23 02:27:54.614
STEP: Creating a pod to test consume secrets 03/02/23 02:27:54.641
Mar  2 02:27:54.727: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-59cfc06d-c70f-48f1-ac9c-1d520cb03130" in namespace "projected-4431" to be "Succeeded or Failed"
Mar  2 02:27:54.746: INFO: Pod "pod-projected-secrets-59cfc06d-c70f-48f1-ac9c-1d520cb03130": Phase="Pending", Reason="", readiness=false. Elapsed: 18.590245ms
Mar  2 02:27:56.767: INFO: Pod "pod-projected-secrets-59cfc06d-c70f-48f1-ac9c-1d520cb03130": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039704651s
Mar  2 02:27:58.762: INFO: Pod "pod-projected-secrets-59cfc06d-c70f-48f1-ac9c-1d520cb03130": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034095756s
Mar  2 02:28:00.771: INFO: Pod "pod-projected-secrets-59cfc06d-c70f-48f1-ac9c-1d520cb03130": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043087473s
STEP: Saw pod success 03/02/23 02:28:00.771
Mar  2 02:28:00.771: INFO: Pod "pod-projected-secrets-59cfc06d-c70f-48f1-ac9c-1d520cb03130" satisfied condition "Succeeded or Failed"
Mar  2 02:28:00.790: INFO: Trying to get logs from node 10.132.92.143 pod pod-projected-secrets-59cfc06d-c70f-48f1-ac9c-1d520cb03130 container secret-volume-test: <nil>
STEP: delete the pod 03/02/23 02:28:00.869
Mar  2 02:28:00.922: INFO: Waiting for pod pod-projected-secrets-59cfc06d-c70f-48f1-ac9c-1d520cb03130 to disappear
Mar  2 02:28:00.940: INFO: Pod pod-projected-secrets-59cfc06d-c70f-48f1-ac9c-1d520cb03130 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Mar  2 02:28:00.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4431" for this suite. 03/02/23 02:28:00.961
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":256,"skipped":4761,"failed":0}
------------------------------
• [SLOW TEST] [6.460 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:27:54.527
    Mar  2 02:27:54.527: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename projected 03/02/23 02:27:54.529
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:27:54.589
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:27:54.6
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:118
    STEP: Creating secret with name projected-secret-test-9aa82e9c-fc38-4830-bece-47a032bd8bac 03/02/23 02:27:54.614
    STEP: Creating a pod to test consume secrets 03/02/23 02:27:54.641
    Mar  2 02:27:54.727: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-59cfc06d-c70f-48f1-ac9c-1d520cb03130" in namespace "projected-4431" to be "Succeeded or Failed"
    Mar  2 02:27:54.746: INFO: Pod "pod-projected-secrets-59cfc06d-c70f-48f1-ac9c-1d520cb03130": Phase="Pending", Reason="", readiness=false. Elapsed: 18.590245ms
    Mar  2 02:27:56.767: INFO: Pod "pod-projected-secrets-59cfc06d-c70f-48f1-ac9c-1d520cb03130": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039704651s
    Mar  2 02:27:58.762: INFO: Pod "pod-projected-secrets-59cfc06d-c70f-48f1-ac9c-1d520cb03130": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034095756s
    Mar  2 02:28:00.771: INFO: Pod "pod-projected-secrets-59cfc06d-c70f-48f1-ac9c-1d520cb03130": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043087473s
    STEP: Saw pod success 03/02/23 02:28:00.771
    Mar  2 02:28:00.771: INFO: Pod "pod-projected-secrets-59cfc06d-c70f-48f1-ac9c-1d520cb03130" satisfied condition "Succeeded or Failed"
    Mar  2 02:28:00.790: INFO: Trying to get logs from node 10.132.92.143 pod pod-projected-secrets-59cfc06d-c70f-48f1-ac9c-1d520cb03130 container secret-volume-test: <nil>
    STEP: delete the pod 03/02/23 02:28:00.869
    Mar  2 02:28:00.922: INFO: Waiting for pod pod-projected-secrets-59cfc06d-c70f-48f1-ac9c-1d520cb03130 to disappear
    Mar  2 02:28:00.940: INFO: Pod pod-projected-secrets-59cfc06d-c70f-48f1-ac9c-1d520cb03130 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Mar  2 02:28:00.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4431" for this suite. 03/02/23 02:28:00.961
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:28:00.989
Mar  2 02:28:00.990: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename sched-preemption 03/02/23 02:28:00.99
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:28:01.053
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:28:01.066
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Mar  2 02:28:01.186: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  2 02:29:01.668: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:29:01.865
Mar  2 02:29:01.865: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename sched-preemption-path 03/02/23 02:29:01.866
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:29:01.99
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:29:02.04
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:496
STEP: Finding an available node 03/02/23 02:29:02.058
STEP: Trying to launch a pod without a label to get a node which can launch it. 03/02/23 02:29:02.059
Mar  2 02:29:02.206: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-9906" to be "running"
Mar  2 02:29:02.266: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 60.477574ms
Mar  2 02:29:04.280: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.074210347s
Mar  2 02:29:06.282: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.07581697s
Mar  2 02:29:06.282: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 03/02/23 02:29:06.294
Mar  2 02:29:06.338: INFO: found a healthy node: 10.132.92.143
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
Mar  2 02:29:22.683: INFO: pods created so far: [1 1 1]
Mar  2 02:29:22.684: INFO: length of pods created so far: 3
Mar  2 02:29:26.755: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:187
Mar  2 02:29:33.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-9906" for this suite. 03/02/23 02:29:33.793
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:470
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Mar  2 02:29:33.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-408" for this suite. 03/02/23 02:29:33.968
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","completed":257,"skipped":4774,"failed":0}
------------------------------
• [SLOW TEST] [93.185 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:458
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:543

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:28:00.989
    Mar  2 02:28:00.990: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename sched-preemption 03/02/23 02:28:00.99
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:28:01.053
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:28:01.066
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Mar  2 02:28:01.186: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar  2 02:29:01.668: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:29:01.865
    Mar  2 02:29:01.865: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename sched-preemption-path 03/02/23 02:29:01.866
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:29:01.99
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:29:02.04
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:496
    STEP: Finding an available node 03/02/23 02:29:02.058
    STEP: Trying to launch a pod without a label to get a node which can launch it. 03/02/23 02:29:02.059
    Mar  2 02:29:02.206: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-9906" to be "running"
    Mar  2 02:29:02.266: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 60.477574ms
    Mar  2 02:29:04.280: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.074210347s
    Mar  2 02:29:06.282: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.07581697s
    Mar  2 02:29:06.282: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 03/02/23 02:29:06.294
    Mar  2 02:29:06.338: INFO: found a healthy node: 10.132.92.143
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:543
    Mar  2 02:29:22.683: INFO: pods created so far: [1 1 1]
    Mar  2 02:29:22.684: INFO: length of pods created so far: 3
    Mar  2 02:29:26.755: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:187
    Mar  2 02:29:33.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-9906" for this suite. 03/02/23 02:29:33.793
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:470
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Mar  2 02:29:33.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-408" for this suite. 03/02/23 02:29:33.968
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:29:34.181
Mar  2 02:29:34.181: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename kubectl 03/02/23 02:29:34.183
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:29:34.245
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:29:34.257
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
STEP: Starting the proxy 03/02/23 02:29:34.272
Mar  2 02:29:34.273: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-9539 proxy --unix-socket=/tmp/kubectl-proxy-unix2636360432/test'
STEP: retrieving proxy /api/ output 03/02/23 02:29:34.35
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  2 02:29:34.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9539" for this suite. 03/02/23 02:29:34.374
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","completed":258,"skipped":4782,"failed":0}
------------------------------
• [0.232 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:29:34.181
    Mar  2 02:29:34.181: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename kubectl 03/02/23 02:29:34.183
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:29:34.245
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:29:34.257
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1810
    STEP: Starting the proxy 03/02/23 02:29:34.272
    Mar  2 02:29:34.273: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-9539 proxy --unix-socket=/tmp/kubectl-proxy-unix2636360432/test'
    STEP: retrieving proxy /api/ output 03/02/23 02:29:34.35
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  2 02:29:34.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9539" for this suite. 03/02/23 02:29:34.374
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:29:34.413
Mar  2 02:29:34.413: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename pod-network-test 03/02/23 02:29:34.415
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:29:34.475
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:29:34.49
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-3162 03/02/23 02:29:34.503
STEP: creating a selector 03/02/23 02:29:34.503
STEP: Creating the service pods in kubernetes 03/02/23 02:29:34.503
Mar  2 02:29:34.504: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar  2 02:29:34.711: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3162" to be "running and ready"
Mar  2 02:29:34.726: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.861228ms
Mar  2 02:29:34.726: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 02:29:36.751: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.039945232s
Mar  2 02:29:36.751: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 02:29:38.743: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.032138445s
Mar  2 02:29:38.744: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 02:29:40.757: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.045624573s
Mar  2 02:29:40.757: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 02:29:42.744: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.032647759s
Mar  2 02:29:42.744: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 02:29:44.743: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.031726998s
Mar  2 02:29:44.743: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 02:29:46.742: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.030719348s
Mar  2 02:29:46.742: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 02:29:48.742: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.030256838s
Mar  2 02:29:48.742: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 02:29:50.746: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.034536288s
Mar  2 02:29:50.746: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 02:29:52.745: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.033895776s
Mar  2 02:29:52.745: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 02:29:54.766: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.055064245s
Mar  2 02:29:54.782: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar  2 02:29:56.750: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.038619136s
Mar  2 02:29:56.750: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Mar  2 02:29:56.750: INFO: Pod "netserver-0" satisfied condition "running and ready"
Mar  2 02:29:56.763: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3162" to be "running and ready"
Mar  2 02:29:56.777: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 13.218259ms
Mar  2 02:29:56.777: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Mar  2 02:29:56.777: INFO: Pod "netserver-1" satisfied condition "running and ready"
Mar  2 02:29:56.791: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-3162" to be "running and ready"
Mar  2 02:29:56.805: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 13.733068ms
Mar  2 02:29:56.805: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Mar  2 02:29:56.805: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 03/02/23 02:29:56.84
Mar  2 02:29:56.900: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3162" to be "running"
Mar  2 02:29:56.915: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.82267ms
Mar  2 02:29:58.930: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.029643373s
Mar  2 02:29:58.930: INFO: Pod "test-container-pod" satisfied condition "running"
Mar  2 02:29:58.943: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar  2 02:29:58.943: INFO: Breadth first check of 172.30.156.110 on host 10.132.92.143...
Mar  2 02:29:58.956: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.156.111:9080/dial?request=hostname&protocol=http&host=172.30.156.110&port=8083&tries=1'] Namespace:pod-network-test-3162 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 02:29:58.956: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
Mar  2 02:29:58.957: INFO: ExecWithOptions: Clientset creation
Mar  2 02:29:58.957: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-3162/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.156.111%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.156.110%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar  2 02:29:59.173: INFO: Waiting for responses: map[]
Mar  2 02:29:59.173: INFO: reached 172.30.156.110 after 0/1 tries
Mar  2 02:29:59.173: INFO: Breadth first check of 172.30.62.255 on host 10.132.92.186...
Mar  2 02:29:59.187: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.156.111:9080/dial?request=hostname&protocol=http&host=172.30.62.255&port=8083&tries=1'] Namespace:pod-network-test-3162 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 02:29:59.187: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
Mar  2 02:29:59.188: INFO: ExecWithOptions: Clientset creation
Mar  2 02:29:59.188: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-3162/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.156.111%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.62.255%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar  2 02:29:59.418: INFO: Waiting for responses: map[]
Mar  2 02:29:59.418: INFO: reached 172.30.62.255 after 0/1 tries
Mar  2 02:29:59.418: INFO: Breadth first check of 172.30.201.242 on host 10.132.92.188...
Mar  2 02:29:59.432: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.156.111:9080/dial?request=hostname&protocol=http&host=172.30.201.242&port=8083&tries=1'] Namespace:pod-network-test-3162 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 02:29:59.432: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
Mar  2 02:29:59.432: INFO: ExecWithOptions: Clientset creation
Mar  2 02:29:59.432: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-3162/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.156.111%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.201.242%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar  2 02:29:59.618: INFO: Waiting for responses: map[]
Mar  2 02:29:59.618: INFO: reached 172.30.201.242 after 0/1 tries
Mar  2 02:29:59.618: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Mar  2 02:29:59.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3162" for this suite. 03/02/23 02:29:59.64
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","completed":259,"skipped":4786,"failed":0}
------------------------------
• [SLOW TEST] [25.250 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:29:34.413
    Mar  2 02:29:34.413: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename pod-network-test 03/02/23 02:29:34.415
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:29:34.475
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:29:34.49
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-3162 03/02/23 02:29:34.503
    STEP: creating a selector 03/02/23 02:29:34.503
    STEP: Creating the service pods in kubernetes 03/02/23 02:29:34.503
    Mar  2 02:29:34.504: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Mar  2 02:29:34.711: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3162" to be "running and ready"
    Mar  2 02:29:34.726: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.861228ms
    Mar  2 02:29:34.726: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 02:29:36.751: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.039945232s
    Mar  2 02:29:36.751: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 02:29:38.743: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.032138445s
    Mar  2 02:29:38.744: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 02:29:40.757: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.045624573s
    Mar  2 02:29:40.757: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 02:29:42.744: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.032647759s
    Mar  2 02:29:42.744: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 02:29:44.743: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.031726998s
    Mar  2 02:29:44.743: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 02:29:46.742: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.030719348s
    Mar  2 02:29:46.742: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 02:29:48.742: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.030256838s
    Mar  2 02:29:48.742: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 02:29:50.746: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.034536288s
    Mar  2 02:29:50.746: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 02:29:52.745: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.033895776s
    Mar  2 02:29:52.745: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 02:29:54.766: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.055064245s
    Mar  2 02:29:54.782: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar  2 02:29:56.750: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.038619136s
    Mar  2 02:29:56.750: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Mar  2 02:29:56.750: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Mar  2 02:29:56.763: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3162" to be "running and ready"
    Mar  2 02:29:56.777: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 13.218259ms
    Mar  2 02:29:56.777: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Mar  2 02:29:56.777: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Mar  2 02:29:56.791: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-3162" to be "running and ready"
    Mar  2 02:29:56.805: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 13.733068ms
    Mar  2 02:29:56.805: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Mar  2 02:29:56.805: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 03/02/23 02:29:56.84
    Mar  2 02:29:56.900: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3162" to be "running"
    Mar  2 02:29:56.915: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.82267ms
    Mar  2 02:29:58.930: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.029643373s
    Mar  2 02:29:58.930: INFO: Pod "test-container-pod" satisfied condition "running"
    Mar  2 02:29:58.943: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Mar  2 02:29:58.943: INFO: Breadth first check of 172.30.156.110 on host 10.132.92.143...
    Mar  2 02:29:58.956: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.156.111:9080/dial?request=hostname&protocol=http&host=172.30.156.110&port=8083&tries=1'] Namespace:pod-network-test-3162 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 02:29:58.956: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    Mar  2 02:29:58.957: INFO: ExecWithOptions: Clientset creation
    Mar  2 02:29:58.957: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-3162/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.156.111%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.156.110%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar  2 02:29:59.173: INFO: Waiting for responses: map[]
    Mar  2 02:29:59.173: INFO: reached 172.30.156.110 after 0/1 tries
    Mar  2 02:29:59.173: INFO: Breadth first check of 172.30.62.255 on host 10.132.92.186...
    Mar  2 02:29:59.187: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.156.111:9080/dial?request=hostname&protocol=http&host=172.30.62.255&port=8083&tries=1'] Namespace:pod-network-test-3162 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 02:29:59.187: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    Mar  2 02:29:59.188: INFO: ExecWithOptions: Clientset creation
    Mar  2 02:29:59.188: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-3162/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.156.111%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.62.255%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar  2 02:29:59.418: INFO: Waiting for responses: map[]
    Mar  2 02:29:59.418: INFO: reached 172.30.62.255 after 0/1 tries
    Mar  2 02:29:59.418: INFO: Breadth first check of 172.30.201.242 on host 10.132.92.188...
    Mar  2 02:29:59.432: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.156.111:9080/dial?request=hostname&protocol=http&host=172.30.201.242&port=8083&tries=1'] Namespace:pod-network-test-3162 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 02:29:59.432: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    Mar  2 02:29:59.432: INFO: ExecWithOptions: Clientset creation
    Mar  2 02:29:59.432: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-3162/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.156.111%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.201.242%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar  2 02:29:59.618: INFO: Waiting for responses: map[]
    Mar  2 02:29:59.618: INFO: reached 172.30.201.242 after 0/1 tries
    Mar  2 02:29:59.618: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Mar  2 02:29:59.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-3162" for this suite. 03/02/23 02:29:59.64
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:29:59.664
Mar  2 02:29:59.664: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename dns 03/02/23 02:29:59.665
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:29:59.724
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:29:59.736
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5027.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-5027.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 03/02/23 02:29:59.75
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5027.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-5027.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 03/02/23 02:29:59.751
STEP: creating a pod to probe /etc/hosts 03/02/23 02:29:59.751
STEP: submitting the pod to kubernetes 03/02/23 02:29:59.751
Mar  2 02:29:59.819: INFO: Waiting up to 15m0s for pod "dns-test-6d81db79-5ec3-46bb-9190-caefd7713a7d" in namespace "dns-5027" to be "running"
Mar  2 02:29:59.839: INFO: Pod "dns-test-6d81db79-5ec3-46bb-9190-caefd7713a7d": Phase="Pending", Reason="", readiness=false. Elapsed: 20.204388ms
Mar  2 02:30:01.865: INFO: Pod "dns-test-6d81db79-5ec3-46bb-9190-caefd7713a7d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045841136s
Mar  2 02:30:03.854: INFO: Pod "dns-test-6d81db79-5ec3-46bb-9190-caefd7713a7d": Phase="Running", Reason="", readiness=true. Elapsed: 4.034799391s
Mar  2 02:30:03.854: INFO: Pod "dns-test-6d81db79-5ec3-46bb-9190-caefd7713a7d" satisfied condition "running"
STEP: retrieving the pod 03/02/23 02:30:03.854
STEP: looking for the results for each expected name from probers 03/02/23 02:30:03.875
Mar  2 02:30:04.028: INFO: DNS probes using dns-5027/dns-test-6d81db79-5ec3-46bb-9190-caefd7713a7d succeeded

STEP: deleting the pod 03/02/23 02:30:04.028
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar  2 02:30:04.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5027" for this suite. 03/02/23 02:30:04.094
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]","completed":260,"skipped":4794,"failed":0}
------------------------------
• [4.490 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:29:59.664
    Mar  2 02:29:59.664: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename dns 03/02/23 02:29:59.665
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:29:59.724
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:29:59.736
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5027.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-5027.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     03/02/23 02:29:59.75
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5027.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-5027.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     03/02/23 02:29:59.751
    STEP: creating a pod to probe /etc/hosts 03/02/23 02:29:59.751
    STEP: submitting the pod to kubernetes 03/02/23 02:29:59.751
    Mar  2 02:29:59.819: INFO: Waiting up to 15m0s for pod "dns-test-6d81db79-5ec3-46bb-9190-caefd7713a7d" in namespace "dns-5027" to be "running"
    Mar  2 02:29:59.839: INFO: Pod "dns-test-6d81db79-5ec3-46bb-9190-caefd7713a7d": Phase="Pending", Reason="", readiness=false. Elapsed: 20.204388ms
    Mar  2 02:30:01.865: INFO: Pod "dns-test-6d81db79-5ec3-46bb-9190-caefd7713a7d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045841136s
    Mar  2 02:30:03.854: INFO: Pod "dns-test-6d81db79-5ec3-46bb-9190-caefd7713a7d": Phase="Running", Reason="", readiness=true. Elapsed: 4.034799391s
    Mar  2 02:30:03.854: INFO: Pod "dns-test-6d81db79-5ec3-46bb-9190-caefd7713a7d" satisfied condition "running"
    STEP: retrieving the pod 03/02/23 02:30:03.854
    STEP: looking for the results for each expected name from probers 03/02/23 02:30:03.875
    Mar  2 02:30:04.028: INFO: DNS probes using dns-5027/dns-test-6d81db79-5ec3-46bb-9190-caefd7713a7d succeeded

    STEP: deleting the pod 03/02/23 02:30:04.028
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar  2 02:30:04.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-5027" for this suite. 03/02/23 02:30:04.094
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:30:04.159
Mar  2 02:30:04.160: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename services 03/02/23 02:30:04.161
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:30:04.244
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:30:04.272
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
STEP: creating service in namespace services-613 03/02/23 02:30:04.318
STEP: creating service affinity-nodeport-transition in namespace services-613 03/02/23 02:30:04.319
STEP: creating replication controller affinity-nodeport-transition in namespace services-613 03/02/23 02:30:04.389
I0302 02:30:04.416683      20 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-613, replica count: 3
I0302 02:30:07.468194      20 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 02:30:10.469081      20 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 02:30:10.580: INFO: Creating new exec pod
Mar  2 02:30:10.692: INFO: Waiting up to 5m0s for pod "execpod-affinityvvghw" in namespace "services-613" to be "running"
Mar  2 02:30:10.724: INFO: Pod "execpod-affinityvvghw": Phase="Pending", Reason="", readiness=false. Elapsed: 31.991909ms
Mar  2 02:30:12.743: INFO: Pod "execpod-affinityvvghw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051505252s
Mar  2 02:30:14.740: INFO: Pod "execpod-affinityvvghw": Phase="Running", Reason="", readiness=true. Elapsed: 4.048269829s
Mar  2 02:30:14.740: INFO: Pod "execpod-affinityvvghw" satisfied condition "running"
Mar  2 02:30:15.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-613 exec execpod-affinityvvghw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Mar  2 02:30:16.139: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Mar  2 02:30:16.140: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 02:30:16.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-613 exec execpod-affinityvvghw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.187.0 80'
Mar  2 02:30:16.518: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.187.0 80\nConnection to 172.21.187.0 80 port [tcp/http] succeeded!\n"
Mar  2 02:30:16.518: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 02:30:16.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-613 exec execpod-affinityvvghw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.132.92.143 32222'
Mar  2 02:30:16.860: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.132.92.143 32222\nConnection to 10.132.92.143 32222 port [tcp/*] succeeded!\n"
Mar  2 02:30:16.860: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 02:30:16.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-613 exec execpod-affinityvvghw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.132.92.186 32222'
Mar  2 02:30:17.248: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.132.92.186 32222\nConnection to 10.132.92.186 32222 port [tcp/*] succeeded!\n"
Mar  2 02:30:17.249: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 02:30:17.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-613 exec execpod-affinityvvghw -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.132.92.143:32222/ ; done'
Mar  2 02:30:17.780: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n"
Mar  2 02:30:17.780: INFO: stdout: "\naffinity-nodeport-transition-9hvsh\naffinity-nodeport-transition-ggwf9\naffinity-nodeport-transition-ggwf9\naffinity-nodeport-transition-ggwf9\naffinity-nodeport-transition-8cwjq\naffinity-nodeport-transition-ggwf9\naffinity-nodeport-transition-8cwjq\naffinity-nodeport-transition-ggwf9\naffinity-nodeport-transition-9hvsh\naffinity-nodeport-transition-ggwf9\naffinity-nodeport-transition-ggwf9\naffinity-nodeport-transition-ggwf9\naffinity-nodeport-transition-8cwjq\naffinity-nodeport-transition-ggwf9\naffinity-nodeport-transition-8cwjq\naffinity-nodeport-transition-8cwjq"
Mar  2 02:30:17.780: INFO: Received response from host: affinity-nodeport-transition-9hvsh
Mar  2 02:30:17.780: INFO: Received response from host: affinity-nodeport-transition-ggwf9
Mar  2 02:30:17.780: INFO: Received response from host: affinity-nodeport-transition-ggwf9
Mar  2 02:30:17.780: INFO: Received response from host: affinity-nodeport-transition-ggwf9
Mar  2 02:30:17.780: INFO: Received response from host: affinity-nodeport-transition-8cwjq
Mar  2 02:30:17.780: INFO: Received response from host: affinity-nodeport-transition-ggwf9
Mar  2 02:30:17.780: INFO: Received response from host: affinity-nodeport-transition-8cwjq
Mar  2 02:30:17.780: INFO: Received response from host: affinity-nodeport-transition-ggwf9
Mar  2 02:30:17.780: INFO: Received response from host: affinity-nodeport-transition-9hvsh
Mar  2 02:30:17.780: INFO: Received response from host: affinity-nodeport-transition-ggwf9
Mar  2 02:30:17.780: INFO: Received response from host: affinity-nodeport-transition-ggwf9
Mar  2 02:30:17.780: INFO: Received response from host: affinity-nodeport-transition-ggwf9
Mar  2 02:30:17.780: INFO: Received response from host: affinity-nodeport-transition-8cwjq
Mar  2 02:30:17.780: INFO: Received response from host: affinity-nodeport-transition-ggwf9
Mar  2 02:30:17.780: INFO: Received response from host: affinity-nodeport-transition-8cwjq
Mar  2 02:30:17.780: INFO: Received response from host: affinity-nodeport-transition-8cwjq
Mar  2 02:30:17.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-613 exec execpod-affinityvvghw -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.132.92.143:32222/ ; done'
Mar  2 02:30:18.218: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n"
Mar  2 02:30:18.218: INFO: stdout: "\naffinity-nodeport-transition-9hvsh\naffinity-nodeport-transition-9hvsh\naffinity-nodeport-transition-9hvsh\naffinity-nodeport-transition-9hvsh\naffinity-nodeport-transition-9hvsh\naffinity-nodeport-transition-9hvsh\naffinity-nodeport-transition-9hvsh\naffinity-nodeport-transition-9hvsh\naffinity-nodeport-transition-9hvsh\naffinity-nodeport-transition-9hvsh\naffinity-nodeport-transition-9hvsh\naffinity-nodeport-transition-9hvsh\naffinity-nodeport-transition-9hvsh\naffinity-nodeport-transition-9hvsh\naffinity-nodeport-transition-9hvsh\naffinity-nodeport-transition-9hvsh"
Mar  2 02:30:18.218: INFO: Received response from host: affinity-nodeport-transition-9hvsh
Mar  2 02:30:18.218: INFO: Received response from host: affinity-nodeport-transition-9hvsh
Mar  2 02:30:18.218: INFO: Received response from host: affinity-nodeport-transition-9hvsh
Mar  2 02:30:18.218: INFO: Received response from host: affinity-nodeport-transition-9hvsh
Mar  2 02:30:18.218: INFO: Received response from host: affinity-nodeport-transition-9hvsh
Mar  2 02:30:18.218: INFO: Received response from host: affinity-nodeport-transition-9hvsh
Mar  2 02:30:18.218: INFO: Received response from host: affinity-nodeport-transition-9hvsh
Mar  2 02:30:18.218: INFO: Received response from host: affinity-nodeport-transition-9hvsh
Mar  2 02:30:18.218: INFO: Received response from host: affinity-nodeport-transition-9hvsh
Mar  2 02:30:18.218: INFO: Received response from host: affinity-nodeport-transition-9hvsh
Mar  2 02:30:18.218: INFO: Received response from host: affinity-nodeport-transition-9hvsh
Mar  2 02:30:18.218: INFO: Received response from host: affinity-nodeport-transition-9hvsh
Mar  2 02:30:18.218: INFO: Received response from host: affinity-nodeport-transition-9hvsh
Mar  2 02:30:18.218: INFO: Received response from host: affinity-nodeport-transition-9hvsh
Mar  2 02:30:18.218: INFO: Received response from host: affinity-nodeport-transition-9hvsh
Mar  2 02:30:18.218: INFO: Received response from host: affinity-nodeport-transition-9hvsh
Mar  2 02:30:18.218: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-613, will wait for the garbage collector to delete the pods 03/02/23 02:30:18.274
Mar  2 02:30:18.365: INFO: Deleting ReplicationController affinity-nodeport-transition took: 24.497699ms
Mar  2 02:30:18.466: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.771827ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  2 02:30:21.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-613" for this suite. 03/02/23 02:30:21.572
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","completed":261,"skipped":4813,"failed":0}
------------------------------
• [SLOW TEST] [17.443 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:30:04.159
    Mar  2 02:30:04.160: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename services 03/02/23 02:30:04.161
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:30:04.244
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:30:04.272
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2237
    STEP: creating service in namespace services-613 03/02/23 02:30:04.318
    STEP: creating service affinity-nodeport-transition in namespace services-613 03/02/23 02:30:04.319
    STEP: creating replication controller affinity-nodeport-transition in namespace services-613 03/02/23 02:30:04.389
    I0302 02:30:04.416683      20 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-613, replica count: 3
    I0302 02:30:07.468194      20 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0302 02:30:10.469081      20 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar  2 02:30:10.580: INFO: Creating new exec pod
    Mar  2 02:30:10.692: INFO: Waiting up to 5m0s for pod "execpod-affinityvvghw" in namespace "services-613" to be "running"
    Mar  2 02:30:10.724: INFO: Pod "execpod-affinityvvghw": Phase="Pending", Reason="", readiness=false. Elapsed: 31.991909ms
    Mar  2 02:30:12.743: INFO: Pod "execpod-affinityvvghw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051505252s
    Mar  2 02:30:14.740: INFO: Pod "execpod-affinityvvghw": Phase="Running", Reason="", readiness=true. Elapsed: 4.048269829s
    Mar  2 02:30:14.740: INFO: Pod "execpod-affinityvvghw" satisfied condition "running"
    Mar  2 02:30:15.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-613 exec execpod-affinityvvghw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
    Mar  2 02:30:16.139: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Mar  2 02:30:16.140: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  2 02:30:16.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-613 exec execpod-affinityvvghw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.187.0 80'
    Mar  2 02:30:16.518: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.187.0 80\nConnection to 172.21.187.0 80 port [tcp/http] succeeded!\n"
    Mar  2 02:30:16.518: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  2 02:30:16.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-613 exec execpod-affinityvvghw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.132.92.143 32222'
    Mar  2 02:30:16.860: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.132.92.143 32222\nConnection to 10.132.92.143 32222 port [tcp/*] succeeded!\n"
    Mar  2 02:30:16.860: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  2 02:30:16.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-613 exec execpod-affinityvvghw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.132.92.186 32222'
    Mar  2 02:30:17.248: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.132.92.186 32222\nConnection to 10.132.92.186 32222 port [tcp/*] succeeded!\n"
    Mar  2 02:30:17.249: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar  2 02:30:17.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-613 exec execpod-affinityvvghw -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.132.92.143:32222/ ; done'
    Mar  2 02:30:17.780: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n"
    Mar  2 02:30:17.780: INFO: stdout: "\naffinity-nodeport-transition-9hvsh\naffinity-nodeport-transition-ggwf9\naffinity-nodeport-transition-ggwf9\naffinity-nodeport-transition-ggwf9\naffinity-nodeport-transition-8cwjq\naffinity-nodeport-transition-ggwf9\naffinity-nodeport-transition-8cwjq\naffinity-nodeport-transition-ggwf9\naffinity-nodeport-transition-9hvsh\naffinity-nodeport-transition-ggwf9\naffinity-nodeport-transition-ggwf9\naffinity-nodeport-transition-ggwf9\naffinity-nodeport-transition-8cwjq\naffinity-nodeport-transition-ggwf9\naffinity-nodeport-transition-8cwjq\naffinity-nodeport-transition-8cwjq"
    Mar  2 02:30:17.780: INFO: Received response from host: affinity-nodeport-transition-9hvsh
    Mar  2 02:30:17.780: INFO: Received response from host: affinity-nodeport-transition-ggwf9
    Mar  2 02:30:17.780: INFO: Received response from host: affinity-nodeport-transition-ggwf9
    Mar  2 02:30:17.780: INFO: Received response from host: affinity-nodeport-transition-ggwf9
    Mar  2 02:30:17.780: INFO: Received response from host: affinity-nodeport-transition-8cwjq
    Mar  2 02:30:17.780: INFO: Received response from host: affinity-nodeport-transition-ggwf9
    Mar  2 02:30:17.780: INFO: Received response from host: affinity-nodeport-transition-8cwjq
    Mar  2 02:30:17.780: INFO: Received response from host: affinity-nodeport-transition-ggwf9
    Mar  2 02:30:17.780: INFO: Received response from host: affinity-nodeport-transition-9hvsh
    Mar  2 02:30:17.780: INFO: Received response from host: affinity-nodeport-transition-ggwf9
    Mar  2 02:30:17.780: INFO: Received response from host: affinity-nodeport-transition-ggwf9
    Mar  2 02:30:17.780: INFO: Received response from host: affinity-nodeport-transition-ggwf9
    Mar  2 02:30:17.780: INFO: Received response from host: affinity-nodeport-transition-8cwjq
    Mar  2 02:30:17.780: INFO: Received response from host: affinity-nodeport-transition-ggwf9
    Mar  2 02:30:17.780: INFO: Received response from host: affinity-nodeport-transition-8cwjq
    Mar  2 02:30:17.780: INFO: Received response from host: affinity-nodeport-transition-8cwjq
    Mar  2 02:30:17.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-613 exec execpod-affinityvvghw -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.132.92.143:32222/ ; done'
    Mar  2 02:30:18.218: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.132.92.143:32222/\n"
    Mar  2 02:30:18.218: INFO: stdout: "\naffinity-nodeport-transition-9hvsh\naffinity-nodeport-transition-9hvsh\naffinity-nodeport-transition-9hvsh\naffinity-nodeport-transition-9hvsh\naffinity-nodeport-transition-9hvsh\naffinity-nodeport-transition-9hvsh\naffinity-nodeport-transition-9hvsh\naffinity-nodeport-transition-9hvsh\naffinity-nodeport-transition-9hvsh\naffinity-nodeport-transition-9hvsh\naffinity-nodeport-transition-9hvsh\naffinity-nodeport-transition-9hvsh\naffinity-nodeport-transition-9hvsh\naffinity-nodeport-transition-9hvsh\naffinity-nodeport-transition-9hvsh\naffinity-nodeport-transition-9hvsh"
    Mar  2 02:30:18.218: INFO: Received response from host: affinity-nodeport-transition-9hvsh
    Mar  2 02:30:18.218: INFO: Received response from host: affinity-nodeport-transition-9hvsh
    Mar  2 02:30:18.218: INFO: Received response from host: affinity-nodeport-transition-9hvsh
    Mar  2 02:30:18.218: INFO: Received response from host: affinity-nodeport-transition-9hvsh
    Mar  2 02:30:18.218: INFO: Received response from host: affinity-nodeport-transition-9hvsh
    Mar  2 02:30:18.218: INFO: Received response from host: affinity-nodeport-transition-9hvsh
    Mar  2 02:30:18.218: INFO: Received response from host: affinity-nodeport-transition-9hvsh
    Mar  2 02:30:18.218: INFO: Received response from host: affinity-nodeport-transition-9hvsh
    Mar  2 02:30:18.218: INFO: Received response from host: affinity-nodeport-transition-9hvsh
    Mar  2 02:30:18.218: INFO: Received response from host: affinity-nodeport-transition-9hvsh
    Mar  2 02:30:18.218: INFO: Received response from host: affinity-nodeport-transition-9hvsh
    Mar  2 02:30:18.218: INFO: Received response from host: affinity-nodeport-transition-9hvsh
    Mar  2 02:30:18.218: INFO: Received response from host: affinity-nodeport-transition-9hvsh
    Mar  2 02:30:18.218: INFO: Received response from host: affinity-nodeport-transition-9hvsh
    Mar  2 02:30:18.218: INFO: Received response from host: affinity-nodeport-transition-9hvsh
    Mar  2 02:30:18.218: INFO: Received response from host: affinity-nodeport-transition-9hvsh
    Mar  2 02:30:18.218: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-613, will wait for the garbage collector to delete the pods 03/02/23 02:30:18.274
    Mar  2 02:30:18.365: INFO: Deleting ReplicationController affinity-nodeport-transition took: 24.497699ms
    Mar  2 02:30:18.466: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.771827ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  2 02:30:21.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-613" for this suite. 03/02/23 02:30:21.572
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:30:21.604
Mar  2 02:30:21.605: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename job 03/02/23 02:30:21.606
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:30:21.676
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:30:21.69
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
STEP: Creating a suspended job 03/02/23 02:30:21.728
W0302 02:30:21.756561      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Patching the Job 03/02/23 02:30:21.756
STEP: Watching for Job to be patched 03/02/23 02:30:21.839
Mar  2 02:30:21.847: INFO: Event ADDED observed for Job e2e-9gnbp in namespace job-9197 with labels: map[e2e-job-label:e2e-9gnbp] and annotations: map[batch.kubernetes.io/job-tracking:]
Mar  2 02:30:21.847: INFO: Event MODIFIED observed for Job e2e-9gnbp in namespace job-9197 with labels: map[e2e-job-label:e2e-9gnbp] and annotations: map[batch.kubernetes.io/job-tracking:]
Mar  2 02:30:21.847: INFO: Event MODIFIED found for Job e2e-9gnbp in namespace job-9197 with labels: map[e2e-9gnbp:patched e2e-job-label:e2e-9gnbp] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 03/02/23 02:30:21.847
STEP: Watching for Job to be updated 03/02/23 02:30:21.948
Mar  2 02:30:21.954: INFO: Event MODIFIED found for Job e2e-9gnbp in namespace job-9197 with labels: map[e2e-9gnbp:patched e2e-job-label:e2e-9gnbp] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar  2 02:30:21.954: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 03/02/23 02:30:21.954
Mar  2 02:30:21.971: INFO: Job: e2e-9gnbp as labels: map[e2e-9gnbp:patched e2e-job-label:e2e-9gnbp]
STEP: Waiting for job to complete 03/02/23 02:30:21.971
STEP: Delete a job collection with a labelselector 03/02/23 02:30:31.991
STEP: Watching for Job to be deleted 03/02/23 02:30:32.044
Mar  2 02:30:32.050: INFO: Event MODIFIED observed for Job e2e-9gnbp in namespace job-9197 with labels: map[e2e-9gnbp:patched e2e-job-label:e2e-9gnbp] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar  2 02:30:32.051: INFO: Event MODIFIED observed for Job e2e-9gnbp in namespace job-9197 with labels: map[e2e-9gnbp:patched e2e-job-label:e2e-9gnbp] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar  2 02:30:32.051: INFO: Event MODIFIED observed for Job e2e-9gnbp in namespace job-9197 with labels: map[e2e-9gnbp:patched e2e-job-label:e2e-9gnbp] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar  2 02:30:32.051: INFO: Event MODIFIED observed for Job e2e-9gnbp in namespace job-9197 with labels: map[e2e-9gnbp:patched e2e-job-label:e2e-9gnbp] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar  2 02:30:32.051: INFO: Event MODIFIED observed for Job e2e-9gnbp in namespace job-9197 with labels: map[e2e-9gnbp:patched e2e-job-label:e2e-9gnbp] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar  2 02:30:32.051: INFO: Event MODIFIED observed for Job e2e-9gnbp in namespace job-9197 with labels: map[e2e-9gnbp:patched e2e-job-label:e2e-9gnbp] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar  2 02:30:32.053: INFO: Event MODIFIED observed for Job e2e-9gnbp in namespace job-9197 with labels: map[e2e-9gnbp:patched e2e-job-label:e2e-9gnbp] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar  2 02:30:32.053: INFO: Event DELETED found for Job e2e-9gnbp in namespace job-9197 with labels: map[e2e-9gnbp:patched e2e-job-label:e2e-9gnbp] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 03/02/23 02:30:32.053
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Mar  2 02:30:32.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9197" for this suite. 03/02/23 02:30:32.113
{"msg":"PASSED [sig-apps] Job should manage the lifecycle of a job [Conformance]","completed":262,"skipped":4833,"failed":0}
------------------------------
• [SLOW TEST] [10.538 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:30:21.604
    Mar  2 02:30:21.605: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename job 03/02/23 02:30:21.606
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:30:21.676
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:30:21.69
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:531
    STEP: Creating a suspended job 03/02/23 02:30:21.728
    W0302 02:30:21.756561      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Patching the Job 03/02/23 02:30:21.756
    STEP: Watching for Job to be patched 03/02/23 02:30:21.839
    Mar  2 02:30:21.847: INFO: Event ADDED observed for Job e2e-9gnbp in namespace job-9197 with labels: map[e2e-job-label:e2e-9gnbp] and annotations: map[batch.kubernetes.io/job-tracking:]
    Mar  2 02:30:21.847: INFO: Event MODIFIED observed for Job e2e-9gnbp in namespace job-9197 with labels: map[e2e-job-label:e2e-9gnbp] and annotations: map[batch.kubernetes.io/job-tracking:]
    Mar  2 02:30:21.847: INFO: Event MODIFIED found for Job e2e-9gnbp in namespace job-9197 with labels: map[e2e-9gnbp:patched e2e-job-label:e2e-9gnbp] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 03/02/23 02:30:21.847
    STEP: Watching for Job to be updated 03/02/23 02:30:21.948
    Mar  2 02:30:21.954: INFO: Event MODIFIED found for Job e2e-9gnbp in namespace job-9197 with labels: map[e2e-9gnbp:patched e2e-job-label:e2e-9gnbp] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar  2 02:30:21.954: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 03/02/23 02:30:21.954
    Mar  2 02:30:21.971: INFO: Job: e2e-9gnbp as labels: map[e2e-9gnbp:patched e2e-job-label:e2e-9gnbp]
    STEP: Waiting for job to complete 03/02/23 02:30:21.971
    STEP: Delete a job collection with a labelselector 03/02/23 02:30:31.991
    STEP: Watching for Job to be deleted 03/02/23 02:30:32.044
    Mar  2 02:30:32.050: INFO: Event MODIFIED observed for Job e2e-9gnbp in namespace job-9197 with labels: map[e2e-9gnbp:patched e2e-job-label:e2e-9gnbp] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar  2 02:30:32.051: INFO: Event MODIFIED observed for Job e2e-9gnbp in namespace job-9197 with labels: map[e2e-9gnbp:patched e2e-job-label:e2e-9gnbp] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar  2 02:30:32.051: INFO: Event MODIFIED observed for Job e2e-9gnbp in namespace job-9197 with labels: map[e2e-9gnbp:patched e2e-job-label:e2e-9gnbp] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar  2 02:30:32.051: INFO: Event MODIFIED observed for Job e2e-9gnbp in namespace job-9197 with labels: map[e2e-9gnbp:patched e2e-job-label:e2e-9gnbp] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar  2 02:30:32.051: INFO: Event MODIFIED observed for Job e2e-9gnbp in namespace job-9197 with labels: map[e2e-9gnbp:patched e2e-job-label:e2e-9gnbp] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar  2 02:30:32.051: INFO: Event MODIFIED observed for Job e2e-9gnbp in namespace job-9197 with labels: map[e2e-9gnbp:patched e2e-job-label:e2e-9gnbp] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar  2 02:30:32.053: INFO: Event MODIFIED observed for Job e2e-9gnbp in namespace job-9197 with labels: map[e2e-9gnbp:patched e2e-job-label:e2e-9gnbp] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar  2 02:30:32.053: INFO: Event DELETED found for Job e2e-9gnbp in namespace job-9197 with labels: map[e2e-9gnbp:patched e2e-job-label:e2e-9gnbp] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 03/02/23 02:30:32.053
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Mar  2 02:30:32.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-9197" for this suite. 03/02/23 02:30:32.113
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:30:32.144
Mar  2 02:30:32.144: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename kubectl 03/02/23 02:30:32.146
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:30:32.224
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:30:32.241
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1732
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/02/23 02:30:32.254
Mar  2 02:30:32.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1246 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Mar  2 02:30:32.561: INFO: stderr: ""
Mar  2 02:30:32.561: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 03/02/23 02:30:32.561
STEP: verifying the pod e2e-test-httpd-pod was created 03/02/23 02:30:37.62
Mar  2 02:30:37.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1246 get pod e2e-test-httpd-pod -o json'
Mar  2 02:30:37.730: INFO: stderr: ""
Mar  2 02:30:37.730: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"648105f61fc9d79d1b8c0c5a761c0d1662eb5ec6375eeb9a8ca8ebca3ec5d92b\",\n            \"cni.projectcalico.org/podIP\": \"172.30.156.84/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.30.156.84/32\",\n            \"k8s.v1.cni.cncf.io/network-status\": \"[{\\n    \\\"name\\\": \\\"k8s-pod-network\\\",\\n    \\\"ips\\\": [\\n        \\\"172.30.156.84\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"k8s-pod-network\\\",\\n    \\\"ips\\\": [\\n        \\\"172.30.156.84\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2023-03-02T02:30:32Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-1246\",\n        \"resourceVersion\": \"128835\",\n        \"uid\": \"31bd9e14-1499-43c6-809a-efdafb8fd866\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    }\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-r7js5\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"10.132.92.143\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c60,c45\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-r7js5\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"service-ca.crt\",\n                                        \"path\": \"service-ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"openshift-service-ca.crt\"\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-02T02:30:32Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-02T02:30:34Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-02T02:30:34Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-02T02:30:32Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://979ce224ae78ba5a144084b989c495404baaafbdd5d5313b375ab4f58609b80e\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-03-02T02:30:33Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.132.92.143\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.30.156.84\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.30.156.84\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-03-02T02:30:32Z\"\n    }\n}\n"
STEP: replace the image in the pod 03/02/23 02:30:37.731
Mar  2 02:30:37.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1246 replace -f -'
Mar  2 02:30:38.541: INFO: stderr: ""
Mar  2 02:30:38.541: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 03/02/23 02:30:38.541
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1736
Mar  2 02:30:38.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1246 delete pods e2e-test-httpd-pod'
Mar  2 02:30:40.304: INFO: stderr: ""
Mar  2 02:30:40.304: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  2 02:30:40.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1246" for this suite. 03/02/23 02:30:40.325
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","completed":263,"skipped":4850,"failed":0}
------------------------------
• [SLOW TEST] [8.206 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1729
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1745

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:30:32.144
    Mar  2 02:30:32.144: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename kubectl 03/02/23 02:30:32.146
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:30:32.224
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:30:32.241
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1732
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1745
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/02/23 02:30:32.254
    Mar  2 02:30:32.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1246 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Mar  2 02:30:32.561: INFO: stderr: ""
    Mar  2 02:30:32.561: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 03/02/23 02:30:32.561
    STEP: verifying the pod e2e-test-httpd-pod was created 03/02/23 02:30:37.62
    Mar  2 02:30:37.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1246 get pod e2e-test-httpd-pod -o json'
    Mar  2 02:30:37.730: INFO: stderr: ""
    Mar  2 02:30:37.730: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"648105f61fc9d79d1b8c0c5a761c0d1662eb5ec6375eeb9a8ca8ebca3ec5d92b\",\n            \"cni.projectcalico.org/podIP\": \"172.30.156.84/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.30.156.84/32\",\n            \"k8s.v1.cni.cncf.io/network-status\": \"[{\\n    \\\"name\\\": \\\"k8s-pod-network\\\",\\n    \\\"ips\\\": [\\n        \\\"172.30.156.84\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"k8s-pod-network\\\",\\n    \\\"ips\\\": [\\n        \\\"172.30.156.84\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2023-03-02T02:30:32Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-1246\",\n        \"resourceVersion\": \"128835\",\n        \"uid\": \"31bd9e14-1499-43c6-809a-efdafb8fd866\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    }\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-r7js5\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"10.132.92.143\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c60,c45\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-r7js5\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"service-ca.crt\",\n                                        \"path\": \"service-ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"openshift-service-ca.crt\"\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-02T02:30:32Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-02T02:30:34Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-02T02:30:34Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-02T02:30:32Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://979ce224ae78ba5a144084b989c495404baaafbdd5d5313b375ab4f58609b80e\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-03-02T02:30:33Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.132.92.143\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.30.156.84\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.30.156.84\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-03-02T02:30:32Z\"\n    }\n}\n"
    STEP: replace the image in the pod 03/02/23 02:30:37.731
    Mar  2 02:30:37.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1246 replace -f -'
    Mar  2 02:30:38.541: INFO: stderr: ""
    Mar  2 02:30:38.541: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 03/02/23 02:30:38.541
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1736
    Mar  2 02:30:38.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-1246 delete pods e2e-test-httpd-pod'
    Mar  2 02:30:40.304: INFO: stderr: ""
    Mar  2 02:30:40.304: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  2 02:30:40.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1246" for this suite. 03/02/23 02:30:40.325
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:30:40.352
Mar  2 02:30:40.352: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename configmap 03/02/23 02:30:40.353
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:30:40.411
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:30:40.429
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
STEP: Creating configMap configmap-6243/configmap-test-dfd70007-ba4c-4211-bd2f-24561eb7371e 03/02/23 02:30:40.443
STEP: Creating a pod to test consume configMaps 03/02/23 02:30:40.492
Mar  2 02:30:40.551: INFO: Waiting up to 5m0s for pod "pod-configmaps-fbe35a97-1e4d-416a-be44-932972e5a0fe" in namespace "configmap-6243" to be "Succeeded or Failed"
Mar  2 02:30:40.575: INFO: Pod "pod-configmaps-fbe35a97-1e4d-416a-be44-932972e5a0fe": Phase="Pending", Reason="", readiness=false. Elapsed: 23.597891ms
Mar  2 02:30:42.591: INFO: Pod "pod-configmaps-fbe35a97-1e4d-416a-be44-932972e5a0fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040077899s
Mar  2 02:30:44.589: INFO: Pod "pod-configmaps-fbe35a97-1e4d-416a-be44-932972e5a0fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03737568s
STEP: Saw pod success 03/02/23 02:30:44.589
Mar  2 02:30:44.589: INFO: Pod "pod-configmaps-fbe35a97-1e4d-416a-be44-932972e5a0fe" satisfied condition "Succeeded or Failed"
Mar  2 02:30:44.604: INFO: Trying to get logs from node 10.132.92.143 pod pod-configmaps-fbe35a97-1e4d-416a-be44-932972e5a0fe container env-test: <nil>
STEP: delete the pod 03/02/23 02:30:44.69
Mar  2 02:30:44.750: INFO: Waiting for pod pod-configmaps-fbe35a97-1e4d-416a-be44-932972e5a0fe to disappear
Mar  2 02:30:44.763: INFO: Pod pod-configmaps-fbe35a97-1e4d-416a-be44-932972e5a0fe no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Mar  2 02:30:44.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6243" for this suite. 03/02/23 02:30:44.782
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","completed":264,"skipped":4870,"failed":0}
------------------------------
• [4.453 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:30:40.352
    Mar  2 02:30:40.352: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename configmap 03/02/23 02:30:40.353
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:30:40.411
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:30:40.429
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:44
    STEP: Creating configMap configmap-6243/configmap-test-dfd70007-ba4c-4211-bd2f-24561eb7371e 03/02/23 02:30:40.443
    STEP: Creating a pod to test consume configMaps 03/02/23 02:30:40.492
    Mar  2 02:30:40.551: INFO: Waiting up to 5m0s for pod "pod-configmaps-fbe35a97-1e4d-416a-be44-932972e5a0fe" in namespace "configmap-6243" to be "Succeeded or Failed"
    Mar  2 02:30:40.575: INFO: Pod "pod-configmaps-fbe35a97-1e4d-416a-be44-932972e5a0fe": Phase="Pending", Reason="", readiness=false. Elapsed: 23.597891ms
    Mar  2 02:30:42.591: INFO: Pod "pod-configmaps-fbe35a97-1e4d-416a-be44-932972e5a0fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040077899s
    Mar  2 02:30:44.589: INFO: Pod "pod-configmaps-fbe35a97-1e4d-416a-be44-932972e5a0fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03737568s
    STEP: Saw pod success 03/02/23 02:30:44.589
    Mar  2 02:30:44.589: INFO: Pod "pod-configmaps-fbe35a97-1e4d-416a-be44-932972e5a0fe" satisfied condition "Succeeded or Failed"
    Mar  2 02:30:44.604: INFO: Trying to get logs from node 10.132.92.143 pod pod-configmaps-fbe35a97-1e4d-416a-be44-932972e5a0fe container env-test: <nil>
    STEP: delete the pod 03/02/23 02:30:44.69
    Mar  2 02:30:44.750: INFO: Waiting for pod pod-configmaps-fbe35a97-1e4d-416a-be44-932972e5a0fe to disappear
    Mar  2 02:30:44.763: INFO: Pod pod-configmaps-fbe35a97-1e4d-416a-be44-932972e5a0fe no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  2 02:30:44.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6243" for this suite. 03/02/23 02:30:44.782
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:30:44.805
Mar  2 02:30:44.805: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename deployment 03/02/23 02:30:44.806
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:30:44.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:30:44.886
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Mar  2 02:30:44.899: INFO: Creating simple deployment test-new-deployment
W0302 02:30:44.924707      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Mar  2 02:30:44.973: INFO: deployment "test-new-deployment" doesn't have the required revision set
Mar  2 02:30:47.020: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 2, 30, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 30, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 2, 30, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 30, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-845c8977d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: getting scale subresource 03/02/23 02:30:49.063
STEP: updating a scale subresource 03/02/23 02:30:49.077
STEP: verifying the deployment Spec.Replicas was modified 03/02/23 02:30:49.098
STEP: Patch a scale subresource 03/02/23 02:30:49.113
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar  2 02:30:49.174: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-9494  2cca1f8b-c28a-4436-babd-ec2a121b5e95 129117 3 2023-03-02 02:30:44 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-03-02 02:30:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 02:30:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0029c1b78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-02 02:30:47 +0000 UTC,LastTransitionTime:2023-03-02 02:30:47 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-03-02 02:30:47 +0000 UTC,LastTransitionTime:2023-03-02 02:30:44 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar  2 02:30:49.191: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-9494  e1b87a31-2b65-4380-ae27-b6134a4d1c75 129120 3 2023-03-02 02:30:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 2cca1f8b-c28a-4436-babd-ec2a121b5e95 0xc0029c1fc7 0xc0029c1fc8}] [] [{kube-controller-manager Update apps/v1 2023-03-02 02:30:47 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-03-02 02:30:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2cca1f8b-c28a-4436-babd-ec2a121b5e95\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006e70058 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar  2 02:30:49.233: INFO: Pod "test-new-deployment-845c8977d9-g7t7j" is available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-g7t7j test-new-deployment-845c8977d9- deployment-9494  697901e6-5d09-4e91-b42d-e8ec26a415ee 129083 0 2023-03-02 02:30:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:9f5b0b246e88359597db79b7687066f2eafca99b9b43d05acc21a358385808d7 cni.projectcalico.org/podIP:172.30.156.120/32 cni.projectcalico.org/podIPs:172.30.156.120/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.156.120"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.156.120"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 e1b87a31-2b65-4380-ae27-b6134a4d1c75 0xc006e70427 0xc006e70428}] [] [{kube-controller-manager Update v1 2023-03-02 02:30:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e1b87a31-2b65-4380-ae27-b6134a4d1c75\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 02:30:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 02:30:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-02 02:30:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.156.120\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gckls,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gckls,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.132.92.143,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c55,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:30:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:30:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:30:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:30:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.132.92.143,PodIP:172.30.156.120,StartTime:2023-03-02 02:30:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 02:30:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://ad3a9ba76f1574fc6dc61835949130c0e9d4323ddc5f9628ac786087bde4c835,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.156.120,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 02:30:49.234: INFO: Pod "test-new-deployment-845c8977d9-lqhks" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-lqhks test-new-deployment-845c8977d9- deployment-9494  ad94bf18-0419-47e2-a74c-9c4879c724fa 129122 0 2023-03-02 02:30:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 e1b87a31-2b65-4380-ae27-b6134a4d1c75 0xc006e70697 0xc006e70698}] [] [{kube-controller-manager Update v1 2023-03-02 02:30:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e1b87a31-2b65-4380-ae27-b6134a4d1c75\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8vstf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8vstf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.132.92.188,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c55,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9tb49,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:30:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar  2 02:30:49.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9494" for this suite. 03/02/23 02:30:49.341
{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","completed":265,"skipped":4877,"failed":0}
------------------------------
• [4.567 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:30:44.805
    Mar  2 02:30:44.805: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename deployment 03/02/23 02:30:44.806
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:30:44.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:30:44.886
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Mar  2 02:30:44.899: INFO: Creating simple deployment test-new-deployment
    W0302 02:30:44.924707      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Mar  2 02:30:44.973: INFO: deployment "test-new-deployment" doesn't have the required revision set
    Mar  2 02:30:47.020: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 2, 30, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 30, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 2, 30, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 30, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-845c8977d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: getting scale subresource 03/02/23 02:30:49.063
    STEP: updating a scale subresource 03/02/23 02:30:49.077
    STEP: verifying the deployment Spec.Replicas was modified 03/02/23 02:30:49.098
    STEP: Patch a scale subresource 03/02/23 02:30:49.113
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar  2 02:30:49.174: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-9494  2cca1f8b-c28a-4436-babd-ec2a121b5e95 129117 3 2023-03-02 02:30:44 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-03-02 02:30:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 02:30:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0029c1b78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-02 02:30:47 +0000 UTC,LastTransitionTime:2023-03-02 02:30:47 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-03-02 02:30:47 +0000 UTC,LastTransitionTime:2023-03-02 02:30:44 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Mar  2 02:30:49.191: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-9494  e1b87a31-2b65-4380-ae27-b6134a4d1c75 129120 3 2023-03-02 02:30:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 2cca1f8b-c28a-4436-babd-ec2a121b5e95 0xc0029c1fc7 0xc0029c1fc8}] [] [{kube-controller-manager Update apps/v1 2023-03-02 02:30:47 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-03-02 02:30:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2cca1f8b-c28a-4436-babd-ec2a121b5e95\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006e70058 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar  2 02:30:49.233: INFO: Pod "test-new-deployment-845c8977d9-g7t7j" is available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-g7t7j test-new-deployment-845c8977d9- deployment-9494  697901e6-5d09-4e91-b42d-e8ec26a415ee 129083 0 2023-03-02 02:30:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:9f5b0b246e88359597db79b7687066f2eafca99b9b43d05acc21a358385808d7 cni.projectcalico.org/podIP:172.30.156.120/32 cni.projectcalico.org/podIPs:172.30.156.120/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.156.120"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.156.120"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 e1b87a31-2b65-4380-ae27-b6134a4d1c75 0xc006e70427 0xc006e70428}] [] [{kube-controller-manager Update v1 2023-03-02 02:30:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e1b87a31-2b65-4380-ae27-b6134a4d1c75\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 02:30:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 02:30:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-02 02:30:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.156.120\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gckls,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gckls,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.132.92.143,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c55,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:30:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:30:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:30:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:30:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.132.92.143,PodIP:172.30.156.120,StartTime:2023-03-02 02:30:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 02:30:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://ad3a9ba76f1574fc6dc61835949130c0e9d4323ddc5f9628ac786087bde4c835,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.156.120,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar  2 02:30:49.234: INFO: Pod "test-new-deployment-845c8977d9-lqhks" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-lqhks test-new-deployment-845c8977d9- deployment-9494  ad94bf18-0419-47e2-a74c-9c4879c724fa 129122 0 2023-03-02 02:30:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 e1b87a31-2b65-4380-ae27-b6134a4d1c75 0xc006e70697 0xc006e70698}] [] [{kube-controller-manager Update v1 2023-03-02 02:30:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e1b87a31-2b65-4380-ae27-b6134a4d1c75\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8vstf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8vstf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.132.92.188,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c55,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9tb49,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:30:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar  2 02:30:49.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-9494" for this suite. 03/02/23 02:30:49.341
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:30:49.378
Mar  2 02:30:49.379: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename tables 03/02/23 02:30:49.381
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:30:49.435
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:30:49.456
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:187
Mar  2 02:30:49.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-3923" for this suite. 03/02/23 02:30:49.508
{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","completed":266,"skipped":4923,"failed":0}
------------------------------
• [0.157 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:30:49.378
    Mar  2 02:30:49.379: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename tables 03/02/23 02:30:49.381
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:30:49.435
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:30:49.456
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:187
    Mar  2 02:30:49.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "tables-3923" for this suite. 03/02/23 02:30:49.508
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:30:49.537
Mar  2 02:30:49.537: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename ingress 03/02/23 02:30:49.538
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:30:49.61
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:30:49.626
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 03/02/23 02:30:49.638
STEP: getting /apis/networking.k8s.io 03/02/23 02:30:49.65
STEP: getting /apis/networking.k8s.iov1 03/02/23 02:30:49.657
STEP: creating 03/02/23 02:30:49.662
STEP: getting 03/02/23 02:30:49.734
STEP: listing 03/02/23 02:30:49.753
STEP: watching 03/02/23 02:30:49.775
Mar  2 02:30:49.775: INFO: starting watch
STEP: cluster-wide listing 03/02/23 02:30:49.781
STEP: cluster-wide watching 03/02/23 02:30:49.804
Mar  2 02:30:49.805: INFO: starting watch
STEP: patching 03/02/23 02:30:49.81
STEP: updating 03/02/23 02:30:49.833
Mar  2 02:30:49.871: INFO: waiting for watch events with expected annotations
Mar  2 02:30:49.871: INFO: saw patched and updated annotations
STEP: patching /status 03/02/23 02:30:49.871
STEP: updating /status 03/02/23 02:30:49.895
STEP: get /status 03/02/23 02:30:49.933
STEP: deleting 03/02/23 02:30:49.951
STEP: deleting a collection 03/02/23 02:30:50.04
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:187
Mar  2 02:30:50.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-6372" for this suite. 03/02/23 02:30:50.138
{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","completed":267,"skipped":4943,"failed":0}
------------------------------
• [0.633 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:30:49.537
    Mar  2 02:30:49.537: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename ingress 03/02/23 02:30:49.538
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:30:49.61
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:30:49.626
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 03/02/23 02:30:49.638
    STEP: getting /apis/networking.k8s.io 03/02/23 02:30:49.65
    STEP: getting /apis/networking.k8s.iov1 03/02/23 02:30:49.657
    STEP: creating 03/02/23 02:30:49.662
    STEP: getting 03/02/23 02:30:49.734
    STEP: listing 03/02/23 02:30:49.753
    STEP: watching 03/02/23 02:30:49.775
    Mar  2 02:30:49.775: INFO: starting watch
    STEP: cluster-wide listing 03/02/23 02:30:49.781
    STEP: cluster-wide watching 03/02/23 02:30:49.804
    Mar  2 02:30:49.805: INFO: starting watch
    STEP: patching 03/02/23 02:30:49.81
    STEP: updating 03/02/23 02:30:49.833
    Mar  2 02:30:49.871: INFO: waiting for watch events with expected annotations
    Mar  2 02:30:49.871: INFO: saw patched and updated annotations
    STEP: patching /status 03/02/23 02:30:49.871
    STEP: updating /status 03/02/23 02:30:49.895
    STEP: get /status 03/02/23 02:30:49.933
    STEP: deleting 03/02/23 02:30:49.951
    STEP: deleting a collection 03/02/23 02:30:50.04
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:187
    Mar  2 02:30:50.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingress-6372" for this suite. 03/02/23 02:30:50.138
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:30:50.171
Mar  2 02:30:50.172: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename webhook 03/02/23 02:30:50.173
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:30:50.233
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:30:50.249
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/02/23 02:30:50.384
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 02:30:50.732
STEP: Deploying the webhook pod 03/02/23 02:30:50.807
STEP: Wait for the deployment to be ready 03/02/23 02:30:50.874
Mar  2 02:30:50.910: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 02:30:52.958: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 2, 30, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 30, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 2, 30, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 30, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/02/23 02:30:54.976
STEP: Verifying the service has paired with the endpoint 03/02/23 02:30:55.043
Mar  2 02:30:56.045: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 03/02/23 02:30:56.065
STEP: create a pod that should be updated by the webhook 03/02/23 02:30:56.393
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 02:30:56.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5767" for this suite. 03/02/23 02:30:56.885
STEP: Destroying namespace "webhook-5767-markers" for this suite. 03/02/23 02:30:56.968
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","completed":268,"skipped":4956,"failed":0}
------------------------------
• [SLOW TEST] [7.115 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:30:50.171
    Mar  2 02:30:50.172: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename webhook 03/02/23 02:30:50.173
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:30:50.233
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:30:50.249
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/02/23 02:30:50.384
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 02:30:50.732
    STEP: Deploying the webhook pod 03/02/23 02:30:50.807
    STEP: Wait for the deployment to be ready 03/02/23 02:30:50.874
    Mar  2 02:30:50.910: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Mar  2 02:30:52.958: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 2, 30, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 30, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 2, 30, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 30, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/02/23 02:30:54.976
    STEP: Verifying the service has paired with the endpoint 03/02/23 02:30:55.043
    Mar  2 02:30:56.045: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:263
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 03/02/23 02:30:56.065
    STEP: create a pod that should be updated by the webhook 03/02/23 02:30:56.393
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 02:30:56.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5767" for this suite. 03/02/23 02:30:56.885
    STEP: Destroying namespace "webhook-5767-markers" for this suite. 03/02/23 02:30:56.968
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:30:57.29
Mar  2 02:30:57.290: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename services 03/02/23 02:30:57.296
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:30:57.361
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:30:57.381
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6733 03/02/23 02:30:57.396
STEP: changing the ExternalName service to type=NodePort 03/02/23 02:30:57.429
STEP: creating replication controller externalname-service in namespace services-6733 03/02/23 02:30:57.555
I0302 02:30:57.585820      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6733, replica count: 2
I0302 02:31:00.636445      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 02:31:00.636: INFO: Creating new exec pod
Mar  2 02:31:00.795: INFO: Waiting up to 5m0s for pod "execpod672hz" in namespace "services-6733" to be "running"
Mar  2 02:31:00.820: INFO: Pod "execpod672hz": Phase="Pending", Reason="", readiness=false. Elapsed: 24.606276ms
Mar  2 02:31:02.842: INFO: Pod "execpod672hz": Phase="Running", Reason="", readiness=true. Elapsed: 2.04692895s
Mar  2 02:31:02.842: INFO: Pod "execpod672hz" satisfied condition "running"
Mar  2 02:31:03.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-6733 exec execpod672hz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Mar  2 02:31:04.289: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar  2 02:31:04.289: INFO: stdout: ""
Mar  2 02:31:05.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-6733 exec execpod672hz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Mar  2 02:31:05.758: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar  2 02:31:05.758: INFO: stdout: "externalname-service-9hg6r"
Mar  2 02:31:05.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-6733 exec execpod672hz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.193.21 80'
Mar  2 02:31:06.128: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.193.21 80\nConnection to 172.21.193.21 80 port [tcp/http] succeeded!\n"
Mar  2 02:31:06.129: INFO: stdout: ""
Mar  2 02:31:07.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-6733 exec execpod672hz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.193.21 80'
Mar  2 02:31:07.683: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.193.21 80\nConnection to 172.21.193.21 80 port [tcp/http] succeeded!\n"
Mar  2 02:31:07.683: INFO: stdout: ""
Mar  2 02:31:08.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-6733 exec execpod672hz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.193.21 80'
Mar  2 02:31:08.469: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.193.21 80\nConnection to 172.21.193.21 80 port [tcp/http] succeeded!\n"
Mar  2 02:31:08.469: INFO: stdout: ""
Mar  2 02:31:09.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-6733 exec execpod672hz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.193.21 80'
Mar  2 02:31:09.466: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.193.21 80\nConnection to 172.21.193.21 80 port [tcp/http] succeeded!\n"
Mar  2 02:31:09.466: INFO: stdout: ""
Mar  2 02:31:10.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-6733 exec execpod672hz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.193.21 80'
Mar  2 02:31:10.508: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.193.21 80\nConnection to 172.21.193.21 80 port [tcp/http] succeeded!\n"
Mar  2 02:31:10.508: INFO: stdout: ""
Mar  2 02:31:11.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-6733 exec execpod672hz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.193.21 80'
Mar  2 02:31:11.601: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.193.21 80\nConnection to 172.21.193.21 80 port [tcp/http] succeeded!\n"
Mar  2 02:31:11.601: INFO: stdout: ""
Mar  2 02:31:12.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-6733 exec execpod672hz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.193.21 80'
Mar  2 02:31:12.496: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.193.21 80\nConnection to 172.21.193.21 80 port [tcp/http] succeeded!\n"
Mar  2 02:31:12.496: INFO: stdout: "externalname-service-9hg6r"
Mar  2 02:31:12.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-6733 exec execpod672hz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.132.92.186 31514'
Mar  2 02:31:12.936: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.132.92.186 31514\nConnection to 10.132.92.186 31514 port [tcp/*] succeeded!\n"
Mar  2 02:31:12.936: INFO: stdout: ""
Mar  2 02:31:13.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-6733 exec execpod672hz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.132.92.186 31514'
Mar  2 02:31:14.284: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.132.92.186 31514\nConnection to 10.132.92.186 31514 port [tcp/*] succeeded!\n"
Mar  2 02:31:14.284: INFO: stdout: "externalname-service-9sk7b"
Mar  2 02:31:14.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-6733 exec execpod672hz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.132.92.188 31514'
Mar  2 02:31:14.645: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.132.92.188 31514\nConnection to 10.132.92.188 31514 port [tcp/*] succeeded!\n"
Mar  2 02:31:14.645: INFO: stdout: "externalname-service-9hg6r"
Mar  2 02:31:14.645: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  2 02:31:14.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6733" for this suite. 03/02/23 02:31:14.757
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","completed":269,"skipped":4972,"failed":0}
------------------------------
• [SLOW TEST] [17.520 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:30:57.29
    Mar  2 02:30:57.290: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename services 03/02/23 02:30:57.296
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:30:57.361
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:30:57.381
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1443
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-6733 03/02/23 02:30:57.396
    STEP: changing the ExternalName service to type=NodePort 03/02/23 02:30:57.429
    STEP: creating replication controller externalname-service in namespace services-6733 03/02/23 02:30:57.555
    I0302 02:30:57.585820      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6733, replica count: 2
    I0302 02:31:00.636445      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar  2 02:31:00.636: INFO: Creating new exec pod
    Mar  2 02:31:00.795: INFO: Waiting up to 5m0s for pod "execpod672hz" in namespace "services-6733" to be "running"
    Mar  2 02:31:00.820: INFO: Pod "execpod672hz": Phase="Pending", Reason="", readiness=false. Elapsed: 24.606276ms
    Mar  2 02:31:02.842: INFO: Pod "execpod672hz": Phase="Running", Reason="", readiness=true. Elapsed: 2.04692895s
    Mar  2 02:31:02.842: INFO: Pod "execpod672hz" satisfied condition "running"
    Mar  2 02:31:03.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-6733 exec execpod672hz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Mar  2 02:31:04.289: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Mar  2 02:31:04.289: INFO: stdout: ""
    Mar  2 02:31:05.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-6733 exec execpod672hz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Mar  2 02:31:05.758: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Mar  2 02:31:05.758: INFO: stdout: "externalname-service-9hg6r"
    Mar  2 02:31:05.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-6733 exec execpod672hz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.193.21 80'
    Mar  2 02:31:06.128: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.193.21 80\nConnection to 172.21.193.21 80 port [tcp/http] succeeded!\n"
    Mar  2 02:31:06.129: INFO: stdout: ""
    Mar  2 02:31:07.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-6733 exec execpod672hz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.193.21 80'
    Mar  2 02:31:07.683: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.193.21 80\nConnection to 172.21.193.21 80 port [tcp/http] succeeded!\n"
    Mar  2 02:31:07.683: INFO: stdout: ""
    Mar  2 02:31:08.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-6733 exec execpod672hz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.193.21 80'
    Mar  2 02:31:08.469: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.193.21 80\nConnection to 172.21.193.21 80 port [tcp/http] succeeded!\n"
    Mar  2 02:31:08.469: INFO: stdout: ""
    Mar  2 02:31:09.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-6733 exec execpod672hz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.193.21 80'
    Mar  2 02:31:09.466: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.193.21 80\nConnection to 172.21.193.21 80 port [tcp/http] succeeded!\n"
    Mar  2 02:31:09.466: INFO: stdout: ""
    Mar  2 02:31:10.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-6733 exec execpod672hz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.193.21 80'
    Mar  2 02:31:10.508: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.193.21 80\nConnection to 172.21.193.21 80 port [tcp/http] succeeded!\n"
    Mar  2 02:31:10.508: INFO: stdout: ""
    Mar  2 02:31:11.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-6733 exec execpod672hz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.193.21 80'
    Mar  2 02:31:11.601: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.193.21 80\nConnection to 172.21.193.21 80 port [tcp/http] succeeded!\n"
    Mar  2 02:31:11.601: INFO: stdout: ""
    Mar  2 02:31:12.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-6733 exec execpod672hz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.193.21 80'
    Mar  2 02:31:12.496: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.193.21 80\nConnection to 172.21.193.21 80 port [tcp/http] succeeded!\n"
    Mar  2 02:31:12.496: INFO: stdout: "externalname-service-9hg6r"
    Mar  2 02:31:12.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-6733 exec execpod672hz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.132.92.186 31514'
    Mar  2 02:31:12.936: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.132.92.186 31514\nConnection to 10.132.92.186 31514 port [tcp/*] succeeded!\n"
    Mar  2 02:31:12.936: INFO: stdout: ""
    Mar  2 02:31:13.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-6733 exec execpod672hz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.132.92.186 31514'
    Mar  2 02:31:14.284: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.132.92.186 31514\nConnection to 10.132.92.186 31514 port [tcp/*] succeeded!\n"
    Mar  2 02:31:14.284: INFO: stdout: "externalname-service-9sk7b"
    Mar  2 02:31:14.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-6733 exec execpod672hz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.132.92.188 31514'
    Mar  2 02:31:14.645: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.132.92.188 31514\nConnection to 10.132.92.188 31514 port [tcp/*] succeeded!\n"
    Mar  2 02:31:14.645: INFO: stdout: "externalname-service-9hg6r"
    Mar  2 02:31:14.645: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  2 02:31:14.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6733" for this suite. 03/02/23 02:31:14.757
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:31:14.812
Mar  2 02:31:14.812: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename job 03/02/23 02:31:14.814
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:31:14.946
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:31:14.959
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
STEP: Creating Indexed job 03/02/23 02:31:14.975
STEP: Ensuring job reaches completions 03/02/23 02:31:15.005
STEP: Ensuring pods with index for job exist 03/02/23 02:31:27.038
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Mar  2 02:31:27.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3614" for this suite. 03/02/23 02:31:27.368
{"msg":"PASSED [sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]","completed":270,"skipped":4991,"failed":0}
------------------------------
• [SLOW TEST] [12.621 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:31:14.812
    Mar  2 02:31:14.812: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename job 03/02/23 02:31:14.814
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:31:14.946
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:31:14.959
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:194
    STEP: Creating Indexed job 03/02/23 02:31:14.975
    STEP: Ensuring job reaches completions 03/02/23 02:31:15.005
    STEP: Ensuring pods with index for job exist 03/02/23 02:31:27.038
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Mar  2 02:31:27.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-3614" for this suite. 03/02/23 02:31:27.368
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:31:27.433
Mar  2 02:31:27.433: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename configmap 03/02/23 02:31:27.435
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:31:27.508
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:31:27.58
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
STEP: Creating configMap with name configmap-test-volume-map-6de068c1-bccf-4e9e-bd04-e5a708358e62 03/02/23 02:31:27.614
STEP: Creating a pod to test consume configMaps 03/02/23 02:31:27.636
Mar  2 02:31:27.733: INFO: Waiting up to 5m0s for pod "pod-configmaps-9f09e308-13e1-4d21-8a24-8d18a576389b" in namespace "configmap-1758" to be "Succeeded or Failed"
Mar  2 02:31:27.809: INFO: Pod "pod-configmaps-9f09e308-13e1-4d21-8a24-8d18a576389b": Phase="Pending", Reason="", readiness=false. Elapsed: 76.266104ms
Mar  2 02:31:29.825: INFO: Pod "pod-configmaps-9f09e308-13e1-4d21-8a24-8d18a576389b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.091632456s
Mar  2 02:31:31.828: INFO: Pod "pod-configmaps-9f09e308-13e1-4d21-8a24-8d18a576389b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.094732153s
STEP: Saw pod success 03/02/23 02:31:31.828
Mar  2 02:31:31.828: INFO: Pod "pod-configmaps-9f09e308-13e1-4d21-8a24-8d18a576389b" satisfied condition "Succeeded or Failed"
Mar  2 02:31:31.841: INFO: Trying to get logs from node 10.132.92.143 pod pod-configmaps-9f09e308-13e1-4d21-8a24-8d18a576389b container agnhost-container: <nil>
STEP: delete the pod 03/02/23 02:31:31.874
Mar  2 02:31:31.923: INFO: Waiting for pod pod-configmaps-9f09e308-13e1-4d21-8a24-8d18a576389b to disappear
Mar  2 02:31:31.944: INFO: Pod pod-configmaps-9f09e308-13e1-4d21-8a24-8d18a576389b no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar  2 02:31:31.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1758" for this suite. 03/02/23 02:31:31.965
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":271,"skipped":4995,"failed":0}
------------------------------
• [4.597 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:31:27.433
    Mar  2 02:31:27.433: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename configmap 03/02/23 02:31:27.435
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:31:27.508
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:31:27.58
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:88
    STEP: Creating configMap with name configmap-test-volume-map-6de068c1-bccf-4e9e-bd04-e5a708358e62 03/02/23 02:31:27.614
    STEP: Creating a pod to test consume configMaps 03/02/23 02:31:27.636
    Mar  2 02:31:27.733: INFO: Waiting up to 5m0s for pod "pod-configmaps-9f09e308-13e1-4d21-8a24-8d18a576389b" in namespace "configmap-1758" to be "Succeeded or Failed"
    Mar  2 02:31:27.809: INFO: Pod "pod-configmaps-9f09e308-13e1-4d21-8a24-8d18a576389b": Phase="Pending", Reason="", readiness=false. Elapsed: 76.266104ms
    Mar  2 02:31:29.825: INFO: Pod "pod-configmaps-9f09e308-13e1-4d21-8a24-8d18a576389b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.091632456s
    Mar  2 02:31:31.828: INFO: Pod "pod-configmaps-9f09e308-13e1-4d21-8a24-8d18a576389b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.094732153s
    STEP: Saw pod success 03/02/23 02:31:31.828
    Mar  2 02:31:31.828: INFO: Pod "pod-configmaps-9f09e308-13e1-4d21-8a24-8d18a576389b" satisfied condition "Succeeded or Failed"
    Mar  2 02:31:31.841: INFO: Trying to get logs from node 10.132.92.143 pod pod-configmaps-9f09e308-13e1-4d21-8a24-8d18a576389b container agnhost-container: <nil>
    STEP: delete the pod 03/02/23 02:31:31.874
    Mar  2 02:31:31.923: INFO: Waiting for pod pod-configmaps-9f09e308-13e1-4d21-8a24-8d18a576389b to disappear
    Mar  2 02:31:31.944: INFO: Pod pod-configmaps-9f09e308-13e1-4d21-8a24-8d18a576389b no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  2 02:31:31.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1758" for this suite. 03/02/23 02:31:31.965
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:31:32.031
Mar  2 02:31:32.031: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename taint-single-pod 03/02/23 02:31:32.033
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:31:32.125
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:31:32.14
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:166
Mar  2 02:31:32.173: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  2 02:32:32.478: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
Mar  2 02:32:32.507: INFO: Starting informer...
STEP: Starting pod... 03/02/23 02:32:32.507
Mar  2 02:32:32.806: INFO: Pod is running on 10.132.92.143. Tainting Node
STEP: Trying to apply a taint on the Node 03/02/23 02:32:32.806
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/02/23 02:32:32.866
STEP: Waiting short time to make sure Pod is queued for deletion 03/02/23 02:32:32.928
Mar  2 02:32:32.928: INFO: Pod wasn't evicted. Proceeding
Mar  2 02:32:32.928: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/02/23 02:32:33.063
STEP: Waiting some time to make sure that toleration time passed. 03/02/23 02:32:33.082
Mar  2 02:33:48.083: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:187
Mar  2 02:33:48.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-3833" for this suite. 03/02/23 02:33:48.107
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","completed":272,"skipped":4998,"failed":0}
------------------------------
• [SLOW TEST] [136.101 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:31:32.031
    Mar  2 02:31:32.031: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename taint-single-pod 03/02/23 02:31:32.033
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:31:32.125
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:31:32.14
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:166
    Mar  2 02:31:32.173: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar  2 02:32:32.478: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:289
    Mar  2 02:32:32.507: INFO: Starting informer...
    STEP: Starting pod... 03/02/23 02:32:32.507
    Mar  2 02:32:32.806: INFO: Pod is running on 10.132.92.143. Tainting Node
    STEP: Trying to apply a taint on the Node 03/02/23 02:32:32.806
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/02/23 02:32:32.866
    STEP: Waiting short time to make sure Pod is queued for deletion 03/02/23 02:32:32.928
    Mar  2 02:32:32.928: INFO: Pod wasn't evicted. Proceeding
    Mar  2 02:32:32.928: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/02/23 02:32:33.063
    STEP: Waiting some time to make sure that toleration time passed. 03/02/23 02:32:33.082
    Mar  2 02:33:48.083: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:187
    Mar  2 02:33:48.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-single-pod-3833" for this suite. 03/02/23 02:33:48.107
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:33:48.135
Mar  2 02:33:48.135: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename webhook 03/02/23 02:33:48.136
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:33:48.195
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:33:48.207
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/02/23 02:33:48.296
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 02:33:48.891
STEP: Deploying the webhook pod 03/02/23 02:33:48.929
STEP: Wait for the deployment to be ready 03/02/23 02:33:48.966
Mar  2 02:33:48.994: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 02:33:51.048: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 2, 33, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 33, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 2, 33, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 33, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/02/23 02:33:53.063
STEP: Verifying the service has paired with the endpoint 03/02/23 02:33:53.098
Mar  2 02:33:54.099: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
STEP: fetching the /apis discovery document 03/02/23 02:33:54.118
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 03/02/23 02:33:54.127
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 03/02/23 02:33:54.127
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 03/02/23 02:33:54.127
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 03/02/23 02:33:54.133
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 03/02/23 02:33:54.133
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 03/02/23 02:33:54.138
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 02:33:54.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6193" for this suite. 03/02/23 02:33:54.173
STEP: Destroying namespace "webhook-6193-markers" for this suite. 03/02/23 02:33:54.206
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","completed":273,"skipped":5012,"failed":0}
------------------------------
• [SLOW TEST] [6.273 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:33:48.135
    Mar  2 02:33:48.135: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename webhook 03/02/23 02:33:48.136
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:33:48.195
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:33:48.207
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/02/23 02:33:48.296
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 02:33:48.891
    STEP: Deploying the webhook pod 03/02/23 02:33:48.929
    STEP: Wait for the deployment to be ready 03/02/23 02:33:48.966
    Mar  2 02:33:48.994: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Mar  2 02:33:51.048: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 2, 33, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 33, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 2, 33, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 33, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/02/23 02:33:53.063
    STEP: Verifying the service has paired with the endpoint 03/02/23 02:33:53.098
    Mar  2 02:33:54.099: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:116
    STEP: fetching the /apis discovery document 03/02/23 02:33:54.118
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 03/02/23 02:33:54.127
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 03/02/23 02:33:54.127
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 03/02/23 02:33:54.127
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 03/02/23 02:33:54.133
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 03/02/23 02:33:54.133
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 03/02/23 02:33:54.138
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 02:33:54.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6193" for this suite. 03/02/23 02:33:54.173
    STEP: Destroying namespace "webhook-6193-markers" for this suite. 03/02/23 02:33:54.206
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:33:54.409
Mar  2 02:33:54.410: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename security-context-test 03/02/23 02:33:54.412
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:33:54.474
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:33:54.489
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
Mar  2 02:33:54.577: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-8ce478bb-1f8f-4ea9-b539-9ea766152076" in namespace "security-context-test-2702" to be "Succeeded or Failed"
Mar  2 02:33:54.599: INFO: Pod "alpine-nnp-false-8ce478bb-1f8f-4ea9-b539-9ea766152076": Phase="Pending", Reason="", readiness=false. Elapsed: 21.947424ms
Mar  2 02:33:56.615: INFO: Pod "alpine-nnp-false-8ce478bb-1f8f-4ea9-b539-9ea766152076": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038141051s
Mar  2 02:33:58.613: INFO: Pod "alpine-nnp-false-8ce478bb-1f8f-4ea9-b539-9ea766152076": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035513546s
Mar  2 02:34:00.615: INFO: Pod "alpine-nnp-false-8ce478bb-1f8f-4ea9-b539-9ea766152076": Phase="Pending", Reason="", readiness=false. Elapsed: 6.038092784s
Mar  2 02:34:02.615: INFO: Pod "alpine-nnp-false-8ce478bb-1f8f-4ea9-b539-9ea766152076": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.037679852s
Mar  2 02:34:02.615: INFO: Pod "alpine-nnp-false-8ce478bb-1f8f-4ea9-b539-9ea766152076" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Mar  2 02:34:02.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2702" for this suite. 03/02/23 02:34:02.738
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","completed":274,"skipped":5023,"failed":0}
------------------------------
• [SLOW TEST] [8.354 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:554
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:608

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:33:54.409
    Mar  2 02:33:54.410: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename security-context-test 03/02/23 02:33:54.412
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:33:54.474
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:33:54.489
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:608
    Mar  2 02:33:54.577: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-8ce478bb-1f8f-4ea9-b539-9ea766152076" in namespace "security-context-test-2702" to be "Succeeded or Failed"
    Mar  2 02:33:54.599: INFO: Pod "alpine-nnp-false-8ce478bb-1f8f-4ea9-b539-9ea766152076": Phase="Pending", Reason="", readiness=false. Elapsed: 21.947424ms
    Mar  2 02:33:56.615: INFO: Pod "alpine-nnp-false-8ce478bb-1f8f-4ea9-b539-9ea766152076": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038141051s
    Mar  2 02:33:58.613: INFO: Pod "alpine-nnp-false-8ce478bb-1f8f-4ea9-b539-9ea766152076": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035513546s
    Mar  2 02:34:00.615: INFO: Pod "alpine-nnp-false-8ce478bb-1f8f-4ea9-b539-9ea766152076": Phase="Pending", Reason="", readiness=false. Elapsed: 6.038092784s
    Mar  2 02:34:02.615: INFO: Pod "alpine-nnp-false-8ce478bb-1f8f-4ea9-b539-9ea766152076": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.037679852s
    Mar  2 02:34:02.615: INFO: Pod "alpine-nnp-false-8ce478bb-1f8f-4ea9-b539-9ea766152076" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Mar  2 02:34:02.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-2702" for this suite. 03/02/23 02:34:02.738
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:34:02.764
Mar  2 02:34:02.765: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename daemonsets 03/02/23 02:34:02.766
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:34:02.843
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:34:02.857
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
STEP: Creating simple DaemonSet "daemon-set" 03/02/23 02:34:03.039
STEP: Check that daemon pods launch on every node of the cluster. 03/02/23 02:34:03.063
Mar  2 02:34:03.103: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 02:34:03.103: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
Mar  2 02:34:04.154: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 02:34:04.154: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
Mar  2 02:34:05.141: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar  2 02:34:05.141: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
Mar  2 02:34:06.143: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar  2 02:34:06.143: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets 03/02/23 02:34:06.171
STEP: DeleteCollection of the DaemonSets 03/02/23 02:34:06.206
STEP: Verify that ReplicaSets have been deleted 03/02/23 02:34:06.248
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
Mar  2 02:34:06.311: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"131393"},"items":null}

Mar  2 02:34:06.365: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"131394"},"items":[{"metadata":{"name":"daemon-set-8bmww","generateName":"daemon-set-","namespace":"daemonsets-1074","uid":"72ed9b98-7057-4aff-b0e5-9a87ce151011","resourceVersion":"131380","creationTimestamp":"2023-03-02T02:34:03Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"73bd4b6aed7be967baf644a5f192965268edc4b47a8bcf820394d5733e5e9079","cni.projectcalico.org/podIP":"172.30.62.234/32","cni.projectcalico.org/podIPs":"172.30.62.234/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.62.234\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.62.234\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"489421f0-8b21-474e-aefb-f2bc4fde72e7","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-02T02:34:03Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"489421f0-8b21-474e-aefb-f2bc4fde72e7\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-03-02T02:34:04Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-03-02T02:34:04Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-02T02:34:05Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.62.234\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-xlgbf","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-xlgbf","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.132.92.186","securityContext":{"seLinuxOptions":{"level":"s0:c61,c55"}},"imagePullSecrets":[{"name":"default-dockercfg-wst9s"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.132.92.186"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T02:34:03Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T02:34:05Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T02:34:05Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T02:34:03Z"}],"hostIP":"10.132.92.186","podIP":"172.30.62.234","podIPs":[{"ip":"172.30.62.234"}],"startTime":"2023-03-02T02:34:03Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-02T02:34:05Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://e536df1c7111dc54d704d4980484d118dc0c6483c82b182a6c0b38e275add1ea","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-vj5m2","generateName":"daemon-set-","namespace":"daemonsets-1074","uid":"0da95cb2-e443-44b0-ac08-f385b740d175","resourceVersion":"131384","creationTimestamp":"2023-03-02T02:34:03Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"9d9ea11d95f08f2a9e5a046715452cd4f11f3ec16467e7dcaecaa70dbcdce1e9","cni.projectcalico.org/podIP":"172.30.156.123/32","cni.projectcalico.org/podIPs":"172.30.156.123/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.156.123\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.156.123\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"489421f0-8b21-474e-aefb-f2bc4fde72e7","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-02T02:34:03Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"489421f0-8b21-474e-aefb-f2bc4fde72e7\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-03-02T02:34:04Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-03-02T02:34:04Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-02T02:34:05Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.156.123\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-nnf4r","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-nnf4r","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.132.92.143","securityContext":{"seLinuxOptions":{"level":"s0:c61,c55"}},"imagePullSecrets":[{"name":"default-dockercfg-wst9s"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.132.92.143"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T02:34:03Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T02:34:05Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T02:34:05Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T02:34:03Z"}],"hostIP":"10.132.92.143","podIP":"172.30.156.123","podIPs":[{"ip":"172.30.156.123"}],"startTime":"2023-03-02T02:34:03Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-02T02:34:04Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://32b08b9de0eef78d23b4aa65a314ecb167ba816505e5b751b14edda9beecc24b","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-xsvtr","generateName":"daemon-set-","namespace":"daemonsets-1074","uid":"279aeb1d-f455-41b9-b254-19783afb7fe1","resourceVersion":"131376","creationTimestamp":"2023-03-02T02:34:03Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"d1c5c2e75de76ed6c2576ce2a9f890d06d9836432454a1abff4bb6e285693031","cni.projectcalico.org/podIP":"172.30.201.254/32","cni.projectcalico.org/podIPs":"172.30.201.254/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.201.254\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.201.254\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"489421f0-8b21-474e-aefb-f2bc4fde72e7","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-02T02:34:03Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"489421f0-8b21-474e-aefb-f2bc4fde72e7\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-03-02T02:34:04Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-02T02:34:04Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.201.254\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-03-02T02:34:04Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-t7dj7","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-t7dj7","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.132.92.188","securityContext":{"seLinuxOptions":{"level":"s0:c61,c55"}},"imagePullSecrets":[{"name":"default-dockercfg-wst9s"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.132.92.188"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T02:34:03Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T02:34:04Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T02:34:04Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T02:34:03Z"}],"hostIP":"10.132.92.188","podIP":"172.30.201.254","podIPs":[{"ip":"172.30.201.254"}],"startTime":"2023-03-02T02:34:03Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-02T02:34:04Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://f1dfc349e59f32b028ae836bb19f20ef82887c996ea8e4f3899388c0620c2ddb","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Mar  2 02:34:06.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1074" for this suite. 03/02/23 02:34:06.513
{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","completed":275,"skipped":5036,"failed":0}
------------------------------
• [3.779 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:34:02.764
    Mar  2 02:34:02.765: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename daemonsets 03/02/23 02:34:02.766
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:34:02.843
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:34:02.857
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:822
    STEP: Creating simple DaemonSet "daemon-set" 03/02/23 02:34:03.039
    STEP: Check that daemon pods launch on every node of the cluster. 03/02/23 02:34:03.063
    Mar  2 02:34:03.103: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 02:34:03.103: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
    Mar  2 02:34:04.154: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar  2 02:34:04.154: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
    Mar  2 02:34:05.141: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar  2 02:34:05.141: INFO: Node 10.132.92.143 is running 0 daemon pod, expected 1
    Mar  2 02:34:06.143: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar  2 02:34:06.143: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: listing all DeamonSets 03/02/23 02:34:06.171
    STEP: DeleteCollection of the DaemonSets 03/02/23 02:34:06.206
    STEP: Verify that ReplicaSets have been deleted 03/02/23 02:34:06.248
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    Mar  2 02:34:06.311: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"131393"},"items":null}

    Mar  2 02:34:06.365: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"131394"},"items":[{"metadata":{"name":"daemon-set-8bmww","generateName":"daemon-set-","namespace":"daemonsets-1074","uid":"72ed9b98-7057-4aff-b0e5-9a87ce151011","resourceVersion":"131380","creationTimestamp":"2023-03-02T02:34:03Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"73bd4b6aed7be967baf644a5f192965268edc4b47a8bcf820394d5733e5e9079","cni.projectcalico.org/podIP":"172.30.62.234/32","cni.projectcalico.org/podIPs":"172.30.62.234/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.62.234\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.62.234\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"489421f0-8b21-474e-aefb-f2bc4fde72e7","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-02T02:34:03Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"489421f0-8b21-474e-aefb-f2bc4fde72e7\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-03-02T02:34:04Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-03-02T02:34:04Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-02T02:34:05Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.62.234\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-xlgbf","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-xlgbf","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.132.92.186","securityContext":{"seLinuxOptions":{"level":"s0:c61,c55"}},"imagePullSecrets":[{"name":"default-dockercfg-wst9s"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.132.92.186"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T02:34:03Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T02:34:05Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T02:34:05Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T02:34:03Z"}],"hostIP":"10.132.92.186","podIP":"172.30.62.234","podIPs":[{"ip":"172.30.62.234"}],"startTime":"2023-03-02T02:34:03Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-02T02:34:05Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://e536df1c7111dc54d704d4980484d118dc0c6483c82b182a6c0b38e275add1ea","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-vj5m2","generateName":"daemon-set-","namespace":"daemonsets-1074","uid":"0da95cb2-e443-44b0-ac08-f385b740d175","resourceVersion":"131384","creationTimestamp":"2023-03-02T02:34:03Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"9d9ea11d95f08f2a9e5a046715452cd4f11f3ec16467e7dcaecaa70dbcdce1e9","cni.projectcalico.org/podIP":"172.30.156.123/32","cni.projectcalico.org/podIPs":"172.30.156.123/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.156.123\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.156.123\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"489421f0-8b21-474e-aefb-f2bc4fde72e7","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-02T02:34:03Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"489421f0-8b21-474e-aefb-f2bc4fde72e7\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-03-02T02:34:04Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-03-02T02:34:04Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-02T02:34:05Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.156.123\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-nnf4r","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-nnf4r","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.132.92.143","securityContext":{"seLinuxOptions":{"level":"s0:c61,c55"}},"imagePullSecrets":[{"name":"default-dockercfg-wst9s"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.132.92.143"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T02:34:03Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T02:34:05Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T02:34:05Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T02:34:03Z"}],"hostIP":"10.132.92.143","podIP":"172.30.156.123","podIPs":[{"ip":"172.30.156.123"}],"startTime":"2023-03-02T02:34:03Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-02T02:34:04Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://32b08b9de0eef78d23b4aa65a314ecb167ba816505e5b751b14edda9beecc24b","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-xsvtr","generateName":"daemon-set-","namespace":"daemonsets-1074","uid":"279aeb1d-f455-41b9-b254-19783afb7fe1","resourceVersion":"131376","creationTimestamp":"2023-03-02T02:34:03Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"d1c5c2e75de76ed6c2576ce2a9f890d06d9836432454a1abff4bb6e285693031","cni.projectcalico.org/podIP":"172.30.201.254/32","cni.projectcalico.org/podIPs":"172.30.201.254/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.201.254\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.201.254\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"489421f0-8b21-474e-aefb-f2bc4fde72e7","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-02T02:34:03Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"489421f0-8b21-474e-aefb-f2bc4fde72e7\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-03-02T02:34:04Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-02T02:34:04Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.201.254\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-03-02T02:34:04Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-t7dj7","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-t7dj7","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.132.92.188","securityContext":{"seLinuxOptions":{"level":"s0:c61,c55"}},"imagePullSecrets":[{"name":"default-dockercfg-wst9s"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.132.92.188"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T02:34:03Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T02:34:04Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T02:34:04Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T02:34:03Z"}],"hostIP":"10.132.92.188","podIP":"172.30.201.254","podIPs":[{"ip":"172.30.201.254"}],"startTime":"2023-03-02T02:34:03Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-02T02:34:04Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://f1dfc349e59f32b028ae836bb19f20ef82887c996ea8e4f3899388c0620c2ddb","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Mar  2 02:34:06.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-1074" for this suite. 03/02/23 02:34:06.513
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:34:06.545
Mar  2 02:34:06.546: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename configmap 03/02/23 02:34:06.548
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:34:06.611
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:34:06.626
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
STEP: Creating configMap configmap-5994/configmap-test-3e79081d-ae25-40c5-9e0d-37b8cdb687a1 03/02/23 02:34:06.643
STEP: Creating a pod to test consume configMaps 03/02/23 02:34:06.666
Mar  2 02:34:06.755: INFO: Waiting up to 5m0s for pod "pod-configmaps-11d2e9d9-e47f-4c74-b4cc-57d2a46ec46e" in namespace "configmap-5994" to be "Succeeded or Failed"
Mar  2 02:34:06.773: INFO: Pod "pod-configmaps-11d2e9d9-e47f-4c74-b4cc-57d2a46ec46e": Phase="Pending", Reason="", readiness=false. Elapsed: 17.099863ms
Mar  2 02:34:08.791: INFO: Pod "pod-configmaps-11d2e9d9-e47f-4c74-b4cc-57d2a46ec46e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035131199s
Mar  2 02:34:10.803: INFO: Pod "pod-configmaps-11d2e9d9-e47f-4c74-b4cc-57d2a46ec46e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047270844s
STEP: Saw pod success 03/02/23 02:34:10.803
Mar  2 02:34:10.803: INFO: Pod "pod-configmaps-11d2e9d9-e47f-4c74-b4cc-57d2a46ec46e" satisfied condition "Succeeded or Failed"
Mar  2 02:34:10.824: INFO: Trying to get logs from node 10.132.92.143 pod pod-configmaps-11d2e9d9-e47f-4c74-b4cc-57d2a46ec46e container env-test: <nil>
STEP: delete the pod 03/02/23 02:34:10.872
Mar  2 02:34:10.932: INFO: Waiting for pod pod-configmaps-11d2e9d9-e47f-4c74-b4cc-57d2a46ec46e to disappear
Mar  2 02:34:10.960: INFO: Pod pod-configmaps-11d2e9d9-e47f-4c74-b4cc-57d2a46ec46e no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Mar  2 02:34:10.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5994" for this suite. 03/02/23 02:34:10.993
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","completed":276,"skipped":5037,"failed":0}
------------------------------
• [4.483 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:34:06.545
    Mar  2 02:34:06.546: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename configmap 03/02/23 02:34:06.548
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:34:06.611
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:34:06.626
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:92
    STEP: Creating configMap configmap-5994/configmap-test-3e79081d-ae25-40c5-9e0d-37b8cdb687a1 03/02/23 02:34:06.643
    STEP: Creating a pod to test consume configMaps 03/02/23 02:34:06.666
    Mar  2 02:34:06.755: INFO: Waiting up to 5m0s for pod "pod-configmaps-11d2e9d9-e47f-4c74-b4cc-57d2a46ec46e" in namespace "configmap-5994" to be "Succeeded or Failed"
    Mar  2 02:34:06.773: INFO: Pod "pod-configmaps-11d2e9d9-e47f-4c74-b4cc-57d2a46ec46e": Phase="Pending", Reason="", readiness=false. Elapsed: 17.099863ms
    Mar  2 02:34:08.791: INFO: Pod "pod-configmaps-11d2e9d9-e47f-4c74-b4cc-57d2a46ec46e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035131199s
    Mar  2 02:34:10.803: INFO: Pod "pod-configmaps-11d2e9d9-e47f-4c74-b4cc-57d2a46ec46e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047270844s
    STEP: Saw pod success 03/02/23 02:34:10.803
    Mar  2 02:34:10.803: INFO: Pod "pod-configmaps-11d2e9d9-e47f-4c74-b4cc-57d2a46ec46e" satisfied condition "Succeeded or Failed"
    Mar  2 02:34:10.824: INFO: Trying to get logs from node 10.132.92.143 pod pod-configmaps-11d2e9d9-e47f-4c74-b4cc-57d2a46ec46e container env-test: <nil>
    STEP: delete the pod 03/02/23 02:34:10.872
    Mar  2 02:34:10.932: INFO: Waiting for pod pod-configmaps-11d2e9d9-e47f-4c74-b4cc-57d2a46ec46e to disappear
    Mar  2 02:34:10.960: INFO: Pod pod-configmaps-11d2e9d9-e47f-4c74-b4cc-57d2a46ec46e no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  2 02:34:10.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-5994" for this suite. 03/02/23 02:34:10.993
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:34:11.032
Mar  2 02:34:11.032: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 03/02/23 02:34:11.033
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:34:11.183
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:34:11.215
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 03/02/23 02:34:11.248
STEP: Creating hostNetwork=false pod 03/02/23 02:34:11.248
Mar  2 02:34:11.341: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-5438" to be "running and ready"
Mar  2 02:34:11.396: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 54.805055ms
Mar  2 02:34:11.396: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Mar  2 02:34:13.410: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0683314s
Mar  2 02:34:13.410: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Mar  2 02:34:15.411: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.069667127s
Mar  2 02:34:15.411: INFO: The phase of Pod test-pod is Running (Ready = true)
Mar  2 02:34:15.411: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 03/02/23 02:34:15.425
Mar  2 02:34:15.469: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-5438" to be "running and ready"
Mar  2 02:34:15.493: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 24.750309ms
Mar  2 02:34:15.493: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Mar  2 02:34:17.508: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.03900956s
Mar  2 02:34:17.508: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Mar  2 02:34:17.508: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 03/02/23 02:34:17.521
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 03/02/23 02:34:17.521
Mar  2 02:34:17.521: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5438 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 02:34:17.521: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
Mar  2 02:34:17.522: INFO: ExecWithOptions: Clientset creation
Mar  2 02:34:17.522: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5438/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar  2 02:34:17.719: INFO: Exec stderr: ""
Mar  2 02:34:17.719: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5438 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 02:34:17.719: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
Mar  2 02:34:17.720: INFO: ExecWithOptions: Clientset creation
Mar  2 02:34:17.720: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5438/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar  2 02:34:17.924: INFO: Exec stderr: ""
Mar  2 02:34:17.924: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5438 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 02:34:17.924: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
Mar  2 02:34:17.925: INFO: ExecWithOptions: Clientset creation
Mar  2 02:34:17.925: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5438/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar  2 02:34:18.098: INFO: Exec stderr: ""
Mar  2 02:34:18.098: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5438 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 02:34:18.098: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
Mar  2 02:34:18.099: INFO: ExecWithOptions: Clientset creation
Mar  2 02:34:18.100: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5438/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar  2 02:34:18.299: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 03/02/23 02:34:18.299
Mar  2 02:34:18.299: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5438 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 02:34:18.299: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
Mar  2 02:34:18.300: INFO: ExecWithOptions: Clientset creation
Mar  2 02:34:18.300: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5438/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Mar  2 02:34:18.496: INFO: Exec stderr: ""
Mar  2 02:34:18.496: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5438 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 02:34:18.496: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
Mar  2 02:34:18.498: INFO: ExecWithOptions: Clientset creation
Mar  2 02:34:18.498: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5438/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Mar  2 02:34:18.663: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 03/02/23 02:34:18.663
Mar  2 02:34:18.663: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5438 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 02:34:18.663: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
Mar  2 02:34:18.664: INFO: ExecWithOptions: Clientset creation
Mar  2 02:34:18.664: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5438/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar  2 02:34:18.867: INFO: Exec stderr: ""
Mar  2 02:34:18.867: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5438 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 02:34:18.867: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
Mar  2 02:34:18.868: INFO: ExecWithOptions: Clientset creation
Mar  2 02:34:18.868: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5438/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar  2 02:34:19.065: INFO: Exec stderr: ""
Mar  2 02:34:19.065: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5438 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 02:34:19.065: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
Mar  2 02:34:19.066: INFO: ExecWithOptions: Clientset creation
Mar  2 02:34:19.066: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5438/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar  2 02:34:19.278: INFO: Exec stderr: ""
Mar  2 02:34:19.279: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5438 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 02:34:19.279: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
Mar  2 02:34:19.279: INFO: ExecWithOptions: Clientset creation
Mar  2 02:34:19.280: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5438/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar  2 02:34:19.478: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:187
Mar  2 02:34:19.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-5438" for this suite. 03/02/23 02:34:19.499
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","completed":277,"skipped":5082,"failed":0}
------------------------------
• [SLOW TEST] [8.492 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:34:11.032
    Mar  2 02:34:11.032: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 03/02/23 02:34:11.033
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:34:11.183
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:34:11.215
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 03/02/23 02:34:11.248
    STEP: Creating hostNetwork=false pod 03/02/23 02:34:11.248
    Mar  2 02:34:11.341: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-5438" to be "running and ready"
    Mar  2 02:34:11.396: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 54.805055ms
    Mar  2 02:34:11.396: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 02:34:13.410: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0683314s
    Mar  2 02:34:13.410: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 02:34:15.411: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.069667127s
    Mar  2 02:34:15.411: INFO: The phase of Pod test-pod is Running (Ready = true)
    Mar  2 02:34:15.411: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 03/02/23 02:34:15.425
    Mar  2 02:34:15.469: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-5438" to be "running and ready"
    Mar  2 02:34:15.493: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 24.750309ms
    Mar  2 02:34:15.493: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 02:34:17.508: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.03900956s
    Mar  2 02:34:17.508: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Mar  2 02:34:17.508: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 03/02/23 02:34:17.521
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 03/02/23 02:34:17.521
    Mar  2 02:34:17.521: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5438 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 02:34:17.521: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    Mar  2 02:34:17.522: INFO: ExecWithOptions: Clientset creation
    Mar  2 02:34:17.522: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5438/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Mar  2 02:34:17.719: INFO: Exec stderr: ""
    Mar  2 02:34:17.719: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5438 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 02:34:17.719: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    Mar  2 02:34:17.720: INFO: ExecWithOptions: Clientset creation
    Mar  2 02:34:17.720: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5438/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Mar  2 02:34:17.924: INFO: Exec stderr: ""
    Mar  2 02:34:17.924: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5438 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 02:34:17.924: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    Mar  2 02:34:17.925: INFO: ExecWithOptions: Clientset creation
    Mar  2 02:34:17.925: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5438/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Mar  2 02:34:18.098: INFO: Exec stderr: ""
    Mar  2 02:34:18.098: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5438 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 02:34:18.098: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    Mar  2 02:34:18.099: INFO: ExecWithOptions: Clientset creation
    Mar  2 02:34:18.100: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5438/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Mar  2 02:34:18.299: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 03/02/23 02:34:18.299
    Mar  2 02:34:18.299: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5438 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 02:34:18.299: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    Mar  2 02:34:18.300: INFO: ExecWithOptions: Clientset creation
    Mar  2 02:34:18.300: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5438/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Mar  2 02:34:18.496: INFO: Exec stderr: ""
    Mar  2 02:34:18.496: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5438 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 02:34:18.496: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    Mar  2 02:34:18.498: INFO: ExecWithOptions: Clientset creation
    Mar  2 02:34:18.498: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5438/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Mar  2 02:34:18.663: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 03/02/23 02:34:18.663
    Mar  2 02:34:18.663: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5438 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 02:34:18.663: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    Mar  2 02:34:18.664: INFO: ExecWithOptions: Clientset creation
    Mar  2 02:34:18.664: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5438/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Mar  2 02:34:18.867: INFO: Exec stderr: ""
    Mar  2 02:34:18.867: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5438 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 02:34:18.867: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    Mar  2 02:34:18.868: INFO: ExecWithOptions: Clientset creation
    Mar  2 02:34:18.868: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5438/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Mar  2 02:34:19.065: INFO: Exec stderr: ""
    Mar  2 02:34:19.065: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5438 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 02:34:19.065: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    Mar  2 02:34:19.066: INFO: ExecWithOptions: Clientset creation
    Mar  2 02:34:19.066: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5438/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Mar  2 02:34:19.278: INFO: Exec stderr: ""
    Mar  2 02:34:19.279: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5438 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 02:34:19.279: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    Mar  2 02:34:19.279: INFO: ExecWithOptions: Clientset creation
    Mar  2 02:34:19.280: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5438/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Mar  2 02:34:19.478: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:187
    Mar  2 02:34:19.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-5438" for this suite. 03/02/23 02:34:19.499
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:34:19.527
Mar  2 02:34:19.527: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename emptydir 03/02/23 02:34:19.528
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:34:19.59
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:34:19.606
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
STEP: Creating a pod to test emptydir 0644 on tmpfs 03/02/23 02:34:19.618
Mar  2 02:34:19.707: INFO: Waiting up to 5m0s for pod "pod-bc5fa481-40eb-4ef8-a1ca-552d63429180" in namespace "emptydir-9722" to be "Succeeded or Failed"
Mar  2 02:34:19.727: INFO: Pod "pod-bc5fa481-40eb-4ef8-a1ca-552d63429180": Phase="Pending", Reason="", readiness=false. Elapsed: 20.228634ms
Mar  2 02:34:21.743: INFO: Pod "pod-bc5fa481-40eb-4ef8-a1ca-552d63429180": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035405474s
Mar  2 02:34:23.742: INFO: Pod "pod-bc5fa481-40eb-4ef8-a1ca-552d63429180": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034957093s
STEP: Saw pod success 03/02/23 02:34:23.742
Mar  2 02:34:23.742: INFO: Pod "pod-bc5fa481-40eb-4ef8-a1ca-552d63429180" satisfied condition "Succeeded or Failed"
Mar  2 02:34:23.756: INFO: Trying to get logs from node 10.132.92.143 pod pod-bc5fa481-40eb-4ef8-a1ca-552d63429180 container test-container: <nil>
STEP: delete the pod 03/02/23 02:34:23.792
Mar  2 02:34:23.827: INFO: Waiting for pod pod-bc5fa481-40eb-4ef8-a1ca-552d63429180 to disappear
Mar  2 02:34:23.840: INFO: Pod pod-bc5fa481-40eb-4ef8-a1ca-552d63429180 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  2 02:34:23.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9722" for this suite. 03/02/23 02:34:23.862
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":278,"skipped":5109,"failed":0}
------------------------------
• [4.359 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:34:19.527
    Mar  2 02:34:19.527: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename emptydir 03/02/23 02:34:19.528
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:34:19.59
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:34:19.606
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:126
    STEP: Creating a pod to test emptydir 0644 on tmpfs 03/02/23 02:34:19.618
    Mar  2 02:34:19.707: INFO: Waiting up to 5m0s for pod "pod-bc5fa481-40eb-4ef8-a1ca-552d63429180" in namespace "emptydir-9722" to be "Succeeded or Failed"
    Mar  2 02:34:19.727: INFO: Pod "pod-bc5fa481-40eb-4ef8-a1ca-552d63429180": Phase="Pending", Reason="", readiness=false. Elapsed: 20.228634ms
    Mar  2 02:34:21.743: INFO: Pod "pod-bc5fa481-40eb-4ef8-a1ca-552d63429180": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035405474s
    Mar  2 02:34:23.742: INFO: Pod "pod-bc5fa481-40eb-4ef8-a1ca-552d63429180": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034957093s
    STEP: Saw pod success 03/02/23 02:34:23.742
    Mar  2 02:34:23.742: INFO: Pod "pod-bc5fa481-40eb-4ef8-a1ca-552d63429180" satisfied condition "Succeeded or Failed"
    Mar  2 02:34:23.756: INFO: Trying to get logs from node 10.132.92.143 pod pod-bc5fa481-40eb-4ef8-a1ca-552d63429180 container test-container: <nil>
    STEP: delete the pod 03/02/23 02:34:23.792
    Mar  2 02:34:23.827: INFO: Waiting for pod pod-bc5fa481-40eb-4ef8-a1ca-552d63429180 to disappear
    Mar  2 02:34:23.840: INFO: Pod pod-bc5fa481-40eb-4ef8-a1ca-552d63429180 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  2 02:34:23.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9722" for this suite. 03/02/23 02:34:23.862
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:34:23.896
Mar  2 02:34:23.896: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename var-expansion 03/02/23 02:34:23.897
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:34:23.956
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:34:23.97
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
STEP: Creating a pod to test env composition 03/02/23 02:34:23.981
Mar  2 02:34:24.040: INFO: Waiting up to 5m0s for pod "var-expansion-5b0f4c8c-20bd-4e59-b7ce-1f52cbbde2d3" in namespace "var-expansion-2187" to be "Succeeded or Failed"
Mar  2 02:34:24.055: INFO: Pod "var-expansion-5b0f4c8c-20bd-4e59-b7ce-1f52cbbde2d3": Phase="Pending", Reason="", readiness=false. Elapsed: 14.516373ms
Mar  2 02:34:26.070: INFO: Pod "var-expansion-5b0f4c8c-20bd-4e59-b7ce-1f52cbbde2d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029985088s
Mar  2 02:34:28.071: INFO: Pod "var-expansion-5b0f4c8c-20bd-4e59-b7ce-1f52cbbde2d3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03036699s
Mar  2 02:34:30.071: INFO: Pod "var-expansion-5b0f4c8c-20bd-4e59-b7ce-1f52cbbde2d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.03025798s
STEP: Saw pod success 03/02/23 02:34:30.071
Mar  2 02:34:30.071: INFO: Pod "var-expansion-5b0f4c8c-20bd-4e59-b7ce-1f52cbbde2d3" satisfied condition "Succeeded or Failed"
Mar  2 02:34:30.084: INFO: Trying to get logs from node 10.132.92.143 pod var-expansion-5b0f4c8c-20bd-4e59-b7ce-1f52cbbde2d3 container dapi-container: <nil>
STEP: delete the pod 03/02/23 02:34:30.117
Mar  2 02:34:30.153: INFO: Waiting for pod var-expansion-5b0f4c8c-20bd-4e59-b7ce-1f52cbbde2d3 to disappear
Mar  2 02:34:30.168: INFO: Pod var-expansion-5b0f4c8c-20bd-4e59-b7ce-1f52cbbde2d3 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar  2 02:34:30.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2187" for this suite. 03/02/23 02:34:30.197
{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","completed":279,"skipped":5140,"failed":0}
------------------------------
• [SLOW TEST] [6.350 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:34:23.896
    Mar  2 02:34:23.896: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename var-expansion 03/02/23 02:34:23.897
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:34:23.956
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:34:23.97
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:43
    STEP: Creating a pod to test env composition 03/02/23 02:34:23.981
    Mar  2 02:34:24.040: INFO: Waiting up to 5m0s for pod "var-expansion-5b0f4c8c-20bd-4e59-b7ce-1f52cbbde2d3" in namespace "var-expansion-2187" to be "Succeeded or Failed"
    Mar  2 02:34:24.055: INFO: Pod "var-expansion-5b0f4c8c-20bd-4e59-b7ce-1f52cbbde2d3": Phase="Pending", Reason="", readiness=false. Elapsed: 14.516373ms
    Mar  2 02:34:26.070: INFO: Pod "var-expansion-5b0f4c8c-20bd-4e59-b7ce-1f52cbbde2d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029985088s
    Mar  2 02:34:28.071: INFO: Pod "var-expansion-5b0f4c8c-20bd-4e59-b7ce-1f52cbbde2d3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03036699s
    Mar  2 02:34:30.071: INFO: Pod "var-expansion-5b0f4c8c-20bd-4e59-b7ce-1f52cbbde2d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.03025798s
    STEP: Saw pod success 03/02/23 02:34:30.071
    Mar  2 02:34:30.071: INFO: Pod "var-expansion-5b0f4c8c-20bd-4e59-b7ce-1f52cbbde2d3" satisfied condition "Succeeded or Failed"
    Mar  2 02:34:30.084: INFO: Trying to get logs from node 10.132.92.143 pod var-expansion-5b0f4c8c-20bd-4e59-b7ce-1f52cbbde2d3 container dapi-container: <nil>
    STEP: delete the pod 03/02/23 02:34:30.117
    Mar  2 02:34:30.153: INFO: Waiting for pod var-expansion-5b0f4c8c-20bd-4e59-b7ce-1f52cbbde2d3 to disappear
    Mar  2 02:34:30.168: INFO: Pod var-expansion-5b0f4c8c-20bd-4e59-b7ce-1f52cbbde2d3 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar  2 02:34:30.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-2187" for this suite. 03/02/23 02:34:30.197
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:34:30.248
Mar  2 02:34:30.248: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename secrets 03/02/23 02:34:30.249
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:34:30.309
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:34:30.323
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
STEP: Creating secret with name secret-test-map-d972f860-36fa-439e-8ab4-3fb7c8284030 03/02/23 02:34:30.337
STEP: Creating a pod to test consume secrets 03/02/23 02:34:30.357
Mar  2 02:34:30.432: INFO: Waiting up to 5m0s for pod "pod-secrets-ac39a4db-488d-4d5c-a295-9de52f597df4" in namespace "secrets-5958" to be "Succeeded or Failed"
Mar  2 02:34:30.469: INFO: Pod "pod-secrets-ac39a4db-488d-4d5c-a295-9de52f597df4": Phase="Pending", Reason="", readiness=false. Elapsed: 36.31418ms
Mar  2 02:34:32.485: INFO: Pod "pod-secrets-ac39a4db-488d-4d5c-a295-9de52f597df4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051983866s
Mar  2 02:34:34.484: INFO: Pod "pod-secrets-ac39a4db-488d-4d5c-a295-9de52f597df4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051063359s
Mar  2 02:34:36.494: INFO: Pod "pod-secrets-ac39a4db-488d-4d5c-a295-9de52f597df4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.061074025s
STEP: Saw pod success 03/02/23 02:34:36.494
Mar  2 02:34:36.494: INFO: Pod "pod-secrets-ac39a4db-488d-4d5c-a295-9de52f597df4" satisfied condition "Succeeded or Failed"
Mar  2 02:34:36.531: INFO: Trying to get logs from node 10.132.92.143 pod pod-secrets-ac39a4db-488d-4d5c-a295-9de52f597df4 container secret-volume-test: <nil>
STEP: delete the pod 03/02/23 02:34:36.565
Mar  2 02:34:36.652: INFO: Waiting for pod pod-secrets-ac39a4db-488d-4d5c-a295-9de52f597df4 to disappear
Mar  2 02:34:36.711: INFO: Pod pod-secrets-ac39a4db-488d-4d5c-a295-9de52f597df4 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar  2 02:34:36.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5958" for this suite. 03/02/23 02:34:36.799
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":280,"skipped":5154,"failed":0}
------------------------------
• [SLOW TEST] [6.657 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:34:30.248
    Mar  2 02:34:30.248: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename secrets 03/02/23 02:34:30.249
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:34:30.309
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:34:30.323
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:78
    STEP: Creating secret with name secret-test-map-d972f860-36fa-439e-8ab4-3fb7c8284030 03/02/23 02:34:30.337
    STEP: Creating a pod to test consume secrets 03/02/23 02:34:30.357
    Mar  2 02:34:30.432: INFO: Waiting up to 5m0s for pod "pod-secrets-ac39a4db-488d-4d5c-a295-9de52f597df4" in namespace "secrets-5958" to be "Succeeded or Failed"
    Mar  2 02:34:30.469: INFO: Pod "pod-secrets-ac39a4db-488d-4d5c-a295-9de52f597df4": Phase="Pending", Reason="", readiness=false. Elapsed: 36.31418ms
    Mar  2 02:34:32.485: INFO: Pod "pod-secrets-ac39a4db-488d-4d5c-a295-9de52f597df4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051983866s
    Mar  2 02:34:34.484: INFO: Pod "pod-secrets-ac39a4db-488d-4d5c-a295-9de52f597df4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051063359s
    Mar  2 02:34:36.494: INFO: Pod "pod-secrets-ac39a4db-488d-4d5c-a295-9de52f597df4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.061074025s
    STEP: Saw pod success 03/02/23 02:34:36.494
    Mar  2 02:34:36.494: INFO: Pod "pod-secrets-ac39a4db-488d-4d5c-a295-9de52f597df4" satisfied condition "Succeeded or Failed"
    Mar  2 02:34:36.531: INFO: Trying to get logs from node 10.132.92.143 pod pod-secrets-ac39a4db-488d-4d5c-a295-9de52f597df4 container secret-volume-test: <nil>
    STEP: delete the pod 03/02/23 02:34:36.565
    Mar  2 02:34:36.652: INFO: Waiting for pod pod-secrets-ac39a4db-488d-4d5c-a295-9de52f597df4 to disappear
    Mar  2 02:34:36.711: INFO: Pod pod-secrets-ac39a4db-488d-4d5c-a295-9de52f597df4 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar  2 02:34:36.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5958" for this suite. 03/02/23 02:34:36.799
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:34:36.907
Mar  2 02:34:36.907: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename svcaccounts 03/02/23 02:34:36.909
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:34:36.995
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:34:37.012
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
Mar  2 02:34:37.151: INFO: Waiting up to 5m0s for pod "pod-service-account-2d296f13-94c9-436c-b0ff-8cdc50695e88" in namespace "svcaccounts-8802" to be "running"
Mar  2 02:34:37.173: INFO: Pod "pod-service-account-2d296f13-94c9-436c-b0ff-8cdc50695e88": Phase="Pending", Reason="", readiness=false. Elapsed: 22.560288ms
Mar  2 02:34:39.192: INFO: Pod "pod-service-account-2d296f13-94c9-436c-b0ff-8cdc50695e88": Phase="Running", Reason="", readiness=true. Elapsed: 2.04092519s
Mar  2 02:34:39.192: INFO: Pod "pod-service-account-2d296f13-94c9-436c-b0ff-8cdc50695e88" satisfied condition "running"
STEP: reading a file in the container 03/02/23 02:34:39.192
Mar  2 02:34:39.192: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8802 pod-service-account-2d296f13-94c9-436c-b0ff-8cdc50695e88 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 03/02/23 02:34:39.629
Mar  2 02:34:39.629: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8802 pod-service-account-2d296f13-94c9-436c-b0ff-8cdc50695e88 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 03/02/23 02:34:39.942
Mar  2 02:34:39.942: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8802 pod-service-account-2d296f13-94c9-436c-b0ff-8cdc50695e88 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Mar  2 02:34:40.289: INFO: Got root ca configmap in namespace "svcaccounts-8802"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Mar  2 02:34:40.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8802" for this suite. 03/02/23 02:34:40.322
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","completed":281,"skipped":5159,"failed":0}
------------------------------
• [3.442 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:34:36.907
    Mar  2 02:34:36.907: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename svcaccounts 03/02/23 02:34:36.909
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:34:36.995
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:34:37.012
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:75
    Mar  2 02:34:37.151: INFO: Waiting up to 5m0s for pod "pod-service-account-2d296f13-94c9-436c-b0ff-8cdc50695e88" in namespace "svcaccounts-8802" to be "running"
    Mar  2 02:34:37.173: INFO: Pod "pod-service-account-2d296f13-94c9-436c-b0ff-8cdc50695e88": Phase="Pending", Reason="", readiness=false. Elapsed: 22.560288ms
    Mar  2 02:34:39.192: INFO: Pod "pod-service-account-2d296f13-94c9-436c-b0ff-8cdc50695e88": Phase="Running", Reason="", readiness=true. Elapsed: 2.04092519s
    Mar  2 02:34:39.192: INFO: Pod "pod-service-account-2d296f13-94c9-436c-b0ff-8cdc50695e88" satisfied condition "running"
    STEP: reading a file in the container 03/02/23 02:34:39.192
    Mar  2 02:34:39.192: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8802 pod-service-account-2d296f13-94c9-436c-b0ff-8cdc50695e88 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 03/02/23 02:34:39.629
    Mar  2 02:34:39.629: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8802 pod-service-account-2d296f13-94c9-436c-b0ff-8cdc50695e88 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 03/02/23 02:34:39.942
    Mar  2 02:34:39.942: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8802 pod-service-account-2d296f13-94c9-436c-b0ff-8cdc50695e88 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Mar  2 02:34:40.289: INFO: Got root ca configmap in namespace "svcaccounts-8802"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Mar  2 02:34:40.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-8802" for this suite. 03/02/23 02:34:40.322
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:34:40.349
Mar  2 02:34:40.349: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename emptydir 03/02/23 02:34:40.351
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:34:40.419
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:34:40.433
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
STEP: Creating a pod to test emptydir 0644 on node default medium 03/02/23 02:34:40.449
Mar  2 02:34:40.524: INFO: Waiting up to 5m0s for pod "pod-6ae5552f-32a2-463e-b2cd-e467dbc0843a" in namespace "emptydir-3935" to be "Succeeded or Failed"
Mar  2 02:34:40.552: INFO: Pod "pod-6ae5552f-32a2-463e-b2cd-e467dbc0843a": Phase="Pending", Reason="", readiness=false. Elapsed: 28.293597ms
Mar  2 02:34:42.572: INFO: Pod "pod-6ae5552f-32a2-463e-b2cd-e467dbc0843a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048165544s
Mar  2 02:34:44.570: INFO: Pod "pod-6ae5552f-32a2-463e-b2cd-e467dbc0843a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046118222s
Mar  2 02:34:46.572: INFO: Pod "pod-6ae5552f-32a2-463e-b2cd-e467dbc0843a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.047929819s
STEP: Saw pod success 03/02/23 02:34:46.572
Mar  2 02:34:46.572: INFO: Pod "pod-6ae5552f-32a2-463e-b2cd-e467dbc0843a" satisfied condition "Succeeded or Failed"
Mar  2 02:34:46.589: INFO: Trying to get logs from node 10.132.92.143 pod pod-6ae5552f-32a2-463e-b2cd-e467dbc0843a container test-container: <nil>
STEP: delete the pod 03/02/23 02:34:46.673
Mar  2 02:34:46.715: INFO: Waiting for pod pod-6ae5552f-32a2-463e-b2cd-e467dbc0843a to disappear
Mar  2 02:34:46.747: INFO: Pod pod-6ae5552f-32a2-463e-b2cd-e467dbc0843a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  2 02:34:46.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3935" for this suite. 03/02/23 02:34:46.769
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":282,"skipped":5166,"failed":0}
------------------------------
• [SLOW TEST] [6.491 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:34:40.349
    Mar  2 02:34:40.349: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename emptydir 03/02/23 02:34:40.351
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:34:40.419
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:34:40.433
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:196
    STEP: Creating a pod to test emptydir 0644 on node default medium 03/02/23 02:34:40.449
    Mar  2 02:34:40.524: INFO: Waiting up to 5m0s for pod "pod-6ae5552f-32a2-463e-b2cd-e467dbc0843a" in namespace "emptydir-3935" to be "Succeeded or Failed"
    Mar  2 02:34:40.552: INFO: Pod "pod-6ae5552f-32a2-463e-b2cd-e467dbc0843a": Phase="Pending", Reason="", readiness=false. Elapsed: 28.293597ms
    Mar  2 02:34:42.572: INFO: Pod "pod-6ae5552f-32a2-463e-b2cd-e467dbc0843a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048165544s
    Mar  2 02:34:44.570: INFO: Pod "pod-6ae5552f-32a2-463e-b2cd-e467dbc0843a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046118222s
    Mar  2 02:34:46.572: INFO: Pod "pod-6ae5552f-32a2-463e-b2cd-e467dbc0843a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.047929819s
    STEP: Saw pod success 03/02/23 02:34:46.572
    Mar  2 02:34:46.572: INFO: Pod "pod-6ae5552f-32a2-463e-b2cd-e467dbc0843a" satisfied condition "Succeeded or Failed"
    Mar  2 02:34:46.589: INFO: Trying to get logs from node 10.132.92.143 pod pod-6ae5552f-32a2-463e-b2cd-e467dbc0843a container test-container: <nil>
    STEP: delete the pod 03/02/23 02:34:46.673
    Mar  2 02:34:46.715: INFO: Waiting for pod pod-6ae5552f-32a2-463e-b2cd-e467dbc0843a to disappear
    Mar  2 02:34:46.747: INFO: Pod pod-6ae5552f-32a2-463e-b2cd-e467dbc0843a no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  2 02:34:46.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-3935" for this suite. 03/02/23 02:34:46.769
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:34:46.841
Mar  2 02:34:46.841: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename webhook 03/02/23 02:34:46.842
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:34:46.9
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:34:46.913
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/02/23 02:34:47.078
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 02:34:47.889
STEP: Deploying the webhook pod 03/02/23 02:34:47.931
STEP: Wait for the deployment to be ready 03/02/23 02:34:48.058
Mar  2 02:34:48.093: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 02:34:50.135: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 2, 34, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 34, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 2, 34, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 34, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/02/23 02:34:52.167
STEP: Verifying the service has paired with the endpoint 03/02/23 02:34:52.217
Mar  2 02:34:53.218: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
Mar  2 02:34:53.235: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9942-crds.webhook.example.com via the AdmissionRegistration API 03/02/23 02:34:53.775
STEP: Creating a custom resource that should be mutated by the webhook 03/02/23 02:34:53.846
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 02:34:56.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-167" for this suite. 03/02/23 02:34:56.661
STEP: Destroying namespace "webhook-167-markers" for this suite. 03/02/23 02:34:56.686
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","completed":283,"skipped":5168,"failed":0}
------------------------------
• [SLOW TEST] [10.059 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:34:46.841
    Mar  2 02:34:46.841: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename webhook 03/02/23 02:34:46.842
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:34:46.9
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:34:46.913
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/02/23 02:34:47.078
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 02:34:47.889
    STEP: Deploying the webhook pod 03/02/23 02:34:47.931
    STEP: Wait for the deployment to be ready 03/02/23 02:34:48.058
    Mar  2 02:34:48.093: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Mar  2 02:34:50.135: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 2, 34, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 34, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 2, 34, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 34, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/02/23 02:34:52.167
    STEP: Verifying the service has paired with the endpoint 03/02/23 02:34:52.217
    Mar  2 02:34:53.218: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:290
    Mar  2 02:34:53.235: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9942-crds.webhook.example.com via the AdmissionRegistration API 03/02/23 02:34:53.775
    STEP: Creating a custom resource that should be mutated by the webhook 03/02/23 02:34:53.846
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 02:34:56.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-167" for this suite. 03/02/23 02:34:56.661
    STEP: Destroying namespace "webhook-167-markers" for this suite. 03/02/23 02:34:56.686
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:34:56.901
Mar  2 02:34:56.901: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename emptydir 03/02/23 02:34:56.903
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:34:57.007
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:34:57.026
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
STEP: Creating Pod 03/02/23 02:34:57.046
Mar  2 02:34:57.113: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-23777967-d05d-42f7-9c64-fff04a9586d4" in namespace "emptydir-1266" to be "running"
Mar  2 02:34:57.169: INFO: Pod "pod-sharedvolume-23777967-d05d-42f7-9c64-fff04a9586d4": Phase="Pending", Reason="", readiness=false. Elapsed: 55.637667ms
Mar  2 02:34:59.184: INFO: Pod "pod-sharedvolume-23777967-d05d-42f7-9c64-fff04a9586d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.07128792s
Mar  2 02:35:01.185: INFO: Pod "pod-sharedvolume-23777967-d05d-42f7-9c64-fff04a9586d4": Phase="Running", Reason="", readiness=false. Elapsed: 4.071468334s
Mar  2 02:35:01.185: INFO: Pod "pod-sharedvolume-23777967-d05d-42f7-9c64-fff04a9586d4" satisfied condition "running"
STEP: Reading file content from the nginx-container 03/02/23 02:35:01.185
Mar  2 02:35:01.185: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-1266 PodName:pod-sharedvolume-23777967-d05d-42f7-9c64-fff04a9586d4 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 02:35:01.185: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
Mar  2 02:35:01.185: INFO: ExecWithOptions: Clientset creation
Mar  2 02:35:01.185: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/emptydir-1266/pods/pod-sharedvolume-23777967-d05d-42f7-9c64-fff04a9586d4/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Mar  2 02:35:01.391: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  2 02:35:01.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1266" for this suite. 03/02/23 02:35:01.418
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","completed":284,"skipped":5172,"failed":0}
------------------------------
• [4.542 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:34:56.901
    Mar  2 02:34:56.901: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename emptydir 03/02/23 02:34:56.903
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:34:57.007
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:34:57.026
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:226
    STEP: Creating Pod 03/02/23 02:34:57.046
    Mar  2 02:34:57.113: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-23777967-d05d-42f7-9c64-fff04a9586d4" in namespace "emptydir-1266" to be "running"
    Mar  2 02:34:57.169: INFO: Pod "pod-sharedvolume-23777967-d05d-42f7-9c64-fff04a9586d4": Phase="Pending", Reason="", readiness=false. Elapsed: 55.637667ms
    Mar  2 02:34:59.184: INFO: Pod "pod-sharedvolume-23777967-d05d-42f7-9c64-fff04a9586d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.07128792s
    Mar  2 02:35:01.185: INFO: Pod "pod-sharedvolume-23777967-d05d-42f7-9c64-fff04a9586d4": Phase="Running", Reason="", readiness=false. Elapsed: 4.071468334s
    Mar  2 02:35:01.185: INFO: Pod "pod-sharedvolume-23777967-d05d-42f7-9c64-fff04a9586d4" satisfied condition "running"
    STEP: Reading file content from the nginx-container 03/02/23 02:35:01.185
    Mar  2 02:35:01.185: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-1266 PodName:pod-sharedvolume-23777967-d05d-42f7-9c64-fff04a9586d4 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar  2 02:35:01.185: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    Mar  2 02:35:01.185: INFO: ExecWithOptions: Clientset creation
    Mar  2 02:35:01.185: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/emptydir-1266/pods/pod-sharedvolume-23777967-d05d-42f7-9c64-fff04a9586d4/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Mar  2 02:35:01.391: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  2 02:35:01.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1266" for this suite. 03/02/23 02:35:01.418
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:35:01.444
Mar  2 02:35:01.444: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename configmap 03/02/23 02:35:01.446
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:35:01.536
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:35:01.555
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
STEP: Creating configMap with name configmap-test-volume-d93e3223-4d33-4d6b-bfd2-58d7cc204266 03/02/23 02:35:01.567
STEP: Creating a pod to test consume configMaps 03/02/23 02:35:01.611
Mar  2 02:35:01.701: INFO: Waiting up to 5m0s for pod "pod-configmaps-b68b1df8-0d57-4a07-a84e-ea156c09ad0f" in namespace "configmap-299" to be "Succeeded or Failed"
Mar  2 02:35:01.717: INFO: Pod "pod-configmaps-b68b1df8-0d57-4a07-a84e-ea156c09ad0f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.8369ms
Mar  2 02:35:03.750: INFO: Pod "pod-configmaps-b68b1df8-0d57-4a07-a84e-ea156c09ad0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049121555s
Mar  2 02:35:05.753: INFO: Pod "pod-configmaps-b68b1df8-0d57-4a07-a84e-ea156c09ad0f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052974959s
Mar  2 02:35:07.739: INFO: Pod "pod-configmaps-b68b1df8-0d57-4a07-a84e-ea156c09ad0f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038186291s
STEP: Saw pod success 03/02/23 02:35:07.739
Mar  2 02:35:07.739: INFO: Pod "pod-configmaps-b68b1df8-0d57-4a07-a84e-ea156c09ad0f" satisfied condition "Succeeded or Failed"
Mar  2 02:35:07.751: INFO: Trying to get logs from node 10.132.92.143 pod pod-configmaps-b68b1df8-0d57-4a07-a84e-ea156c09ad0f container agnhost-container: <nil>
STEP: delete the pod 03/02/23 02:35:07.784
Mar  2 02:35:07.823: INFO: Waiting for pod pod-configmaps-b68b1df8-0d57-4a07-a84e-ea156c09ad0f to disappear
Mar  2 02:35:07.834: INFO: Pod pod-configmaps-b68b1df8-0d57-4a07-a84e-ea156c09ad0f no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar  2 02:35:07.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-299" for this suite. 03/02/23 02:35:07.853
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":285,"skipped":5187,"failed":0}
------------------------------
• [SLOW TEST] [6.432 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:35:01.444
    Mar  2 02:35:01.444: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename configmap 03/02/23 02:35:01.446
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:35:01.536
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:35:01.555
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:46
    STEP: Creating configMap with name configmap-test-volume-d93e3223-4d33-4d6b-bfd2-58d7cc204266 03/02/23 02:35:01.567
    STEP: Creating a pod to test consume configMaps 03/02/23 02:35:01.611
    Mar  2 02:35:01.701: INFO: Waiting up to 5m0s for pod "pod-configmaps-b68b1df8-0d57-4a07-a84e-ea156c09ad0f" in namespace "configmap-299" to be "Succeeded or Failed"
    Mar  2 02:35:01.717: INFO: Pod "pod-configmaps-b68b1df8-0d57-4a07-a84e-ea156c09ad0f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.8369ms
    Mar  2 02:35:03.750: INFO: Pod "pod-configmaps-b68b1df8-0d57-4a07-a84e-ea156c09ad0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049121555s
    Mar  2 02:35:05.753: INFO: Pod "pod-configmaps-b68b1df8-0d57-4a07-a84e-ea156c09ad0f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052974959s
    Mar  2 02:35:07.739: INFO: Pod "pod-configmaps-b68b1df8-0d57-4a07-a84e-ea156c09ad0f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038186291s
    STEP: Saw pod success 03/02/23 02:35:07.739
    Mar  2 02:35:07.739: INFO: Pod "pod-configmaps-b68b1df8-0d57-4a07-a84e-ea156c09ad0f" satisfied condition "Succeeded or Failed"
    Mar  2 02:35:07.751: INFO: Trying to get logs from node 10.132.92.143 pod pod-configmaps-b68b1df8-0d57-4a07-a84e-ea156c09ad0f container agnhost-container: <nil>
    STEP: delete the pod 03/02/23 02:35:07.784
    Mar  2 02:35:07.823: INFO: Waiting for pod pod-configmaps-b68b1df8-0d57-4a07-a84e-ea156c09ad0f to disappear
    Mar  2 02:35:07.834: INFO: Pod pod-configmaps-b68b1df8-0d57-4a07-a84e-ea156c09ad0f no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  2 02:35:07.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-299" for this suite. 03/02/23 02:35:07.853
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:35:07.878
Mar  2 02:35:07.878: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename projected 03/02/23 02:35:07.879
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:35:07.934
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:35:07.949
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
STEP: Creating a pod to test downward API volume plugin 03/02/23 02:35:07.961
Mar  2 02:35:08.021: INFO: Waiting up to 5m0s for pod "downwardapi-volume-60774e0c-90cf-4396-bf72-dece3bb61dbd" in namespace "projected-3833" to be "Succeeded or Failed"
Mar  2 02:35:08.050: INFO: Pod "downwardapi-volume-60774e0c-90cf-4396-bf72-dece3bb61dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 29.473212ms
Mar  2 02:35:10.077: INFO: Pod "downwardapi-volume-60774e0c-90cf-4396-bf72-dece3bb61dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056365093s
Mar  2 02:35:12.068: INFO: Pod "downwardapi-volume-60774e0c-90cf-4396-bf72-dece3bb61dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047918825s
Mar  2 02:35:14.083: INFO: Pod "downwardapi-volume-60774e0c-90cf-4396-bf72-dece3bb61dbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.06239102s
STEP: Saw pod success 03/02/23 02:35:14.083
Mar  2 02:35:14.083: INFO: Pod "downwardapi-volume-60774e0c-90cf-4396-bf72-dece3bb61dbd" satisfied condition "Succeeded or Failed"
Mar  2 02:35:14.102: INFO: Trying to get logs from node 10.132.92.143 pod downwardapi-volume-60774e0c-90cf-4396-bf72-dece3bb61dbd container client-container: <nil>
STEP: delete the pod 03/02/23 02:35:14.139
Mar  2 02:35:14.198: INFO: Waiting for pod downwardapi-volume-60774e0c-90cf-4396-bf72-dece3bb61dbd to disappear
Mar  2 02:35:14.232: INFO: Pod downwardapi-volume-60774e0c-90cf-4396-bf72-dece3bb61dbd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar  2 02:35:14.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3833" for this suite. 03/02/23 02:35:14.267
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","completed":286,"skipped":5208,"failed":0}
------------------------------
• [SLOW TEST] [6.418 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:35:07.878
    Mar  2 02:35:07.878: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename projected 03/02/23 02:35:07.879
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:35:07.934
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:35:07.949
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:52
    STEP: Creating a pod to test downward API volume plugin 03/02/23 02:35:07.961
    Mar  2 02:35:08.021: INFO: Waiting up to 5m0s for pod "downwardapi-volume-60774e0c-90cf-4396-bf72-dece3bb61dbd" in namespace "projected-3833" to be "Succeeded or Failed"
    Mar  2 02:35:08.050: INFO: Pod "downwardapi-volume-60774e0c-90cf-4396-bf72-dece3bb61dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 29.473212ms
    Mar  2 02:35:10.077: INFO: Pod "downwardapi-volume-60774e0c-90cf-4396-bf72-dece3bb61dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056365093s
    Mar  2 02:35:12.068: INFO: Pod "downwardapi-volume-60774e0c-90cf-4396-bf72-dece3bb61dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047918825s
    Mar  2 02:35:14.083: INFO: Pod "downwardapi-volume-60774e0c-90cf-4396-bf72-dece3bb61dbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.06239102s
    STEP: Saw pod success 03/02/23 02:35:14.083
    Mar  2 02:35:14.083: INFO: Pod "downwardapi-volume-60774e0c-90cf-4396-bf72-dece3bb61dbd" satisfied condition "Succeeded or Failed"
    Mar  2 02:35:14.102: INFO: Trying to get logs from node 10.132.92.143 pod downwardapi-volume-60774e0c-90cf-4396-bf72-dece3bb61dbd container client-container: <nil>
    STEP: delete the pod 03/02/23 02:35:14.139
    Mar  2 02:35:14.198: INFO: Waiting for pod downwardapi-volume-60774e0c-90cf-4396-bf72-dece3bb61dbd to disappear
    Mar  2 02:35:14.232: INFO: Pod downwardapi-volume-60774e0c-90cf-4396-bf72-dece3bb61dbd no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar  2 02:35:14.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3833" for this suite. 03/02/23 02:35:14.267
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:35:14.3
Mar  2 02:35:14.300: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename proxy 03/02/23 02:35:14.301
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:35:14.371
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:35:14.385
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Mar  2 02:35:14.399: INFO: Creating pod...
Mar  2 02:35:14.537: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-4062" to be "running"
Mar  2 02:35:14.558: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 21.866642ms
Mar  2 02:35:16.580: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04372077s
Mar  2 02:35:18.573: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.036341648s
Mar  2 02:35:18.573: INFO: Pod "agnhost" satisfied condition "running"
Mar  2 02:35:18.573: INFO: Creating service...
Mar  2 02:35:18.610: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4062/pods/agnhost/proxy?method=DELETE
Mar  2 02:35:18.641: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar  2 02:35:18.641: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4062/pods/agnhost/proxy?method=OPTIONS
Mar  2 02:35:18.665: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar  2 02:35:18.665: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4062/pods/agnhost/proxy?method=PATCH
Mar  2 02:35:18.689: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar  2 02:35:18.689: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4062/pods/agnhost/proxy?method=POST
Mar  2 02:35:18.711: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar  2 02:35:18.711: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4062/pods/agnhost/proxy?method=PUT
Mar  2 02:35:18.732: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Mar  2 02:35:18.732: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4062/services/e2e-proxy-test-service/proxy?method=DELETE
Mar  2 02:35:18.765: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar  2 02:35:18.765: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4062/services/e2e-proxy-test-service/proxy?method=OPTIONS
Mar  2 02:35:18.803: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar  2 02:35:18.803: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4062/services/e2e-proxy-test-service/proxy?method=PATCH
Mar  2 02:35:18.843: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar  2 02:35:18.843: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4062/services/e2e-proxy-test-service/proxy?method=POST
Mar  2 02:35:18.876: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar  2 02:35:18.876: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4062/services/e2e-proxy-test-service/proxy?method=PUT
Mar  2 02:35:18.903: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Mar  2 02:35:18.903: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4062/pods/agnhost/proxy?method=GET
Mar  2 02:35:18.916: INFO: http.Client request:GET StatusCode:301
Mar  2 02:35:18.916: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4062/services/e2e-proxy-test-service/proxy?method=GET
Mar  2 02:35:18.937: INFO: http.Client request:GET StatusCode:301
Mar  2 02:35:18.937: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4062/pods/agnhost/proxy?method=HEAD
Mar  2 02:35:18.950: INFO: http.Client request:HEAD StatusCode:301
Mar  2 02:35:18.950: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4062/services/e2e-proxy-test-service/proxy?method=HEAD
Mar  2 02:35:18.969: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Mar  2 02:35:18.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4062" for this suite. 03/02/23 02:35:18.989
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]","completed":287,"skipped":5230,"failed":0}
------------------------------
• [4.714 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:35:14.3
    Mar  2 02:35:14.300: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename proxy 03/02/23 02:35:14.301
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:35:14.371
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:35:14.385
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Mar  2 02:35:14.399: INFO: Creating pod...
    Mar  2 02:35:14.537: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-4062" to be "running"
    Mar  2 02:35:14.558: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 21.866642ms
    Mar  2 02:35:16.580: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04372077s
    Mar  2 02:35:18.573: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.036341648s
    Mar  2 02:35:18.573: INFO: Pod "agnhost" satisfied condition "running"
    Mar  2 02:35:18.573: INFO: Creating service...
    Mar  2 02:35:18.610: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4062/pods/agnhost/proxy?method=DELETE
    Mar  2 02:35:18.641: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Mar  2 02:35:18.641: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4062/pods/agnhost/proxy?method=OPTIONS
    Mar  2 02:35:18.665: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Mar  2 02:35:18.665: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4062/pods/agnhost/proxy?method=PATCH
    Mar  2 02:35:18.689: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Mar  2 02:35:18.689: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4062/pods/agnhost/proxy?method=POST
    Mar  2 02:35:18.711: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Mar  2 02:35:18.711: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4062/pods/agnhost/proxy?method=PUT
    Mar  2 02:35:18.732: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Mar  2 02:35:18.732: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4062/services/e2e-proxy-test-service/proxy?method=DELETE
    Mar  2 02:35:18.765: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Mar  2 02:35:18.765: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4062/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Mar  2 02:35:18.803: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Mar  2 02:35:18.803: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4062/services/e2e-proxy-test-service/proxy?method=PATCH
    Mar  2 02:35:18.843: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Mar  2 02:35:18.843: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4062/services/e2e-proxy-test-service/proxy?method=POST
    Mar  2 02:35:18.876: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Mar  2 02:35:18.876: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4062/services/e2e-proxy-test-service/proxy?method=PUT
    Mar  2 02:35:18.903: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Mar  2 02:35:18.903: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4062/pods/agnhost/proxy?method=GET
    Mar  2 02:35:18.916: INFO: http.Client request:GET StatusCode:301
    Mar  2 02:35:18.916: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4062/services/e2e-proxy-test-service/proxy?method=GET
    Mar  2 02:35:18.937: INFO: http.Client request:GET StatusCode:301
    Mar  2 02:35:18.937: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4062/pods/agnhost/proxy?method=HEAD
    Mar  2 02:35:18.950: INFO: http.Client request:HEAD StatusCode:301
    Mar  2 02:35:18.950: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4062/services/e2e-proxy-test-service/proxy?method=HEAD
    Mar  2 02:35:18.969: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Mar  2 02:35:18.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-4062" for this suite. 03/02/23 02:35:18.989
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:35:19.016
Mar  2 02:35:19.016: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename projected 03/02/23 02:35:19.017
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:35:19.075
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:35:19.086
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
Mar  2 02:35:19.126: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-b70d277f-74e7-4962-b7bf-0af63a2d8b84 03/02/23 02:35:19.126
STEP: Creating configMap with name cm-test-opt-upd-dba872c1-84c6-473e-8e94-782d6385d8a4 03/02/23 02:35:19.156
STEP: Creating the pod 03/02/23 02:35:19.181
Mar  2 02:35:19.366: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e9ba963e-22e5-49d1-8779-0b51832e907f" in namespace "projected-1777" to be "running and ready"
Mar  2 02:35:19.392: INFO: Pod "pod-projected-configmaps-e9ba963e-22e5-49d1-8779-0b51832e907f": Phase="Pending", Reason="", readiness=false. Elapsed: 25.133868ms
Mar  2 02:35:19.392: INFO: The phase of Pod pod-projected-configmaps-e9ba963e-22e5-49d1-8779-0b51832e907f is Pending, waiting for it to be Running (with Ready = true)
Mar  2 02:35:21.411: INFO: Pod "pod-projected-configmaps-e9ba963e-22e5-49d1-8779-0b51832e907f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044816899s
Mar  2 02:35:21.411: INFO: The phase of Pod pod-projected-configmaps-e9ba963e-22e5-49d1-8779-0b51832e907f is Pending, waiting for it to be Running (with Ready = true)
Mar  2 02:35:23.406: INFO: Pod "pod-projected-configmaps-e9ba963e-22e5-49d1-8779-0b51832e907f": Phase="Running", Reason="", readiness=true. Elapsed: 4.03999941s
Mar  2 02:35:23.406: INFO: The phase of Pod pod-projected-configmaps-e9ba963e-22e5-49d1-8779-0b51832e907f is Running (Ready = true)
Mar  2 02:35:23.406: INFO: Pod "pod-projected-configmaps-e9ba963e-22e5-49d1-8779-0b51832e907f" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-b70d277f-74e7-4962-b7bf-0af63a2d8b84 03/02/23 02:35:23.52
STEP: Updating configmap cm-test-opt-upd-dba872c1-84c6-473e-8e94-782d6385d8a4 03/02/23 02:35:23.551
STEP: Creating configMap with name cm-test-opt-create-01997bf6-d95e-4a70-b63b-2abde4bb4bd0 03/02/23 02:35:23.573
STEP: waiting to observe update in volume 03/02/23 02:35:23.593
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar  2 02:36:33.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1777" for this suite. 03/02/23 02:36:33.393
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":288,"skipped":5243,"failed":0}
------------------------------
• [SLOW TEST] [74.404 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:35:19.016
    Mar  2 02:35:19.016: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename projected 03/02/23 02:35:19.017
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:35:19.075
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:35:19.086
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:173
    Mar  2 02:35:19.126: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating configMap with name cm-test-opt-del-b70d277f-74e7-4962-b7bf-0af63a2d8b84 03/02/23 02:35:19.126
    STEP: Creating configMap with name cm-test-opt-upd-dba872c1-84c6-473e-8e94-782d6385d8a4 03/02/23 02:35:19.156
    STEP: Creating the pod 03/02/23 02:35:19.181
    Mar  2 02:35:19.366: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e9ba963e-22e5-49d1-8779-0b51832e907f" in namespace "projected-1777" to be "running and ready"
    Mar  2 02:35:19.392: INFO: Pod "pod-projected-configmaps-e9ba963e-22e5-49d1-8779-0b51832e907f": Phase="Pending", Reason="", readiness=false. Elapsed: 25.133868ms
    Mar  2 02:35:19.392: INFO: The phase of Pod pod-projected-configmaps-e9ba963e-22e5-49d1-8779-0b51832e907f is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 02:35:21.411: INFO: Pod "pod-projected-configmaps-e9ba963e-22e5-49d1-8779-0b51832e907f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044816899s
    Mar  2 02:35:21.411: INFO: The phase of Pod pod-projected-configmaps-e9ba963e-22e5-49d1-8779-0b51832e907f is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 02:35:23.406: INFO: Pod "pod-projected-configmaps-e9ba963e-22e5-49d1-8779-0b51832e907f": Phase="Running", Reason="", readiness=true. Elapsed: 4.03999941s
    Mar  2 02:35:23.406: INFO: The phase of Pod pod-projected-configmaps-e9ba963e-22e5-49d1-8779-0b51832e907f is Running (Ready = true)
    Mar  2 02:35:23.406: INFO: Pod "pod-projected-configmaps-e9ba963e-22e5-49d1-8779-0b51832e907f" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-b70d277f-74e7-4962-b7bf-0af63a2d8b84 03/02/23 02:35:23.52
    STEP: Updating configmap cm-test-opt-upd-dba872c1-84c6-473e-8e94-782d6385d8a4 03/02/23 02:35:23.551
    STEP: Creating configMap with name cm-test-opt-create-01997bf6-d95e-4a70-b63b-2abde4bb4bd0 03/02/23 02:35:23.573
    STEP: waiting to observe update in volume 03/02/23 02:35:23.593
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar  2 02:36:33.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1777" for this suite. 03/02/23 02:36:33.393
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:36:33.423
Mar  2 02:36:33.423: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename watch 03/02/23 02:36:33.424
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:36:33.476
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:36:33.49
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 03/02/23 02:36:33.505
STEP: creating a watch on configmaps with label B 03/02/23 02:36:33.513
STEP: creating a watch on configmaps with label A or B 03/02/23 02:36:33.519
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 03/02/23 02:36:33.525
Mar  2 02:36:33.545: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9430  81585615-e788-44de-9f5f-9ef7e81f5fb3 133381 0 2023-03-02 02:36:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-02 02:36:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 02:36:33.546: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9430  81585615-e788-44de-9f5f-9ef7e81f5fb3 133381 0 2023-03-02 02:36:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-02 02:36:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 03/02/23 02:36:33.546
Mar  2 02:36:33.593: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9430  81585615-e788-44de-9f5f-9ef7e81f5fb3 133386 0 2023-03-02 02:36:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-02 02:36:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 02:36:33.594: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9430  81585615-e788-44de-9f5f-9ef7e81f5fb3 133386 0 2023-03-02 02:36:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-02 02:36:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 03/02/23 02:36:33.594
Mar  2 02:36:33.673: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9430  81585615-e788-44de-9f5f-9ef7e81f5fb3 133393 0 2023-03-02 02:36:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-02 02:36:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 02:36:33.673: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9430  81585615-e788-44de-9f5f-9ef7e81f5fb3 133393 0 2023-03-02 02:36:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-02 02:36:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 03/02/23 02:36:33.673
Mar  2 02:36:33.712: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9430  81585615-e788-44de-9f5f-9ef7e81f5fb3 133394 0 2023-03-02 02:36:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-02 02:36:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 02:36:33.712: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9430  81585615-e788-44de-9f5f-9ef7e81f5fb3 133394 0 2023-03-02 02:36:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-02 02:36:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 03/02/23 02:36:33.712
Mar  2 02:36:33.733: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9430  35fc6a46-bf93-4fd6-8b2a-2f107abb8bd2 133395 0 2023-03-02 02:36:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-02 02:36:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 02:36:33.733: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9430  35fc6a46-bf93-4fd6-8b2a-2f107abb8bd2 133395 0 2023-03-02 02:36:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-02 02:36:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 03/02/23 02:36:43.734
Mar  2 02:36:43.765: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9430  35fc6a46-bf93-4fd6-8b2a-2f107abb8bd2 133512 0 2023-03-02 02:36:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-02 02:36:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 02:36:43.765: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9430  35fc6a46-bf93-4fd6-8b2a-2f107abb8bd2 133512 0 2023-03-02 02:36:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-02 02:36:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Mar  2 02:36:53.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9430" for this suite. 03/02/23 02:36:53.787
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","completed":289,"skipped":5274,"failed":0}
------------------------------
• [SLOW TEST] [20.388 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:36:33.423
    Mar  2 02:36:33.423: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename watch 03/02/23 02:36:33.424
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:36:33.476
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:36:33.49
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 03/02/23 02:36:33.505
    STEP: creating a watch on configmaps with label B 03/02/23 02:36:33.513
    STEP: creating a watch on configmaps with label A or B 03/02/23 02:36:33.519
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 03/02/23 02:36:33.525
    Mar  2 02:36:33.545: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9430  81585615-e788-44de-9f5f-9ef7e81f5fb3 133381 0 2023-03-02 02:36:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-02 02:36:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar  2 02:36:33.546: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9430  81585615-e788-44de-9f5f-9ef7e81f5fb3 133381 0 2023-03-02 02:36:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-02 02:36:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 03/02/23 02:36:33.546
    Mar  2 02:36:33.593: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9430  81585615-e788-44de-9f5f-9ef7e81f5fb3 133386 0 2023-03-02 02:36:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-02 02:36:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar  2 02:36:33.594: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9430  81585615-e788-44de-9f5f-9ef7e81f5fb3 133386 0 2023-03-02 02:36:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-02 02:36:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 03/02/23 02:36:33.594
    Mar  2 02:36:33.673: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9430  81585615-e788-44de-9f5f-9ef7e81f5fb3 133393 0 2023-03-02 02:36:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-02 02:36:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar  2 02:36:33.673: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9430  81585615-e788-44de-9f5f-9ef7e81f5fb3 133393 0 2023-03-02 02:36:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-02 02:36:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 03/02/23 02:36:33.673
    Mar  2 02:36:33.712: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9430  81585615-e788-44de-9f5f-9ef7e81f5fb3 133394 0 2023-03-02 02:36:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-02 02:36:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar  2 02:36:33.712: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9430  81585615-e788-44de-9f5f-9ef7e81f5fb3 133394 0 2023-03-02 02:36:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-02 02:36:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 03/02/23 02:36:33.712
    Mar  2 02:36:33.733: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9430  35fc6a46-bf93-4fd6-8b2a-2f107abb8bd2 133395 0 2023-03-02 02:36:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-02 02:36:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar  2 02:36:33.733: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9430  35fc6a46-bf93-4fd6-8b2a-2f107abb8bd2 133395 0 2023-03-02 02:36:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-02 02:36:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 03/02/23 02:36:43.734
    Mar  2 02:36:43.765: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9430  35fc6a46-bf93-4fd6-8b2a-2f107abb8bd2 133512 0 2023-03-02 02:36:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-02 02:36:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar  2 02:36:43.765: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9430  35fc6a46-bf93-4fd6-8b2a-2f107abb8bd2 133512 0 2023-03-02 02:36:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-02 02:36:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Mar  2 02:36:53.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-9430" for this suite. 03/02/23 02:36:53.787
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:36:53.812
Mar  2 02:36:53.812: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename webhook 03/02/23 02:36:53.813
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:36:53.874
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:36:53.887
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/02/23 02:36:53.971
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 02:36:54.577
STEP: Deploying the webhook pod 03/02/23 02:36:54.614
STEP: Wait for the deployment to be ready 03/02/23 02:36:54.656
Mar  2 02:36:54.683: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 02:36:56.729: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 2, 36, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 36, 54, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 2, 36, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 36, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/02/23 02:36:58.745
STEP: Verifying the service has paired with the endpoint 03/02/23 02:36:58.801
Mar  2 02:36:59.802: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
STEP: Registering the webhook via the AdmissionRegistration API 03/02/23 02:36:59.817
STEP: create a pod that should be denied by the webhook 03/02/23 02:36:59.887
STEP: create a pod that causes the webhook to hang 03/02/23 02:36:59.964
STEP: create a configmap that should be denied by the webhook 03/02/23 02:37:10.023
STEP: create a configmap that should be admitted by the webhook 03/02/23 02:37:10.102
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 03/02/23 02:37:10.155
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 03/02/23 02:37:10.198
STEP: create a namespace that bypass the webhook 03/02/23 02:37:10.224
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 03/02/23 02:37:10.254
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 02:37:10.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2238" for this suite. 03/02/23 02:37:10.474
STEP: Destroying namespace "webhook-2238-markers" for this suite. 03/02/23 02:37:10.499
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","completed":290,"skipped":5282,"failed":0}
------------------------------
• [SLOW TEST] [17.095 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:36:53.812
    Mar  2 02:36:53.812: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename webhook 03/02/23 02:36:53.813
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:36:53.874
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:36:53.887
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/02/23 02:36:53.971
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 02:36:54.577
    STEP: Deploying the webhook pod 03/02/23 02:36:54.614
    STEP: Wait for the deployment to be ready 03/02/23 02:36:54.656
    Mar  2 02:36:54.683: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Mar  2 02:36:56.729: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 2, 36, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 36, 54, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 2, 36, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 36, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/02/23 02:36:58.745
    STEP: Verifying the service has paired with the endpoint 03/02/23 02:36:58.801
    Mar  2 02:36:59.802: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:196
    STEP: Registering the webhook via the AdmissionRegistration API 03/02/23 02:36:59.817
    STEP: create a pod that should be denied by the webhook 03/02/23 02:36:59.887
    STEP: create a pod that causes the webhook to hang 03/02/23 02:36:59.964
    STEP: create a configmap that should be denied by the webhook 03/02/23 02:37:10.023
    STEP: create a configmap that should be admitted by the webhook 03/02/23 02:37:10.102
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 03/02/23 02:37:10.155
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 03/02/23 02:37:10.198
    STEP: create a namespace that bypass the webhook 03/02/23 02:37:10.224
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 03/02/23 02:37:10.254
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 02:37:10.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2238" for this suite. 03/02/23 02:37:10.474
    STEP: Destroying namespace "webhook-2238-markers" for this suite. 03/02/23 02:37:10.499
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:37:10.909
Mar  2 02:37:10.909: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename webhook 03/02/23 02:37:10.91
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:37:10.984
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:37:11.001
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/02/23 02:37:11.082
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 02:37:11.443
STEP: Deploying the webhook pod 03/02/23 02:37:11.477
STEP: Wait for the deployment to be ready 03/02/23 02:37:11.556
Mar  2 02:37:11.594: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/02/23 02:37:13.666
STEP: Verifying the service has paired with the endpoint 03/02/23 02:37:13.71
Mar  2 02:37:14.711: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
Mar  2 02:37:14.725: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5870-crds.webhook.example.com via the AdmissionRegistration API 03/02/23 02:37:15.28
STEP: Creating a custom resource while v1 is storage version 03/02/23 02:37:15.362
STEP: Patching Custom Resource Definition to set v2 as storage 03/02/23 02:37:17.802
STEP: Patching the custom resource while v2 is storage version 03/02/23 02:37:17.961
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 02:37:18.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6934" for this suite. 03/02/23 02:37:18.793
STEP: Destroying namespace "webhook-6934-markers" for this suite. 03/02/23 02:37:18.823
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","completed":291,"skipped":5295,"failed":0}
------------------------------
• [SLOW TEST] [8.253 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:37:10.909
    Mar  2 02:37:10.909: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename webhook 03/02/23 02:37:10.91
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:37:10.984
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:37:11.001
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/02/23 02:37:11.082
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 02:37:11.443
    STEP: Deploying the webhook pod 03/02/23 02:37:11.477
    STEP: Wait for the deployment to be ready 03/02/23 02:37:11.556
    Mar  2 02:37:11.594: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/02/23 02:37:13.666
    STEP: Verifying the service has paired with the endpoint 03/02/23 02:37:13.71
    Mar  2 02:37:14.711: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:322
    Mar  2 02:37:14.725: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5870-crds.webhook.example.com via the AdmissionRegistration API 03/02/23 02:37:15.28
    STEP: Creating a custom resource while v1 is storage version 03/02/23 02:37:15.362
    STEP: Patching Custom Resource Definition to set v2 as storage 03/02/23 02:37:17.802
    STEP: Patching the custom resource while v2 is storage version 03/02/23 02:37:17.961
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 02:37:18.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6934" for this suite. 03/02/23 02:37:18.793
    STEP: Destroying namespace "webhook-6934-markers" for this suite. 03/02/23 02:37:18.823
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:37:19.184
Mar  2 02:37:19.184: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename watch 03/02/23 02:37:19.186
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:37:19.289
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:37:19.336
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 03/02/23 02:37:19.381
STEP: starting a background goroutine to produce watch events 03/02/23 02:37:19.45
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 03/02/23 02:37:19.451
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Mar  2 02:37:23.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5131" for this suite. 03/02/23 02:37:23.242
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","completed":292,"skipped":5306,"failed":0}
------------------------------
• [4.085 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:37:19.184
    Mar  2 02:37:19.184: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename watch 03/02/23 02:37:19.186
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:37:19.289
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:37:19.336
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 03/02/23 02:37:19.381
    STEP: starting a background goroutine to produce watch events 03/02/23 02:37:19.45
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 03/02/23 02:37:19.451
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Mar  2 02:37:23.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-5131" for this suite. 03/02/23 02:37:23.242
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:37:23.27
Mar  2 02:37:23.270: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename secrets 03/02/23 02:37:23.272
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:37:23.352
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:37:23.366
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
STEP: Creating secret with name secret-test-023a7b13-2a34-4def-a46b-338b65e3f79a 03/02/23 02:37:23.566
STEP: Creating a pod to test consume secrets 03/02/23 02:37:23.638
Mar  2 02:37:23.843: INFO: Waiting up to 5m0s for pod "pod-secrets-818854cb-5c80-4d96-936a-63fe5107a918" in namespace "secrets-6255" to be "Succeeded or Failed"
Mar  2 02:37:23.897: INFO: Pod "pod-secrets-818854cb-5c80-4d96-936a-63fe5107a918": Phase="Pending", Reason="", readiness=false. Elapsed: 54.202594ms
Mar  2 02:37:25.940: INFO: Pod "pod-secrets-818854cb-5c80-4d96-936a-63fe5107a918": Phase="Pending", Reason="", readiness=false. Elapsed: 2.096653465s
Mar  2 02:37:27.912: INFO: Pod "pod-secrets-818854cb-5c80-4d96-936a-63fe5107a918": Phase="Pending", Reason="", readiness=false. Elapsed: 4.069087124s
Mar  2 02:37:29.921: INFO: Pod "pod-secrets-818854cb-5c80-4d96-936a-63fe5107a918": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.077819325s
STEP: Saw pod success 03/02/23 02:37:29.921
Mar  2 02:37:29.921: INFO: Pod "pod-secrets-818854cb-5c80-4d96-936a-63fe5107a918" satisfied condition "Succeeded or Failed"
Mar  2 02:37:29.939: INFO: Trying to get logs from node 10.132.92.143 pod pod-secrets-818854cb-5c80-4d96-936a-63fe5107a918 container secret-volume-test: <nil>
STEP: delete the pod 03/02/23 02:37:30.056
Mar  2 02:37:30.097: INFO: Waiting for pod pod-secrets-818854cb-5c80-4d96-936a-63fe5107a918 to disappear
Mar  2 02:37:30.148: INFO: Pod pod-secrets-818854cb-5c80-4d96-936a-63fe5107a918 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar  2 02:37:30.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6255" for this suite. 03/02/23 02:37:30.208
STEP: Destroying namespace "secret-namespace-4442" for this suite. 03/02/23 02:37:30.25
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","completed":293,"skipped":5318,"failed":0}
------------------------------
• [SLOW TEST] [7.006 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:37:23.27
    Mar  2 02:37:23.270: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename secrets 03/02/23 02:37:23.272
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:37:23.352
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:37:23.366
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:98
    STEP: Creating secret with name secret-test-023a7b13-2a34-4def-a46b-338b65e3f79a 03/02/23 02:37:23.566
    STEP: Creating a pod to test consume secrets 03/02/23 02:37:23.638
    Mar  2 02:37:23.843: INFO: Waiting up to 5m0s for pod "pod-secrets-818854cb-5c80-4d96-936a-63fe5107a918" in namespace "secrets-6255" to be "Succeeded or Failed"
    Mar  2 02:37:23.897: INFO: Pod "pod-secrets-818854cb-5c80-4d96-936a-63fe5107a918": Phase="Pending", Reason="", readiness=false. Elapsed: 54.202594ms
    Mar  2 02:37:25.940: INFO: Pod "pod-secrets-818854cb-5c80-4d96-936a-63fe5107a918": Phase="Pending", Reason="", readiness=false. Elapsed: 2.096653465s
    Mar  2 02:37:27.912: INFO: Pod "pod-secrets-818854cb-5c80-4d96-936a-63fe5107a918": Phase="Pending", Reason="", readiness=false. Elapsed: 4.069087124s
    Mar  2 02:37:29.921: INFO: Pod "pod-secrets-818854cb-5c80-4d96-936a-63fe5107a918": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.077819325s
    STEP: Saw pod success 03/02/23 02:37:29.921
    Mar  2 02:37:29.921: INFO: Pod "pod-secrets-818854cb-5c80-4d96-936a-63fe5107a918" satisfied condition "Succeeded or Failed"
    Mar  2 02:37:29.939: INFO: Trying to get logs from node 10.132.92.143 pod pod-secrets-818854cb-5c80-4d96-936a-63fe5107a918 container secret-volume-test: <nil>
    STEP: delete the pod 03/02/23 02:37:30.056
    Mar  2 02:37:30.097: INFO: Waiting for pod pod-secrets-818854cb-5c80-4d96-936a-63fe5107a918 to disappear
    Mar  2 02:37:30.148: INFO: Pod pod-secrets-818854cb-5c80-4d96-936a-63fe5107a918 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar  2 02:37:30.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-6255" for this suite. 03/02/23 02:37:30.208
    STEP: Destroying namespace "secret-namespace-4442" for this suite. 03/02/23 02:37:30.25
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:37:30.279
Mar  2 02:37:30.279: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename job 03/02/23 02:37:30.28
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:37:30.372
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:37:30.386
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
STEP: Creating a job 03/02/23 02:37:30.398
STEP: Ensuring job reaches completions 03/02/23 02:37:30.429
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Mar  2 02:37:44.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-625" for this suite. 03/02/23 02:37:44.468
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","completed":294,"skipped":5337,"failed":0}
------------------------------
• [SLOW TEST] [14.217 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:37:30.279
    Mar  2 02:37:30.279: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename job 03/02/23 02:37:30.28
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:37:30.372
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:37:30.386
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:254
    STEP: Creating a job 03/02/23 02:37:30.398
    STEP: Ensuring job reaches completions 03/02/23 02:37:30.429
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Mar  2 02:37:44.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-625" for this suite. 03/02/23 02:37:44.468
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:37:44.497
Mar  2 02:37:44.500: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename resourcequota 03/02/23 02:37:44.501
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:37:44.562
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:37:44.576
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
STEP: Discovering how many secrets are in namespace by default 03/02/23 02:37:44.587
STEP: Counting existing ResourceQuota 03/02/23 02:37:50.615
STEP: Creating a ResourceQuota 03/02/23 02:37:55.628
STEP: Ensuring resource quota status is calculated 03/02/23 02:37:55.653
STEP: Creating a Secret 03/02/23 02:37:57.669
STEP: Ensuring resource quota status captures secret creation 03/02/23 02:37:57.713
STEP: Deleting a secret 03/02/23 02:37:59.728
STEP: Ensuring resource quota status released usage 03/02/23 02:37:59.75
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar  2 02:38:01.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6723" for this suite. 03/02/23 02:38:01.79
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","completed":295,"skipped":5340,"failed":0}
------------------------------
• [SLOW TEST] [17.328 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:37:44.497
    Mar  2 02:37:44.500: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename resourcequota 03/02/23 02:37:44.501
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:37:44.562
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:37:44.576
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:150
    STEP: Discovering how many secrets are in namespace by default 03/02/23 02:37:44.587
    STEP: Counting existing ResourceQuota 03/02/23 02:37:50.615
    STEP: Creating a ResourceQuota 03/02/23 02:37:55.628
    STEP: Ensuring resource quota status is calculated 03/02/23 02:37:55.653
    STEP: Creating a Secret 03/02/23 02:37:57.669
    STEP: Ensuring resource quota status captures secret creation 03/02/23 02:37:57.713
    STEP: Deleting a secret 03/02/23 02:37:59.728
    STEP: Ensuring resource quota status released usage 03/02/23 02:37:59.75
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar  2 02:38:01.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-6723" for this suite. 03/02/23 02:38:01.79
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:38:01.829
Mar  2 02:38:01.829: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename statefulset 03/02/23 02:38:01.83
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:38:01.9
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:38:01.933
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-6237 03/02/23 02:38:01.945
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
STEP: Creating statefulset ss in namespace statefulset-6237 03/02/23 02:38:01.973
Mar  2 02:38:02.025: INFO: Found 0 stateful pods, waiting for 1
Mar  2 02:38:12.041: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 03/02/23 02:38:12.066
STEP: updating a scale subresource 03/02/23 02:38:12.079
STEP: verifying the statefulset Spec.Replicas was modified 03/02/23 02:38:12.099
STEP: Patch a scale subresource 03/02/23 02:38:12.113
STEP: verifying the statefulset Spec.Replicas was modified 03/02/23 02:38:12.134
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar  2 02:38:12.149: INFO: Deleting all statefulset in ns statefulset-6237
Mar  2 02:38:12.164: INFO: Scaling statefulset ss to 0
Mar  2 02:38:22.229: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 02:38:22.244: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar  2 02:38:22.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6237" for this suite. 03/02/23 02:38:22.379
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","completed":296,"skipped":5369,"failed":0}
------------------------------
• [SLOW TEST] [20.574 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:846

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:38:01.829
    Mar  2 02:38:01.829: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename statefulset 03/02/23 02:38:01.83
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:38:01.9
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:38:01.933
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-6237 03/02/23 02:38:01.945
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:846
    STEP: Creating statefulset ss in namespace statefulset-6237 03/02/23 02:38:01.973
    Mar  2 02:38:02.025: INFO: Found 0 stateful pods, waiting for 1
    Mar  2 02:38:12.041: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 03/02/23 02:38:12.066
    STEP: updating a scale subresource 03/02/23 02:38:12.079
    STEP: verifying the statefulset Spec.Replicas was modified 03/02/23 02:38:12.099
    STEP: Patch a scale subresource 03/02/23 02:38:12.113
    STEP: verifying the statefulset Spec.Replicas was modified 03/02/23 02:38:12.134
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar  2 02:38:12.149: INFO: Deleting all statefulset in ns statefulset-6237
    Mar  2 02:38:12.164: INFO: Scaling statefulset ss to 0
    Mar  2 02:38:22.229: INFO: Waiting for statefulset status.replicas updated to 0
    Mar  2 02:38:22.244: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar  2 02:38:22.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-6237" for this suite. 03/02/23 02:38:22.379
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:38:22.404
Mar  2 02:38:22.405: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename gc 03/02/23 02:38:22.407
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:38:22.482
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:38:22.502
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 03/02/23 02:38:22.515
STEP: Wait for the Deployment to create new ReplicaSet 03/02/23 02:38:22.544
STEP: delete the deployment 03/02/23 02:38:22.568
STEP: wait for all rs to be garbage collected 03/02/23 02:38:22.649
STEP: expected 0 pods, got 2 pods 03/02/23 02:38:22.79
STEP: Gathering metrics 03/02/23 02:38:23.338
W0302 02:38:23.398262      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Mar  2 02:38:23.398: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Mar  2 02:38:23.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1039" for this suite. 03/02/23 02:38:23.448
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","completed":297,"skipped":5380,"failed":0}
------------------------------
• [1.071 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:38:22.404
    Mar  2 02:38:22.405: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename gc 03/02/23 02:38:22.407
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:38:22.482
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:38:22.502
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 03/02/23 02:38:22.515
    STEP: Wait for the Deployment to create new ReplicaSet 03/02/23 02:38:22.544
    STEP: delete the deployment 03/02/23 02:38:22.568
    STEP: wait for all rs to be garbage collected 03/02/23 02:38:22.649
    STEP: expected 0 pods, got 2 pods 03/02/23 02:38:22.79
    STEP: Gathering metrics 03/02/23 02:38:23.338
    W0302 02:38:23.398262      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Mar  2 02:38:23.398: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Mar  2 02:38:23.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-1039" for this suite. 03/02/23 02:38:23.448
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:38:23.477
Mar  2 02:38:23.477: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename downward-api 03/02/23 02:38:23.478
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:38:23.531
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:38:23.55
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
STEP: Creating a pod to test downward API volume plugin 03/02/23 02:38:23.603
Mar  2 02:38:23.716: INFO: Waiting up to 5m0s for pod "downwardapi-volume-17c024e7-c032-43b7-b018-751f1317798e" in namespace "downward-api-3456" to be "Succeeded or Failed"
Mar  2 02:38:23.754: INFO: Pod "downwardapi-volume-17c024e7-c032-43b7-b018-751f1317798e": Phase="Pending", Reason="", readiness=false. Elapsed: 38.267793ms
Mar  2 02:38:25.784: INFO: Pod "downwardapi-volume-17c024e7-c032-43b7-b018-751f1317798e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.068545969s
Mar  2 02:38:27.784: INFO: Pod "downwardapi-volume-17c024e7-c032-43b7-b018-751f1317798e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.068310462s
Mar  2 02:38:29.771: INFO: Pod "downwardapi-volume-17c024e7-c032-43b7-b018-751f1317798e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.055873358s
STEP: Saw pod success 03/02/23 02:38:29.772
Mar  2 02:38:29.772: INFO: Pod "downwardapi-volume-17c024e7-c032-43b7-b018-751f1317798e" satisfied condition "Succeeded or Failed"
Mar  2 02:38:29.786: INFO: Trying to get logs from node 10.132.92.143 pod downwardapi-volume-17c024e7-c032-43b7-b018-751f1317798e container client-container: <nil>
STEP: delete the pod 03/02/23 02:38:29.823
Mar  2 02:38:29.869: INFO: Waiting for pod downwardapi-volume-17c024e7-c032-43b7-b018-751f1317798e to disappear
Mar  2 02:38:29.882: INFO: Pod downwardapi-volume-17c024e7-c032-43b7-b018-751f1317798e no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar  2 02:38:29.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3456" for this suite. 03/02/23 02:38:29.907
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","completed":298,"skipped":5390,"failed":0}
------------------------------
• [SLOW TEST] [6.453 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:38:23.477
    Mar  2 02:38:23.477: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename downward-api 03/02/23 02:38:23.478
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:38:23.531
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:38:23.55
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:220
    STEP: Creating a pod to test downward API volume plugin 03/02/23 02:38:23.603
    Mar  2 02:38:23.716: INFO: Waiting up to 5m0s for pod "downwardapi-volume-17c024e7-c032-43b7-b018-751f1317798e" in namespace "downward-api-3456" to be "Succeeded or Failed"
    Mar  2 02:38:23.754: INFO: Pod "downwardapi-volume-17c024e7-c032-43b7-b018-751f1317798e": Phase="Pending", Reason="", readiness=false. Elapsed: 38.267793ms
    Mar  2 02:38:25.784: INFO: Pod "downwardapi-volume-17c024e7-c032-43b7-b018-751f1317798e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.068545969s
    Mar  2 02:38:27.784: INFO: Pod "downwardapi-volume-17c024e7-c032-43b7-b018-751f1317798e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.068310462s
    Mar  2 02:38:29.771: INFO: Pod "downwardapi-volume-17c024e7-c032-43b7-b018-751f1317798e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.055873358s
    STEP: Saw pod success 03/02/23 02:38:29.772
    Mar  2 02:38:29.772: INFO: Pod "downwardapi-volume-17c024e7-c032-43b7-b018-751f1317798e" satisfied condition "Succeeded or Failed"
    Mar  2 02:38:29.786: INFO: Trying to get logs from node 10.132.92.143 pod downwardapi-volume-17c024e7-c032-43b7-b018-751f1317798e container client-container: <nil>
    STEP: delete the pod 03/02/23 02:38:29.823
    Mar  2 02:38:29.869: INFO: Waiting for pod downwardapi-volume-17c024e7-c032-43b7-b018-751f1317798e to disappear
    Mar  2 02:38:29.882: INFO: Pod downwardapi-volume-17c024e7-c032-43b7-b018-751f1317798e no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar  2 02:38:29.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-3456" for this suite. 03/02/23 02:38:29.907
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:38:29.93
Mar  2 02:38:29.930: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename namespaces 03/02/23 02:38:29.931
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:38:29.998
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:38:30.021
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
STEP: creating a Namespace 03/02/23 02:38:30.036
STEP: patching the Namespace 03/02/23 02:38:30.092
STEP: get the Namespace and ensuring it has the label 03/02/23 02:38:30.134
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Mar  2 02:38:30.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8687" for this suite. 03/02/23 02:38:30.2
STEP: Destroying namespace "nspatchtest-36d8059b-8065-4951-9a50-8505c9ceca52-6398" for this suite. 03/02/23 02:38:30.248
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","completed":299,"skipped":5396,"failed":0}
------------------------------
• [0.343 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:38:29.93
    Mar  2 02:38:29.930: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename namespaces 03/02/23 02:38:29.931
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:38:29.998
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:38:30.021
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:267
    STEP: creating a Namespace 03/02/23 02:38:30.036
    STEP: patching the Namespace 03/02/23 02:38:30.092
    STEP: get the Namespace and ensuring it has the label 03/02/23 02:38:30.134
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Mar  2 02:38:30.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-8687" for this suite. 03/02/23 02:38:30.2
    STEP: Destroying namespace "nspatchtest-36d8059b-8065-4951-9a50-8505c9ceca52-6398" for this suite. 03/02/23 02:38:30.248
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:38:30.276
Mar  2 02:38:30.277: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename security-context 03/02/23 02:38:30.278
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:38:30.335
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:38:30.348
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 03/02/23 02:38:30.361
Mar  2 02:38:30.425: INFO: Waiting up to 5m0s for pod "security-context-309be9c0-6b1c-4811-969d-a6ecbbf2176e" in namespace "security-context-8743" to be "Succeeded or Failed"
Mar  2 02:38:30.448: INFO: Pod "security-context-309be9c0-6b1c-4811-969d-a6ecbbf2176e": Phase="Pending", Reason="", readiness=false. Elapsed: 22.100641ms
Mar  2 02:38:32.466: INFO: Pod "security-context-309be9c0-6b1c-4811-969d-a6ecbbf2176e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040323467s
Mar  2 02:38:34.468: INFO: Pod "security-context-309be9c0-6b1c-4811-969d-a6ecbbf2176e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042173254s
Mar  2 02:38:36.474: INFO: Pod "security-context-309be9c0-6b1c-4811-969d-a6ecbbf2176e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.048380585s
STEP: Saw pod success 03/02/23 02:38:36.474
Mar  2 02:38:36.474: INFO: Pod "security-context-309be9c0-6b1c-4811-969d-a6ecbbf2176e" satisfied condition "Succeeded or Failed"
Mar  2 02:38:36.499: INFO: Trying to get logs from node 10.132.92.143 pod security-context-309be9c0-6b1c-4811-969d-a6ecbbf2176e container test-container: <nil>
STEP: delete the pod 03/02/23 02:38:36.535
Mar  2 02:38:36.592: INFO: Waiting for pod security-context-309be9c0-6b1c-4811-969d-a6ecbbf2176e to disappear
Mar  2 02:38:36.611: INFO: Pod security-context-309be9c0-6b1c-4811-969d-a6ecbbf2176e no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Mar  2 02:38:36.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-8743" for this suite. 03/02/23 02:38:36.641
{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":300,"skipped":5426,"failed":0}
------------------------------
• [SLOW TEST] [6.401 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:38:30.276
    Mar  2 02:38:30.277: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename security-context 03/02/23 02:38:30.278
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:38:30.335
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:38:30.348
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:132
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 03/02/23 02:38:30.361
    Mar  2 02:38:30.425: INFO: Waiting up to 5m0s for pod "security-context-309be9c0-6b1c-4811-969d-a6ecbbf2176e" in namespace "security-context-8743" to be "Succeeded or Failed"
    Mar  2 02:38:30.448: INFO: Pod "security-context-309be9c0-6b1c-4811-969d-a6ecbbf2176e": Phase="Pending", Reason="", readiness=false. Elapsed: 22.100641ms
    Mar  2 02:38:32.466: INFO: Pod "security-context-309be9c0-6b1c-4811-969d-a6ecbbf2176e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040323467s
    Mar  2 02:38:34.468: INFO: Pod "security-context-309be9c0-6b1c-4811-969d-a6ecbbf2176e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042173254s
    Mar  2 02:38:36.474: INFO: Pod "security-context-309be9c0-6b1c-4811-969d-a6ecbbf2176e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.048380585s
    STEP: Saw pod success 03/02/23 02:38:36.474
    Mar  2 02:38:36.474: INFO: Pod "security-context-309be9c0-6b1c-4811-969d-a6ecbbf2176e" satisfied condition "Succeeded or Failed"
    Mar  2 02:38:36.499: INFO: Trying to get logs from node 10.132.92.143 pod security-context-309be9c0-6b1c-4811-969d-a6ecbbf2176e container test-container: <nil>
    STEP: delete the pod 03/02/23 02:38:36.535
    Mar  2 02:38:36.592: INFO: Waiting for pod security-context-309be9c0-6b1c-4811-969d-a6ecbbf2176e to disappear
    Mar  2 02:38:36.611: INFO: Pod security-context-309be9c0-6b1c-4811-969d-a6ecbbf2176e no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Mar  2 02:38:36.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-8743" for this suite. 03/02/23 02:38:36.641
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:38:36.678
Mar  2 02:38:36.678: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename pods 03/02/23 02:38:36.679
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:38:36.737
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:38:36.751
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
STEP: creating the pod 03/02/23 02:38:36.764
STEP: submitting the pod to kubernetes 03/02/23 02:38:36.764
Mar  2 02:38:36.868: INFO: Waiting up to 5m0s for pod "pod-update-2019bed3-8b43-44fc-bce0-710cc13da1e3" in namespace "pods-4102" to be "running and ready"
Mar  2 02:38:36.907: INFO: Pod "pod-update-2019bed3-8b43-44fc-bce0-710cc13da1e3": Phase="Pending", Reason="", readiness=false. Elapsed: 38.604139ms
Mar  2 02:38:36.907: INFO: The phase of Pod pod-update-2019bed3-8b43-44fc-bce0-710cc13da1e3 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 02:38:38.922: INFO: Pod "pod-update-2019bed3-8b43-44fc-bce0-710cc13da1e3": Phase="Running", Reason="", readiness=true. Elapsed: 2.053742303s
Mar  2 02:38:38.922: INFO: The phase of Pod pod-update-2019bed3-8b43-44fc-bce0-710cc13da1e3 is Running (Ready = true)
Mar  2 02:38:38.923: INFO: Pod "pod-update-2019bed3-8b43-44fc-bce0-710cc13da1e3" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 03/02/23 02:38:38.938
STEP: updating the pod 03/02/23 02:38:38.951
Mar  2 02:38:39.519: INFO: Successfully updated pod "pod-update-2019bed3-8b43-44fc-bce0-710cc13da1e3"
Mar  2 02:38:39.519: INFO: Waiting up to 5m0s for pod "pod-update-2019bed3-8b43-44fc-bce0-710cc13da1e3" in namespace "pods-4102" to be "running"
Mar  2 02:38:39.532: INFO: Pod "pod-update-2019bed3-8b43-44fc-bce0-710cc13da1e3": Phase="Running", Reason="", readiness=true. Elapsed: 12.9576ms
Mar  2 02:38:39.532: INFO: Pod "pod-update-2019bed3-8b43-44fc-bce0-710cc13da1e3" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 03/02/23 02:38:39.532
Mar  2 02:38:39.546: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar  2 02:38:39.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4102" for this suite. 03/02/23 02:38:39.569
{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","completed":301,"skipped":5438,"failed":0}
------------------------------
• [2.915 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:38:36.678
    Mar  2 02:38:36.678: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename pods 03/02/23 02:38:36.679
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:38:36.737
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:38:36.751
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:343
    STEP: creating the pod 03/02/23 02:38:36.764
    STEP: submitting the pod to kubernetes 03/02/23 02:38:36.764
    Mar  2 02:38:36.868: INFO: Waiting up to 5m0s for pod "pod-update-2019bed3-8b43-44fc-bce0-710cc13da1e3" in namespace "pods-4102" to be "running and ready"
    Mar  2 02:38:36.907: INFO: Pod "pod-update-2019bed3-8b43-44fc-bce0-710cc13da1e3": Phase="Pending", Reason="", readiness=false. Elapsed: 38.604139ms
    Mar  2 02:38:36.907: INFO: The phase of Pod pod-update-2019bed3-8b43-44fc-bce0-710cc13da1e3 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 02:38:38.922: INFO: Pod "pod-update-2019bed3-8b43-44fc-bce0-710cc13da1e3": Phase="Running", Reason="", readiness=true. Elapsed: 2.053742303s
    Mar  2 02:38:38.922: INFO: The phase of Pod pod-update-2019bed3-8b43-44fc-bce0-710cc13da1e3 is Running (Ready = true)
    Mar  2 02:38:38.923: INFO: Pod "pod-update-2019bed3-8b43-44fc-bce0-710cc13da1e3" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 03/02/23 02:38:38.938
    STEP: updating the pod 03/02/23 02:38:38.951
    Mar  2 02:38:39.519: INFO: Successfully updated pod "pod-update-2019bed3-8b43-44fc-bce0-710cc13da1e3"
    Mar  2 02:38:39.519: INFO: Waiting up to 5m0s for pod "pod-update-2019bed3-8b43-44fc-bce0-710cc13da1e3" in namespace "pods-4102" to be "running"
    Mar  2 02:38:39.532: INFO: Pod "pod-update-2019bed3-8b43-44fc-bce0-710cc13da1e3": Phase="Running", Reason="", readiness=true. Elapsed: 12.9576ms
    Mar  2 02:38:39.532: INFO: Pod "pod-update-2019bed3-8b43-44fc-bce0-710cc13da1e3" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 03/02/23 02:38:39.532
    Mar  2 02:38:39.546: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar  2 02:38:39.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4102" for this suite. 03/02/23 02:38:39.569
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:38:39.594
Mar  2 02:38:39.595: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename webhook 03/02/23 02:38:39.596
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:38:39.65
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:38:39.665
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/02/23 02:38:39.753
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 02:38:40.497
STEP: Deploying the webhook pod 03/02/23 02:38:40.531
STEP: Wait for the deployment to be ready 03/02/23 02:38:40.573
Mar  2 02:38:40.603: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 02:38:42.655: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 2, 38, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 38, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 2, 38, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 38, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/02/23 02:38:44.67
STEP: Verifying the service has paired with the endpoint 03/02/23 02:38:44.712
Mar  2 02:38:45.722: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 03/02/23 02:38:45.735
STEP: create a namespace for the webhook 03/02/23 02:38:45.832
STEP: create a configmap should be unconditionally rejected by the webhook 03/02/23 02:38:45.859
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 02:38:45.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9537" for this suite. 03/02/23 02:38:46.005
STEP: Destroying namespace "webhook-9537-markers" for this suite. 03/02/23 02:38:46.028
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","completed":302,"skipped":5448,"failed":0}
------------------------------
• [SLOW TEST] [6.598 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:38:39.594
    Mar  2 02:38:39.595: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename webhook 03/02/23 02:38:39.596
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:38:39.65
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:38:39.665
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/02/23 02:38:39.753
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 02:38:40.497
    STEP: Deploying the webhook pod 03/02/23 02:38:40.531
    STEP: Wait for the deployment to be ready 03/02/23 02:38:40.573
    Mar  2 02:38:40.603: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Mar  2 02:38:42.655: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 2, 38, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 38, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 2, 38, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 38, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/02/23 02:38:44.67
    STEP: Verifying the service has paired with the endpoint 03/02/23 02:38:44.712
    Mar  2 02:38:45.722: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:238
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 03/02/23 02:38:45.735
    STEP: create a namespace for the webhook 03/02/23 02:38:45.832
    STEP: create a configmap should be unconditionally rejected by the webhook 03/02/23 02:38:45.859
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 02:38:45.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9537" for this suite. 03/02/23 02:38:46.005
    STEP: Destroying namespace "webhook-9537-markers" for this suite. 03/02/23 02:38:46.028
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:38:46.193
Mar  2 02:38:46.193: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename crd-publish-openapi 03/02/23 02:38:46.195
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:38:46.256
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:38:46.274
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
STEP: set up a multi version CRD 03/02/23 02:38:46.295
Mar  2 02:38:46.296: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: rename a version 03/02/23 02:39:19.429
STEP: check the new version name is served 03/02/23 02:39:19.476
STEP: check the old version name is removed 03/02/23 02:39:34.17
STEP: check the other version is not changed 03/02/23 02:39:39.562
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 02:40:06.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7612" for this suite. 03/02/23 02:40:06.182
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","completed":303,"skipped":5448,"failed":0}
------------------------------
• [SLOW TEST] [80.009 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:38:46.193
    Mar  2 02:38:46.193: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename crd-publish-openapi 03/02/23 02:38:46.195
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:38:46.256
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:38:46.274
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:390
    STEP: set up a multi version CRD 03/02/23 02:38:46.295
    Mar  2 02:38:46.296: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: rename a version 03/02/23 02:39:19.429
    STEP: check the new version name is served 03/02/23 02:39:19.476
    STEP: check the old version name is removed 03/02/23 02:39:34.17
    STEP: check the other version is not changed 03/02/23 02:39:39.562
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 02:40:06.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-7612" for this suite. 03/02/23 02:40:06.182
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:40:06.205
Mar  2 02:40:06.205: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename cronjob 03/02/23 02:40:06.206
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:40:06.307
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:40:06.335
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 03/02/23 02:40:06.351
W0302 02:40:06.388889      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring a job is scheduled 03/02/23 02:40:06.389
STEP: Ensuring exactly one is scheduled 03/02/23 02:41:00.401
STEP: Ensuring exactly one running job exists by listing jobs explicitly 03/02/23 02:41:00.413
STEP: Ensuring the job is replaced with a new one 03/02/23 02:41:00.429
STEP: Removing cronjob 03/02/23 02:42:00.447
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Mar  2 02:42:00.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-3911" for this suite. 03/02/23 02:42:00.498
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","completed":304,"skipped":5468,"failed":0}
------------------------------
• [SLOW TEST] [114.313 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:40:06.205
    Mar  2 02:40:06.205: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename cronjob 03/02/23 02:40:06.206
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:40:06.307
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:40:06.335
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 03/02/23 02:40:06.351
    W0302 02:40:06.388889      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring a job is scheduled 03/02/23 02:40:06.389
    STEP: Ensuring exactly one is scheduled 03/02/23 02:41:00.401
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 03/02/23 02:41:00.413
    STEP: Ensuring the job is replaced with a new one 03/02/23 02:41:00.429
    STEP: Removing cronjob 03/02/23 02:42:00.447
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Mar  2 02:42:00.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-3911" for this suite. 03/02/23 02:42:00.498
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:42:00.519
Mar  2 02:42:00.519: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename deployment 03/02/23 02:42:00.52
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:42:00.563
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:42:00.576
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 03/02/23 02:42:00.685
Mar  2 02:42:00.686: INFO: Creating simple deployment test-deployment-9f4l4
Mar  2 02:42:00.752: INFO: deployment "test-deployment-9f4l4" doesn't have the required revision set
STEP: Getting /status 03/02/23 02:42:02.871
Mar  2 02:42:02.900: INFO: Deployment test-deployment-9f4l4 has Conditions: [{Available True 2023-03-02 02:42:02 +0000 UTC 2023-03-02 02:42:02 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-03-02 02:42:02 +0000 UTC 2023-03-02 02:42:00 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-9f4l4-777898ffcc" has successfully progressed.}]
STEP: updating Deployment Status 03/02/23 02:42:02.9
Mar  2 02:42:02.972: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 2, 42, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 42, 2, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 2, 42, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 42, 0, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-9f4l4-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 03/02/23 02:42:02.972
Mar  2 02:42:02.985: INFO: Observed &Deployment event: ADDED
Mar  2 02:42:02.986: INFO: Observed Deployment test-deployment-9f4l4 in namespace deployment-304 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-02 02:42:00 +0000 UTC 2023-03-02 02:42:00 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-9f4l4-777898ffcc"}
Mar  2 02:42:02.986: INFO: Observed &Deployment event: MODIFIED
Mar  2 02:42:02.986: INFO: Observed Deployment test-deployment-9f4l4 in namespace deployment-304 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-02 02:42:00 +0000 UTC 2023-03-02 02:42:00 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-9f4l4-777898ffcc"}
Mar  2 02:42:02.986: INFO: Observed Deployment test-deployment-9f4l4 in namespace deployment-304 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-02 02:42:00 +0000 UTC 2023-03-02 02:42:00 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar  2 02:42:02.987: INFO: Observed &Deployment event: MODIFIED
Mar  2 02:42:02.988: INFO: Observed Deployment test-deployment-9f4l4 in namespace deployment-304 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-02 02:42:00 +0000 UTC 2023-03-02 02:42:00 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar  2 02:42:02.988: INFO: Observed Deployment test-deployment-9f4l4 in namespace deployment-304 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-02 02:42:00 +0000 UTC 2023-03-02 02:42:00 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-9f4l4-777898ffcc" is progressing.}
Mar  2 02:42:02.988: INFO: Observed &Deployment event: MODIFIED
Mar  2 02:42:02.988: INFO: Observed Deployment test-deployment-9f4l4 in namespace deployment-304 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-02 02:42:02 +0000 UTC 2023-03-02 02:42:02 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar  2 02:42:02.988: INFO: Observed Deployment test-deployment-9f4l4 in namespace deployment-304 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-02 02:42:02 +0000 UTC 2023-03-02 02:42:00 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-9f4l4-777898ffcc" has successfully progressed.}
Mar  2 02:42:02.989: INFO: Observed &Deployment event: MODIFIED
Mar  2 02:42:02.989: INFO: Observed Deployment test-deployment-9f4l4 in namespace deployment-304 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-02 02:42:02 +0000 UTC 2023-03-02 02:42:02 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar  2 02:42:02.989: INFO: Observed Deployment test-deployment-9f4l4 in namespace deployment-304 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-02 02:42:02 +0000 UTC 2023-03-02 02:42:00 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-9f4l4-777898ffcc" has successfully progressed.}
Mar  2 02:42:02.989: INFO: Found Deployment test-deployment-9f4l4 in namespace deployment-304 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar  2 02:42:02.989: INFO: Deployment test-deployment-9f4l4 has an updated status
STEP: patching the Statefulset Status 03/02/23 02:42:02.989
Mar  2 02:42:02.990: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Mar  2 02:42:03.038: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 03/02/23 02:42:03.038
Mar  2 02:42:03.044: INFO: Observed &Deployment event: ADDED
Mar  2 02:42:03.044: INFO: Observed deployment test-deployment-9f4l4 in namespace deployment-304 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-02 02:42:00 +0000 UTC 2023-03-02 02:42:00 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-9f4l4-777898ffcc"}
Mar  2 02:42:03.045: INFO: Observed &Deployment event: MODIFIED
Mar  2 02:42:03.045: INFO: Observed deployment test-deployment-9f4l4 in namespace deployment-304 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-02 02:42:00 +0000 UTC 2023-03-02 02:42:00 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-9f4l4-777898ffcc"}
Mar  2 02:42:03.045: INFO: Observed deployment test-deployment-9f4l4 in namespace deployment-304 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-02 02:42:00 +0000 UTC 2023-03-02 02:42:00 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar  2 02:42:03.045: INFO: Observed &Deployment event: MODIFIED
Mar  2 02:42:03.045: INFO: Observed deployment test-deployment-9f4l4 in namespace deployment-304 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-02 02:42:00 +0000 UTC 2023-03-02 02:42:00 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar  2 02:42:03.045: INFO: Observed deployment test-deployment-9f4l4 in namespace deployment-304 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-02 02:42:00 +0000 UTC 2023-03-02 02:42:00 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-9f4l4-777898ffcc" is progressing.}
Mar  2 02:42:03.046: INFO: Observed &Deployment event: MODIFIED
Mar  2 02:42:03.046: INFO: Observed deployment test-deployment-9f4l4 in namespace deployment-304 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-02 02:42:02 +0000 UTC 2023-03-02 02:42:02 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar  2 02:42:03.046: INFO: Observed deployment test-deployment-9f4l4 in namespace deployment-304 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-02 02:42:02 +0000 UTC 2023-03-02 02:42:00 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-9f4l4-777898ffcc" has successfully progressed.}
Mar  2 02:42:03.046: INFO: Observed &Deployment event: MODIFIED
Mar  2 02:42:03.046: INFO: Observed deployment test-deployment-9f4l4 in namespace deployment-304 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-02 02:42:02 +0000 UTC 2023-03-02 02:42:02 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar  2 02:42:03.046: INFO: Observed deployment test-deployment-9f4l4 in namespace deployment-304 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-02 02:42:02 +0000 UTC 2023-03-02 02:42:00 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-9f4l4-777898ffcc" has successfully progressed.}
Mar  2 02:42:03.046: INFO: Observed deployment test-deployment-9f4l4 in namespace deployment-304 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar  2 02:42:03.046: INFO: Observed &Deployment event: MODIFIED
Mar  2 02:42:03.047: INFO: Found deployment test-deployment-9f4l4 in namespace deployment-304 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Mar  2 02:42:03.047: INFO: Deployment test-deployment-9f4l4 has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar  2 02:42:03.071: INFO: Deployment "test-deployment-9f4l4":
&Deployment{ObjectMeta:{test-deployment-9f4l4  deployment-304  f6ec8fd8-19b0-40b9-b5ef-d964fa988d1c 137080 1 2023-03-02 02:42:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-03-02 02:42:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-03-02 02:42:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-03-02 02:42:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0094ae2d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-9f4l4-777898ffcc",LastUpdateTime:2023-03-02 02:42:03 +0000 UTC,LastTransitionTime:2023-03-02 02:42:03 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar  2 02:42:03.091: INFO: New ReplicaSet "test-deployment-9f4l4-777898ffcc" of Deployment "test-deployment-9f4l4":
&ReplicaSet{ObjectMeta:{test-deployment-9f4l4-777898ffcc  deployment-304  9bd022a0-11f8-4c33-b70d-1d90d7b68891 137071 1 2023-03-02 02:42:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-9f4l4 f6ec8fd8-19b0-40b9-b5ef-d964fa988d1c 0xc0077b46c0 0xc0077b46c1}] [] [{kube-controller-manager Update apps/v1 2023-03-02 02:42:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f6ec8fd8-19b0-40b9-b5ef-d964fa988d1c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 02:42:02 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0077b4768 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar  2 02:42:03.134: INFO: Pod "test-deployment-9f4l4-777898ffcc-rq92s" is available:
&Pod{ObjectMeta:{test-deployment-9f4l4-777898ffcc-rq92s test-deployment-9f4l4-777898ffcc- deployment-304  c7197f4c-8f06-4c97-a2ab-331f5886989e 137070 0 2023-03-02 02:42:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cni.projectcalico.org/containerID:e7a9e66a576efdc5bf3e9af046ba17675da0d710702ba525de0c494d1b57b11a cni.projectcalico.org/podIP:172.30.156.112/32 cni.projectcalico.org/podIPs:172.30.156.112/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.156.112"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.156.112"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-9f4l4-777898ffcc 9bd022a0-11f8-4c33-b70d-1d90d7b68891 0xc0077b4b47 0xc0077b4b48}] [] [{kube-controller-manager Update v1 2023-03-02 02:42:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9bd022a0-11f8-4c33-b70d-1d90d7b68891\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 02:42:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-02 02:42:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.156.112\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-03-02 02:42:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w5vjp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w5vjp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.132.92.143,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c65,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:42:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:42:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:42:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:42:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.132.92.143,PodIP:172.30.156.112,StartTime:2023-03-02 02:42:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 02:42:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://33f9469e9bd1446e448dfc77eab0af3803be0e668f682eb473378d99970cc253,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.156.112,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar  2 02:42:03.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-304" for this suite. 03/02/23 02:42:03.165
{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","completed":305,"skipped":5484,"failed":0}
------------------------------
• [2.668 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:42:00.519
    Mar  2 02:42:00.519: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename deployment 03/02/23 02:42:00.52
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:42:00.563
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:42:00.576
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 03/02/23 02:42:00.685
    Mar  2 02:42:00.686: INFO: Creating simple deployment test-deployment-9f4l4
    Mar  2 02:42:00.752: INFO: deployment "test-deployment-9f4l4" doesn't have the required revision set
    STEP: Getting /status 03/02/23 02:42:02.871
    Mar  2 02:42:02.900: INFO: Deployment test-deployment-9f4l4 has Conditions: [{Available True 2023-03-02 02:42:02 +0000 UTC 2023-03-02 02:42:02 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-03-02 02:42:02 +0000 UTC 2023-03-02 02:42:00 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-9f4l4-777898ffcc" has successfully progressed.}]
    STEP: updating Deployment Status 03/02/23 02:42:02.9
    Mar  2 02:42:02.972: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 2, 42, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 42, 2, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 2, 42, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 42, 0, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-9f4l4-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 03/02/23 02:42:02.972
    Mar  2 02:42:02.985: INFO: Observed &Deployment event: ADDED
    Mar  2 02:42:02.986: INFO: Observed Deployment test-deployment-9f4l4 in namespace deployment-304 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-02 02:42:00 +0000 UTC 2023-03-02 02:42:00 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-9f4l4-777898ffcc"}
    Mar  2 02:42:02.986: INFO: Observed &Deployment event: MODIFIED
    Mar  2 02:42:02.986: INFO: Observed Deployment test-deployment-9f4l4 in namespace deployment-304 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-02 02:42:00 +0000 UTC 2023-03-02 02:42:00 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-9f4l4-777898ffcc"}
    Mar  2 02:42:02.986: INFO: Observed Deployment test-deployment-9f4l4 in namespace deployment-304 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-02 02:42:00 +0000 UTC 2023-03-02 02:42:00 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Mar  2 02:42:02.987: INFO: Observed &Deployment event: MODIFIED
    Mar  2 02:42:02.988: INFO: Observed Deployment test-deployment-9f4l4 in namespace deployment-304 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-02 02:42:00 +0000 UTC 2023-03-02 02:42:00 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Mar  2 02:42:02.988: INFO: Observed Deployment test-deployment-9f4l4 in namespace deployment-304 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-02 02:42:00 +0000 UTC 2023-03-02 02:42:00 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-9f4l4-777898ffcc" is progressing.}
    Mar  2 02:42:02.988: INFO: Observed &Deployment event: MODIFIED
    Mar  2 02:42:02.988: INFO: Observed Deployment test-deployment-9f4l4 in namespace deployment-304 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-02 02:42:02 +0000 UTC 2023-03-02 02:42:02 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Mar  2 02:42:02.988: INFO: Observed Deployment test-deployment-9f4l4 in namespace deployment-304 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-02 02:42:02 +0000 UTC 2023-03-02 02:42:00 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-9f4l4-777898ffcc" has successfully progressed.}
    Mar  2 02:42:02.989: INFO: Observed &Deployment event: MODIFIED
    Mar  2 02:42:02.989: INFO: Observed Deployment test-deployment-9f4l4 in namespace deployment-304 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-02 02:42:02 +0000 UTC 2023-03-02 02:42:02 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Mar  2 02:42:02.989: INFO: Observed Deployment test-deployment-9f4l4 in namespace deployment-304 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-02 02:42:02 +0000 UTC 2023-03-02 02:42:00 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-9f4l4-777898ffcc" has successfully progressed.}
    Mar  2 02:42:02.989: INFO: Found Deployment test-deployment-9f4l4 in namespace deployment-304 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar  2 02:42:02.989: INFO: Deployment test-deployment-9f4l4 has an updated status
    STEP: patching the Statefulset Status 03/02/23 02:42:02.989
    Mar  2 02:42:02.990: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Mar  2 02:42:03.038: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 03/02/23 02:42:03.038
    Mar  2 02:42:03.044: INFO: Observed &Deployment event: ADDED
    Mar  2 02:42:03.044: INFO: Observed deployment test-deployment-9f4l4 in namespace deployment-304 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-02 02:42:00 +0000 UTC 2023-03-02 02:42:00 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-9f4l4-777898ffcc"}
    Mar  2 02:42:03.045: INFO: Observed &Deployment event: MODIFIED
    Mar  2 02:42:03.045: INFO: Observed deployment test-deployment-9f4l4 in namespace deployment-304 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-02 02:42:00 +0000 UTC 2023-03-02 02:42:00 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-9f4l4-777898ffcc"}
    Mar  2 02:42:03.045: INFO: Observed deployment test-deployment-9f4l4 in namespace deployment-304 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-02 02:42:00 +0000 UTC 2023-03-02 02:42:00 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Mar  2 02:42:03.045: INFO: Observed &Deployment event: MODIFIED
    Mar  2 02:42:03.045: INFO: Observed deployment test-deployment-9f4l4 in namespace deployment-304 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-02 02:42:00 +0000 UTC 2023-03-02 02:42:00 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Mar  2 02:42:03.045: INFO: Observed deployment test-deployment-9f4l4 in namespace deployment-304 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-02 02:42:00 +0000 UTC 2023-03-02 02:42:00 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-9f4l4-777898ffcc" is progressing.}
    Mar  2 02:42:03.046: INFO: Observed &Deployment event: MODIFIED
    Mar  2 02:42:03.046: INFO: Observed deployment test-deployment-9f4l4 in namespace deployment-304 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-02 02:42:02 +0000 UTC 2023-03-02 02:42:02 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Mar  2 02:42:03.046: INFO: Observed deployment test-deployment-9f4l4 in namespace deployment-304 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-02 02:42:02 +0000 UTC 2023-03-02 02:42:00 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-9f4l4-777898ffcc" has successfully progressed.}
    Mar  2 02:42:03.046: INFO: Observed &Deployment event: MODIFIED
    Mar  2 02:42:03.046: INFO: Observed deployment test-deployment-9f4l4 in namespace deployment-304 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-02 02:42:02 +0000 UTC 2023-03-02 02:42:02 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Mar  2 02:42:03.046: INFO: Observed deployment test-deployment-9f4l4 in namespace deployment-304 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-02 02:42:02 +0000 UTC 2023-03-02 02:42:00 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-9f4l4-777898ffcc" has successfully progressed.}
    Mar  2 02:42:03.046: INFO: Observed deployment test-deployment-9f4l4 in namespace deployment-304 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar  2 02:42:03.046: INFO: Observed &Deployment event: MODIFIED
    Mar  2 02:42:03.047: INFO: Found deployment test-deployment-9f4l4 in namespace deployment-304 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Mar  2 02:42:03.047: INFO: Deployment test-deployment-9f4l4 has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar  2 02:42:03.071: INFO: Deployment "test-deployment-9f4l4":
    &Deployment{ObjectMeta:{test-deployment-9f4l4  deployment-304  f6ec8fd8-19b0-40b9-b5ef-d964fa988d1c 137080 1 2023-03-02 02:42:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-03-02 02:42:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-03-02 02:42:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-03-02 02:42:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0094ae2d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-9f4l4-777898ffcc",LastUpdateTime:2023-03-02 02:42:03 +0000 UTC,LastTransitionTime:2023-03-02 02:42:03 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Mar  2 02:42:03.091: INFO: New ReplicaSet "test-deployment-9f4l4-777898ffcc" of Deployment "test-deployment-9f4l4":
    &ReplicaSet{ObjectMeta:{test-deployment-9f4l4-777898ffcc  deployment-304  9bd022a0-11f8-4c33-b70d-1d90d7b68891 137071 1 2023-03-02 02:42:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-9f4l4 f6ec8fd8-19b0-40b9-b5ef-d964fa988d1c 0xc0077b46c0 0xc0077b46c1}] [] [{kube-controller-manager Update apps/v1 2023-03-02 02:42:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f6ec8fd8-19b0-40b9-b5ef-d964fa988d1c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 02:42:02 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0077b4768 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar  2 02:42:03.134: INFO: Pod "test-deployment-9f4l4-777898ffcc-rq92s" is available:
    &Pod{ObjectMeta:{test-deployment-9f4l4-777898ffcc-rq92s test-deployment-9f4l4-777898ffcc- deployment-304  c7197f4c-8f06-4c97-a2ab-331f5886989e 137070 0 2023-03-02 02:42:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cni.projectcalico.org/containerID:e7a9e66a576efdc5bf3e9af046ba17675da0d710702ba525de0c494d1b57b11a cni.projectcalico.org/podIP:172.30.156.112/32 cni.projectcalico.org/podIPs:172.30.156.112/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.156.112"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.156.112"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-9f4l4-777898ffcc 9bd022a0-11f8-4c33-b70d-1d90d7b68891 0xc0077b4b47 0xc0077b4b48}] [] [{kube-controller-manager Update v1 2023-03-02 02:42:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9bd022a0-11f8-4c33-b70d-1d90d7b68891\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-02 02:42:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-02 02:42:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.156.112\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-03-02 02:42:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w5vjp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w5vjp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.132.92.143,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c65,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:42:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:42:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:42:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:42:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.132.92.143,PodIP:172.30.156.112,StartTime:2023-03-02 02:42:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 02:42:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://33f9469e9bd1446e448dfc77eab0af3803be0e668f682eb473378d99970cc253,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.156.112,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar  2 02:42:03.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-304" for this suite. 03/02/23 02:42:03.165
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:42:03.193
Mar  2 02:42:03.193: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename configmap 03/02/23 02:42:03.194
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:42:03.25
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:42:03.259
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
STEP: Creating configMap with name configmap-test-volume-map-5901ac9e-540a-40f7-85de-2766a6d76476 03/02/23 02:42:03.272
STEP: Creating a pod to test consume configMaps 03/02/23 02:42:03.309
Mar  2 02:42:03.379: INFO: Waiting up to 5m0s for pod "pod-configmaps-d236d73f-d499-422b-aefe-d4f91c24293d" in namespace "configmap-8446" to be "Succeeded or Failed"
Mar  2 02:42:03.410: INFO: Pod "pod-configmaps-d236d73f-d499-422b-aefe-d4f91c24293d": Phase="Pending", Reason="", readiness=false. Elapsed: 31.144982ms
Mar  2 02:42:05.427: INFO: Pod "pod-configmaps-d236d73f-d499-422b-aefe-d4f91c24293d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047970764s
Mar  2 02:42:07.454: INFO: Pod "pod-configmaps-d236d73f-d499-422b-aefe-d4f91c24293d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.075447016s
Mar  2 02:42:09.427: INFO: Pod "pod-configmaps-d236d73f-d499-422b-aefe-d4f91c24293d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.048814548s
STEP: Saw pod success 03/02/23 02:42:09.427
Mar  2 02:42:09.428: INFO: Pod "pod-configmaps-d236d73f-d499-422b-aefe-d4f91c24293d" satisfied condition "Succeeded or Failed"
Mar  2 02:42:09.446: INFO: Trying to get logs from node 10.132.92.143 pod pod-configmaps-d236d73f-d499-422b-aefe-d4f91c24293d container agnhost-container: <nil>
STEP: delete the pod 03/02/23 02:42:09.513
Mar  2 02:42:09.576: INFO: Waiting for pod pod-configmaps-d236d73f-d499-422b-aefe-d4f91c24293d to disappear
Mar  2 02:42:09.623: INFO: Pod pod-configmaps-d236d73f-d499-422b-aefe-d4f91c24293d no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar  2 02:42:09.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8446" for this suite. 03/02/23 02:42:09.674
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":306,"skipped":5573,"failed":0}
------------------------------
• [SLOW TEST] [6.503 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:42:03.193
    Mar  2 02:42:03.193: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename configmap 03/02/23 02:42:03.194
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:42:03.25
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:42:03.259
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:98
    STEP: Creating configMap with name configmap-test-volume-map-5901ac9e-540a-40f7-85de-2766a6d76476 03/02/23 02:42:03.272
    STEP: Creating a pod to test consume configMaps 03/02/23 02:42:03.309
    Mar  2 02:42:03.379: INFO: Waiting up to 5m0s for pod "pod-configmaps-d236d73f-d499-422b-aefe-d4f91c24293d" in namespace "configmap-8446" to be "Succeeded or Failed"
    Mar  2 02:42:03.410: INFO: Pod "pod-configmaps-d236d73f-d499-422b-aefe-d4f91c24293d": Phase="Pending", Reason="", readiness=false. Elapsed: 31.144982ms
    Mar  2 02:42:05.427: INFO: Pod "pod-configmaps-d236d73f-d499-422b-aefe-d4f91c24293d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047970764s
    Mar  2 02:42:07.454: INFO: Pod "pod-configmaps-d236d73f-d499-422b-aefe-d4f91c24293d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.075447016s
    Mar  2 02:42:09.427: INFO: Pod "pod-configmaps-d236d73f-d499-422b-aefe-d4f91c24293d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.048814548s
    STEP: Saw pod success 03/02/23 02:42:09.427
    Mar  2 02:42:09.428: INFO: Pod "pod-configmaps-d236d73f-d499-422b-aefe-d4f91c24293d" satisfied condition "Succeeded or Failed"
    Mar  2 02:42:09.446: INFO: Trying to get logs from node 10.132.92.143 pod pod-configmaps-d236d73f-d499-422b-aefe-d4f91c24293d container agnhost-container: <nil>
    STEP: delete the pod 03/02/23 02:42:09.513
    Mar  2 02:42:09.576: INFO: Waiting for pod pod-configmaps-d236d73f-d499-422b-aefe-d4f91c24293d to disappear
    Mar  2 02:42:09.623: INFO: Pod pod-configmaps-d236d73f-d499-422b-aefe-d4f91c24293d no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  2 02:42:09.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-8446" for this suite. 03/02/23 02:42:09.674
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:42:09.701
Mar  2 02:42:09.701: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename container-lifecycle-hook 03/02/23 02:42:09.702
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:42:09.761
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:42:09.773
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 03/02/23 02:42:09.821
Mar  2 02:42:09.881: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-521" to be "running and ready"
Mar  2 02:42:09.937: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 55.765083ms
Mar  2 02:42:09.937: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar  2 02:42:11.952: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.071035159s
Mar  2 02:42:11.952: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar  2 02:42:13.955: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.074322989s
Mar  2 02:42:13.955: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Mar  2 02:42:13.955: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
STEP: create the pod with lifecycle hook 03/02/23 02:42:13.971
Mar  2 02:42:14.010: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-521" to be "running and ready"
Mar  2 02:42:14.027: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 17.512979ms
Mar  2 02:42:14.027: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Mar  2 02:42:16.045: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035723828s
Mar  2 02:42:16.046: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Mar  2 02:42:18.048: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.038678821s
Mar  2 02:42:18.048: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Mar  2 02:42:18.048: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 03/02/23 02:42:18.063
Mar  2 02:42:18.091: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  2 02:42:18.107: INFO: Pod pod-with-prestop-http-hook still exists
Mar  2 02:42:20.109: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  2 02:42:20.127: INFO: Pod pod-with-prestop-http-hook still exists
Mar  2 02:42:22.108: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  2 02:42:22.135: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 03/02/23 02:42:22.135
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Mar  2 02:42:22.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-521" for this suite. 03/02/23 02:42:22.236
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","completed":307,"skipped":5645,"failed":0}
------------------------------
• [SLOW TEST] [12.555 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:42:09.701
    Mar  2 02:42:09.701: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename container-lifecycle-hook 03/02/23 02:42:09.702
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:42:09.761
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:42:09.773
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 03/02/23 02:42:09.821
    Mar  2 02:42:09.881: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-521" to be "running and ready"
    Mar  2 02:42:09.937: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 55.765083ms
    Mar  2 02:42:09.937: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 02:42:11.952: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.071035159s
    Mar  2 02:42:11.952: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 02:42:13.955: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.074322989s
    Mar  2 02:42:13.955: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Mar  2 02:42:13.955: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:152
    STEP: create the pod with lifecycle hook 03/02/23 02:42:13.971
    Mar  2 02:42:14.010: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-521" to be "running and ready"
    Mar  2 02:42:14.027: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 17.512979ms
    Mar  2 02:42:14.027: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 02:42:16.045: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035723828s
    Mar  2 02:42:16.046: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 02:42:18.048: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.038678821s
    Mar  2 02:42:18.048: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Mar  2 02:42:18.048: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 03/02/23 02:42:18.063
    Mar  2 02:42:18.091: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Mar  2 02:42:18.107: INFO: Pod pod-with-prestop-http-hook still exists
    Mar  2 02:42:20.109: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Mar  2 02:42:20.127: INFO: Pod pod-with-prestop-http-hook still exists
    Mar  2 02:42:22.108: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Mar  2 02:42:22.135: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 03/02/23 02:42:22.135
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Mar  2 02:42:22.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-521" for this suite. 03/02/23 02:42:22.236
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:42:22.256
Mar  2 02:42:22.256: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename services 03/02/23 02:42:22.257
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:42:22.308
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:42:22.323
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
STEP: fetching services 03/02/23 02:42:22.34
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  2 02:42:22.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-992" for this suite. 03/02/23 02:42:22.396
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","completed":308,"skipped":5650,"failed":0}
------------------------------
• [0.160 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:42:22.256
    Mar  2 02:42:22.256: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename services 03/02/23 02:42:22.257
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:42:22.308
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:42:22.323
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3206
    STEP: fetching services 03/02/23 02:42:22.34
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  2 02:42:22.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-992" for this suite. 03/02/23 02:42:22.396
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:42:22.42
Mar  2 02:42:22.420: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename runtimeclass 03/02/23 02:42:22.429
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:42:22.479
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:42:22.491
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Mar  2 02:42:22.624: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-1501 to be scheduled
Mar  2 02:42:22.641: INFO: 1 pods are not scheduled: [runtimeclass-1501/test-runtimeclass-runtimeclass-1501-preconfigured-handler-x8lz7(0e354c85-2da3-427b-87d4-e83591d89131)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Mar  2 02:42:24.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-1501" for this suite. 03/02/23 02:42:24.706
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]","completed":309,"skipped":5687,"failed":0}
------------------------------
• [2.306 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:42:22.42
    Mar  2 02:42:22.420: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename runtimeclass 03/02/23 02:42:22.429
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:42:22.479
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:42:22.491
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Mar  2 02:42:22.624: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-1501 to be scheduled
    Mar  2 02:42:22.641: INFO: 1 pods are not scheduled: [runtimeclass-1501/test-runtimeclass-runtimeclass-1501-preconfigured-handler-x8lz7(0e354c85-2da3-427b-87d4-e83591d89131)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Mar  2 02:42:24.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-1501" for this suite. 03/02/23 02:42:24.706
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:42:24.73
Mar  2 02:42:24.731: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename container-probe 03/02/23 02:42:24.733
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:42:24.784
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:42:24.796
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
STEP: Creating pod liveness-9565e5e9-ab71-4bb4-b52a-967b6451c4f9 in namespace container-probe-501 03/02/23 02:42:24.811
Mar  2 02:42:24.876: INFO: Waiting up to 5m0s for pod "liveness-9565e5e9-ab71-4bb4-b52a-967b6451c4f9" in namespace "container-probe-501" to be "not pending"
Mar  2 02:42:24.897: INFO: Pod "liveness-9565e5e9-ab71-4bb4-b52a-967b6451c4f9": Phase="Pending", Reason="", readiness=false. Elapsed: 21.368843ms
Mar  2 02:42:26.919: INFO: Pod "liveness-9565e5e9-ab71-4bb4-b52a-967b6451c4f9": Phase="Running", Reason="", readiness=true. Elapsed: 2.042632378s
Mar  2 02:42:26.919: INFO: Pod "liveness-9565e5e9-ab71-4bb4-b52a-967b6451c4f9" satisfied condition "not pending"
Mar  2 02:42:26.919: INFO: Started pod liveness-9565e5e9-ab71-4bb4-b52a-967b6451c4f9 in namespace container-probe-501
STEP: checking the pod's current state and verifying that restartCount is present 03/02/23 02:42:26.919
Mar  2 02:42:26.934: INFO: Initial restart count of pod liveness-9565e5e9-ab71-4bb4-b52a-967b6451c4f9 is 0
STEP: deleting the pod 03/02/23 02:46:27.734
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar  2 02:46:27.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-501" for this suite. 03/02/23 02:46:27.917
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","completed":310,"skipped":5692,"failed":0}
------------------------------
• [SLOW TEST] [243.221 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:42:24.73
    Mar  2 02:42:24.731: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename container-probe 03/02/23 02:42:24.733
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:42:24.784
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:42:24.796
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:180
    STEP: Creating pod liveness-9565e5e9-ab71-4bb4-b52a-967b6451c4f9 in namespace container-probe-501 03/02/23 02:42:24.811
    Mar  2 02:42:24.876: INFO: Waiting up to 5m0s for pod "liveness-9565e5e9-ab71-4bb4-b52a-967b6451c4f9" in namespace "container-probe-501" to be "not pending"
    Mar  2 02:42:24.897: INFO: Pod "liveness-9565e5e9-ab71-4bb4-b52a-967b6451c4f9": Phase="Pending", Reason="", readiness=false. Elapsed: 21.368843ms
    Mar  2 02:42:26.919: INFO: Pod "liveness-9565e5e9-ab71-4bb4-b52a-967b6451c4f9": Phase="Running", Reason="", readiness=true. Elapsed: 2.042632378s
    Mar  2 02:42:26.919: INFO: Pod "liveness-9565e5e9-ab71-4bb4-b52a-967b6451c4f9" satisfied condition "not pending"
    Mar  2 02:42:26.919: INFO: Started pod liveness-9565e5e9-ab71-4bb4-b52a-967b6451c4f9 in namespace container-probe-501
    STEP: checking the pod's current state and verifying that restartCount is present 03/02/23 02:42:26.919
    Mar  2 02:42:26.934: INFO: Initial restart count of pod liveness-9565e5e9-ab71-4bb4-b52a-967b6451c4f9 is 0
    STEP: deleting the pod 03/02/23 02:46:27.734
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar  2 02:46:27.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-501" for this suite. 03/02/23 02:46:27.917
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:46:27.952
Mar  2 02:46:27.952: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename svcaccounts 03/02/23 02:46:27.954
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:46:28.295
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:46:28.415
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
Mar  2 02:46:28.508: INFO: Got root ca configmap in namespace "svcaccounts-9369"
Mar  2 02:46:28.571: INFO: Deleted root ca configmap in namespace "svcaccounts-9369"
STEP: waiting for a new root ca configmap created 03/02/23 02:46:29.076
Mar  2 02:46:29.124: INFO: Recreated root ca configmap in namespace "svcaccounts-9369"
Mar  2 02:46:29.190: INFO: Updated root ca configmap in namespace "svcaccounts-9369"
STEP: waiting for the root ca configmap reconciled 03/02/23 02:46:29.69
Mar  2 02:46:29.708: INFO: Reconciled root ca configmap in namespace "svcaccounts-9369"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Mar  2 02:46:29.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9369" for this suite. 03/02/23 02:46:29.73
{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","completed":311,"skipped":5700,"failed":0}
------------------------------
• [1.801 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:46:27.952
    Mar  2 02:46:27.952: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename svcaccounts 03/02/23 02:46:27.954
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:46:28.295
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:46:28.415
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:739
    Mar  2 02:46:28.508: INFO: Got root ca configmap in namespace "svcaccounts-9369"
    Mar  2 02:46:28.571: INFO: Deleted root ca configmap in namespace "svcaccounts-9369"
    STEP: waiting for a new root ca configmap created 03/02/23 02:46:29.076
    Mar  2 02:46:29.124: INFO: Recreated root ca configmap in namespace "svcaccounts-9369"
    Mar  2 02:46:29.190: INFO: Updated root ca configmap in namespace "svcaccounts-9369"
    STEP: waiting for the root ca configmap reconciled 03/02/23 02:46:29.69
    Mar  2 02:46:29.708: INFO: Reconciled root ca configmap in namespace "svcaccounts-9369"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Mar  2 02:46:29.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-9369" for this suite. 03/02/23 02:46:29.73
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:46:29.758
Mar  2 02:46:29.758: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename services 03/02/23 02:46:29.759
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:46:29.813
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:46:29.828
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
STEP: creating a Service 03/02/23 02:46:29.866
STEP: watching for the Service to be added 03/02/23 02:46:29.911
Mar  2 02:46:29.923: INFO: Found Service test-service-d2jlv in namespace services-3615 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Mar  2 02:46:29.923: INFO: Service test-service-d2jlv created
STEP: Getting /status 03/02/23 02:46:29.923
Mar  2 02:46:29.947: INFO: Service test-service-d2jlv has LoadBalancer: {[]}
STEP: patching the ServiceStatus 03/02/23 02:46:29.947
STEP: watching for the Service to be patched 03/02/23 02:46:29.976
Mar  2 02:46:29.982: INFO: observed Service test-service-d2jlv in namespace services-3615 with annotations: map[] & LoadBalancer: {[]}
Mar  2 02:46:29.982: INFO: Found Service test-service-d2jlv in namespace services-3615 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Mar  2 02:46:29.982: INFO: Service test-service-d2jlv has service status patched
STEP: updating the ServiceStatus 03/02/23 02:46:29.982
Mar  2 02:46:30.025: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 03/02/23 02:46:30.025
Mar  2 02:46:30.033: INFO: Observed Service test-service-d2jlv in namespace services-3615 with annotations: map[] & Conditions: {[]}
Mar  2 02:46:30.033: INFO: Observed event: &Service{ObjectMeta:{test-service-d2jlv  services-3615  da11bd7f-fbf7-45a6-9ec8-df6619433fa7 138917 0 2023-03-02 02:46:29 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-03-02 02:46:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-03-02 02:46:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:172.21.121.31,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[172.21.121.31],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Mar  2 02:46:30.033: INFO: Found Service test-service-d2jlv in namespace services-3615 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar  2 02:46:30.033: INFO: Service test-service-d2jlv has service status updated
STEP: patching the service 03/02/23 02:46:30.033
STEP: watching for the Service to be patched 03/02/23 02:46:30.061
Mar  2 02:46:30.077: INFO: observed Service test-service-d2jlv in namespace services-3615 with labels: map[test-service-static:true]
Mar  2 02:46:30.077: INFO: observed Service test-service-d2jlv in namespace services-3615 with labels: map[test-service-static:true]
Mar  2 02:46:30.077: INFO: observed Service test-service-d2jlv in namespace services-3615 with labels: map[test-service-static:true]
Mar  2 02:46:30.077: INFO: Found Service test-service-d2jlv in namespace services-3615 with labels: map[test-service:patched test-service-static:true]
Mar  2 02:46:30.077: INFO: Service test-service-d2jlv patched
STEP: deleting the service 03/02/23 02:46:30.077
STEP: watching for the Service to be deleted 03/02/23 02:46:30.121
Mar  2 02:46:30.128: INFO: Observed event: ADDED
Mar  2 02:46:30.128: INFO: Observed event: MODIFIED
Mar  2 02:46:30.128: INFO: Observed event: MODIFIED
Mar  2 02:46:30.128: INFO: Observed event: MODIFIED
Mar  2 02:46:30.128: INFO: Found Service test-service-d2jlv in namespace services-3615 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Mar  2 02:46:30.128: INFO: Service test-service-d2jlv deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  2 02:46:30.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3615" for this suite. 03/02/23 02:46:30.152
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","completed":312,"skipped":5721,"failed":0}
------------------------------
• [0.413 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:46:29.758
    Mar  2 02:46:29.758: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename services 03/02/23 02:46:29.759
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:46:29.813
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:46:29.828
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3415
    STEP: creating a Service 03/02/23 02:46:29.866
    STEP: watching for the Service to be added 03/02/23 02:46:29.911
    Mar  2 02:46:29.923: INFO: Found Service test-service-d2jlv in namespace services-3615 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Mar  2 02:46:29.923: INFO: Service test-service-d2jlv created
    STEP: Getting /status 03/02/23 02:46:29.923
    Mar  2 02:46:29.947: INFO: Service test-service-d2jlv has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 03/02/23 02:46:29.947
    STEP: watching for the Service to be patched 03/02/23 02:46:29.976
    Mar  2 02:46:29.982: INFO: observed Service test-service-d2jlv in namespace services-3615 with annotations: map[] & LoadBalancer: {[]}
    Mar  2 02:46:29.982: INFO: Found Service test-service-d2jlv in namespace services-3615 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Mar  2 02:46:29.982: INFO: Service test-service-d2jlv has service status patched
    STEP: updating the ServiceStatus 03/02/23 02:46:29.982
    Mar  2 02:46:30.025: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 03/02/23 02:46:30.025
    Mar  2 02:46:30.033: INFO: Observed Service test-service-d2jlv in namespace services-3615 with annotations: map[] & Conditions: {[]}
    Mar  2 02:46:30.033: INFO: Observed event: &Service{ObjectMeta:{test-service-d2jlv  services-3615  da11bd7f-fbf7-45a6-9ec8-df6619433fa7 138917 0 2023-03-02 02:46:29 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-03-02 02:46:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-03-02 02:46:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:172.21.121.31,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[172.21.121.31],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Mar  2 02:46:30.033: INFO: Found Service test-service-d2jlv in namespace services-3615 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Mar  2 02:46:30.033: INFO: Service test-service-d2jlv has service status updated
    STEP: patching the service 03/02/23 02:46:30.033
    STEP: watching for the Service to be patched 03/02/23 02:46:30.061
    Mar  2 02:46:30.077: INFO: observed Service test-service-d2jlv in namespace services-3615 with labels: map[test-service-static:true]
    Mar  2 02:46:30.077: INFO: observed Service test-service-d2jlv in namespace services-3615 with labels: map[test-service-static:true]
    Mar  2 02:46:30.077: INFO: observed Service test-service-d2jlv in namespace services-3615 with labels: map[test-service-static:true]
    Mar  2 02:46:30.077: INFO: Found Service test-service-d2jlv in namespace services-3615 with labels: map[test-service:patched test-service-static:true]
    Mar  2 02:46:30.077: INFO: Service test-service-d2jlv patched
    STEP: deleting the service 03/02/23 02:46:30.077
    STEP: watching for the Service to be deleted 03/02/23 02:46:30.121
    Mar  2 02:46:30.128: INFO: Observed event: ADDED
    Mar  2 02:46:30.128: INFO: Observed event: MODIFIED
    Mar  2 02:46:30.128: INFO: Observed event: MODIFIED
    Mar  2 02:46:30.128: INFO: Observed event: MODIFIED
    Mar  2 02:46:30.128: INFO: Found Service test-service-d2jlv in namespace services-3615 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Mar  2 02:46:30.128: INFO: Service test-service-d2jlv deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  2 02:46:30.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3615" for this suite. 03/02/23 02:46:30.152
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:46:30.172
Mar  2 02:46:30.173: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename downward-api 03/02/23 02:46:30.175
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:46:30.265
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:46:30.293
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
STEP: Creating a pod to test downward API volume plugin 03/02/23 02:46:30.333
Mar  2 02:46:30.443: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f1937b3a-604a-4014-b184-ff4155f05259" in namespace "downward-api-7855" to be "Succeeded or Failed"
Mar  2 02:46:30.500: INFO: Pod "downwardapi-volume-f1937b3a-604a-4014-b184-ff4155f05259": Phase="Pending", Reason="", readiness=false. Elapsed: 56.221335ms
Mar  2 02:46:32.560: INFO: Pod "downwardapi-volume-f1937b3a-604a-4014-b184-ff4155f05259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.117045484s
Mar  2 02:46:34.517: INFO: Pod "downwardapi-volume-f1937b3a-604a-4014-b184-ff4155f05259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.074032955s
Mar  2 02:46:36.541: INFO: Pod "downwardapi-volume-f1937b3a-604a-4014-b184-ff4155f05259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.097169877s
STEP: Saw pod success 03/02/23 02:46:36.541
Mar  2 02:46:36.541: INFO: Pod "downwardapi-volume-f1937b3a-604a-4014-b184-ff4155f05259" satisfied condition "Succeeded or Failed"
Mar  2 02:46:36.556: INFO: Trying to get logs from node 10.132.92.143 pod downwardapi-volume-f1937b3a-604a-4014-b184-ff4155f05259 container client-container: <nil>
STEP: delete the pod 03/02/23 02:46:36.614
Mar  2 02:46:36.677: INFO: Waiting for pod downwardapi-volume-f1937b3a-604a-4014-b184-ff4155f05259 to disappear
Mar  2 02:46:36.693: INFO: Pod downwardapi-volume-f1937b3a-604a-4014-b184-ff4155f05259 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar  2 02:46:36.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7855" for this suite. 03/02/23 02:46:36.721
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","completed":313,"skipped":5722,"failed":0}
------------------------------
• [SLOW TEST] [6.579 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:46:30.172
    Mar  2 02:46:30.173: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename downward-api 03/02/23 02:46:30.175
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:46:30.265
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:46:30.293
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:206
    STEP: Creating a pod to test downward API volume plugin 03/02/23 02:46:30.333
    Mar  2 02:46:30.443: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f1937b3a-604a-4014-b184-ff4155f05259" in namespace "downward-api-7855" to be "Succeeded or Failed"
    Mar  2 02:46:30.500: INFO: Pod "downwardapi-volume-f1937b3a-604a-4014-b184-ff4155f05259": Phase="Pending", Reason="", readiness=false. Elapsed: 56.221335ms
    Mar  2 02:46:32.560: INFO: Pod "downwardapi-volume-f1937b3a-604a-4014-b184-ff4155f05259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.117045484s
    Mar  2 02:46:34.517: INFO: Pod "downwardapi-volume-f1937b3a-604a-4014-b184-ff4155f05259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.074032955s
    Mar  2 02:46:36.541: INFO: Pod "downwardapi-volume-f1937b3a-604a-4014-b184-ff4155f05259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.097169877s
    STEP: Saw pod success 03/02/23 02:46:36.541
    Mar  2 02:46:36.541: INFO: Pod "downwardapi-volume-f1937b3a-604a-4014-b184-ff4155f05259" satisfied condition "Succeeded or Failed"
    Mar  2 02:46:36.556: INFO: Trying to get logs from node 10.132.92.143 pod downwardapi-volume-f1937b3a-604a-4014-b184-ff4155f05259 container client-container: <nil>
    STEP: delete the pod 03/02/23 02:46:36.614
    Mar  2 02:46:36.677: INFO: Waiting for pod downwardapi-volume-f1937b3a-604a-4014-b184-ff4155f05259 to disappear
    Mar  2 02:46:36.693: INFO: Pod downwardapi-volume-f1937b3a-604a-4014-b184-ff4155f05259 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar  2 02:46:36.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7855" for this suite. 03/02/23 02:46:36.721
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:46:36.752
Mar  2 02:46:36.753: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename projected 03/02/23 02:46:36.754
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:46:36.807
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:46:36.82
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
STEP: Creating a pod to test downward API volume plugin 03/02/23 02:46:36.845
Mar  2 02:46:36.900: INFO: Waiting up to 5m0s for pod "downwardapi-volume-091d1504-19e8-46c4-ae0f-962948d173a9" in namespace "projected-6740" to be "Succeeded or Failed"
Mar  2 02:46:36.940: INFO: Pod "downwardapi-volume-091d1504-19e8-46c4-ae0f-962948d173a9": Phase="Pending", Reason="", readiness=false. Elapsed: 39.916568ms
Mar  2 02:46:38.956: INFO: Pod "downwardapi-volume-091d1504-19e8-46c4-ae0f-962948d173a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056342095s
Mar  2 02:46:40.961: INFO: Pod "downwardapi-volume-091d1504-19e8-46c4-ae0f-962948d173a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.060877687s
STEP: Saw pod success 03/02/23 02:46:40.961
Mar  2 02:46:40.961: INFO: Pod "downwardapi-volume-091d1504-19e8-46c4-ae0f-962948d173a9" satisfied condition "Succeeded or Failed"
Mar  2 02:46:40.976: INFO: Trying to get logs from node 10.132.92.143 pod downwardapi-volume-091d1504-19e8-46c4-ae0f-962948d173a9 container client-container: <nil>
STEP: delete the pod 03/02/23 02:46:41.012
Mar  2 02:46:41.054: INFO: Waiting for pod downwardapi-volume-091d1504-19e8-46c4-ae0f-962948d173a9 to disappear
Mar  2 02:46:41.072: INFO: Pod downwardapi-volume-091d1504-19e8-46c4-ae0f-962948d173a9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar  2 02:46:41.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6740" for this suite. 03/02/23 02:46:41.116
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":314,"skipped":5732,"failed":0}
------------------------------
• [4.385 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:46:36.752
    Mar  2 02:46:36.753: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename projected 03/02/23 02:46:36.754
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:46:36.807
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:46:36.82
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:248
    STEP: Creating a pod to test downward API volume plugin 03/02/23 02:46:36.845
    Mar  2 02:46:36.900: INFO: Waiting up to 5m0s for pod "downwardapi-volume-091d1504-19e8-46c4-ae0f-962948d173a9" in namespace "projected-6740" to be "Succeeded or Failed"
    Mar  2 02:46:36.940: INFO: Pod "downwardapi-volume-091d1504-19e8-46c4-ae0f-962948d173a9": Phase="Pending", Reason="", readiness=false. Elapsed: 39.916568ms
    Mar  2 02:46:38.956: INFO: Pod "downwardapi-volume-091d1504-19e8-46c4-ae0f-962948d173a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056342095s
    Mar  2 02:46:40.961: INFO: Pod "downwardapi-volume-091d1504-19e8-46c4-ae0f-962948d173a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.060877687s
    STEP: Saw pod success 03/02/23 02:46:40.961
    Mar  2 02:46:40.961: INFO: Pod "downwardapi-volume-091d1504-19e8-46c4-ae0f-962948d173a9" satisfied condition "Succeeded or Failed"
    Mar  2 02:46:40.976: INFO: Trying to get logs from node 10.132.92.143 pod downwardapi-volume-091d1504-19e8-46c4-ae0f-962948d173a9 container client-container: <nil>
    STEP: delete the pod 03/02/23 02:46:41.012
    Mar  2 02:46:41.054: INFO: Waiting for pod downwardapi-volume-091d1504-19e8-46c4-ae0f-962948d173a9 to disappear
    Mar  2 02:46:41.072: INFO: Pod downwardapi-volume-091d1504-19e8-46c4-ae0f-962948d173a9 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar  2 02:46:41.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6740" for this suite. 03/02/23 02:46:41.116
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:46:41.139
Mar  2 02:46:41.139: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename resourcequota 03/02/23 02:46:41.141
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:46:41.196
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:46:41.209
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
STEP: Counting existing ResourceQuota 03/02/23 02:46:41.223
STEP: Creating a ResourceQuota 03/02/23 02:46:46.235
STEP: Ensuring resource quota status is calculated 03/02/23 02:46:46.255
STEP: Creating a Pod that fits quota 03/02/23 02:46:48.269
STEP: Ensuring ResourceQuota status captures the pod usage 03/02/23 02:46:48.335
STEP: Not allowing a pod to be created that exceeds remaining quota 03/02/23 02:46:50.348
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 03/02/23 02:46:50.409
STEP: Ensuring a pod cannot update its resource requirements 03/02/23 02:46:50.442
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 03/02/23 02:46:50.474
STEP: Deleting the pod 03/02/23 02:46:52.487
STEP: Ensuring resource quota status released the pod usage 03/02/23 02:46:52.537
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar  2 02:46:54.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7500" for this suite. 03/02/23 02:46:54.572
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","completed":315,"skipped":5743,"failed":0}
------------------------------
• [SLOW TEST] [13.455 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:46:41.139
    Mar  2 02:46:41.139: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename resourcequota 03/02/23 02:46:41.141
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:46:41.196
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:46:41.209
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:220
    STEP: Counting existing ResourceQuota 03/02/23 02:46:41.223
    STEP: Creating a ResourceQuota 03/02/23 02:46:46.235
    STEP: Ensuring resource quota status is calculated 03/02/23 02:46:46.255
    STEP: Creating a Pod that fits quota 03/02/23 02:46:48.269
    STEP: Ensuring ResourceQuota status captures the pod usage 03/02/23 02:46:48.335
    STEP: Not allowing a pod to be created that exceeds remaining quota 03/02/23 02:46:50.348
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 03/02/23 02:46:50.409
    STEP: Ensuring a pod cannot update its resource requirements 03/02/23 02:46:50.442
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 03/02/23 02:46:50.474
    STEP: Deleting the pod 03/02/23 02:46:52.487
    STEP: Ensuring resource quota status released the pod usage 03/02/23 02:46:52.537
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar  2 02:46:54.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-7500" for this suite. 03/02/23 02:46:54.572
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:46:54.595
Mar  2 02:46:54.595: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename crd-publish-openapi 03/02/23 02:46:54.597
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:46:54.655
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:46:54.666
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 03/02/23 02:46:54.68
Mar  2 02:46:54.681: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 03/02/23 02:47:50.894
Mar  2 02:47:50.898: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
Mar  2 02:48:06.994: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 02:49:02.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3646" for this suite. 03/02/23 02:49:02.286
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","completed":316,"skipped":5749,"failed":0}
------------------------------
• [SLOW TEST] [127.719 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:46:54.595
    Mar  2 02:46:54.595: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename crd-publish-openapi 03/02/23 02:46:54.597
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:46:54.655
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:46:54.666
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:308
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 03/02/23 02:46:54.68
    Mar  2 02:46:54.681: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 03/02/23 02:47:50.894
    Mar  2 02:47:50.898: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    Mar  2 02:48:06.994: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 02:49:02.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-3646" for this suite. 03/02/23 02:49:02.286
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:49:02.318
Mar  2 02:49:02.318: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename container-lifecycle-hook 03/02/23 02:49:02.319
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:49:02.378
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:49:02.39
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 03/02/23 02:49:02.449
Mar  2 02:49:02.536: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4036" to be "running and ready"
Mar  2 02:49:02.555: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 18.699832ms
Mar  2 02:49:02.555: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar  2 02:49:04.569: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033381113s
Mar  2 02:49:04.569: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar  2 02:49:06.568: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.032130492s
Mar  2 02:49:06.568: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Mar  2 02:49:06.568: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
STEP: create the pod with lifecycle hook 03/02/23 02:49:06.581
Mar  2 02:49:06.619: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-4036" to be "running and ready"
Mar  2 02:49:06.630: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 11.543271ms
Mar  2 02:49:06.630: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Mar  2 02:49:08.645: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026380255s
Mar  2 02:49:08.645: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Mar  2 02:49:10.647: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.028627267s
Mar  2 02:49:10.647: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Mar  2 02:49:10.647: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 03/02/23 02:49:10.661
STEP: delete the pod with lifecycle hook 03/02/23 02:49:10.727
Mar  2 02:49:10.751: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  2 02:49:10.765: INFO: Pod pod-with-poststart-http-hook still exists
Mar  2 02:49:12.766: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  2 02:49:12.781: INFO: Pod pod-with-poststart-http-hook still exists
Mar  2 02:49:14.766: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  2 02:49:14.782: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Mar  2 02:49:14.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4036" for this suite. 03/02/23 02:49:14.805
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","completed":317,"skipped":5779,"failed":0}
------------------------------
• [SLOW TEST] [12.513 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:49:02.318
    Mar  2 02:49:02.318: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename container-lifecycle-hook 03/02/23 02:49:02.319
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:49:02.378
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:49:02.39
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 03/02/23 02:49:02.449
    Mar  2 02:49:02.536: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4036" to be "running and ready"
    Mar  2 02:49:02.555: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 18.699832ms
    Mar  2 02:49:02.555: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 02:49:04.569: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033381113s
    Mar  2 02:49:04.569: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 02:49:06.568: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.032130492s
    Mar  2 02:49:06.568: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Mar  2 02:49:06.568: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:130
    STEP: create the pod with lifecycle hook 03/02/23 02:49:06.581
    Mar  2 02:49:06.619: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-4036" to be "running and ready"
    Mar  2 02:49:06.630: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 11.543271ms
    Mar  2 02:49:06.630: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 02:49:08.645: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026380255s
    Mar  2 02:49:08.645: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 02:49:10.647: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.028627267s
    Mar  2 02:49:10.647: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Mar  2 02:49:10.647: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 03/02/23 02:49:10.661
    STEP: delete the pod with lifecycle hook 03/02/23 02:49:10.727
    Mar  2 02:49:10.751: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Mar  2 02:49:10.765: INFO: Pod pod-with-poststart-http-hook still exists
    Mar  2 02:49:12.766: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Mar  2 02:49:12.781: INFO: Pod pod-with-poststart-http-hook still exists
    Mar  2 02:49:14.766: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Mar  2 02:49:14.782: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Mar  2 02:49:14.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-4036" for this suite. 03/02/23 02:49:14.805
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:49:14.834
Mar  2 02:49:14.834: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename cronjob 03/02/23 02:49:14.835
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:49:14.898
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:49:14.924
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 03/02/23 02:49:14.94
STEP: creating 03/02/23 02:49:14.94
STEP: getting 03/02/23 02:49:14.961
STEP: listing 03/02/23 02:49:14.978
STEP: watching 03/02/23 02:49:14.994
Mar  2 02:49:14.994: INFO: starting watch
STEP: cluster-wide listing 03/02/23 02:49:15
STEP: cluster-wide watching 03/02/23 02:49:15.025
Mar  2 02:49:15.025: INFO: starting watch
STEP: patching 03/02/23 02:49:15.032
STEP: updating 03/02/23 02:49:15.061
Mar  2 02:49:15.101: INFO: waiting for watch events with expected annotations
Mar  2 02:49:15.101: INFO: saw patched and updated annotations
STEP: patching /status 03/02/23 02:49:15.102
STEP: updating /status 03/02/23 02:49:15.125
STEP: get /status 03/02/23 02:49:15.167
STEP: deleting 03/02/23 02:49:15.187
STEP: deleting a collection 03/02/23 02:49:15.272
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Mar  2 02:49:15.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-9745" for this suite. 03/02/23 02:49:15.36
{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","completed":318,"skipped":5798,"failed":0}
------------------------------
• [0.559 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:49:14.834
    Mar  2 02:49:14.834: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename cronjob 03/02/23 02:49:14.835
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:49:14.898
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:49:14.924
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 03/02/23 02:49:14.94
    STEP: creating 03/02/23 02:49:14.94
    STEP: getting 03/02/23 02:49:14.961
    STEP: listing 03/02/23 02:49:14.978
    STEP: watching 03/02/23 02:49:14.994
    Mar  2 02:49:14.994: INFO: starting watch
    STEP: cluster-wide listing 03/02/23 02:49:15
    STEP: cluster-wide watching 03/02/23 02:49:15.025
    Mar  2 02:49:15.025: INFO: starting watch
    STEP: patching 03/02/23 02:49:15.032
    STEP: updating 03/02/23 02:49:15.061
    Mar  2 02:49:15.101: INFO: waiting for watch events with expected annotations
    Mar  2 02:49:15.101: INFO: saw patched and updated annotations
    STEP: patching /status 03/02/23 02:49:15.102
    STEP: updating /status 03/02/23 02:49:15.125
    STEP: get /status 03/02/23 02:49:15.167
    STEP: deleting 03/02/23 02:49:15.187
    STEP: deleting a collection 03/02/23 02:49:15.272
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Mar  2 02:49:15.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-9745" for this suite. 03/02/23 02:49:15.36
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:49:15.397
Mar  2 02:49:15.397: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename endpointslice 03/02/23 02:49:15.398
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:49:15.454
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:49:15.468
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
STEP: referencing a single matching pod 03/02/23 02:49:20.833
STEP: referencing matching pods with named port 03/02/23 02:49:25.863
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 03/02/23 02:49:30.9
STEP: recreating EndpointSlices after they've been deleted 03/02/23 02:49:35.965
Mar  2 02:49:36.050: INFO: EndpointSlice for Service endpointslice-5565/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Mar  2 02:49:46.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-5565" for this suite. 03/02/23 02:49:46.104
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","completed":319,"skipped":5831,"failed":0}
------------------------------
• [SLOW TEST] [30.732 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:49:15.397
    Mar  2 02:49:15.397: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename endpointslice 03/02/23 02:49:15.398
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:49:15.454
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:49:15.468
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:204
    STEP: referencing a single matching pod 03/02/23 02:49:20.833
    STEP: referencing matching pods with named port 03/02/23 02:49:25.863
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 03/02/23 02:49:30.9
    STEP: recreating EndpointSlices after they've been deleted 03/02/23 02:49:35.965
    Mar  2 02:49:36.050: INFO: EndpointSlice for Service endpointslice-5565/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Mar  2 02:49:46.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-5565" for this suite. 03/02/23 02:49:46.104
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:49:46.129
Mar  2 02:49:46.129: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename downward-api 03/02/23 02:49:46.131
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:49:46.21
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:49:46.221
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
STEP: Creating a pod to test downward API volume plugin 03/02/23 02:49:46.234
Mar  2 02:49:46.316: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c94492ab-6962-49fa-93a0-4b4609330a46" in namespace "downward-api-3927" to be "Succeeded or Failed"
Mar  2 02:49:46.333: INFO: Pod "downwardapi-volume-c94492ab-6962-49fa-93a0-4b4609330a46": Phase="Pending", Reason="", readiness=false. Elapsed: 16.608972ms
Mar  2 02:49:48.348: INFO: Pod "downwardapi-volume-c94492ab-6962-49fa-93a0-4b4609330a46": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031929705s
Mar  2 02:49:50.348: INFO: Pod "downwardapi-volume-c94492ab-6962-49fa-93a0-4b4609330a46": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031921816s
Mar  2 02:49:52.348: INFO: Pod "downwardapi-volume-c94492ab-6962-49fa-93a0-4b4609330a46": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032025977s
STEP: Saw pod success 03/02/23 02:49:52.348
Mar  2 02:49:52.348: INFO: Pod "downwardapi-volume-c94492ab-6962-49fa-93a0-4b4609330a46" satisfied condition "Succeeded or Failed"
Mar  2 02:49:52.362: INFO: Trying to get logs from node 10.132.92.143 pod downwardapi-volume-c94492ab-6962-49fa-93a0-4b4609330a46 container client-container: <nil>
STEP: delete the pod 03/02/23 02:49:52.525
Mar  2 02:49:52.566: INFO: Waiting for pod downwardapi-volume-c94492ab-6962-49fa-93a0-4b4609330a46 to disappear
Mar  2 02:49:52.580: INFO: Pod downwardapi-volume-c94492ab-6962-49fa-93a0-4b4609330a46 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar  2 02:49:52.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3927" for this suite. 03/02/23 02:49:52.603
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","completed":320,"skipped":5836,"failed":0}
------------------------------
• [SLOW TEST] [6.513 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:49:46.129
    Mar  2 02:49:46.129: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename downward-api 03/02/23 02:49:46.131
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:49:46.21
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:49:46.221
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:234
    STEP: Creating a pod to test downward API volume plugin 03/02/23 02:49:46.234
    Mar  2 02:49:46.316: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c94492ab-6962-49fa-93a0-4b4609330a46" in namespace "downward-api-3927" to be "Succeeded or Failed"
    Mar  2 02:49:46.333: INFO: Pod "downwardapi-volume-c94492ab-6962-49fa-93a0-4b4609330a46": Phase="Pending", Reason="", readiness=false. Elapsed: 16.608972ms
    Mar  2 02:49:48.348: INFO: Pod "downwardapi-volume-c94492ab-6962-49fa-93a0-4b4609330a46": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031929705s
    Mar  2 02:49:50.348: INFO: Pod "downwardapi-volume-c94492ab-6962-49fa-93a0-4b4609330a46": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031921816s
    Mar  2 02:49:52.348: INFO: Pod "downwardapi-volume-c94492ab-6962-49fa-93a0-4b4609330a46": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032025977s
    STEP: Saw pod success 03/02/23 02:49:52.348
    Mar  2 02:49:52.348: INFO: Pod "downwardapi-volume-c94492ab-6962-49fa-93a0-4b4609330a46" satisfied condition "Succeeded or Failed"
    Mar  2 02:49:52.362: INFO: Trying to get logs from node 10.132.92.143 pod downwardapi-volume-c94492ab-6962-49fa-93a0-4b4609330a46 container client-container: <nil>
    STEP: delete the pod 03/02/23 02:49:52.525
    Mar  2 02:49:52.566: INFO: Waiting for pod downwardapi-volume-c94492ab-6962-49fa-93a0-4b4609330a46 to disappear
    Mar  2 02:49:52.580: INFO: Pod downwardapi-volume-c94492ab-6962-49fa-93a0-4b4609330a46 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar  2 02:49:52.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-3927" for this suite. 03/02/23 02:49:52.603
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:49:52.649
Mar  2 02:49:52.649: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename kubectl 03/02/23 02:49:52.651
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:49:52.719
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:49:52.736
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
STEP: creating Agnhost RC 03/02/23 02:49:52.761
Mar  2 02:49:52.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-6805 create -f -'
Mar  2 02:49:55.694: INFO: stderr: ""
Mar  2 02:49:55.694: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 03/02/23 02:49:55.694
Mar  2 02:49:56.708: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 02:49:56.708: INFO: Found 0 / 1
Mar  2 02:49:57.707: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 02:49:57.707: INFO: Found 1 / 1
Mar  2 02:49:57.707: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 03/02/23 02:49:57.707
Mar  2 02:49:57.719: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 02:49:57.719: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  2 02:49:57.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-6805 patch pod agnhost-primary-hb422 -p {"metadata":{"annotations":{"x":"y"}}}'
Mar  2 02:49:57.904: INFO: stderr: ""
Mar  2 02:49:57.904: INFO: stdout: "pod/agnhost-primary-hb422 patched\n"
STEP: checking annotations 03/02/23 02:49:57.904
Mar  2 02:49:57.917: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 02:49:57.917: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  2 02:49:57.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6805" for this suite. 03/02/23 02:49:57.94
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","completed":321,"skipped":5859,"failed":0}
------------------------------
• [SLOW TEST] [5.313 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1644
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:49:52.649
    Mar  2 02:49:52.649: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename kubectl 03/02/23 02:49:52.651
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:49:52.719
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:49:52.736
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1650
    STEP: creating Agnhost RC 03/02/23 02:49:52.761
    Mar  2 02:49:52.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-6805 create -f -'
    Mar  2 02:49:55.694: INFO: stderr: ""
    Mar  2 02:49:55.694: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 03/02/23 02:49:55.694
    Mar  2 02:49:56.708: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar  2 02:49:56.708: INFO: Found 0 / 1
    Mar  2 02:49:57.707: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar  2 02:49:57.707: INFO: Found 1 / 1
    Mar  2 02:49:57.707: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 03/02/23 02:49:57.707
    Mar  2 02:49:57.719: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar  2 02:49:57.719: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Mar  2 02:49:57.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-6805 patch pod agnhost-primary-hb422 -p {"metadata":{"annotations":{"x":"y"}}}'
    Mar  2 02:49:57.904: INFO: stderr: ""
    Mar  2 02:49:57.904: INFO: stdout: "pod/agnhost-primary-hb422 patched\n"
    STEP: checking annotations 03/02/23 02:49:57.904
    Mar  2 02:49:57.917: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar  2 02:49:57.917: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  2 02:49:57.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6805" for this suite. 03/02/23 02:49:57.94
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:49:57.964
Mar  2 02:49:57.964: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename container-probe 03/02/23 02:49:57.965
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:49:58.061
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:49:58.078
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
STEP: Creating pod busybox-b07d4143-98cd-406c-8772-c4326fb36fef in namespace container-probe-8247 03/02/23 02:49:58.092
Mar  2 02:49:58.198: INFO: Waiting up to 5m0s for pod "busybox-b07d4143-98cd-406c-8772-c4326fb36fef" in namespace "container-probe-8247" to be "not pending"
Mar  2 02:49:58.211: INFO: Pod "busybox-b07d4143-98cd-406c-8772-c4326fb36fef": Phase="Pending", Reason="", readiness=false. Elapsed: 13.811624ms
Mar  2 02:50:00.226: INFO: Pod "busybox-b07d4143-98cd-406c-8772-c4326fb36fef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028814687s
Mar  2 02:50:02.228: INFO: Pod "busybox-b07d4143-98cd-406c-8772-c4326fb36fef": Phase="Running", Reason="", readiness=true. Elapsed: 4.030632394s
Mar  2 02:50:02.228: INFO: Pod "busybox-b07d4143-98cd-406c-8772-c4326fb36fef" satisfied condition "not pending"
Mar  2 02:50:02.228: INFO: Started pod busybox-b07d4143-98cd-406c-8772-c4326fb36fef in namespace container-probe-8247
STEP: checking the pod's current state and verifying that restartCount is present 03/02/23 02:50:02.228
Mar  2 02:50:02.248: INFO: Initial restart count of pod busybox-b07d4143-98cd-406c-8772-c4326fb36fef is 0
Mar  2 02:50:50.693: INFO: Restart count of pod container-probe-8247/busybox-b07d4143-98cd-406c-8772-c4326fb36fef is now 1 (48.44507542s elapsed)
STEP: deleting the pod 03/02/23 02:50:50.693
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar  2 02:50:50.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8247" for this suite. 03/02/23 02:50:50.794
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":322,"skipped":5877,"failed":0}
------------------------------
• [SLOW TEST] [52.859 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:49:57.964
    Mar  2 02:49:57.964: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename container-probe 03/02/23 02:49:57.965
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:49:58.061
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:49:58.078
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:131
    STEP: Creating pod busybox-b07d4143-98cd-406c-8772-c4326fb36fef in namespace container-probe-8247 03/02/23 02:49:58.092
    Mar  2 02:49:58.198: INFO: Waiting up to 5m0s for pod "busybox-b07d4143-98cd-406c-8772-c4326fb36fef" in namespace "container-probe-8247" to be "not pending"
    Mar  2 02:49:58.211: INFO: Pod "busybox-b07d4143-98cd-406c-8772-c4326fb36fef": Phase="Pending", Reason="", readiness=false. Elapsed: 13.811624ms
    Mar  2 02:50:00.226: INFO: Pod "busybox-b07d4143-98cd-406c-8772-c4326fb36fef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028814687s
    Mar  2 02:50:02.228: INFO: Pod "busybox-b07d4143-98cd-406c-8772-c4326fb36fef": Phase="Running", Reason="", readiness=true. Elapsed: 4.030632394s
    Mar  2 02:50:02.228: INFO: Pod "busybox-b07d4143-98cd-406c-8772-c4326fb36fef" satisfied condition "not pending"
    Mar  2 02:50:02.228: INFO: Started pod busybox-b07d4143-98cd-406c-8772-c4326fb36fef in namespace container-probe-8247
    STEP: checking the pod's current state and verifying that restartCount is present 03/02/23 02:50:02.228
    Mar  2 02:50:02.248: INFO: Initial restart count of pod busybox-b07d4143-98cd-406c-8772-c4326fb36fef is 0
    Mar  2 02:50:50.693: INFO: Restart count of pod container-probe-8247/busybox-b07d4143-98cd-406c-8772-c4326fb36fef is now 1 (48.44507542s elapsed)
    STEP: deleting the pod 03/02/23 02:50:50.693
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar  2 02:50:50.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-8247" for this suite. 03/02/23 02:50:50.794
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:50:50.824
Mar  2 02:50:50.824: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename init-container 03/02/23 02:50:50.825
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:50:50.895
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:50:50.916
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
STEP: creating the pod 03/02/23 02:50:50.933
Mar  2 02:50:50.933: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Mar  2 02:50:57.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3665" for this suite. 03/02/23 02:50:57.065
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","completed":323,"skipped":5879,"failed":0}
------------------------------
• [SLOW TEST] [6.271 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:50:50.824
    Mar  2 02:50:50.824: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename init-container 03/02/23 02:50:50.825
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:50:50.895
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:50:50.916
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:176
    STEP: creating the pod 03/02/23 02:50:50.933
    Mar  2 02:50:50.933: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Mar  2 02:50:57.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-3665" for this suite. 03/02/23 02:50:57.065
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:50:57.102
Mar  2 02:50:57.103: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename pods 03/02/23 02:50:57.106
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:50:57.2
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:50:57.214
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
Mar  2 02:50:57.236: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: creating the pod 03/02/23 02:50:57.236
STEP: submitting the pod to kubernetes 03/02/23 02:50:57.237
Mar  2 02:50:57.303: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-14be5dbb-83b1-46aa-8e53-6e9450df726c" in namespace "pods-905" to be "running and ready"
Mar  2 02:50:57.317: INFO: Pod "pod-exec-websocket-14be5dbb-83b1-46aa-8e53-6e9450df726c": Phase="Pending", Reason="", readiness=false. Elapsed: 13.416236ms
Mar  2 02:50:57.317: INFO: The phase of Pod pod-exec-websocket-14be5dbb-83b1-46aa-8e53-6e9450df726c is Pending, waiting for it to be Running (with Ready = true)
Mar  2 02:50:59.332: INFO: Pod "pod-exec-websocket-14be5dbb-83b1-46aa-8e53-6e9450df726c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028249243s
Mar  2 02:50:59.332: INFO: The phase of Pod pod-exec-websocket-14be5dbb-83b1-46aa-8e53-6e9450df726c is Pending, waiting for it to be Running (with Ready = true)
Mar  2 02:51:01.357: INFO: Pod "pod-exec-websocket-14be5dbb-83b1-46aa-8e53-6e9450df726c": Phase="Running", Reason="", readiness=true. Elapsed: 4.047354196s
Mar  2 02:51:01.357: INFO: The phase of Pod pod-exec-websocket-14be5dbb-83b1-46aa-8e53-6e9450df726c is Running (Ready = true)
Mar  2 02:51:01.358: INFO: Pod "pod-exec-websocket-14be5dbb-83b1-46aa-8e53-6e9450df726c" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar  2 02:51:01.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-905" for this suite. 03/02/23 02:51:01.697
{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","completed":324,"skipped":5889,"failed":0}
------------------------------
• [4.619 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:50:57.102
    Mar  2 02:50:57.103: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename pods 03/02/23 02:50:57.106
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:50:57.2
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:50:57.214
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:535
    Mar  2 02:50:57.236: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: creating the pod 03/02/23 02:50:57.236
    STEP: submitting the pod to kubernetes 03/02/23 02:50:57.237
    Mar  2 02:50:57.303: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-14be5dbb-83b1-46aa-8e53-6e9450df726c" in namespace "pods-905" to be "running and ready"
    Mar  2 02:50:57.317: INFO: Pod "pod-exec-websocket-14be5dbb-83b1-46aa-8e53-6e9450df726c": Phase="Pending", Reason="", readiness=false. Elapsed: 13.416236ms
    Mar  2 02:50:57.317: INFO: The phase of Pod pod-exec-websocket-14be5dbb-83b1-46aa-8e53-6e9450df726c is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 02:50:59.332: INFO: Pod "pod-exec-websocket-14be5dbb-83b1-46aa-8e53-6e9450df726c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028249243s
    Mar  2 02:50:59.332: INFO: The phase of Pod pod-exec-websocket-14be5dbb-83b1-46aa-8e53-6e9450df726c is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 02:51:01.357: INFO: Pod "pod-exec-websocket-14be5dbb-83b1-46aa-8e53-6e9450df726c": Phase="Running", Reason="", readiness=true. Elapsed: 4.047354196s
    Mar  2 02:51:01.357: INFO: The phase of Pod pod-exec-websocket-14be5dbb-83b1-46aa-8e53-6e9450df726c is Running (Ready = true)
    Mar  2 02:51:01.358: INFO: Pod "pod-exec-websocket-14be5dbb-83b1-46aa-8e53-6e9450df726c" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar  2 02:51:01.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-905" for this suite. 03/02/23 02:51:01.697
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:51:01.722
Mar  2 02:51:01.723: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename podtemplate 03/02/23 02:51:01.724
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:51:01.789
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:51:01.803
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 03/02/23 02:51:01.825
Mar  2 02:51:01.862: INFO: created test-podtemplate-1
Mar  2 02:51:01.895: INFO: created test-podtemplate-2
Mar  2 02:51:01.919: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 03/02/23 02:51:01.919
STEP: delete collection of pod templates 03/02/23 02:51:01.941
Mar  2 02:51:01.941: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 03/02/23 02:51:02.052
Mar  2 02:51:02.052: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Mar  2 02:51:02.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-5790" for this suite. 03/02/23 02:51:02.119
{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","completed":325,"skipped":5890,"failed":0}
------------------------------
• [0.431 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:51:01.722
    Mar  2 02:51:01.723: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename podtemplate 03/02/23 02:51:01.724
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:51:01.789
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:51:01.803
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 03/02/23 02:51:01.825
    Mar  2 02:51:01.862: INFO: created test-podtemplate-1
    Mar  2 02:51:01.895: INFO: created test-podtemplate-2
    Mar  2 02:51:01.919: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 03/02/23 02:51:01.919
    STEP: delete collection of pod templates 03/02/23 02:51:01.941
    Mar  2 02:51:01.941: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 03/02/23 02:51:02.052
    Mar  2 02:51:02.052: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Mar  2 02:51:02.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-5790" for this suite. 03/02/23 02:51:02.119
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:51:02.156
Mar  2 02:51:02.156: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename statefulset 03/02/23 02:51:02.158
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:51:02.219
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:51:02.232
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-591 03/02/23 02:51:02.248
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
STEP: Creating a new StatefulSet 03/02/23 02:51:02.268
W0302 02:51:02.308458      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Mar  2 02:51:02.326: INFO: Found 0 stateful pods, waiting for 3
Mar  2 02:51:12.344: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 02:51:12.345: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 02:51:12.345: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 02:51:12.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=statefulset-591 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 02:51:12.842: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 02:51:12.842: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 02:51:12.842: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 03/02/23 02:51:22.906
Mar  2 02:51:22.955: INFO: Updating stateful set ss2
STEP: Creating a new revision 03/02/23 02:51:22.955
STEP: Updating Pods in reverse ordinal order 03/02/23 02:51:33.018
Mar  2 02:51:33.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=statefulset-591 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 02:51:33.350: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  2 02:51:33.350: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 02:51:33.350: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 02:51:33.426: INFO: Waiting for StatefulSet statefulset-591/ss2 to complete update
Mar  2 02:51:33.426: INFO: Waiting for Pod statefulset-591/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Mar  2 02:51:33.426: INFO: Waiting for Pod statefulset-591/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Mar  2 02:51:33.426: INFO: Waiting for Pod statefulset-591/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Mar  2 02:51:43.463: INFO: Waiting for StatefulSet statefulset-591/ss2 to complete update
Mar  2 02:51:43.463: INFO: Waiting for Pod statefulset-591/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
STEP: Rolling back to a previous revision 03/02/23 02:51:53.459
Mar  2 02:51:53.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=statefulset-591 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 02:51:53.816: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 02:51:53.816: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 02:51:53.816: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 02:52:03.952: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 03/02/23 02:52:14.037
Mar  2 02:52:14.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=statefulset-591 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 02:52:14.500: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  2 02:52:14.500: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 02:52:14.500: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 02:52:24.599: INFO: Waiting for StatefulSet statefulset-591/ss2 to complete update
Mar  2 02:52:24.599: INFO: Waiting for Pod statefulset-591/ss2-0 to have revision ss2-6557876d87 update revision ss2-5d8c6ff87d
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar  2 02:52:34.632: INFO: Deleting all statefulset in ns statefulset-591
Mar  2 02:52:34.645: INFO: Scaling statefulset ss2 to 0
Mar  2 02:52:44.711: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 02:52:44.725: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar  2 02:52:44.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-591" for this suite. 03/02/23 02:52:44.799
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","completed":326,"skipped":5893,"failed":0}
------------------------------
• [SLOW TEST] [102.673 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:304

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:51:02.156
    Mar  2 02:51:02.156: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename statefulset 03/02/23 02:51:02.158
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:51:02.219
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:51:02.232
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-591 03/02/23 02:51:02.248
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:304
    STEP: Creating a new StatefulSet 03/02/23 02:51:02.268
    W0302 02:51:02.308458      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Mar  2 02:51:02.326: INFO: Found 0 stateful pods, waiting for 3
    Mar  2 02:51:12.344: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar  2 02:51:12.345: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar  2 02:51:12.345: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Mar  2 02:51:12.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=statefulset-591 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar  2 02:51:12.842: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar  2 02:51:12.842: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar  2 02:51:12.842: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 03/02/23 02:51:22.906
    Mar  2 02:51:22.955: INFO: Updating stateful set ss2
    STEP: Creating a new revision 03/02/23 02:51:22.955
    STEP: Updating Pods in reverse ordinal order 03/02/23 02:51:33.018
    Mar  2 02:51:33.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=statefulset-591 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar  2 02:51:33.350: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar  2 02:51:33.350: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar  2 02:51:33.350: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar  2 02:51:33.426: INFO: Waiting for StatefulSet statefulset-591/ss2 to complete update
    Mar  2 02:51:33.426: INFO: Waiting for Pod statefulset-591/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Mar  2 02:51:33.426: INFO: Waiting for Pod statefulset-591/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Mar  2 02:51:33.426: INFO: Waiting for Pod statefulset-591/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Mar  2 02:51:43.463: INFO: Waiting for StatefulSet statefulset-591/ss2 to complete update
    Mar  2 02:51:43.463: INFO: Waiting for Pod statefulset-591/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    STEP: Rolling back to a previous revision 03/02/23 02:51:53.459
    Mar  2 02:51:53.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=statefulset-591 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar  2 02:51:53.816: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar  2 02:51:53.816: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar  2 02:51:53.816: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar  2 02:52:03.952: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 03/02/23 02:52:14.037
    Mar  2 02:52:14.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=statefulset-591 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar  2 02:52:14.500: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar  2 02:52:14.500: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar  2 02:52:14.500: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar  2 02:52:24.599: INFO: Waiting for StatefulSet statefulset-591/ss2 to complete update
    Mar  2 02:52:24.599: INFO: Waiting for Pod statefulset-591/ss2-0 to have revision ss2-6557876d87 update revision ss2-5d8c6ff87d
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar  2 02:52:34.632: INFO: Deleting all statefulset in ns statefulset-591
    Mar  2 02:52:34.645: INFO: Scaling statefulset ss2 to 0
    Mar  2 02:52:44.711: INFO: Waiting for statefulset status.replicas updated to 0
    Mar  2 02:52:44.725: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar  2 02:52:44.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-591" for this suite. 03/02/23 02:52:44.799
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:52:44.832
Mar  2 02:52:44.832: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename container-probe 03/02/23 02:52:44.833
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:52:44.898
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:52:44.908
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
STEP: Creating pod busybox-8d3c9a9b-1785-496b-947d-3df23e8b991a in namespace container-probe-379 03/02/23 02:52:44.924
Mar  2 02:52:44.985: INFO: Waiting up to 5m0s for pod "busybox-8d3c9a9b-1785-496b-947d-3df23e8b991a" in namespace "container-probe-379" to be "not pending"
Mar  2 02:52:45.028: INFO: Pod "busybox-8d3c9a9b-1785-496b-947d-3df23e8b991a": Phase="Pending", Reason="", readiness=false. Elapsed: 43.165472ms
Mar  2 02:52:47.043: INFO: Pod "busybox-8d3c9a9b-1785-496b-947d-3df23e8b991a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058194133s
Mar  2 02:52:49.059: INFO: Pod "busybox-8d3c9a9b-1785-496b-947d-3df23e8b991a": Phase="Running", Reason="", readiness=true. Elapsed: 4.07408366s
Mar  2 02:52:49.059: INFO: Pod "busybox-8d3c9a9b-1785-496b-947d-3df23e8b991a" satisfied condition "not pending"
Mar  2 02:52:49.059: INFO: Started pod busybox-8d3c9a9b-1785-496b-947d-3df23e8b991a in namespace container-probe-379
STEP: checking the pod's current state and verifying that restartCount is present 03/02/23 02:52:49.059
Mar  2 02:52:49.071: INFO: Initial restart count of pod busybox-8d3c9a9b-1785-496b-947d-3df23e8b991a is 0
STEP: deleting the pod 03/02/23 02:56:49.143
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar  2 02:56:49.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-379" for this suite. 03/02/23 02:56:49.221
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":327,"skipped":5947,"failed":0}
------------------------------
• [SLOW TEST] [244.414 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:52:44.832
    Mar  2 02:52:44.832: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename container-probe 03/02/23 02:52:44.833
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:52:44.898
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:52:44.908
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:148
    STEP: Creating pod busybox-8d3c9a9b-1785-496b-947d-3df23e8b991a in namespace container-probe-379 03/02/23 02:52:44.924
    Mar  2 02:52:44.985: INFO: Waiting up to 5m0s for pod "busybox-8d3c9a9b-1785-496b-947d-3df23e8b991a" in namespace "container-probe-379" to be "not pending"
    Mar  2 02:52:45.028: INFO: Pod "busybox-8d3c9a9b-1785-496b-947d-3df23e8b991a": Phase="Pending", Reason="", readiness=false. Elapsed: 43.165472ms
    Mar  2 02:52:47.043: INFO: Pod "busybox-8d3c9a9b-1785-496b-947d-3df23e8b991a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058194133s
    Mar  2 02:52:49.059: INFO: Pod "busybox-8d3c9a9b-1785-496b-947d-3df23e8b991a": Phase="Running", Reason="", readiness=true. Elapsed: 4.07408366s
    Mar  2 02:52:49.059: INFO: Pod "busybox-8d3c9a9b-1785-496b-947d-3df23e8b991a" satisfied condition "not pending"
    Mar  2 02:52:49.059: INFO: Started pod busybox-8d3c9a9b-1785-496b-947d-3df23e8b991a in namespace container-probe-379
    STEP: checking the pod's current state and verifying that restartCount is present 03/02/23 02:52:49.059
    Mar  2 02:52:49.071: INFO: Initial restart count of pod busybox-8d3c9a9b-1785-496b-947d-3df23e8b991a is 0
    STEP: deleting the pod 03/02/23 02:56:49.143
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar  2 02:56:49.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-379" for this suite. 03/02/23 02:56:49.221
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:56:49.249
Mar  2 02:56:49.249: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename webhook 03/02/23 02:56:49.25
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:56:49.331
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:56:49.345
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/02/23 02:56:49.406
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 02:56:50.001
STEP: Deploying the webhook pod 03/02/23 02:56:50.034
STEP: Wait for the deployment to be ready 03/02/23 02:56:50.076
Mar  2 02:56:50.106: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/02/23 02:56:52.154
STEP: Verifying the service has paired with the endpoint 03/02/23 02:56:52.194
Mar  2 02:56:53.195: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
STEP: Listing all of the created validation webhooks 03/02/23 02:56:53.446
STEP: Creating a configMap that should be mutated 03/02/23 02:56:53.508
STEP: Deleting the collection of validation webhooks 03/02/23 02:56:53.654
STEP: Creating a configMap that should not be mutated 03/02/23 02:56:53.964
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 02:56:54.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7197" for this suite. 03/02/23 02:56:54.058
STEP: Destroying namespace "webhook-7197-markers" for this suite. 03/02/23 02:56:54.081
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","completed":328,"skipped":5965,"failed":0}
------------------------------
• [SLOW TEST] [5.055 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:56:49.249
    Mar  2 02:56:49.249: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename webhook 03/02/23 02:56:49.25
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:56:49.331
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:56:49.345
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/02/23 02:56:49.406
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 02:56:50.001
    STEP: Deploying the webhook pod 03/02/23 02:56:50.034
    STEP: Wait for the deployment to be ready 03/02/23 02:56:50.076
    Mar  2 02:56:50.106: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/02/23 02:56:52.154
    STEP: Verifying the service has paired with the endpoint 03/02/23 02:56:52.194
    Mar  2 02:56:53.195: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:655
    STEP: Listing all of the created validation webhooks 03/02/23 02:56:53.446
    STEP: Creating a configMap that should be mutated 03/02/23 02:56:53.508
    STEP: Deleting the collection of validation webhooks 03/02/23 02:56:53.654
    STEP: Creating a configMap that should not be mutated 03/02/23 02:56:53.964
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 02:56:54.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7197" for this suite. 03/02/23 02:56:54.058
    STEP: Destroying namespace "webhook-7197-markers" for this suite. 03/02/23 02:56:54.081
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:56:54.304
Mar  2 02:56:54.304: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename dns 03/02/23 02:56:54.306
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:56:54.473
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:56:54.49
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 03/02/23 02:56:54.512
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1464.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-1464.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 03/02/23 02:56:54.557
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1464.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-1464.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 03/02/23 02:56:54.557
STEP: creating a pod to probe DNS 03/02/23 02:56:54.558
STEP: submitting the pod to kubernetes 03/02/23 02:56:54.558
Mar  2 02:56:54.730: INFO: Waiting up to 15m0s for pod "dns-test-76a46edd-f7c0-4831-bed6-ebd76a1f7edc" in namespace "dns-1464" to be "running"
Mar  2 02:56:54.745: INFO: Pod "dns-test-76a46edd-f7c0-4831-bed6-ebd76a1f7edc": Phase="Pending", Reason="", readiness=false. Elapsed: 15.744045ms
Mar  2 02:56:56.766: INFO: Pod "dns-test-76a46edd-f7c0-4831-bed6-ebd76a1f7edc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036201727s
Mar  2 02:56:58.761: INFO: Pod "dns-test-76a46edd-f7c0-4831-bed6-ebd76a1f7edc": Phase="Running", Reason="", readiness=true. Elapsed: 4.031637271s
Mar  2 02:56:58.761: INFO: Pod "dns-test-76a46edd-f7c0-4831-bed6-ebd76a1f7edc" satisfied condition "running"
STEP: retrieving the pod 03/02/23 02:56:58.762
STEP: looking for the results for each expected name from probers 03/02/23 02:56:58.776
Mar  2 02:56:58.870: INFO: DNS probes using dns-1464/dns-test-76a46edd-f7c0-4831-bed6-ebd76a1f7edc succeeded

STEP: deleting the pod 03/02/23 02:56:58.87
STEP: deleting the test headless service 03/02/23 02:56:58.914
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar  2 02:56:58.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1464" for this suite. 03/02/23 02:56:59.004
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [Conformance]","completed":329,"skipped":5967,"failed":0}
------------------------------
• [4.728 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:56:54.304
    Mar  2 02:56:54.304: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename dns 03/02/23 02:56:54.306
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:56:54.473
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:56:54.49
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 03/02/23 02:56:54.512
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1464.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-1464.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     03/02/23 02:56:54.557
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1464.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-1464.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     03/02/23 02:56:54.557
    STEP: creating a pod to probe DNS 03/02/23 02:56:54.558
    STEP: submitting the pod to kubernetes 03/02/23 02:56:54.558
    Mar  2 02:56:54.730: INFO: Waiting up to 15m0s for pod "dns-test-76a46edd-f7c0-4831-bed6-ebd76a1f7edc" in namespace "dns-1464" to be "running"
    Mar  2 02:56:54.745: INFO: Pod "dns-test-76a46edd-f7c0-4831-bed6-ebd76a1f7edc": Phase="Pending", Reason="", readiness=false. Elapsed: 15.744045ms
    Mar  2 02:56:56.766: INFO: Pod "dns-test-76a46edd-f7c0-4831-bed6-ebd76a1f7edc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036201727s
    Mar  2 02:56:58.761: INFO: Pod "dns-test-76a46edd-f7c0-4831-bed6-ebd76a1f7edc": Phase="Running", Reason="", readiness=true. Elapsed: 4.031637271s
    Mar  2 02:56:58.761: INFO: Pod "dns-test-76a46edd-f7c0-4831-bed6-ebd76a1f7edc" satisfied condition "running"
    STEP: retrieving the pod 03/02/23 02:56:58.762
    STEP: looking for the results for each expected name from probers 03/02/23 02:56:58.776
    Mar  2 02:56:58.870: INFO: DNS probes using dns-1464/dns-test-76a46edd-f7c0-4831-bed6-ebd76a1f7edc succeeded

    STEP: deleting the pod 03/02/23 02:56:58.87
    STEP: deleting the test headless service 03/02/23 02:56:58.914
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar  2 02:56:58.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-1464" for this suite. 03/02/23 02:56:59.004
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:56:59.034
Mar  2 02:56:59.034: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename sched-preemption 03/02/23 02:56:59.036
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:56:59.097
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:56:59.114
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Mar  2 02:56:59.187: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  2 02:57:59.420: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
STEP: Create pods that use 4/5 of node resources. 03/02/23 02:57:59.45
Mar  2 02:57:59.558: INFO: Created pod: pod0-0-sched-preemption-low-priority
Mar  2 02:57:59.612: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Mar  2 02:57:59.720: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Mar  2 02:57:59.765: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Mar  2 02:57:59.850: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Mar  2 02:57:59.902: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 03/02/23 02:57:59.902
Mar  2 02:57:59.903: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-219" to be "running"
Mar  2 02:57:59.915: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 11.836363ms
Mar  2 02:58:01.931: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028059917s
Mar  2 02:58:03.931: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028543885s
Mar  2 02:58:05.935: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.032282985s
Mar  2 02:58:05.935: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Mar  2 02:58:05.935: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-219" to be "running"
Mar  2 02:58:05.957: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 21.55669ms
Mar  2 02:58:05.957: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Mar  2 02:58:05.957: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-219" to be "running"
Mar  2 02:58:05.977: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 20.032321ms
Mar  2 02:58:07.990: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033561662s
Mar  2 02:58:09.991: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.034013576s
Mar  2 02:58:09.991: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Mar  2 02:58:09.991: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-219" to be "running"
Mar  2 02:58:10.003: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 12.008131ms
Mar  2 02:58:10.003: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Mar  2 02:58:10.003: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-219" to be "running"
Mar  2 02:58:10.016: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 13.280315ms
Mar  2 02:58:10.016: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Mar  2 02:58:10.016: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-219" to be "running"
Mar  2 02:58:10.030: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 13.970721ms
Mar  2 02:58:10.030: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 03/02/23 02:58:10.03
Mar  2 02:58:10.070: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Mar  2 02:58:10.082: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 11.589042ms
Mar  2 02:58:12.112: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042053189s
Mar  2 02:58:14.096: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025711076s
Mar  2 02:58:16.098: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.027324377s
Mar  2 02:58:16.098: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Mar  2 02:58:16.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-219" for this suite. 03/02/23 02:58:16.291
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","completed":330,"skipped":5976,"failed":0}
------------------------------
• [SLOW TEST] [77.583 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:56:59.034
    Mar  2 02:56:59.034: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename sched-preemption 03/02/23 02:56:59.036
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:56:59.097
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:56:59.114
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Mar  2 02:56:59.187: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar  2 02:57:59.420: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:218
    STEP: Create pods that use 4/5 of node resources. 03/02/23 02:57:59.45
    Mar  2 02:57:59.558: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Mar  2 02:57:59.612: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Mar  2 02:57:59.720: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Mar  2 02:57:59.765: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Mar  2 02:57:59.850: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Mar  2 02:57:59.902: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 03/02/23 02:57:59.902
    Mar  2 02:57:59.903: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-219" to be "running"
    Mar  2 02:57:59.915: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 11.836363ms
    Mar  2 02:58:01.931: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028059917s
    Mar  2 02:58:03.931: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028543885s
    Mar  2 02:58:05.935: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.032282985s
    Mar  2 02:58:05.935: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Mar  2 02:58:05.935: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-219" to be "running"
    Mar  2 02:58:05.957: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 21.55669ms
    Mar  2 02:58:05.957: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Mar  2 02:58:05.957: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-219" to be "running"
    Mar  2 02:58:05.977: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 20.032321ms
    Mar  2 02:58:07.990: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033561662s
    Mar  2 02:58:09.991: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.034013576s
    Mar  2 02:58:09.991: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Mar  2 02:58:09.991: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-219" to be "running"
    Mar  2 02:58:10.003: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 12.008131ms
    Mar  2 02:58:10.003: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Mar  2 02:58:10.003: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-219" to be "running"
    Mar  2 02:58:10.016: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 13.280315ms
    Mar  2 02:58:10.016: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Mar  2 02:58:10.016: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-219" to be "running"
    Mar  2 02:58:10.030: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 13.970721ms
    Mar  2 02:58:10.030: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 03/02/23 02:58:10.03
    Mar  2 02:58:10.070: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Mar  2 02:58:10.082: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 11.589042ms
    Mar  2 02:58:12.112: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042053189s
    Mar  2 02:58:14.096: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025711076s
    Mar  2 02:58:16.098: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.027324377s
    Mar  2 02:58:16.098: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Mar  2 02:58:16.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-219" for this suite. 03/02/23 02:58:16.291
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:58:16.62
Mar  2 02:58:16.620: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename configmap 03/02/23 02:58:16.622
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:58:16.769
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:58:16.798
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar  2 02:58:17.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3694" for this suite. 03/02/23 02:58:17.129
{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","completed":331,"skipped":5991,"failed":0}
------------------------------
• [0.540 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:58:16.62
    Mar  2 02:58:16.620: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename configmap 03/02/23 02:58:16.622
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:58:16.769
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:58:16.798
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:503
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  2 02:58:17.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-3694" for this suite. 03/02/23 02:58:17.129
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:58:17.164
Mar  2 02:58:17.164: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename sched-pred 03/02/23 02:58:17.165
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:58:17.264
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:58:17.28
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Mar  2 02:58:17.291: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  2 02:58:17.367: INFO: Waiting for terminating namespaces to be deleted...
Mar  2 02:58:17.390: INFO: 
Logging pods the apiserver thinks is on node 10.132.92.143 before test
Mar  2 02:58:17.452: INFO: calico-node-99tft from calico-system started at 2023-03-01 22:49:50 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.453: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 02:58:17.453: INFO: calico-typha-b7f8b755-xlnll from calico-system started at 2023-03-01 22:49:57 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.453: INFO: 	Container calico-typha ready: true, restart count 0
Mar  2 02:58:17.453: INFO: ibm-keepalived-watcher-dp97c from kube-system started at 2023-03-01 22:48:16 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.453: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar  2 02:58:17.453: INFO: ibm-master-proxy-static-10.132.92.143 from kube-system started at 2023-03-01 22:48:15 +0000 UTC (2 container statuses recorded)
Mar  2 02:58:17.453: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar  2 02:58:17.453: INFO: 	Container pause ready: true, restart count 0
Mar  2 02:58:17.453: INFO: ibmcloud-block-storage-driver-zfc8r from kube-system started at 2023-03-01 22:48:23 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.453: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar  2 02:58:17.453: INFO: tuned-w95xr from openshift-cluster-node-tuning-operator started at 2023-03-01 22:55:42 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.454: INFO: 	Container tuned ready: true, restart count 0
Mar  2 02:58:17.454: INFO: dns-default-w2mf4 from openshift-dns started at 2023-03-02 02:32:54 +0000 UTC (2 container statuses recorded)
Mar  2 02:58:17.454: INFO: 	Container dns ready: true, restart count 0
Mar  2 02:58:17.454: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:58:17.454: INFO: node-resolver-7wklz from openshift-dns started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.454: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 02:58:17.454: INFO: node-ca-q4vcl from openshift-image-registry started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.454: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 02:58:17.454: INFO: registry-pvc-permissions-6wblc from openshift-image-registry started at 2023-03-01 22:58:39 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.454: INFO: 	Container pvc-permissions ready: false, restart count 0
Mar  2 02:58:17.454: INFO: ingress-canary-bz9g8 from openshift-ingress-canary started at 2023-03-02 02:32:35 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.454: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Mar  2 02:58:17.454: INFO: openshift-kube-proxy-zdts7 from openshift-kube-proxy started at 2023-03-01 22:49:00 +0000 UTC (2 container statuses recorded)
Mar  2 02:58:17.454: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  2 02:58:17.454: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:58:17.454: INFO: node-exporter-z52nh from openshift-monitoring started at 2023-03-01 22:56:40 +0000 UTC (2 container statuses recorded)
Mar  2 02:58:17.454: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:58:17.454: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 02:58:17.454: INFO: multus-48w86 from openshift-multus started at 2023-03-01 22:48:55 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.454: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 02:58:17.454: INFO: multus-additional-cni-plugins-d2kv5 from openshift-multus started at 2023-03-01 22:48:55 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.455: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Mar  2 02:58:17.455: INFO: network-metrics-daemon-7jfj4 from openshift-multus started at 2023-03-01 22:48:56 +0000 UTC (2 container statuses recorded)
Mar  2 02:58:17.455: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:58:17.455: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Mar  2 02:58:17.455: INFO: network-check-target-w7x6h from openshift-network-diagnostics started at 2023-03-01 22:49:03 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.455: INFO: 	Container network-check-target-container ready: true, restart count 0
Mar  2 02:58:17.455: INFO: pod0-1-sched-preemption-medium-priority from sched-preemption-219 started at 2023-03-02 02:58:02 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.455: INFO: 	Container pod0-1-sched-preemption-medium-priority ready: true, restart count 0
Mar  2 02:58:17.455: INFO: sonobuoy from sonobuoy started at 2023-03-02 01:07:05 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.455: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  2 02:58:17.455: INFO: sonobuoy-systemd-logs-daemon-set-7266eb5e626146f7-4c6tp from sonobuoy started at 2023-03-02 01:07:14 +0000 UTC (2 container statuses recorded)
Mar  2 02:58:17.455: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 02:58:17.455: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 02:58:17.455: INFO: 
Logging pods the apiserver thinks is on node 10.132.92.186 before test
Mar  2 02:58:17.528: INFO: calico-kube-controllers-5b6b964466-vlvht from calico-system started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.528: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar  2 02:58:17.528: INFO: calico-node-5vvbw from calico-system started at 2023-03-01 22:49:50 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.528: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 02:58:17.528: INFO: calico-typha-b7f8b755-bdnm9 from calico-system started at 2023-03-01 22:49:50 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.528: INFO: 	Container calico-typha ready: true, restart count 0
Mar  2 02:58:17.528: INFO: managed-storage-validation-webhooks-757b765d54-f72mn from ibm-odf-validation-webhook started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.528: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Mar  2 02:58:17.528: INFO: managed-storage-validation-webhooks-757b765d54-gshct from ibm-odf-validation-webhook started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.528: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Mar  2 02:58:17.528: INFO: managed-storage-validation-webhooks-757b765d54-h4j5p from ibm-odf-validation-webhook started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.528: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Mar  2 02:58:17.528: INFO: ibm-cloud-provider-ip-169-56-11-242-558f6ff69-s4dz5 from ibm-system started at 2023-03-02 02:09:21 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.528: INFO: 	Container ibm-cloud-provider-ip-169-56-11-242 ready: true, restart count 0
Mar  2 02:58:17.528: INFO: ibm-file-plugin-68bfbbdc7-77m77 from kube-system started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.528: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Mar  2 02:58:17.528: INFO: ibm-keepalived-watcher-knrjl from kube-system started at 2023-03-01 22:48:27 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.528: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar  2 02:58:17.528: INFO: ibm-master-proxy-static-10.132.92.186 from kube-system started at 2023-03-01 22:48:22 +0000 UTC (2 container statuses recorded)
Mar  2 02:58:17.528: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar  2 02:58:17.528: INFO: 	Container pause ready: true, restart count 0
Mar  2 02:58:17.528: INFO: ibm-storage-metrics-agent-6c459b7855-cv8d6 from kube-system started at 2023-03-01 22:51:20 +0000 UTC (2 container statuses recorded)
Mar  2 02:58:17.528: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
Mar  2 02:58:17.528: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Mar  2 02:58:17.528: INFO: ibm-storage-watcher-67589fd788-z5w99 from kube-system started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.528: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Mar  2 02:58:17.528: INFO: ibmcloud-block-storage-driver-l498z from kube-system started at 2023-03-01 22:48:36 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.528: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar  2 02:58:17.528: INFO: ibmcloud-block-storage-plugin-86f4cbdf95-klfqp from kube-system started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.528: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Mar  2 02:58:17.529: INFO: cluster-node-tuning-operator-5df67c68d9-gd552 from openshift-cluster-node-tuning-operator started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.529: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Mar  2 02:58:17.529: INFO: tuned-h4jnq from openshift-cluster-node-tuning-operator started at 2023-03-01 22:55:42 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.529: INFO: 	Container tuned ready: true, restart count 0
Mar  2 02:58:17.529: INFO: cluster-samples-operator-5948f44764-88mzn from openshift-cluster-samples-operator started at 2023-03-01 22:51:20 +0000 UTC (2 container statuses recorded)
Mar  2 02:58:17.529: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Mar  2 02:58:17.529: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Mar  2 02:58:17.529: INFO: cluster-storage-operator-54cf568cb7-smstf from openshift-cluster-storage-operator started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.529: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Mar  2 02:58:17.529: INFO: csi-snapshot-controller-85d666c956-5hhrv from openshift-cluster-storage-operator started at 2023-03-01 22:51:57 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.529: INFO: 	Container snapshot-controller ready: true, restart count 0
Mar  2 02:58:17.529: INFO: csi-snapshot-controller-operator-6b76cb8c94-9bvdj from openshift-cluster-storage-operator started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.529: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Mar  2 02:58:17.529: INFO: csi-snapshot-webhook-6cbf55b956-z69cb from openshift-cluster-storage-operator started at 2023-03-01 22:51:57 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.529: INFO: 	Container webhook ready: true, restart count 0
Mar  2 02:58:17.529: INFO: console-operator-6975f688f8-xwx92 from openshift-console-operator started at 2023-03-01 22:51:20 +0000 UTC (2 container statuses recorded)
Mar  2 02:58:17.529: INFO: 	Container console-operator ready: true, restart count 1
Mar  2 02:58:17.529: INFO: 	Container conversion-webhook-server ready: true, restart count 3
Mar  2 02:58:17.529: INFO: console-6c8dcd4bdd-c2dbn from openshift-console started at 2023-03-01 22:59:49 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.529: INFO: 	Container console ready: true, restart count 0
Mar  2 02:58:17.529: INFO: downloads-8b4bf4d5-pjlj4 from openshift-console started at 2023-03-01 22:51:55 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.529: INFO: 	Container download-server ready: true, restart count 0
Mar  2 02:58:17.529: INFO: dns-operator-b8598b55c-87ghp from openshift-dns-operator started at 2023-03-01 22:51:20 +0000 UTC (2 container statuses recorded)
Mar  2 02:58:17.529: INFO: 	Container dns-operator ready: true, restart count 0
Mar  2 02:58:17.529: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:58:17.529: INFO: dns-default-rdd5h from openshift-dns started at 2023-03-01 22:55:41 +0000 UTC (2 container statuses recorded)
Mar  2 02:58:17.529: INFO: 	Container dns ready: true, restart count 0
Mar  2 02:58:17.529: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:58:17.529: INFO: node-resolver-9wrks from openshift-dns started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.529: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 02:58:17.529: INFO: cluster-image-registry-operator-797c5fb5f8-tvjwj from openshift-image-registry started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.529: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Mar  2 02:58:17.529: INFO: node-ca-6ffsq from openshift-image-registry started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.529: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 02:58:17.529: INFO: ingress-canary-zgjd4 from openshift-ingress-canary started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.529: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Mar  2 02:58:17.529: INFO: ingress-operator-7b8c5ddb4b-wr6sb from openshift-ingress-operator started at 2023-03-01 22:51:20 +0000 UTC (2 container statuses recorded)
Mar  2 02:58:17.529: INFO: 	Container ingress-operator ready: true, restart count 0
Mar  2 02:58:17.530: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:58:17.530: INFO: router-default-68bc8785b7-vl6bd from openshift-ingress started at 2023-03-02 02:09:21 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.530: INFO: 	Container router ready: true, restart count 0
Mar  2 02:58:17.530: INFO: insights-operator-7b97bdcbd8-5zm5p from openshift-insights started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.530: INFO: 	Container insights-operator ready: true, restart count 1
Mar  2 02:58:17.530: INFO: openshift-kube-proxy-xgktp from openshift-kube-proxy started at 2023-03-01 22:49:00 +0000 UTC (2 container statuses recorded)
Mar  2 02:58:17.530: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  2 02:58:17.530: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:58:17.530: INFO: kube-storage-version-migrator-operator-78bc45549c-qmzfr from openshift-kube-storage-version-migrator-operator started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.530: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Mar  2 02:58:17.530: INFO: marketplace-operator-66f5c8895c-l9r8f from openshift-marketplace started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.530: INFO: 	Container marketplace-operator ready: true, restart count 0
Mar  2 02:58:17.530: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-03-02 02:09:26 +0000 UTC (6 container statuses recorded)
Mar  2 02:58:17.530: INFO: 	Container alertmanager ready: true, restart count 1
Mar  2 02:58:17.530: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 02:58:17.530: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 02:58:17.530: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:58:17.530: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Mar  2 02:58:17.530: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 02:58:17.530: INFO: cluster-monitoring-operator-7ffc6575dd-kkb8m from openshift-monitoring started at 2023-03-01 22:51:20 +0000 UTC (2 container statuses recorded)
Mar  2 02:58:17.530: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Mar  2 02:58:17.530: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:58:17.530: INFO: node-exporter-95fzh from openshift-monitoring started at 2023-03-01 22:56:40 +0000 UTC (2 container statuses recorded)
Mar  2 02:58:17.530: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:58:17.530: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 02:58:17.530: INFO: prometheus-adapter-95d69f68c-vtxdk from openshift-monitoring started at 2023-03-02 02:09:21 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.530: INFO: 	Container prometheus-adapter ready: true, restart count 0
Mar  2 02:58:17.530: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-03-02 02:09:26 +0000 UTC (6 container statuses recorded)
Mar  2 02:58:17.530: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 02:58:17.530: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:58:17.530: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Mar  2 02:58:17.530: INFO: 	Container prometheus ready: true, restart count 0
Mar  2 02:58:17.530: INFO: 	Container prometheus-proxy ready: true, restart count 0
Mar  2 02:58:17.530: INFO: 	Container thanos-sidecar ready: true, restart count 0
Mar  2 02:58:17.531: INFO: prometheus-operator-admission-webhook-6d5fbffb86-sm9lf from openshift-monitoring started at 2023-03-02 02:09:21 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.531: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Mar  2 02:58:17.531: INFO: thanos-querier-6cd8656bbb-j85bx from openshift-monitoring started at 2023-03-02 02:09:21 +0000 UTC (6 container statuses recorded)
Mar  2 02:58:17.531: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:58:17.531: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Mar  2 02:58:17.531: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Mar  2 02:58:17.531: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 02:58:17.531: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 02:58:17.531: INFO: 	Container thanos-query ready: true, restart count 0
Mar  2 02:58:17.531: INFO: multus-89qpj from openshift-multus started at 2023-03-01 22:48:55 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.531: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 02:58:17.531: INFO: multus-additional-cni-plugins-ff7kf from openshift-multus started at 2023-03-01 22:48:55 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.531: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Mar  2 02:58:17.531: INFO: multus-admission-controller-6f984f76c7-xsk9f from openshift-multus started at 2023-03-02 02:09:21 +0000 UTC (2 container statuses recorded)
Mar  2 02:58:17.531: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:58:17.531: INFO: 	Container multus-admission-controller ready: true, restart count 0
Mar  2 02:58:17.531: INFO: network-metrics-daemon-7m69v from openshift-multus started at 2023-03-01 22:48:56 +0000 UTC (2 container statuses recorded)
Mar  2 02:58:17.531: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:58:17.531: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Mar  2 02:58:17.531: INFO: network-check-source-848f87f9d4-bxlxt from openshift-network-diagnostics started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.531: INFO: 	Container check-endpoints ready: true, restart count 0
Mar  2 02:58:17.531: INFO: network-check-target-jqnqt from openshift-network-diagnostics started at 2023-03-01 22:49:03 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.531: INFO: 	Container network-check-target-container ready: true, restart count 0
Mar  2 02:58:17.531: INFO: catalog-operator-657ff7bb9f-c7tqq from openshift-operator-lifecycle-manager started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.531: INFO: 	Container catalog-operator ready: true, restart count 0
Mar  2 02:58:17.531: INFO: olm-operator-56ccd7794f-4ffrj from openshift-operator-lifecycle-manager started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.531: INFO: 	Container olm-operator ready: true, restart count 0
Mar  2 02:58:17.531: INFO: package-server-manager-599745f678-rmjg4 from openshift-operator-lifecycle-manager started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.531: INFO: 	Container package-server-manager ready: true, restart count 0
Mar  2 02:58:17.531: INFO: packageserver-6c7c56df95-jq89f from openshift-operator-lifecycle-manager started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.531: INFO: 	Container packageserver ready: true, restart count 0
Mar  2 02:58:17.531: INFO: metrics-784dfd67b7-rjkpg from openshift-roks-metrics started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.531: INFO: 	Container metrics ready: true, restart count 3
Mar  2 02:58:17.531: INFO: push-gateway-646cc86d79-cgtct from openshift-roks-metrics started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.531: INFO: 	Container push-gateway ready: true, restart count 0
Mar  2 02:58:17.532: INFO: service-ca-operator-6f8744cbc5-w5srs from openshift-service-ca-operator started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.532: INFO: 	Container service-ca-operator ready: true, restart count 1
Mar  2 02:58:17.532: INFO: pod1-0-sched-preemption-medium-priority from sched-preemption-219 started at 2023-03-02 02:58:06 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.532: INFO: 	Container pod1-0-sched-preemption-medium-priority ready: true, restart count 0
Mar  2 02:58:17.532: INFO: pod1-1-sched-preemption-medium-priority from sched-preemption-219 started at 2023-03-02 02:58:06 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.532: INFO: 	Container pod1-1-sched-preemption-medium-priority ready: true, restart count 0
Mar  2 02:58:17.532: INFO: sonobuoy-systemd-logs-daemon-set-7266eb5e626146f7-n2dsq from sonobuoy started at 2023-03-02 01:07:14 +0000 UTC (2 container statuses recorded)
Mar  2 02:58:17.532: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 02:58:17.532: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 02:58:17.532: INFO: 
Logging pods the apiserver thinks is on node 10.132.92.188 before test
Mar  2 02:58:17.590: INFO: calico-node-mkvsm from calico-system started at 2023-03-01 22:49:50 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.590: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 02:58:17.590: INFO: ibm-cloud-provider-ip-169-56-11-242-558f6ff69-xvh2v from ibm-system started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.590: INFO: 	Container ibm-cloud-provider-ip-169-56-11-242 ready: true, restart count 0
Mar  2 02:58:17.590: INFO: ibm-keepalived-watcher-zwfb8 from kube-system started at 2023-03-01 22:48:10 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.590: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar  2 02:58:17.590: INFO: ibm-master-proxy-static-10.132.92.188 from kube-system started at 2023-03-01 22:48:07 +0000 UTC (2 container statuses recorded)
Mar  2 02:58:17.590: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar  2 02:58:17.590: INFO: 	Container pause ready: true, restart count 0
Mar  2 02:58:17.590: INFO: ibmcloud-block-storage-driver-qbg9f from kube-system started at 2023-03-01 22:48:16 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.590: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar  2 02:58:17.590: INFO: vpn-f6c799ddd-xls7d from kube-system started at 2023-03-02 02:09:21 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.590: INFO: 	Container vpn ready: true, restart count 0
Mar  2 02:58:17.590: INFO: tuned-5z98r from openshift-cluster-node-tuning-operator started at 2023-03-01 22:55:42 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.590: INFO: 	Container tuned ready: true, restart count 0
Mar  2 02:58:17.590: INFO: csi-snapshot-controller-85d666c956-xsh6c from openshift-cluster-storage-operator started at 2023-03-01 22:51:57 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.590: INFO: 	Container snapshot-controller ready: true, restart count 0
Mar  2 02:58:17.590: INFO: csi-snapshot-webhook-6cbf55b956-wq8bh from openshift-cluster-storage-operator started at 2023-03-01 22:51:57 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.590: INFO: 	Container webhook ready: true, restart count 0
Mar  2 02:58:17.590: INFO: console-6c8dcd4bdd-ddzwf from openshift-console started at 2023-03-02 02:09:21 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.590: INFO: 	Container console ready: true, restart count 0
Mar  2 02:58:17.590: INFO: downloads-8b4bf4d5-b9kfz from openshift-console started at 2023-03-01 22:51:55 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.590: INFO: 	Container download-server ready: true, restart count 0
Mar  2 02:58:17.590: INFO: dns-default-nt82k from openshift-dns started at 2023-03-01 22:55:41 +0000 UTC (2 container statuses recorded)
Mar  2 02:58:17.590: INFO: 	Container dns ready: true, restart count 0
Mar  2 02:58:17.590: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:58:17.591: INFO: node-resolver-wcng9 from openshift-dns started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.591: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 02:58:17.591: INFO: image-pruner-27961920-wd4pv from openshift-image-registry started at 2023-03-02 00:00:00 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.591: INFO: 	Container image-pruner ready: false, restart count 0
Mar  2 02:58:17.591: INFO: image-registry-7fb7bd8df6-lb2bt from openshift-image-registry started at 2023-03-01 22:58:31 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.591: INFO: 	Container registry ready: true, restart count 0
Mar  2 02:58:17.591: INFO: node-ca-qlt4w from openshift-image-registry started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.591: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 02:58:17.591: INFO: ingress-canary-qvcz9 from openshift-ingress-canary started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.591: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Mar  2 02:58:17.591: INFO: router-default-68bc8785b7-wz8ct from openshift-ingress started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.591: INFO: 	Container router ready: true, restart count 0
Mar  2 02:58:17.591: INFO: openshift-kube-proxy-mth7h from openshift-kube-proxy started at 2023-03-01 22:49:00 +0000 UTC (2 container statuses recorded)
Mar  2 02:58:17.591: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  2 02:58:17.591: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:58:17.591: INFO: migrator-5687b466dd-glln7 from openshift-kube-storage-version-migrator started at 2023-03-01 22:51:57 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.591: INFO: 	Container migrator ready: true, restart count 0
Mar  2 02:58:17.591: INFO: certified-operators-8n5vw from openshift-marketplace started at 2023-03-02 00:13:35 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.591: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 02:58:17.591: INFO: community-operators-lngtl from openshift-marketplace started at 2023-03-01 22:54:15 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.591: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 02:58:17.591: INFO: redhat-marketplace-ld8wh from openshift-marketplace started at 2023-03-02 00:27:02 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.591: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 02:58:17.591: INFO: redhat-operators-tk548 from openshift-marketplace started at 2023-03-01 22:54:14 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.591: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 02:58:17.591: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-03-01 22:58:39 +0000 UTC (6 container statuses recorded)
Mar  2 02:58:17.591: INFO: 	Container alertmanager ready: true, restart count 1
Mar  2 02:58:17.591: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 02:58:17.591: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 02:58:17.591: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:58:17.591: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Mar  2 02:58:17.591: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 02:58:17.591: INFO: kube-state-metrics-554994774b-4xpf2 from openshift-monitoring started at 2023-03-02 02:09:21 +0000 UTC (3 container statuses recorded)
Mar  2 02:58:17.591: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Mar  2 02:58:17.591: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Mar  2 02:58:17.591: INFO: 	Container kube-state-metrics ready: true, restart count 0
Mar  2 02:58:17.591: INFO: node-exporter-gj7lr from openshift-monitoring started at 2023-03-01 22:56:39 +0000 UTC (2 container statuses recorded)
Mar  2 02:58:17.591: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:58:17.591: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 02:58:17.592: INFO: openshift-state-metrics-56599d9b6d-6sw48 from openshift-monitoring started at 2023-03-01 22:56:39 +0000 UTC (3 container statuses recorded)
Mar  2 02:58:17.592: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Mar  2 02:58:17.592: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Mar  2 02:58:17.592: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Mar  2 02:58:17.592: INFO: prometheus-adapter-95d69f68c-gtx2d from openshift-monitoring started at 2023-03-01 22:58:02 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.592: INFO: 	Container prometheus-adapter ready: true, restart count 0
Mar  2 02:58:17.592: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-03-01 22:58:30 +0000 UTC (6 container statuses recorded)
Mar  2 02:58:17.592: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 02:58:17.592: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:58:17.592: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Mar  2 02:58:17.592: INFO: 	Container prometheus ready: true, restart count 0
Mar  2 02:58:17.592: INFO: 	Container prometheus-proxy ready: true, restart count 0
Mar  2 02:58:17.592: INFO: 	Container thanos-sidecar ready: true, restart count 0
Mar  2 02:58:17.592: INFO: prometheus-operator-9bf584898-89sgf from openshift-monitoring started at 2023-03-01 22:56:19 +0000 UTC (2 container statuses recorded)
Mar  2 02:58:17.592: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:58:17.592: INFO: 	Container prometheus-operator ready: true, restart count 0
Mar  2 02:58:17.592: INFO: prometheus-operator-admission-webhook-6d5fbffb86-hhfbw from openshift-monitoring started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.592: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Mar  2 02:58:17.592: INFO: telemeter-client-769c487d5b-l5fxr from openshift-monitoring started at 2023-03-02 02:09:21 +0000 UTC (3 container statuses recorded)
Mar  2 02:58:17.592: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:58:17.592: INFO: 	Container reload ready: true, restart count 0
Mar  2 02:58:17.592: INFO: 	Container telemeter-client ready: true, restart count 0
Mar  2 02:58:17.592: INFO: thanos-querier-6cd8656bbb-8hv4v from openshift-monitoring started at 2023-03-01 22:56:48 +0000 UTC (6 container statuses recorded)
Mar  2 02:58:17.592: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:58:17.592: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Mar  2 02:58:17.592: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Mar  2 02:58:17.592: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 02:58:17.592: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 02:58:17.592: INFO: 	Container thanos-query ready: true, restart count 0
Mar  2 02:58:17.593: INFO: multus-additional-cni-plugins-5pjnd from openshift-multus started at 2023-03-01 22:48:55 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.593: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Mar  2 02:58:17.593: INFO: multus-admission-controller-6f984f76c7-hclf4 from openshift-multus started at 2023-03-01 22:55:41 +0000 UTC (2 container statuses recorded)
Mar  2 02:58:17.593: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:58:17.593: INFO: 	Container multus-admission-controller ready: true, restart count 0
Mar  2 02:58:17.593: INFO: multus-g4k8f from openshift-multus started at 2023-03-01 22:48:55 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.593: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 02:58:17.593: INFO: network-metrics-daemon-95rvd from openshift-multus started at 2023-03-01 22:48:56 +0000 UTC (2 container statuses recorded)
Mar  2 02:58:17.593: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:58:17.593: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Mar  2 02:58:17.593: INFO: network-check-target-4nkgt from openshift-network-diagnostics started at 2023-03-01 22:49:03 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.593: INFO: 	Container network-check-target-container ready: true, restart count 0
Mar  2 02:58:17.593: INFO: network-operator-68bbf8cc76-mg97z from openshift-network-operator started at 2023-03-01 22:48:19 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.593: INFO: 	Container network-operator ready: true, restart count 1
Mar  2 02:58:17.593: INFO: collect-profiles-27962055-hkgx6 from openshift-operator-lifecycle-manager started at 2023-03-02 02:15:00 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.593: INFO: 	Container collect-profiles ready: false, restart count 0
Mar  2 02:58:17.593: INFO: collect-profiles-27962070-cvw8h from openshift-operator-lifecycle-manager started at 2023-03-02 02:30:00 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.593: INFO: 	Container collect-profiles ready: false, restart count 0
Mar  2 02:58:17.593: INFO: collect-profiles-27962085-nl5n2 from openshift-operator-lifecycle-manager started at 2023-03-02 02:45:00 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.593: INFO: 	Container collect-profiles ready: false, restart count 0
Mar  2 02:58:17.593: INFO: packageserver-6c7c56df95-7wr7f from openshift-operator-lifecycle-manager started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.593: INFO: 	Container packageserver ready: true, restart count 0
Mar  2 02:58:17.593: INFO: service-ca-578d67f6f8-89gtp from openshift-service-ca started at 2023-03-01 22:52:15 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.593: INFO: 	Container service-ca-controller ready: true, restart count 0
Mar  2 02:58:17.593: INFO: pod2-0-sched-preemption-medium-priority from sched-preemption-219 started at 2023-03-02 02:58:05 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.593: INFO: 	Container pod2-0-sched-preemption-medium-priority ready: true, restart count 0
Mar  2 02:58:17.593: INFO: pod2-1-sched-preemption-medium-priority from sched-preemption-219 started at 2023-03-02 02:58:05 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.593: INFO: 	Container pod2-1-sched-preemption-medium-priority ready: true, restart count 0
Mar  2 02:58:17.593: INFO: sonobuoy-e2e-job-c6c00a6e72aa4160 from sonobuoy started at 2023-03-02 01:07:14 +0000 UTC (2 container statuses recorded)
Mar  2 02:58:17.593: INFO: 	Container e2e ready: true, restart count 0
Mar  2 02:58:17.593: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 02:58:17.593: INFO: sonobuoy-systemd-logs-daemon-set-7266eb5e626146f7-fgqrz from sonobuoy started at 2023-03-02 01:07:14 +0000 UTC (2 container statuses recorded)
Mar  2 02:58:17.593: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 02:58:17.593: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 02:58:17.593: INFO: tigera-operator-84f4f4565b-5w7s7 from tigera-operator started at 2023-03-01 22:48:19 +0000 UTC (1 container statuses recorded)
Mar  2 02:58:17.593: INFO: 	Container tigera-operator ready: true, restart count 2
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
STEP: Trying to schedule Pod with nonempty NodeSelector. 03/02/23 02:58:17.594
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.17487ad0a5b58d1b], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling.] 03/02/23 02:58:17.768
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Mar  2 02:58:18.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8328" for this suite. 03/02/23 02:58:18.792
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","completed":332,"skipped":6027,"failed":0}
------------------------------
• [1.652 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:58:17.164
    Mar  2 02:58:17.164: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename sched-pred 03/02/23 02:58:17.165
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:58:17.264
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:58:17.28
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Mar  2 02:58:17.291: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Mar  2 02:58:17.367: INFO: Waiting for terminating namespaces to be deleted...
    Mar  2 02:58:17.390: INFO: 
    Logging pods the apiserver thinks is on node 10.132.92.143 before test
    Mar  2 02:58:17.452: INFO: calico-node-99tft from calico-system started at 2023-03-01 22:49:50 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.453: INFO: 	Container calico-node ready: true, restart count 0
    Mar  2 02:58:17.453: INFO: calico-typha-b7f8b755-xlnll from calico-system started at 2023-03-01 22:49:57 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.453: INFO: 	Container calico-typha ready: true, restart count 0
    Mar  2 02:58:17.453: INFO: ibm-keepalived-watcher-dp97c from kube-system started at 2023-03-01 22:48:16 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.453: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Mar  2 02:58:17.453: INFO: ibm-master-proxy-static-10.132.92.143 from kube-system started at 2023-03-01 22:48:15 +0000 UTC (2 container statuses recorded)
    Mar  2 02:58:17.453: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Mar  2 02:58:17.453: INFO: 	Container pause ready: true, restart count 0
    Mar  2 02:58:17.453: INFO: ibmcloud-block-storage-driver-zfc8r from kube-system started at 2023-03-01 22:48:23 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.453: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Mar  2 02:58:17.453: INFO: tuned-w95xr from openshift-cluster-node-tuning-operator started at 2023-03-01 22:55:42 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.454: INFO: 	Container tuned ready: true, restart count 0
    Mar  2 02:58:17.454: INFO: dns-default-w2mf4 from openshift-dns started at 2023-03-02 02:32:54 +0000 UTC (2 container statuses recorded)
    Mar  2 02:58:17.454: INFO: 	Container dns ready: true, restart count 0
    Mar  2 02:58:17.454: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:58:17.454: INFO: node-resolver-7wklz from openshift-dns started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.454: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Mar  2 02:58:17.454: INFO: node-ca-q4vcl from openshift-image-registry started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.454: INFO: 	Container node-ca ready: true, restart count 0
    Mar  2 02:58:17.454: INFO: registry-pvc-permissions-6wblc from openshift-image-registry started at 2023-03-01 22:58:39 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.454: INFO: 	Container pvc-permissions ready: false, restart count 0
    Mar  2 02:58:17.454: INFO: ingress-canary-bz9g8 from openshift-ingress-canary started at 2023-03-02 02:32:35 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.454: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Mar  2 02:58:17.454: INFO: openshift-kube-proxy-zdts7 from openshift-kube-proxy started at 2023-03-01 22:49:00 +0000 UTC (2 container statuses recorded)
    Mar  2 02:58:17.454: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar  2 02:58:17.454: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:58:17.454: INFO: node-exporter-z52nh from openshift-monitoring started at 2023-03-01 22:56:40 +0000 UTC (2 container statuses recorded)
    Mar  2 02:58:17.454: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:58:17.454: INFO: 	Container node-exporter ready: true, restart count 0
    Mar  2 02:58:17.454: INFO: multus-48w86 from openshift-multus started at 2023-03-01 22:48:55 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.454: INFO: 	Container kube-multus ready: true, restart count 0
    Mar  2 02:58:17.454: INFO: multus-additional-cni-plugins-d2kv5 from openshift-multus started at 2023-03-01 22:48:55 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.455: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Mar  2 02:58:17.455: INFO: network-metrics-daemon-7jfj4 from openshift-multus started at 2023-03-01 22:48:56 +0000 UTC (2 container statuses recorded)
    Mar  2 02:58:17.455: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:58:17.455: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Mar  2 02:58:17.455: INFO: network-check-target-w7x6h from openshift-network-diagnostics started at 2023-03-01 22:49:03 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.455: INFO: 	Container network-check-target-container ready: true, restart count 0
    Mar  2 02:58:17.455: INFO: pod0-1-sched-preemption-medium-priority from sched-preemption-219 started at 2023-03-02 02:58:02 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.455: INFO: 	Container pod0-1-sched-preemption-medium-priority ready: true, restart count 0
    Mar  2 02:58:17.455: INFO: sonobuoy from sonobuoy started at 2023-03-02 01:07:05 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.455: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Mar  2 02:58:17.455: INFO: sonobuoy-systemd-logs-daemon-set-7266eb5e626146f7-4c6tp from sonobuoy started at 2023-03-02 01:07:14 +0000 UTC (2 container statuses recorded)
    Mar  2 02:58:17.455: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  2 02:58:17.455: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar  2 02:58:17.455: INFO: 
    Logging pods the apiserver thinks is on node 10.132.92.186 before test
    Mar  2 02:58:17.528: INFO: calico-kube-controllers-5b6b964466-vlvht from calico-system started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.528: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Mar  2 02:58:17.528: INFO: calico-node-5vvbw from calico-system started at 2023-03-01 22:49:50 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.528: INFO: 	Container calico-node ready: true, restart count 0
    Mar  2 02:58:17.528: INFO: calico-typha-b7f8b755-bdnm9 from calico-system started at 2023-03-01 22:49:50 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.528: INFO: 	Container calico-typha ready: true, restart count 0
    Mar  2 02:58:17.528: INFO: managed-storage-validation-webhooks-757b765d54-f72mn from ibm-odf-validation-webhook started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.528: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
    Mar  2 02:58:17.528: INFO: managed-storage-validation-webhooks-757b765d54-gshct from ibm-odf-validation-webhook started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.528: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
    Mar  2 02:58:17.528: INFO: managed-storage-validation-webhooks-757b765d54-h4j5p from ibm-odf-validation-webhook started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.528: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
    Mar  2 02:58:17.528: INFO: ibm-cloud-provider-ip-169-56-11-242-558f6ff69-s4dz5 from ibm-system started at 2023-03-02 02:09:21 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.528: INFO: 	Container ibm-cloud-provider-ip-169-56-11-242 ready: true, restart count 0
    Mar  2 02:58:17.528: INFO: ibm-file-plugin-68bfbbdc7-77m77 from kube-system started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.528: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
    Mar  2 02:58:17.528: INFO: ibm-keepalived-watcher-knrjl from kube-system started at 2023-03-01 22:48:27 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.528: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Mar  2 02:58:17.528: INFO: ibm-master-proxy-static-10.132.92.186 from kube-system started at 2023-03-01 22:48:22 +0000 UTC (2 container statuses recorded)
    Mar  2 02:58:17.528: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Mar  2 02:58:17.528: INFO: 	Container pause ready: true, restart count 0
    Mar  2 02:58:17.528: INFO: ibm-storage-metrics-agent-6c459b7855-cv8d6 from kube-system started at 2023-03-01 22:51:20 +0000 UTC (2 container statuses recorded)
    Mar  2 02:58:17.528: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
    Mar  2 02:58:17.528: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
    Mar  2 02:58:17.528: INFO: ibm-storage-watcher-67589fd788-z5w99 from kube-system started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.528: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
    Mar  2 02:58:17.528: INFO: ibmcloud-block-storage-driver-l498z from kube-system started at 2023-03-01 22:48:36 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.528: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Mar  2 02:58:17.528: INFO: ibmcloud-block-storage-plugin-86f4cbdf95-klfqp from kube-system started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.528: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
    Mar  2 02:58:17.529: INFO: cluster-node-tuning-operator-5df67c68d9-gd552 from openshift-cluster-node-tuning-operator started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.529: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
    Mar  2 02:58:17.529: INFO: tuned-h4jnq from openshift-cluster-node-tuning-operator started at 2023-03-01 22:55:42 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.529: INFO: 	Container tuned ready: true, restart count 0
    Mar  2 02:58:17.529: INFO: cluster-samples-operator-5948f44764-88mzn from openshift-cluster-samples-operator started at 2023-03-01 22:51:20 +0000 UTC (2 container statuses recorded)
    Mar  2 02:58:17.529: INFO: 	Container cluster-samples-operator ready: true, restart count 0
    Mar  2 02:58:17.529: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
    Mar  2 02:58:17.529: INFO: cluster-storage-operator-54cf568cb7-smstf from openshift-cluster-storage-operator started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.529: INFO: 	Container cluster-storage-operator ready: true, restart count 1
    Mar  2 02:58:17.529: INFO: csi-snapshot-controller-85d666c956-5hhrv from openshift-cluster-storage-operator started at 2023-03-01 22:51:57 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.529: INFO: 	Container snapshot-controller ready: true, restart count 0
    Mar  2 02:58:17.529: INFO: csi-snapshot-controller-operator-6b76cb8c94-9bvdj from openshift-cluster-storage-operator started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.529: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
    Mar  2 02:58:17.529: INFO: csi-snapshot-webhook-6cbf55b956-z69cb from openshift-cluster-storage-operator started at 2023-03-01 22:51:57 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.529: INFO: 	Container webhook ready: true, restart count 0
    Mar  2 02:58:17.529: INFO: console-operator-6975f688f8-xwx92 from openshift-console-operator started at 2023-03-01 22:51:20 +0000 UTC (2 container statuses recorded)
    Mar  2 02:58:17.529: INFO: 	Container console-operator ready: true, restart count 1
    Mar  2 02:58:17.529: INFO: 	Container conversion-webhook-server ready: true, restart count 3
    Mar  2 02:58:17.529: INFO: console-6c8dcd4bdd-c2dbn from openshift-console started at 2023-03-01 22:59:49 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.529: INFO: 	Container console ready: true, restart count 0
    Mar  2 02:58:17.529: INFO: downloads-8b4bf4d5-pjlj4 from openshift-console started at 2023-03-01 22:51:55 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.529: INFO: 	Container download-server ready: true, restart count 0
    Mar  2 02:58:17.529: INFO: dns-operator-b8598b55c-87ghp from openshift-dns-operator started at 2023-03-01 22:51:20 +0000 UTC (2 container statuses recorded)
    Mar  2 02:58:17.529: INFO: 	Container dns-operator ready: true, restart count 0
    Mar  2 02:58:17.529: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:58:17.529: INFO: dns-default-rdd5h from openshift-dns started at 2023-03-01 22:55:41 +0000 UTC (2 container statuses recorded)
    Mar  2 02:58:17.529: INFO: 	Container dns ready: true, restart count 0
    Mar  2 02:58:17.529: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:58:17.529: INFO: node-resolver-9wrks from openshift-dns started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.529: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Mar  2 02:58:17.529: INFO: cluster-image-registry-operator-797c5fb5f8-tvjwj from openshift-image-registry started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.529: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
    Mar  2 02:58:17.529: INFO: node-ca-6ffsq from openshift-image-registry started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.529: INFO: 	Container node-ca ready: true, restart count 0
    Mar  2 02:58:17.529: INFO: ingress-canary-zgjd4 from openshift-ingress-canary started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.529: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Mar  2 02:58:17.529: INFO: ingress-operator-7b8c5ddb4b-wr6sb from openshift-ingress-operator started at 2023-03-01 22:51:20 +0000 UTC (2 container statuses recorded)
    Mar  2 02:58:17.529: INFO: 	Container ingress-operator ready: true, restart count 0
    Mar  2 02:58:17.530: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:58:17.530: INFO: router-default-68bc8785b7-vl6bd from openshift-ingress started at 2023-03-02 02:09:21 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.530: INFO: 	Container router ready: true, restart count 0
    Mar  2 02:58:17.530: INFO: insights-operator-7b97bdcbd8-5zm5p from openshift-insights started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.530: INFO: 	Container insights-operator ready: true, restart count 1
    Mar  2 02:58:17.530: INFO: openshift-kube-proxy-xgktp from openshift-kube-proxy started at 2023-03-01 22:49:00 +0000 UTC (2 container statuses recorded)
    Mar  2 02:58:17.530: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar  2 02:58:17.530: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:58:17.530: INFO: kube-storage-version-migrator-operator-78bc45549c-qmzfr from openshift-kube-storage-version-migrator-operator started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.530: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
    Mar  2 02:58:17.530: INFO: marketplace-operator-66f5c8895c-l9r8f from openshift-marketplace started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.530: INFO: 	Container marketplace-operator ready: true, restart count 0
    Mar  2 02:58:17.530: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-03-02 02:09:26 +0000 UTC (6 container statuses recorded)
    Mar  2 02:58:17.530: INFO: 	Container alertmanager ready: true, restart count 1
    Mar  2 02:58:17.530: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Mar  2 02:58:17.530: INFO: 	Container config-reloader ready: true, restart count 0
    Mar  2 02:58:17.530: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:58:17.530: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Mar  2 02:58:17.530: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Mar  2 02:58:17.530: INFO: cluster-monitoring-operator-7ffc6575dd-kkb8m from openshift-monitoring started at 2023-03-01 22:51:20 +0000 UTC (2 container statuses recorded)
    Mar  2 02:58:17.530: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
    Mar  2 02:58:17.530: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:58:17.530: INFO: node-exporter-95fzh from openshift-monitoring started at 2023-03-01 22:56:40 +0000 UTC (2 container statuses recorded)
    Mar  2 02:58:17.530: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:58:17.530: INFO: 	Container node-exporter ready: true, restart count 0
    Mar  2 02:58:17.530: INFO: prometheus-adapter-95d69f68c-vtxdk from openshift-monitoring started at 2023-03-02 02:09:21 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.530: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Mar  2 02:58:17.530: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-03-02 02:09:26 +0000 UTC (6 container statuses recorded)
    Mar  2 02:58:17.530: INFO: 	Container config-reloader ready: true, restart count 0
    Mar  2 02:58:17.530: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:58:17.530: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Mar  2 02:58:17.530: INFO: 	Container prometheus ready: true, restart count 0
    Mar  2 02:58:17.530: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Mar  2 02:58:17.530: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Mar  2 02:58:17.531: INFO: prometheus-operator-admission-webhook-6d5fbffb86-sm9lf from openshift-monitoring started at 2023-03-02 02:09:21 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.531: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Mar  2 02:58:17.531: INFO: thanos-querier-6cd8656bbb-j85bx from openshift-monitoring started at 2023-03-02 02:09:21 +0000 UTC (6 container statuses recorded)
    Mar  2 02:58:17.531: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:58:17.531: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Mar  2 02:58:17.531: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Mar  2 02:58:17.531: INFO: 	Container oauth-proxy ready: true, restart count 0
    Mar  2 02:58:17.531: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Mar  2 02:58:17.531: INFO: 	Container thanos-query ready: true, restart count 0
    Mar  2 02:58:17.531: INFO: multus-89qpj from openshift-multus started at 2023-03-01 22:48:55 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.531: INFO: 	Container kube-multus ready: true, restart count 0
    Mar  2 02:58:17.531: INFO: multus-additional-cni-plugins-ff7kf from openshift-multus started at 2023-03-01 22:48:55 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.531: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Mar  2 02:58:17.531: INFO: multus-admission-controller-6f984f76c7-xsk9f from openshift-multus started at 2023-03-02 02:09:21 +0000 UTC (2 container statuses recorded)
    Mar  2 02:58:17.531: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:58:17.531: INFO: 	Container multus-admission-controller ready: true, restart count 0
    Mar  2 02:58:17.531: INFO: network-metrics-daemon-7m69v from openshift-multus started at 2023-03-01 22:48:56 +0000 UTC (2 container statuses recorded)
    Mar  2 02:58:17.531: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:58:17.531: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Mar  2 02:58:17.531: INFO: network-check-source-848f87f9d4-bxlxt from openshift-network-diagnostics started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.531: INFO: 	Container check-endpoints ready: true, restart count 0
    Mar  2 02:58:17.531: INFO: network-check-target-jqnqt from openshift-network-diagnostics started at 2023-03-01 22:49:03 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.531: INFO: 	Container network-check-target-container ready: true, restart count 0
    Mar  2 02:58:17.531: INFO: catalog-operator-657ff7bb9f-c7tqq from openshift-operator-lifecycle-manager started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.531: INFO: 	Container catalog-operator ready: true, restart count 0
    Mar  2 02:58:17.531: INFO: olm-operator-56ccd7794f-4ffrj from openshift-operator-lifecycle-manager started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.531: INFO: 	Container olm-operator ready: true, restart count 0
    Mar  2 02:58:17.531: INFO: package-server-manager-599745f678-rmjg4 from openshift-operator-lifecycle-manager started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.531: INFO: 	Container package-server-manager ready: true, restart count 0
    Mar  2 02:58:17.531: INFO: packageserver-6c7c56df95-jq89f from openshift-operator-lifecycle-manager started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.531: INFO: 	Container packageserver ready: true, restart count 0
    Mar  2 02:58:17.531: INFO: metrics-784dfd67b7-rjkpg from openshift-roks-metrics started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.531: INFO: 	Container metrics ready: true, restart count 3
    Mar  2 02:58:17.531: INFO: push-gateway-646cc86d79-cgtct from openshift-roks-metrics started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.531: INFO: 	Container push-gateway ready: true, restart count 0
    Mar  2 02:58:17.532: INFO: service-ca-operator-6f8744cbc5-w5srs from openshift-service-ca-operator started at 2023-03-01 22:51:20 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.532: INFO: 	Container service-ca-operator ready: true, restart count 1
    Mar  2 02:58:17.532: INFO: pod1-0-sched-preemption-medium-priority from sched-preemption-219 started at 2023-03-02 02:58:06 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.532: INFO: 	Container pod1-0-sched-preemption-medium-priority ready: true, restart count 0
    Mar  2 02:58:17.532: INFO: pod1-1-sched-preemption-medium-priority from sched-preemption-219 started at 2023-03-02 02:58:06 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.532: INFO: 	Container pod1-1-sched-preemption-medium-priority ready: true, restart count 0
    Mar  2 02:58:17.532: INFO: sonobuoy-systemd-logs-daemon-set-7266eb5e626146f7-n2dsq from sonobuoy started at 2023-03-02 01:07:14 +0000 UTC (2 container statuses recorded)
    Mar  2 02:58:17.532: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  2 02:58:17.532: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar  2 02:58:17.532: INFO: 
    Logging pods the apiserver thinks is on node 10.132.92.188 before test
    Mar  2 02:58:17.590: INFO: calico-node-mkvsm from calico-system started at 2023-03-01 22:49:50 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.590: INFO: 	Container calico-node ready: true, restart count 0
    Mar  2 02:58:17.590: INFO: ibm-cloud-provider-ip-169-56-11-242-558f6ff69-xvh2v from ibm-system started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.590: INFO: 	Container ibm-cloud-provider-ip-169-56-11-242 ready: true, restart count 0
    Mar  2 02:58:17.590: INFO: ibm-keepalived-watcher-zwfb8 from kube-system started at 2023-03-01 22:48:10 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.590: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Mar  2 02:58:17.590: INFO: ibm-master-proxy-static-10.132.92.188 from kube-system started at 2023-03-01 22:48:07 +0000 UTC (2 container statuses recorded)
    Mar  2 02:58:17.590: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Mar  2 02:58:17.590: INFO: 	Container pause ready: true, restart count 0
    Mar  2 02:58:17.590: INFO: ibmcloud-block-storage-driver-qbg9f from kube-system started at 2023-03-01 22:48:16 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.590: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Mar  2 02:58:17.590: INFO: vpn-f6c799ddd-xls7d from kube-system started at 2023-03-02 02:09:21 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.590: INFO: 	Container vpn ready: true, restart count 0
    Mar  2 02:58:17.590: INFO: tuned-5z98r from openshift-cluster-node-tuning-operator started at 2023-03-01 22:55:42 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.590: INFO: 	Container tuned ready: true, restart count 0
    Mar  2 02:58:17.590: INFO: csi-snapshot-controller-85d666c956-xsh6c from openshift-cluster-storage-operator started at 2023-03-01 22:51:57 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.590: INFO: 	Container snapshot-controller ready: true, restart count 0
    Mar  2 02:58:17.590: INFO: csi-snapshot-webhook-6cbf55b956-wq8bh from openshift-cluster-storage-operator started at 2023-03-01 22:51:57 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.590: INFO: 	Container webhook ready: true, restart count 0
    Mar  2 02:58:17.590: INFO: console-6c8dcd4bdd-ddzwf from openshift-console started at 2023-03-02 02:09:21 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.590: INFO: 	Container console ready: true, restart count 0
    Mar  2 02:58:17.590: INFO: downloads-8b4bf4d5-b9kfz from openshift-console started at 2023-03-01 22:51:55 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.590: INFO: 	Container download-server ready: true, restart count 0
    Mar  2 02:58:17.590: INFO: dns-default-nt82k from openshift-dns started at 2023-03-01 22:55:41 +0000 UTC (2 container statuses recorded)
    Mar  2 02:58:17.590: INFO: 	Container dns ready: true, restart count 0
    Mar  2 02:58:17.590: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:58:17.591: INFO: node-resolver-wcng9 from openshift-dns started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.591: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Mar  2 02:58:17.591: INFO: image-pruner-27961920-wd4pv from openshift-image-registry started at 2023-03-02 00:00:00 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.591: INFO: 	Container image-pruner ready: false, restart count 0
    Mar  2 02:58:17.591: INFO: image-registry-7fb7bd8df6-lb2bt from openshift-image-registry started at 2023-03-01 22:58:31 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.591: INFO: 	Container registry ready: true, restart count 0
    Mar  2 02:58:17.591: INFO: node-ca-qlt4w from openshift-image-registry started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.591: INFO: 	Container node-ca ready: true, restart count 0
    Mar  2 02:58:17.591: INFO: ingress-canary-qvcz9 from openshift-ingress-canary started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.591: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Mar  2 02:58:17.591: INFO: router-default-68bc8785b7-wz8ct from openshift-ingress started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.591: INFO: 	Container router ready: true, restart count 0
    Mar  2 02:58:17.591: INFO: openshift-kube-proxy-mth7h from openshift-kube-proxy started at 2023-03-01 22:49:00 +0000 UTC (2 container statuses recorded)
    Mar  2 02:58:17.591: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar  2 02:58:17.591: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:58:17.591: INFO: migrator-5687b466dd-glln7 from openshift-kube-storage-version-migrator started at 2023-03-01 22:51:57 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.591: INFO: 	Container migrator ready: true, restart count 0
    Mar  2 02:58:17.591: INFO: certified-operators-8n5vw from openshift-marketplace started at 2023-03-02 00:13:35 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.591: INFO: 	Container registry-server ready: true, restart count 0
    Mar  2 02:58:17.591: INFO: community-operators-lngtl from openshift-marketplace started at 2023-03-01 22:54:15 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.591: INFO: 	Container registry-server ready: true, restart count 0
    Mar  2 02:58:17.591: INFO: redhat-marketplace-ld8wh from openshift-marketplace started at 2023-03-02 00:27:02 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.591: INFO: 	Container registry-server ready: true, restart count 0
    Mar  2 02:58:17.591: INFO: redhat-operators-tk548 from openshift-marketplace started at 2023-03-01 22:54:14 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.591: INFO: 	Container registry-server ready: true, restart count 0
    Mar  2 02:58:17.591: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-03-01 22:58:39 +0000 UTC (6 container statuses recorded)
    Mar  2 02:58:17.591: INFO: 	Container alertmanager ready: true, restart count 1
    Mar  2 02:58:17.591: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Mar  2 02:58:17.591: INFO: 	Container config-reloader ready: true, restart count 0
    Mar  2 02:58:17.591: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:58:17.591: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Mar  2 02:58:17.591: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Mar  2 02:58:17.591: INFO: kube-state-metrics-554994774b-4xpf2 from openshift-monitoring started at 2023-03-02 02:09:21 +0000 UTC (3 container statuses recorded)
    Mar  2 02:58:17.591: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Mar  2 02:58:17.591: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Mar  2 02:58:17.591: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Mar  2 02:58:17.591: INFO: node-exporter-gj7lr from openshift-monitoring started at 2023-03-01 22:56:39 +0000 UTC (2 container statuses recorded)
    Mar  2 02:58:17.591: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:58:17.591: INFO: 	Container node-exporter ready: true, restart count 0
    Mar  2 02:58:17.592: INFO: openshift-state-metrics-56599d9b6d-6sw48 from openshift-monitoring started at 2023-03-01 22:56:39 +0000 UTC (3 container statuses recorded)
    Mar  2 02:58:17.592: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Mar  2 02:58:17.592: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Mar  2 02:58:17.592: INFO: 	Container openshift-state-metrics ready: true, restart count 0
    Mar  2 02:58:17.592: INFO: prometheus-adapter-95d69f68c-gtx2d from openshift-monitoring started at 2023-03-01 22:58:02 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.592: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Mar  2 02:58:17.592: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-03-01 22:58:30 +0000 UTC (6 container statuses recorded)
    Mar  2 02:58:17.592: INFO: 	Container config-reloader ready: true, restart count 0
    Mar  2 02:58:17.592: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:58:17.592: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Mar  2 02:58:17.592: INFO: 	Container prometheus ready: true, restart count 0
    Mar  2 02:58:17.592: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Mar  2 02:58:17.592: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Mar  2 02:58:17.592: INFO: prometheus-operator-9bf584898-89sgf from openshift-monitoring started at 2023-03-01 22:56:19 +0000 UTC (2 container statuses recorded)
    Mar  2 02:58:17.592: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:58:17.592: INFO: 	Container prometheus-operator ready: true, restart count 0
    Mar  2 02:58:17.592: INFO: prometheus-operator-admission-webhook-6d5fbffb86-hhfbw from openshift-monitoring started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.592: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Mar  2 02:58:17.592: INFO: telemeter-client-769c487d5b-l5fxr from openshift-monitoring started at 2023-03-02 02:09:21 +0000 UTC (3 container statuses recorded)
    Mar  2 02:58:17.592: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:58:17.592: INFO: 	Container reload ready: true, restart count 0
    Mar  2 02:58:17.592: INFO: 	Container telemeter-client ready: true, restart count 0
    Mar  2 02:58:17.592: INFO: thanos-querier-6cd8656bbb-8hv4v from openshift-monitoring started at 2023-03-01 22:56:48 +0000 UTC (6 container statuses recorded)
    Mar  2 02:58:17.592: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:58:17.592: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Mar  2 02:58:17.592: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Mar  2 02:58:17.592: INFO: 	Container oauth-proxy ready: true, restart count 0
    Mar  2 02:58:17.592: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Mar  2 02:58:17.592: INFO: 	Container thanos-query ready: true, restart count 0
    Mar  2 02:58:17.593: INFO: multus-additional-cni-plugins-5pjnd from openshift-multus started at 2023-03-01 22:48:55 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.593: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Mar  2 02:58:17.593: INFO: multus-admission-controller-6f984f76c7-hclf4 from openshift-multus started at 2023-03-01 22:55:41 +0000 UTC (2 container statuses recorded)
    Mar  2 02:58:17.593: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:58:17.593: INFO: 	Container multus-admission-controller ready: true, restart count 0
    Mar  2 02:58:17.593: INFO: multus-g4k8f from openshift-multus started at 2023-03-01 22:48:55 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.593: INFO: 	Container kube-multus ready: true, restart count 0
    Mar  2 02:58:17.593: INFO: network-metrics-daemon-95rvd from openshift-multus started at 2023-03-01 22:48:56 +0000 UTC (2 container statuses recorded)
    Mar  2 02:58:17.593: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Mar  2 02:58:17.593: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Mar  2 02:58:17.593: INFO: network-check-target-4nkgt from openshift-network-diagnostics started at 2023-03-01 22:49:03 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.593: INFO: 	Container network-check-target-container ready: true, restart count 0
    Mar  2 02:58:17.593: INFO: network-operator-68bbf8cc76-mg97z from openshift-network-operator started at 2023-03-01 22:48:19 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.593: INFO: 	Container network-operator ready: true, restart count 1
    Mar  2 02:58:17.593: INFO: collect-profiles-27962055-hkgx6 from openshift-operator-lifecycle-manager started at 2023-03-02 02:15:00 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.593: INFO: 	Container collect-profiles ready: false, restart count 0
    Mar  2 02:58:17.593: INFO: collect-profiles-27962070-cvw8h from openshift-operator-lifecycle-manager started at 2023-03-02 02:30:00 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.593: INFO: 	Container collect-profiles ready: false, restart count 0
    Mar  2 02:58:17.593: INFO: collect-profiles-27962085-nl5n2 from openshift-operator-lifecycle-manager started at 2023-03-02 02:45:00 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.593: INFO: 	Container collect-profiles ready: false, restart count 0
    Mar  2 02:58:17.593: INFO: packageserver-6c7c56df95-7wr7f from openshift-operator-lifecycle-manager started at 2023-03-01 22:55:41 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.593: INFO: 	Container packageserver ready: true, restart count 0
    Mar  2 02:58:17.593: INFO: service-ca-578d67f6f8-89gtp from openshift-service-ca started at 2023-03-01 22:52:15 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.593: INFO: 	Container service-ca-controller ready: true, restart count 0
    Mar  2 02:58:17.593: INFO: pod2-0-sched-preemption-medium-priority from sched-preemption-219 started at 2023-03-02 02:58:05 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.593: INFO: 	Container pod2-0-sched-preemption-medium-priority ready: true, restart count 0
    Mar  2 02:58:17.593: INFO: pod2-1-sched-preemption-medium-priority from sched-preemption-219 started at 2023-03-02 02:58:05 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.593: INFO: 	Container pod2-1-sched-preemption-medium-priority ready: true, restart count 0
    Mar  2 02:58:17.593: INFO: sonobuoy-e2e-job-c6c00a6e72aa4160 from sonobuoy started at 2023-03-02 01:07:14 +0000 UTC (2 container statuses recorded)
    Mar  2 02:58:17.593: INFO: 	Container e2e ready: true, restart count 0
    Mar  2 02:58:17.593: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  2 02:58:17.593: INFO: sonobuoy-systemd-logs-daemon-set-7266eb5e626146f7-fgqrz from sonobuoy started at 2023-03-02 01:07:14 +0000 UTC (2 container statuses recorded)
    Mar  2 02:58:17.593: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar  2 02:58:17.593: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar  2 02:58:17.593: INFO: tigera-operator-84f4f4565b-5w7s7 from tigera-operator started at 2023-03-01 22:48:19 +0000 UTC (1 container statuses recorded)
    Mar  2 02:58:17.593: INFO: 	Container tigera-operator ready: true, restart count 2
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:438
    STEP: Trying to schedule Pod with nonempty NodeSelector. 03/02/23 02:58:17.594
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.17487ad0a5b58d1b], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling.] 03/02/23 02:58:17.768
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Mar  2 02:58:18.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-8328" for this suite. 03/02/23 02:58:18.792
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:58:18.821
Mar  2 02:58:18.821: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename job 03/02/23 02:58:18.823
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:58:18.896
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:58:18.911
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
STEP: Creating a job 03/02/23 02:58:18.926
STEP: Ensure pods equal to paralellism count is attached to the job 03/02/23 02:58:18.948
STEP: patching /status 03/02/23 02:58:22.965
STEP: updating /status 03/02/23 02:58:22.987
STEP: get /status 03/02/23 02:58:23.019
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Mar  2 02:58:23.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-422" for this suite. 03/02/23 02:58:23.055
{"msg":"PASSED [sig-apps] Job should apply changes to a job status [Conformance]","completed":333,"skipped":6056,"failed":0}
------------------------------
• [4.261 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:58:18.821
    Mar  2 02:58:18.821: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename job 03/02/23 02:58:18.823
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:58:18.896
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:58:18.911
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:464
    STEP: Creating a job 03/02/23 02:58:18.926
    STEP: Ensure pods equal to paralellism count is attached to the job 03/02/23 02:58:18.948
    STEP: patching /status 03/02/23 02:58:22.965
    STEP: updating /status 03/02/23 02:58:22.987
    STEP: get /status 03/02/23 02:58:23.019
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Mar  2 02:58:23.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-422" for this suite. 03/02/23 02:58:23.055
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:58:23.084
Mar  2 02:58:23.085: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename configmap 03/02/23 02:58:23.086
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:58:23.163
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:58:23.184
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
STEP: Creating configMap with name configmap-test-volume-4635035c-2237-4716-bff0-63e70818268c 03/02/23 02:58:23.206
STEP: Creating a pod to test consume configMaps 03/02/23 02:58:23.236
Mar  2 02:58:23.390: INFO: Waiting up to 5m0s for pod "pod-configmaps-0c7bc4d8-a717-47e2-94f4-864be549cf65" in namespace "configmap-6361" to be "Succeeded or Failed"
Mar  2 02:58:23.406: INFO: Pod "pod-configmaps-0c7bc4d8-a717-47e2-94f4-864be549cf65": Phase="Pending", Reason="", readiness=false. Elapsed: 15.932597ms
Mar  2 02:58:25.427: INFO: Pod "pod-configmaps-0c7bc4d8-a717-47e2-94f4-864be549cf65": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036942982s
Mar  2 02:58:27.423: INFO: Pod "pod-configmaps-0c7bc4d8-a717-47e2-94f4-864be549cf65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033639012s
STEP: Saw pod success 03/02/23 02:58:27.424
Mar  2 02:58:27.424: INFO: Pod "pod-configmaps-0c7bc4d8-a717-47e2-94f4-864be549cf65" satisfied condition "Succeeded or Failed"
Mar  2 02:58:27.437: INFO: Trying to get logs from node 10.132.92.143 pod pod-configmaps-0c7bc4d8-a717-47e2-94f4-864be549cf65 container agnhost-container: <nil>
STEP: delete the pod 03/02/23 02:58:27.52
Mar  2 02:58:27.565: INFO: Waiting for pod pod-configmaps-0c7bc4d8-a717-47e2-94f4-864be549cf65 to disappear
Mar  2 02:58:27.580: INFO: Pod pod-configmaps-0c7bc4d8-a717-47e2-94f4-864be549cf65 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar  2 02:58:27.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6361" for this suite. 03/02/23 02:58:27.635
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":334,"skipped":6077,"failed":0}
------------------------------
• [4.576 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:58:23.084
    Mar  2 02:58:23.085: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename configmap 03/02/23 02:58:23.086
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:58:23.163
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:58:23.184
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:73
    STEP: Creating configMap with name configmap-test-volume-4635035c-2237-4716-bff0-63e70818268c 03/02/23 02:58:23.206
    STEP: Creating a pod to test consume configMaps 03/02/23 02:58:23.236
    Mar  2 02:58:23.390: INFO: Waiting up to 5m0s for pod "pod-configmaps-0c7bc4d8-a717-47e2-94f4-864be549cf65" in namespace "configmap-6361" to be "Succeeded or Failed"
    Mar  2 02:58:23.406: INFO: Pod "pod-configmaps-0c7bc4d8-a717-47e2-94f4-864be549cf65": Phase="Pending", Reason="", readiness=false. Elapsed: 15.932597ms
    Mar  2 02:58:25.427: INFO: Pod "pod-configmaps-0c7bc4d8-a717-47e2-94f4-864be549cf65": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036942982s
    Mar  2 02:58:27.423: INFO: Pod "pod-configmaps-0c7bc4d8-a717-47e2-94f4-864be549cf65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033639012s
    STEP: Saw pod success 03/02/23 02:58:27.424
    Mar  2 02:58:27.424: INFO: Pod "pod-configmaps-0c7bc4d8-a717-47e2-94f4-864be549cf65" satisfied condition "Succeeded or Failed"
    Mar  2 02:58:27.437: INFO: Trying to get logs from node 10.132.92.143 pod pod-configmaps-0c7bc4d8-a717-47e2-94f4-864be549cf65 container agnhost-container: <nil>
    STEP: delete the pod 03/02/23 02:58:27.52
    Mar  2 02:58:27.565: INFO: Waiting for pod pod-configmaps-0c7bc4d8-a717-47e2-94f4-864be549cf65 to disappear
    Mar  2 02:58:27.580: INFO: Pod pod-configmaps-0c7bc4d8-a717-47e2-94f4-864be549cf65 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  2 02:58:27.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6361" for this suite. 03/02/23 02:58:27.635
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:58:27.663
Mar  2 02:58:27.663: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename kubectl 03/02/23 02:58:27.664
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:58:27.789
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:58:27.803
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
STEP: validating cluster-info 03/02/23 02:58:27.818
Mar  2 02:58:27.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-5632 cluster-info'
Mar  2 02:58:27.963: INFO: stderr: ""
Mar  2 02:58:27.963: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  2 02:58:27.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5632" for this suite. 03/02/23 02:58:27.984
{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","completed":335,"skipped":6096,"failed":0}
------------------------------
• [0.347 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1242
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:58:27.663
    Mar  2 02:58:27.663: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename kubectl 03/02/23 02:58:27.664
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:58:27.789
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:58:27.803
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1248
    STEP: validating cluster-info 03/02/23 02:58:27.818
    Mar  2 02:58:27.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-5632 cluster-info'
    Mar  2 02:58:27.963: INFO: stderr: ""
    Mar  2 02:58:27.963: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  2 02:58:27.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5632" for this suite. 03/02/23 02:58:27.984
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:58:28.011
Mar  2 02:58:28.011: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename webhook 03/02/23 02:58:28.012
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:58:28.072
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:58:28.088
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/02/23 02:58:28.217
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 02:58:28.902
STEP: Deploying the webhook pod 03/02/23 02:58:28.931
STEP: Wait for the deployment to be ready 03/02/23 02:58:29.009
Mar  2 02:58:29.035: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar  2 02:58:31.079: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 2, 58, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 58, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 2, 58, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 58, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/02/23 02:58:33.095
STEP: Verifying the service has paired with the endpoint 03/02/23 02:58:33.136
Mar  2 02:58:34.137: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
Mar  2 02:58:34.154: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9072-crds.webhook.example.com via the AdmissionRegistration API 03/02/23 02:58:34.707
STEP: Creating a custom resource that should be mutated by the webhook 03/02/23 02:58:34.773
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 02:58:37.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2256" for this suite. 03/02/23 02:58:37.646
STEP: Destroying namespace "webhook-2256-markers" for this suite. 03/02/23 02:58:37.675
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","completed":336,"skipped":6098,"failed":0}
------------------------------
• [SLOW TEST] [9.844 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:58:28.011
    Mar  2 02:58:28.011: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename webhook 03/02/23 02:58:28.012
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:58:28.072
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:58:28.088
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/02/23 02:58:28.217
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 02:58:28.902
    STEP: Deploying the webhook pod 03/02/23 02:58:28.931
    STEP: Wait for the deployment to be ready 03/02/23 02:58:29.009
    Mar  2 02:58:29.035: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Mar  2 02:58:31.079: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 2, 58, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 58, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 2, 58, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 58, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/02/23 02:58:33.095
    STEP: Verifying the service has paired with the endpoint 03/02/23 02:58:33.136
    Mar  2 02:58:34.137: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:340
    Mar  2 02:58:34.154: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9072-crds.webhook.example.com via the AdmissionRegistration API 03/02/23 02:58:34.707
    STEP: Creating a custom resource that should be mutated by the webhook 03/02/23 02:58:34.773
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 02:58:37.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2256" for this suite. 03/02/23 02:58:37.646
    STEP: Destroying namespace "webhook-2256-markers" for this suite. 03/02/23 02:58:37.675
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:58:37.856
Mar  2 02:58:37.857: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename emptydir 03/02/23 02:58:37.857
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:58:38.01
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:58:38.023
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
STEP: Creating a pod to test emptydir 0644 on node default medium 03/02/23 02:58:38.071
Mar  2 02:58:38.175: INFO: Waiting up to 5m0s for pod "pod-681e2aaf-c39d-4767-8211-c3e752a39f48" in namespace "emptydir-4376" to be "Succeeded or Failed"
Mar  2 02:58:38.219: INFO: Pod "pod-681e2aaf-c39d-4767-8211-c3e752a39f48": Phase="Pending", Reason="", readiness=false. Elapsed: 43.629ms
Mar  2 02:58:40.236: INFO: Pod "pod-681e2aaf-c39d-4767-8211-c3e752a39f48": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06062683s
Mar  2 02:58:42.249: INFO: Pod "pod-681e2aaf-c39d-4767-8211-c3e752a39f48": Phase="Pending", Reason="", readiness=false. Elapsed: 4.073705074s
Mar  2 02:58:44.234: INFO: Pod "pod-681e2aaf-c39d-4767-8211-c3e752a39f48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.058620649s
STEP: Saw pod success 03/02/23 02:58:44.234
Mar  2 02:58:44.235: INFO: Pod "pod-681e2aaf-c39d-4767-8211-c3e752a39f48" satisfied condition "Succeeded or Failed"
Mar  2 02:58:44.247: INFO: Trying to get logs from node 10.132.92.143 pod pod-681e2aaf-c39d-4767-8211-c3e752a39f48 container test-container: <nil>
STEP: delete the pod 03/02/23 02:58:44.29
Mar  2 02:58:44.324: INFO: Waiting for pod pod-681e2aaf-c39d-4767-8211-c3e752a39f48 to disappear
Mar  2 02:58:44.338: INFO: Pod pod-681e2aaf-c39d-4767-8211-c3e752a39f48 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  2 02:58:44.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4376" for this suite. 03/02/23 02:58:44.361
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":337,"skipped":6106,"failed":0}
------------------------------
• [SLOW TEST] [6.529 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:58:37.856
    Mar  2 02:58:37.857: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename emptydir 03/02/23 02:58:37.857
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:58:38.01
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:58:38.023
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:166
    STEP: Creating a pod to test emptydir 0644 on node default medium 03/02/23 02:58:38.071
    Mar  2 02:58:38.175: INFO: Waiting up to 5m0s for pod "pod-681e2aaf-c39d-4767-8211-c3e752a39f48" in namespace "emptydir-4376" to be "Succeeded or Failed"
    Mar  2 02:58:38.219: INFO: Pod "pod-681e2aaf-c39d-4767-8211-c3e752a39f48": Phase="Pending", Reason="", readiness=false. Elapsed: 43.629ms
    Mar  2 02:58:40.236: INFO: Pod "pod-681e2aaf-c39d-4767-8211-c3e752a39f48": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06062683s
    Mar  2 02:58:42.249: INFO: Pod "pod-681e2aaf-c39d-4767-8211-c3e752a39f48": Phase="Pending", Reason="", readiness=false. Elapsed: 4.073705074s
    Mar  2 02:58:44.234: INFO: Pod "pod-681e2aaf-c39d-4767-8211-c3e752a39f48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.058620649s
    STEP: Saw pod success 03/02/23 02:58:44.234
    Mar  2 02:58:44.235: INFO: Pod "pod-681e2aaf-c39d-4767-8211-c3e752a39f48" satisfied condition "Succeeded or Failed"
    Mar  2 02:58:44.247: INFO: Trying to get logs from node 10.132.92.143 pod pod-681e2aaf-c39d-4767-8211-c3e752a39f48 container test-container: <nil>
    STEP: delete the pod 03/02/23 02:58:44.29
    Mar  2 02:58:44.324: INFO: Waiting for pod pod-681e2aaf-c39d-4767-8211-c3e752a39f48 to disappear
    Mar  2 02:58:44.338: INFO: Pod pod-681e2aaf-c39d-4767-8211-c3e752a39f48 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  2 02:58:44.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4376" for this suite. 03/02/23 02:58:44.361
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:58:44.39
Mar  2 02:58:44.390: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename downward-api 03/02/23 02:58:44.391
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:58:44.443
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:58:44.469
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
STEP: Creating a pod to test downward API volume plugin 03/02/23 02:58:44.514
Mar  2 02:58:44.611: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e081e0bc-5716-41a4-9b68-95306b68866a" in namespace "downward-api-73" to be "Succeeded or Failed"
Mar  2 02:58:44.624: INFO: Pod "downwardapi-volume-e081e0bc-5716-41a4-9b68-95306b68866a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.916767ms
Mar  2 02:58:46.638: INFO: Pod "downwardapi-volume-e081e0bc-5716-41a4-9b68-95306b68866a": Phase="Running", Reason="", readiness=true. Elapsed: 2.027618026s
Mar  2 02:58:48.638: INFO: Pod "downwardapi-volume-e081e0bc-5716-41a4-9b68-95306b68866a": Phase="Running", Reason="", readiness=false. Elapsed: 4.027473251s
Mar  2 02:58:50.640: INFO: Pod "downwardapi-volume-e081e0bc-5716-41a4-9b68-95306b68866a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028837181s
STEP: Saw pod success 03/02/23 02:58:50.64
Mar  2 02:58:50.640: INFO: Pod "downwardapi-volume-e081e0bc-5716-41a4-9b68-95306b68866a" satisfied condition "Succeeded or Failed"
Mar  2 02:58:50.653: INFO: Trying to get logs from node 10.132.92.143 pod downwardapi-volume-e081e0bc-5716-41a4-9b68-95306b68866a container client-container: <nil>
STEP: delete the pod 03/02/23 02:58:50.695
Mar  2 02:58:50.729: INFO: Waiting for pod downwardapi-volume-e081e0bc-5716-41a4-9b68-95306b68866a to disappear
Mar  2 02:58:50.741: INFO: Pod downwardapi-volume-e081e0bc-5716-41a4-9b68-95306b68866a no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar  2 02:58:50.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-73" for this suite. 03/02/23 02:58:50.763
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","completed":338,"skipped":6153,"failed":0}
------------------------------
• [SLOW TEST] [6.401 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:58:44.39
    Mar  2 02:58:44.390: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename downward-api 03/02/23 02:58:44.391
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:58:44.443
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:58:44.469
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:192
    STEP: Creating a pod to test downward API volume plugin 03/02/23 02:58:44.514
    Mar  2 02:58:44.611: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e081e0bc-5716-41a4-9b68-95306b68866a" in namespace "downward-api-73" to be "Succeeded or Failed"
    Mar  2 02:58:44.624: INFO: Pod "downwardapi-volume-e081e0bc-5716-41a4-9b68-95306b68866a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.916767ms
    Mar  2 02:58:46.638: INFO: Pod "downwardapi-volume-e081e0bc-5716-41a4-9b68-95306b68866a": Phase="Running", Reason="", readiness=true. Elapsed: 2.027618026s
    Mar  2 02:58:48.638: INFO: Pod "downwardapi-volume-e081e0bc-5716-41a4-9b68-95306b68866a": Phase="Running", Reason="", readiness=false. Elapsed: 4.027473251s
    Mar  2 02:58:50.640: INFO: Pod "downwardapi-volume-e081e0bc-5716-41a4-9b68-95306b68866a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028837181s
    STEP: Saw pod success 03/02/23 02:58:50.64
    Mar  2 02:58:50.640: INFO: Pod "downwardapi-volume-e081e0bc-5716-41a4-9b68-95306b68866a" satisfied condition "Succeeded or Failed"
    Mar  2 02:58:50.653: INFO: Trying to get logs from node 10.132.92.143 pod downwardapi-volume-e081e0bc-5716-41a4-9b68-95306b68866a container client-container: <nil>
    STEP: delete the pod 03/02/23 02:58:50.695
    Mar  2 02:58:50.729: INFO: Waiting for pod downwardapi-volume-e081e0bc-5716-41a4-9b68-95306b68866a to disappear
    Mar  2 02:58:50.741: INFO: Pod downwardapi-volume-e081e0bc-5716-41a4-9b68-95306b68866a no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar  2 02:58:50.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-73" for this suite. 03/02/23 02:58:50.763
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:58:50.792
Mar  2 02:58:50.792: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename services 03/02/23 02:58:50.793
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:58:50.859
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:58:50.872
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
STEP: creating a service nodeport-service with the type=NodePort in namespace services-9246 03/02/23 02:58:50.887
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 03/02/23 02:58:50.953
STEP: creating service externalsvc in namespace services-9246 03/02/23 02:58:50.953
STEP: creating replication controller externalsvc in namespace services-9246 03/02/23 02:58:51.026
I0302 02:58:51.044324      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-9246, replica count: 2
I0302 02:58:54.098852      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 02:58:57.100115      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 03/02/23 02:58:57.113
Mar  2 02:58:57.224: INFO: Creating new exec pod
Mar  2 02:58:57.305: INFO: Waiting up to 5m0s for pod "execpodk6c52" in namespace "services-9246" to be "running"
Mar  2 02:58:57.321: INFO: Pod "execpodk6c52": Phase="Pending", Reason="", readiness=false. Elapsed: 16.297532ms
Mar  2 02:58:59.337: INFO: Pod "execpodk6c52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031586874s
Mar  2 02:59:01.351: INFO: Pod "execpodk6c52": Phase="Running", Reason="", readiness=true. Elapsed: 4.046221249s
Mar  2 02:59:01.352: INFO: Pod "execpodk6c52" satisfied condition "running"
Mar  2 02:59:01.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-9246 exec execpodk6c52 -- /bin/sh -x -c nslookup nodeport-service.services-9246.svc.cluster.local'
Mar  2 02:59:01.793: INFO: stderr: "+ nslookup nodeport-service.services-9246.svc.cluster.local\n"
Mar  2 02:59:01.793: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nnodeport-service.services-9246.svc.cluster.local\tcanonical name = externalsvc.services-9246.svc.cluster.local.\nName:\texternalsvc.services-9246.svc.cluster.local\nAddress: 172.21.105.205\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-9246, will wait for the garbage collector to delete the pods 03/02/23 02:59:01.793
Mar  2 02:59:01.886: INFO: Deleting ReplicationController externalsvc took: 26.811615ms
Mar  2 02:59:02.188: INFO: Terminating ReplicationController externalsvc pods took: 301.911753ms
Mar  2 02:59:06.258: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  2 02:59:06.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9246" for this suite. 03/02/23 02:59:06.33
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","completed":339,"skipped":6154,"failed":0}
------------------------------
• [SLOW TEST] [15.572 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:58:50.792
    Mar  2 02:58:50.792: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename services 03/02/23 02:58:50.793
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:58:50.859
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:58:50.872
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1523
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-9246 03/02/23 02:58:50.887
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 03/02/23 02:58:50.953
    STEP: creating service externalsvc in namespace services-9246 03/02/23 02:58:50.953
    STEP: creating replication controller externalsvc in namespace services-9246 03/02/23 02:58:51.026
    I0302 02:58:51.044324      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-9246, replica count: 2
    I0302 02:58:54.098852      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0302 02:58:57.100115      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 03/02/23 02:58:57.113
    Mar  2 02:58:57.224: INFO: Creating new exec pod
    Mar  2 02:58:57.305: INFO: Waiting up to 5m0s for pod "execpodk6c52" in namespace "services-9246" to be "running"
    Mar  2 02:58:57.321: INFO: Pod "execpodk6c52": Phase="Pending", Reason="", readiness=false. Elapsed: 16.297532ms
    Mar  2 02:58:59.337: INFO: Pod "execpodk6c52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031586874s
    Mar  2 02:59:01.351: INFO: Pod "execpodk6c52": Phase="Running", Reason="", readiness=true. Elapsed: 4.046221249s
    Mar  2 02:59:01.352: INFO: Pod "execpodk6c52" satisfied condition "running"
    Mar  2 02:59:01.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-9246 exec execpodk6c52 -- /bin/sh -x -c nslookup nodeport-service.services-9246.svc.cluster.local'
    Mar  2 02:59:01.793: INFO: stderr: "+ nslookup nodeport-service.services-9246.svc.cluster.local\n"
    Mar  2 02:59:01.793: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nnodeport-service.services-9246.svc.cluster.local\tcanonical name = externalsvc.services-9246.svc.cluster.local.\nName:\texternalsvc.services-9246.svc.cluster.local\nAddress: 172.21.105.205\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-9246, will wait for the garbage collector to delete the pods 03/02/23 02:59:01.793
    Mar  2 02:59:01.886: INFO: Deleting ReplicationController externalsvc took: 26.811615ms
    Mar  2 02:59:02.188: INFO: Terminating ReplicationController externalsvc pods took: 301.911753ms
    Mar  2 02:59:06.258: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  2 02:59:06.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9246" for this suite. 03/02/23 02:59:06.33
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:59:06.379
Mar  2 02:59:06.379: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename webhook 03/02/23 02:59:06.388
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:59:06.519
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:59:06.568
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/02/23 02:59:06.716
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 02:59:07.214
STEP: Deploying the webhook pod 03/02/23 02:59:07.272
STEP: Wait for the deployment to be ready 03/02/23 02:59:07.35
Mar  2 02:59:07.390: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 02:59:09.464: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 2, 59, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 59, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 2, 59, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 59, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/02/23 02:59:11.478
STEP: Verifying the service has paired with the endpoint 03/02/23 02:59:11.522
Mar  2 02:59:12.522: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 03/02/23 02:59:12.536
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 03/02/23 02:59:12.599
STEP: Creating a dummy validating-webhook-configuration object 03/02/23 02:59:12.668
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 03/02/23 02:59:12.726
STEP: Creating a dummy mutating-webhook-configuration object 03/02/23 02:59:12.783
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 03/02/23 02:59:12.827
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 02:59:12.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5352" for this suite. 03/02/23 02:59:12.95
STEP: Destroying namespace "webhook-5352-markers" for this suite. 03/02/23 02:59:12.977
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","completed":340,"skipped":6202,"failed":0}
------------------------------
• [SLOW TEST] [6.824 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:59:06.379
    Mar  2 02:59:06.379: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename webhook 03/02/23 02:59:06.388
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:59:06.519
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:59:06.568
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/02/23 02:59:06.716
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/02/23 02:59:07.214
    STEP: Deploying the webhook pod 03/02/23 02:59:07.272
    STEP: Wait for the deployment to be ready 03/02/23 02:59:07.35
    Mar  2 02:59:07.390: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Mar  2 02:59:09.464: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 2, 59, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 59, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 2, 59, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 59, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/02/23 02:59:11.478
    STEP: Verifying the service has paired with the endpoint 03/02/23 02:59:11.522
    Mar  2 02:59:12.522: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:276
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 03/02/23 02:59:12.536
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 03/02/23 02:59:12.599
    STEP: Creating a dummy validating-webhook-configuration object 03/02/23 02:59:12.668
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 03/02/23 02:59:12.726
    STEP: Creating a dummy mutating-webhook-configuration object 03/02/23 02:59:12.783
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 03/02/23 02:59:12.827
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 02:59:12.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5352" for this suite. 03/02/23 02:59:12.95
    STEP: Destroying namespace "webhook-5352-markers" for this suite. 03/02/23 02:59:12.977
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:59:13.204
Mar  2 02:59:13.205: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename container-runtime 03/02/23 02:59:13.208
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:59:13.27
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:59:13.289
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
STEP: create the container 03/02/23 02:59:13.338
STEP: wait for the container to reach Succeeded 03/02/23 02:59:13.444
STEP: get the container status 03/02/23 02:59:18.541
STEP: the container should be terminated 03/02/23 02:59:18.553
STEP: the termination message should be set 03/02/23 02:59:18.553
Mar  2 02:59:18.554: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 03/02/23 02:59:18.554
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Mar  2 02:59:18.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6096" for this suite. 03/02/23 02:59:18.642
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","completed":341,"skipped":6209,"failed":0}
------------------------------
• [SLOW TEST] [5.463 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:59:13.204
    Mar  2 02:59:13.205: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename container-runtime 03/02/23 02:59:13.208
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:59:13.27
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:59:13.289
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194
    STEP: create the container 03/02/23 02:59:13.338
    STEP: wait for the container to reach Succeeded 03/02/23 02:59:13.444
    STEP: get the container status 03/02/23 02:59:18.541
    STEP: the container should be terminated 03/02/23 02:59:18.553
    STEP: the termination message should be set 03/02/23 02:59:18.553
    Mar  2 02:59:18.554: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 03/02/23 02:59:18.554
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Mar  2 02:59:18.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-6096" for this suite. 03/02/23 02:59:18.642
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:59:18.681
Mar  2 02:59:18.681: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename projected 03/02/23 02:59:18.682
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:59:18.771
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:59:18.784
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
STEP: Creating projection with secret that has name projected-secret-test-13fcdae4-1404-4585-8792-7192709d3144 03/02/23 02:59:18.795
STEP: Creating a pod to test consume secrets 03/02/23 02:59:18.823
Mar  2 02:59:18.906: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-219790c3-c9c2-4869-b90d-e8bc32b65060" in namespace "projected-953" to be "Succeeded or Failed"
Mar  2 02:59:18.922: INFO: Pod "pod-projected-secrets-219790c3-c9c2-4869-b90d-e8bc32b65060": Phase="Pending", Reason="", readiness=false. Elapsed: 15.29495ms
Mar  2 02:59:20.938: INFO: Pod "pod-projected-secrets-219790c3-c9c2-4869-b90d-e8bc32b65060": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031237046s
Mar  2 02:59:22.938: INFO: Pod "pod-projected-secrets-219790c3-c9c2-4869-b90d-e8bc32b65060": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031243916s
STEP: Saw pod success 03/02/23 02:59:22.938
Mar  2 02:59:22.938: INFO: Pod "pod-projected-secrets-219790c3-c9c2-4869-b90d-e8bc32b65060" satisfied condition "Succeeded or Failed"
Mar  2 02:59:22.951: INFO: Trying to get logs from node 10.132.92.143 pod pod-projected-secrets-219790c3-c9c2-4869-b90d-e8bc32b65060 container projected-secret-volume-test: <nil>
STEP: delete the pod 03/02/23 02:59:22.986
Mar  2 02:59:23.029: INFO: Waiting for pod pod-projected-secrets-219790c3-c9c2-4869-b90d-e8bc32b65060 to disappear
Mar  2 02:59:23.042: INFO: Pod pod-projected-secrets-219790c3-c9c2-4869-b90d-e8bc32b65060 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Mar  2 02:59:23.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-953" for this suite. 03/02/23 02:59:23.065
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":342,"skipped":6249,"failed":0}
------------------------------
• [4.408 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:59:18.681
    Mar  2 02:59:18.681: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename projected 03/02/23 02:59:18.682
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:59:18.771
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:59:18.784
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:66
    STEP: Creating projection with secret that has name projected-secret-test-13fcdae4-1404-4585-8792-7192709d3144 03/02/23 02:59:18.795
    STEP: Creating a pod to test consume secrets 03/02/23 02:59:18.823
    Mar  2 02:59:18.906: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-219790c3-c9c2-4869-b90d-e8bc32b65060" in namespace "projected-953" to be "Succeeded or Failed"
    Mar  2 02:59:18.922: INFO: Pod "pod-projected-secrets-219790c3-c9c2-4869-b90d-e8bc32b65060": Phase="Pending", Reason="", readiness=false. Elapsed: 15.29495ms
    Mar  2 02:59:20.938: INFO: Pod "pod-projected-secrets-219790c3-c9c2-4869-b90d-e8bc32b65060": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031237046s
    Mar  2 02:59:22.938: INFO: Pod "pod-projected-secrets-219790c3-c9c2-4869-b90d-e8bc32b65060": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031243916s
    STEP: Saw pod success 03/02/23 02:59:22.938
    Mar  2 02:59:22.938: INFO: Pod "pod-projected-secrets-219790c3-c9c2-4869-b90d-e8bc32b65060" satisfied condition "Succeeded or Failed"
    Mar  2 02:59:22.951: INFO: Trying to get logs from node 10.132.92.143 pod pod-projected-secrets-219790c3-c9c2-4869-b90d-e8bc32b65060 container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/02/23 02:59:22.986
    Mar  2 02:59:23.029: INFO: Waiting for pod pod-projected-secrets-219790c3-c9c2-4869-b90d-e8bc32b65060 to disappear
    Mar  2 02:59:23.042: INFO: Pod pod-projected-secrets-219790c3-c9c2-4869-b90d-e8bc32b65060 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Mar  2 02:59:23.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-953" for this suite. 03/02/23 02:59:23.065
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:59:23.093
Mar  2 02:59:23.093: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename gc 03/02/23 02:59:23.094
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:59:23.15
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:59:23.16
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 03/02/23 02:59:23.178
W0302 02:59:23.196058      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: delete the rc 03/02/23 02:59:28.212
STEP: wait for all pods to be garbage collected 03/02/23 02:59:28.233
STEP: Gathering metrics 03/02/23 02:59:33.259
W0302 02:59:33.291452      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Mar  2 02:59:33.291: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Mar  2 02:59:33.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4898" for this suite. 03/02/23 02:59:33.315
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","completed":343,"skipped":6293,"failed":0}
------------------------------
• [SLOW TEST] [10.255 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:59:23.093
    Mar  2 02:59:23.093: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename gc 03/02/23 02:59:23.094
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:59:23.15
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:59:23.16
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 03/02/23 02:59:23.178
    W0302 02:59:23.196058      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: delete the rc 03/02/23 02:59:28.212
    STEP: wait for all pods to be garbage collected 03/02/23 02:59:28.233
    STEP: Gathering metrics 03/02/23 02:59:33.259
    W0302 02:59:33.291452      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Mar  2 02:59:33.291: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Mar  2 02:59:33.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-4898" for this suite. 03/02/23 02:59:33.315
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:59:33.349
Mar  2 02:59:33.349: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename replicaset 03/02/23 02:59:33.351
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:59:33.424
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:59:33.457
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 03/02/23 02:59:33.477
Mar  2 02:59:33.549: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-5624" to be "running and ready"
Mar  2 02:59:33.565: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 15.339976ms
Mar  2 02:59:33.565: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Mar  2 02:59:35.580: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030347424s
Mar  2 02:59:35.580: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Mar  2 02:59:37.594: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 4.044187874s
Mar  2 02:59:37.594: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Mar  2 02:59:37.594: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 03/02/23 02:59:37.605
STEP: Then the orphan pod is adopted 03/02/23 02:59:37.647
STEP: When the matched label of one of its pods change 03/02/23 02:59:38.695
Mar  2 02:59:38.714: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 03/02/23 02:59:38.783
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Mar  2 02:59:39.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5624" for this suite. 03/02/23 02:59:39.86
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","completed":344,"skipped":6304,"failed":0}
------------------------------
• [SLOW TEST] [6.538 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:59:33.349
    Mar  2 02:59:33.349: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename replicaset 03/02/23 02:59:33.351
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:59:33.424
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:59:33.457
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 03/02/23 02:59:33.477
    Mar  2 02:59:33.549: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-5624" to be "running and ready"
    Mar  2 02:59:33.565: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 15.339976ms
    Mar  2 02:59:33.565: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 02:59:35.580: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030347424s
    Mar  2 02:59:35.580: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 02:59:37.594: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 4.044187874s
    Mar  2 02:59:37.594: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Mar  2 02:59:37.594: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 03/02/23 02:59:37.605
    STEP: Then the orphan pod is adopted 03/02/23 02:59:37.647
    STEP: When the matched label of one of its pods change 03/02/23 02:59:38.695
    Mar  2 02:59:38.714: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 03/02/23 02:59:38.783
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Mar  2 02:59:39.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-5624" for this suite. 03/02/23 02:59:39.86
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:59:39.889
Mar  2 02:59:39.889: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename replicaset 03/02/23 02:59:39.89
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:59:39.961
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:59:39.98
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Mar  2 02:59:39.994: INFO: Creating ReplicaSet my-hostname-basic-c1b9d1a9-e652-49e9-82f2-4b970775be8a
W0302 02:59:40.016132      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "my-hostname-basic-c1b9d1a9-e652-49e9-82f2-4b970775be8a" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "my-hostname-basic-c1b9d1a9-e652-49e9-82f2-4b970775be8a" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "my-hostname-basic-c1b9d1a9-e652-49e9-82f2-4b970775be8a" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "my-hostname-basic-c1b9d1a9-e652-49e9-82f2-4b970775be8a" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Mar  2 02:59:40.028: INFO: Pod name my-hostname-basic-c1b9d1a9-e652-49e9-82f2-4b970775be8a: Found 0 pods out of 1
Mar  2 02:59:45.043: INFO: Pod name my-hostname-basic-c1b9d1a9-e652-49e9-82f2-4b970775be8a: Found 1 pods out of 1
Mar  2 02:59:45.043: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-c1b9d1a9-e652-49e9-82f2-4b970775be8a" is running
Mar  2 02:59:45.043: INFO: Waiting up to 5m0s for pod "my-hostname-basic-c1b9d1a9-e652-49e9-82f2-4b970775be8a-lpvnl" in namespace "replicaset-835" to be "running"
Mar  2 02:59:45.058: INFO: Pod "my-hostname-basic-c1b9d1a9-e652-49e9-82f2-4b970775be8a-lpvnl": Phase="Running", Reason="", readiness=true. Elapsed: 14.991399ms
Mar  2 02:59:45.058: INFO: Pod "my-hostname-basic-c1b9d1a9-e652-49e9-82f2-4b970775be8a-lpvnl" satisfied condition "running"
Mar  2 02:59:45.058: INFO: Pod "my-hostname-basic-c1b9d1a9-e652-49e9-82f2-4b970775be8a-lpvnl" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-02 02:59:40 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-02 02:59:42 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-02 02:59:42 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-02 02:59:40 +0000 UTC Reason: Message:}])
Mar  2 02:59:45.058: INFO: Trying to dial the pod
Mar  2 02:59:50.147: INFO: Controller my-hostname-basic-c1b9d1a9-e652-49e9-82f2-4b970775be8a: Got expected result from replica 1 [my-hostname-basic-c1b9d1a9-e652-49e9-82f2-4b970775be8a-lpvnl]: "my-hostname-basic-c1b9d1a9-e652-49e9-82f2-4b970775be8a-lpvnl", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Mar  2 02:59:50.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-835" for this suite. 03/02/23 02:59:50.17
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","completed":345,"skipped":6327,"failed":0}
------------------------------
• [SLOW TEST] [10.312 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:59:39.889
    Mar  2 02:59:39.889: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename replicaset 03/02/23 02:59:39.89
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:59:39.961
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:59:39.98
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Mar  2 02:59:39.994: INFO: Creating ReplicaSet my-hostname-basic-c1b9d1a9-e652-49e9-82f2-4b970775be8a
    W0302 02:59:40.016132      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "my-hostname-basic-c1b9d1a9-e652-49e9-82f2-4b970775be8a" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "my-hostname-basic-c1b9d1a9-e652-49e9-82f2-4b970775be8a" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "my-hostname-basic-c1b9d1a9-e652-49e9-82f2-4b970775be8a" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "my-hostname-basic-c1b9d1a9-e652-49e9-82f2-4b970775be8a" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Mar  2 02:59:40.028: INFO: Pod name my-hostname-basic-c1b9d1a9-e652-49e9-82f2-4b970775be8a: Found 0 pods out of 1
    Mar  2 02:59:45.043: INFO: Pod name my-hostname-basic-c1b9d1a9-e652-49e9-82f2-4b970775be8a: Found 1 pods out of 1
    Mar  2 02:59:45.043: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-c1b9d1a9-e652-49e9-82f2-4b970775be8a" is running
    Mar  2 02:59:45.043: INFO: Waiting up to 5m0s for pod "my-hostname-basic-c1b9d1a9-e652-49e9-82f2-4b970775be8a-lpvnl" in namespace "replicaset-835" to be "running"
    Mar  2 02:59:45.058: INFO: Pod "my-hostname-basic-c1b9d1a9-e652-49e9-82f2-4b970775be8a-lpvnl": Phase="Running", Reason="", readiness=true. Elapsed: 14.991399ms
    Mar  2 02:59:45.058: INFO: Pod "my-hostname-basic-c1b9d1a9-e652-49e9-82f2-4b970775be8a-lpvnl" satisfied condition "running"
    Mar  2 02:59:45.058: INFO: Pod "my-hostname-basic-c1b9d1a9-e652-49e9-82f2-4b970775be8a-lpvnl" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-02 02:59:40 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-02 02:59:42 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-02 02:59:42 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-02 02:59:40 +0000 UTC Reason: Message:}])
    Mar  2 02:59:45.058: INFO: Trying to dial the pod
    Mar  2 02:59:50.147: INFO: Controller my-hostname-basic-c1b9d1a9-e652-49e9-82f2-4b970775be8a: Got expected result from replica 1 [my-hostname-basic-c1b9d1a9-e652-49e9-82f2-4b970775be8a-lpvnl]: "my-hostname-basic-c1b9d1a9-e652-49e9-82f2-4b970775be8a-lpvnl", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Mar  2 02:59:50.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-835" for this suite. 03/02/23 02:59:50.17
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 02:59:50.204
Mar  2 02:59:50.204: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename container-probe 03/02/23 02:59:50.208
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:59:50.267
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:59:50.279
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
Mar  2 02:59:50.360: INFO: Waiting up to 5m0s for pod "test-webserver-3dabdcc6-7956-47d9-89bb-895e58dd9ca2" in namespace "container-probe-5949" to be "running and ready"
Mar  2 02:59:50.373: INFO: Pod "test-webserver-3dabdcc6-7956-47d9-89bb-895e58dd9ca2": Phase="Pending", Reason="", readiness=false. Elapsed: 13.206647ms
Mar  2 02:59:50.373: INFO: The phase of Pod test-webserver-3dabdcc6-7956-47d9-89bb-895e58dd9ca2 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 02:59:52.389: INFO: Pod "test-webserver-3dabdcc6-7956-47d9-89bb-895e58dd9ca2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029076065s
Mar  2 02:59:52.389: INFO: The phase of Pod test-webserver-3dabdcc6-7956-47d9-89bb-895e58dd9ca2 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 02:59:54.387: INFO: Pod "test-webserver-3dabdcc6-7956-47d9-89bb-895e58dd9ca2": Phase="Running", Reason="", readiness=false. Elapsed: 4.027809359s
Mar  2 02:59:54.388: INFO: The phase of Pod test-webserver-3dabdcc6-7956-47d9-89bb-895e58dd9ca2 is Running (Ready = false)
Mar  2 02:59:56.388: INFO: Pod "test-webserver-3dabdcc6-7956-47d9-89bb-895e58dd9ca2": Phase="Running", Reason="", readiness=false. Elapsed: 6.028818571s
Mar  2 02:59:56.389: INFO: The phase of Pod test-webserver-3dabdcc6-7956-47d9-89bb-895e58dd9ca2 is Running (Ready = false)
Mar  2 02:59:58.388: INFO: Pod "test-webserver-3dabdcc6-7956-47d9-89bb-895e58dd9ca2": Phase="Running", Reason="", readiness=false. Elapsed: 8.028746017s
Mar  2 02:59:58.389: INFO: The phase of Pod test-webserver-3dabdcc6-7956-47d9-89bb-895e58dd9ca2 is Running (Ready = false)
Mar  2 03:00:00.390: INFO: Pod "test-webserver-3dabdcc6-7956-47d9-89bb-895e58dd9ca2": Phase="Running", Reason="", readiness=false. Elapsed: 10.03026754s
Mar  2 03:00:00.390: INFO: The phase of Pod test-webserver-3dabdcc6-7956-47d9-89bb-895e58dd9ca2 is Running (Ready = false)
Mar  2 03:00:02.396: INFO: Pod "test-webserver-3dabdcc6-7956-47d9-89bb-895e58dd9ca2": Phase="Running", Reason="", readiness=false. Elapsed: 12.03592942s
Mar  2 03:00:02.396: INFO: The phase of Pod test-webserver-3dabdcc6-7956-47d9-89bb-895e58dd9ca2 is Running (Ready = false)
Mar  2 03:00:04.434: INFO: Pod "test-webserver-3dabdcc6-7956-47d9-89bb-895e58dd9ca2": Phase="Running", Reason="", readiness=false. Elapsed: 14.073925985s
Mar  2 03:00:04.434: INFO: The phase of Pod test-webserver-3dabdcc6-7956-47d9-89bb-895e58dd9ca2 is Running (Ready = false)
Mar  2 03:00:06.389: INFO: Pod "test-webserver-3dabdcc6-7956-47d9-89bb-895e58dd9ca2": Phase="Running", Reason="", readiness=false. Elapsed: 16.029662254s
Mar  2 03:00:06.389: INFO: The phase of Pod test-webserver-3dabdcc6-7956-47d9-89bb-895e58dd9ca2 is Running (Ready = false)
Mar  2 03:00:08.407: INFO: Pod "test-webserver-3dabdcc6-7956-47d9-89bb-895e58dd9ca2": Phase="Running", Reason="", readiness=false. Elapsed: 18.047494291s
Mar  2 03:00:08.407: INFO: The phase of Pod test-webserver-3dabdcc6-7956-47d9-89bb-895e58dd9ca2 is Running (Ready = false)
Mar  2 03:00:10.388: INFO: Pod "test-webserver-3dabdcc6-7956-47d9-89bb-895e58dd9ca2": Phase="Running", Reason="", readiness=false. Elapsed: 20.028101654s
Mar  2 03:00:10.388: INFO: The phase of Pod test-webserver-3dabdcc6-7956-47d9-89bb-895e58dd9ca2 is Running (Ready = false)
Mar  2 03:00:12.395: INFO: Pod "test-webserver-3dabdcc6-7956-47d9-89bb-895e58dd9ca2": Phase="Running", Reason="", readiness=true. Elapsed: 22.034871395s
Mar  2 03:00:12.395: INFO: The phase of Pod test-webserver-3dabdcc6-7956-47d9-89bb-895e58dd9ca2 is Running (Ready = true)
Mar  2 03:00:12.395: INFO: Pod "test-webserver-3dabdcc6-7956-47d9-89bb-895e58dd9ca2" satisfied condition "running and ready"
Mar  2 03:00:12.407: INFO: Container started at 2023-03-02 02:59:51 +0000 UTC, pod became ready at 2023-03-02 03:00:10 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar  2 03:00:12.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5949" for this suite. 03/02/23 03:00:12.432
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","completed":346,"skipped":6338,"failed":0}
------------------------------
• [SLOW TEST] [22.297 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 02:59:50.204
    Mar  2 02:59:50.204: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename container-probe 03/02/23 02:59:50.208
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 02:59:50.267
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 02:59:50.279
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:68
    Mar  2 02:59:50.360: INFO: Waiting up to 5m0s for pod "test-webserver-3dabdcc6-7956-47d9-89bb-895e58dd9ca2" in namespace "container-probe-5949" to be "running and ready"
    Mar  2 02:59:50.373: INFO: Pod "test-webserver-3dabdcc6-7956-47d9-89bb-895e58dd9ca2": Phase="Pending", Reason="", readiness=false. Elapsed: 13.206647ms
    Mar  2 02:59:50.373: INFO: The phase of Pod test-webserver-3dabdcc6-7956-47d9-89bb-895e58dd9ca2 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 02:59:52.389: INFO: Pod "test-webserver-3dabdcc6-7956-47d9-89bb-895e58dd9ca2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029076065s
    Mar  2 02:59:52.389: INFO: The phase of Pod test-webserver-3dabdcc6-7956-47d9-89bb-895e58dd9ca2 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 02:59:54.387: INFO: Pod "test-webserver-3dabdcc6-7956-47d9-89bb-895e58dd9ca2": Phase="Running", Reason="", readiness=false. Elapsed: 4.027809359s
    Mar  2 02:59:54.388: INFO: The phase of Pod test-webserver-3dabdcc6-7956-47d9-89bb-895e58dd9ca2 is Running (Ready = false)
    Mar  2 02:59:56.388: INFO: Pod "test-webserver-3dabdcc6-7956-47d9-89bb-895e58dd9ca2": Phase="Running", Reason="", readiness=false. Elapsed: 6.028818571s
    Mar  2 02:59:56.389: INFO: The phase of Pod test-webserver-3dabdcc6-7956-47d9-89bb-895e58dd9ca2 is Running (Ready = false)
    Mar  2 02:59:58.388: INFO: Pod "test-webserver-3dabdcc6-7956-47d9-89bb-895e58dd9ca2": Phase="Running", Reason="", readiness=false. Elapsed: 8.028746017s
    Mar  2 02:59:58.389: INFO: The phase of Pod test-webserver-3dabdcc6-7956-47d9-89bb-895e58dd9ca2 is Running (Ready = false)
    Mar  2 03:00:00.390: INFO: Pod "test-webserver-3dabdcc6-7956-47d9-89bb-895e58dd9ca2": Phase="Running", Reason="", readiness=false. Elapsed: 10.03026754s
    Mar  2 03:00:00.390: INFO: The phase of Pod test-webserver-3dabdcc6-7956-47d9-89bb-895e58dd9ca2 is Running (Ready = false)
    Mar  2 03:00:02.396: INFO: Pod "test-webserver-3dabdcc6-7956-47d9-89bb-895e58dd9ca2": Phase="Running", Reason="", readiness=false. Elapsed: 12.03592942s
    Mar  2 03:00:02.396: INFO: The phase of Pod test-webserver-3dabdcc6-7956-47d9-89bb-895e58dd9ca2 is Running (Ready = false)
    Mar  2 03:00:04.434: INFO: Pod "test-webserver-3dabdcc6-7956-47d9-89bb-895e58dd9ca2": Phase="Running", Reason="", readiness=false. Elapsed: 14.073925985s
    Mar  2 03:00:04.434: INFO: The phase of Pod test-webserver-3dabdcc6-7956-47d9-89bb-895e58dd9ca2 is Running (Ready = false)
    Mar  2 03:00:06.389: INFO: Pod "test-webserver-3dabdcc6-7956-47d9-89bb-895e58dd9ca2": Phase="Running", Reason="", readiness=false. Elapsed: 16.029662254s
    Mar  2 03:00:06.389: INFO: The phase of Pod test-webserver-3dabdcc6-7956-47d9-89bb-895e58dd9ca2 is Running (Ready = false)
    Mar  2 03:00:08.407: INFO: Pod "test-webserver-3dabdcc6-7956-47d9-89bb-895e58dd9ca2": Phase="Running", Reason="", readiness=false. Elapsed: 18.047494291s
    Mar  2 03:00:08.407: INFO: The phase of Pod test-webserver-3dabdcc6-7956-47d9-89bb-895e58dd9ca2 is Running (Ready = false)
    Mar  2 03:00:10.388: INFO: Pod "test-webserver-3dabdcc6-7956-47d9-89bb-895e58dd9ca2": Phase="Running", Reason="", readiness=false. Elapsed: 20.028101654s
    Mar  2 03:00:10.388: INFO: The phase of Pod test-webserver-3dabdcc6-7956-47d9-89bb-895e58dd9ca2 is Running (Ready = false)
    Mar  2 03:00:12.395: INFO: Pod "test-webserver-3dabdcc6-7956-47d9-89bb-895e58dd9ca2": Phase="Running", Reason="", readiness=true. Elapsed: 22.034871395s
    Mar  2 03:00:12.395: INFO: The phase of Pod test-webserver-3dabdcc6-7956-47d9-89bb-895e58dd9ca2 is Running (Ready = true)
    Mar  2 03:00:12.395: INFO: Pod "test-webserver-3dabdcc6-7956-47d9-89bb-895e58dd9ca2" satisfied condition "running and ready"
    Mar  2 03:00:12.407: INFO: Container started at 2023-03-02 02:59:51 +0000 UTC, pod became ready at 2023-03-02 03:00:10 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar  2 03:00:12.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-5949" for this suite. 03/02/23 03:00:12.432
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 03:00:12.502
Mar  2 03:00:12.503: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename svcaccounts 03/02/23 03:00:12.504
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 03:00:12.585
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 03:00:12.62
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
Mar  2 03:00:12.746: INFO: created pod
Mar  2 03:00:12.746: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-4371" to be "Succeeded or Failed"
Mar  2 03:00:12.763: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 17.661041ms
Mar  2 03:00:14.779: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033027119s
Mar  2 03:00:16.779: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033326072s
Mar  2 03:00:18.780: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.033996255s
STEP: Saw pod success 03/02/23 03:00:18.78
Mar  2 03:00:18.780: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Mar  2 03:00:48.782: INFO: polling logs
Mar  2 03:00:48.813: INFO: Pod logs: 
I0302 03:00:14.365288       1 log.go:195] OK: Got token
I0302 03:00:14.365571       1 log.go:195] validating with in-cluster discovery
I0302 03:00:14.366282       1 log.go:195] OK: got issuer https://kubernetes.default.svc
I0302 03:00:14.366429       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-4371:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1677726613, NotBefore:1677726013, IssuedAt:1677726013, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4371", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"0947def1-17f7-4233-9ed7-8a6233e6ed55"}}}
I0302 03:00:14.391304       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
I0302 03:00:14.410623       1 log.go:195] OK: Validated signature on JWT
I0302 03:00:14.410816       1 log.go:195] OK: Got valid claims from token!
I0302 03:00:14.410927       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-4371:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1677726613, NotBefore:1677726013, IssuedAt:1677726013, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4371", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"0947def1-17f7-4233-9ed7-8a6233e6ed55"}}}

Mar  2 03:00:48.813: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Mar  2 03:00:48.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4371" for this suite. 03/02/23 03:00:48.863
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","completed":347,"skipped":6358,"failed":0}
------------------------------
• [SLOW TEST] [36.384 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 03:00:12.502
    Mar  2 03:00:12.503: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename svcaccounts 03/02/23 03:00:12.504
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 03:00:12.585
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 03:00:12.62
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:528
    Mar  2 03:00:12.746: INFO: created pod
    Mar  2 03:00:12.746: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-4371" to be "Succeeded or Failed"
    Mar  2 03:00:12.763: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 17.661041ms
    Mar  2 03:00:14.779: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033027119s
    Mar  2 03:00:16.779: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033326072s
    Mar  2 03:00:18.780: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.033996255s
    STEP: Saw pod success 03/02/23 03:00:18.78
    Mar  2 03:00:18.780: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Mar  2 03:00:48.782: INFO: polling logs
    Mar  2 03:00:48.813: INFO: Pod logs: 
    I0302 03:00:14.365288       1 log.go:195] OK: Got token
    I0302 03:00:14.365571       1 log.go:195] validating with in-cluster discovery
    I0302 03:00:14.366282       1 log.go:195] OK: got issuer https://kubernetes.default.svc
    I0302 03:00:14.366429       1 log.go:195] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-4371:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1677726613, NotBefore:1677726013, IssuedAt:1677726013, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4371", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"0947def1-17f7-4233-9ed7-8a6233e6ed55"}}}
    I0302 03:00:14.391304       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
    I0302 03:00:14.410623       1 log.go:195] OK: Validated signature on JWT
    I0302 03:00:14.410816       1 log.go:195] OK: Got valid claims from token!
    I0302 03:00:14.410927       1 log.go:195] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-4371:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1677726613, NotBefore:1677726013, IssuedAt:1677726013, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4371", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"0947def1-17f7-4233-9ed7-8a6233e6ed55"}}}

    Mar  2 03:00:48.813: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Mar  2 03:00:48.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-4371" for this suite. 03/02/23 03:00:48.863
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 03:00:48.889
Mar  2 03:00:48.889: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename kubectl 03/02/23 03:00:48.89
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 03:00:48.976
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 03:00:48.99
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1698
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/02/23 03:00:49.003
Mar  2 03:00:49.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-3643 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
Mar  2 03:00:49.218: INFO: stderr: ""
Mar  2 03:00:49.218: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 03/02/23 03:00:49.218
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1702
Mar  2 03:00:49.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-3643 delete pods e2e-test-httpd-pod'
Mar  2 03:00:53.193: INFO: stderr: ""
Mar  2 03:00:53.193: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar  2 03:00:53.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3643" for this suite. 03/02/23 03:00:53.217
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","completed":348,"skipped":6359,"failed":0}
------------------------------
• [4.368 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1695
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1711

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 03:00:48.889
    Mar  2 03:00:48.889: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename kubectl 03/02/23 03:00:48.89
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 03:00:48.976
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 03:00:48.99
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1698
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1711
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/02/23 03:00:49.003
    Mar  2 03:00:49.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-3643 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
    Mar  2 03:00:49.218: INFO: stderr: ""
    Mar  2 03:00:49.218: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 03/02/23 03:00:49.218
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1702
    Mar  2 03:00:49.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=kubectl-3643 delete pods e2e-test-httpd-pod'
    Mar  2 03:00:53.193: INFO: stderr: ""
    Mar  2 03:00:53.193: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar  2 03:00:53.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3643" for this suite. 03/02/23 03:00:53.217
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 03:00:53.258
Mar  2 03:00:53.259: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename replicaset 03/02/23 03:00:53.26
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 03:00:53.34
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 03:00:53.354
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 03/02/23 03:00:53.371
STEP: Verify that the required pods have come up 03/02/23 03:00:53.393
Mar  2 03:00:53.406: INFO: Pod name sample-pod: Found 0 pods out of 3
Mar  2 03:00:58.423: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 03/02/23 03:00:58.423
Mar  2 03:00:58.437: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 03/02/23 03:00:58.437
STEP: DeleteCollection of the ReplicaSets 03/02/23 03:00:58.468
STEP: After DeleteCollection verify that ReplicaSets have been deleted 03/02/23 03:00:58.506
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Mar  2 03:00:58.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6037" for this suite. 03/02/23 03:00:58.568
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","completed":349,"skipped":6368,"failed":0}
------------------------------
• [SLOW TEST] [5.349 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 03:00:53.258
    Mar  2 03:00:53.259: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename replicaset 03/02/23 03:00:53.26
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 03:00:53.34
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 03:00:53.354
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 03/02/23 03:00:53.371
    STEP: Verify that the required pods have come up 03/02/23 03:00:53.393
    Mar  2 03:00:53.406: INFO: Pod name sample-pod: Found 0 pods out of 3
    Mar  2 03:00:58.423: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 03/02/23 03:00:58.423
    Mar  2 03:00:58.437: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 03/02/23 03:00:58.437
    STEP: DeleteCollection of the ReplicaSets 03/02/23 03:00:58.468
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 03/02/23 03:00:58.506
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Mar  2 03:00:58.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-6037" for this suite. 03/02/23 03:00:58.568
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 03:00:58.608
Mar  2 03:00:58.608: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename var-expansion 03/02/23 03:00:58.609
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 03:00:58.694
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 03:00:58.709
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
STEP: Creating a pod to test substitution in volume subpath 03/02/23 03:00:58.729
Mar  2 03:00:58.798: INFO: Waiting up to 5m0s for pod "var-expansion-ac504518-eecd-420d-92dd-3050a6df892a" in namespace "var-expansion-6099" to be "Succeeded or Failed"
Mar  2 03:00:58.822: INFO: Pod "var-expansion-ac504518-eecd-420d-92dd-3050a6df892a": Phase="Pending", Reason="", readiness=false. Elapsed: 16.013101ms
Mar  2 03:01:00.850: INFO: Pod "var-expansion-ac504518-eecd-420d-92dd-3050a6df892a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043507374s
Mar  2 03:01:02.855: INFO: Pod "var-expansion-ac504518-eecd-420d-92dd-3050a6df892a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04939428s
Mar  2 03:01:04.840: INFO: Pod "var-expansion-ac504518-eecd-420d-92dd-3050a6df892a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.03362426s
STEP: Saw pod success 03/02/23 03:01:04.84
Mar  2 03:01:04.840: INFO: Pod "var-expansion-ac504518-eecd-420d-92dd-3050a6df892a" satisfied condition "Succeeded or Failed"
Mar  2 03:01:04.855: INFO: Trying to get logs from node 10.132.92.143 pod var-expansion-ac504518-eecd-420d-92dd-3050a6df892a container dapi-container: <nil>
STEP: delete the pod 03/02/23 03:01:04.887
Mar  2 03:01:04.931: INFO: Waiting for pod var-expansion-ac504518-eecd-420d-92dd-3050a6df892a to disappear
Mar  2 03:01:04.944: INFO: Pod var-expansion-ac504518-eecd-420d-92dd-3050a6df892a no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar  2 03:01:04.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6099" for this suite. 03/02/23 03:01:04.969
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","completed":350,"skipped":6373,"failed":0}
------------------------------
• [SLOW TEST] [6.393 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 03:00:58.608
    Mar  2 03:00:58.608: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename var-expansion 03/02/23 03:00:58.609
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 03:00:58.694
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 03:00:58.709
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:111
    STEP: Creating a pod to test substitution in volume subpath 03/02/23 03:00:58.729
    Mar  2 03:00:58.798: INFO: Waiting up to 5m0s for pod "var-expansion-ac504518-eecd-420d-92dd-3050a6df892a" in namespace "var-expansion-6099" to be "Succeeded or Failed"
    Mar  2 03:00:58.822: INFO: Pod "var-expansion-ac504518-eecd-420d-92dd-3050a6df892a": Phase="Pending", Reason="", readiness=false. Elapsed: 16.013101ms
    Mar  2 03:01:00.850: INFO: Pod "var-expansion-ac504518-eecd-420d-92dd-3050a6df892a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043507374s
    Mar  2 03:01:02.855: INFO: Pod "var-expansion-ac504518-eecd-420d-92dd-3050a6df892a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04939428s
    Mar  2 03:01:04.840: INFO: Pod "var-expansion-ac504518-eecd-420d-92dd-3050a6df892a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.03362426s
    STEP: Saw pod success 03/02/23 03:01:04.84
    Mar  2 03:01:04.840: INFO: Pod "var-expansion-ac504518-eecd-420d-92dd-3050a6df892a" satisfied condition "Succeeded or Failed"
    Mar  2 03:01:04.855: INFO: Trying to get logs from node 10.132.92.143 pod var-expansion-ac504518-eecd-420d-92dd-3050a6df892a container dapi-container: <nil>
    STEP: delete the pod 03/02/23 03:01:04.887
    Mar  2 03:01:04.931: INFO: Waiting for pod var-expansion-ac504518-eecd-420d-92dd-3050a6df892a to disappear
    Mar  2 03:01:04.944: INFO: Pod var-expansion-ac504518-eecd-420d-92dd-3050a6df892a no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar  2 03:01:04.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-6099" for this suite. 03/02/23 03:01:04.969
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 03:01:05.005
Mar  2 03:01:05.005: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename kubelet-test 03/02/23 03:01:05.008
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 03:01:05.074
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 03:01:05.095
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Mar  2 03:01:05.175: INFO: Waiting up to 5m0s for pod "busybox-scheduling-bc8947c3-6711-4030-8898-79b818f02d3f" in namespace "kubelet-test-2202" to be "running and ready"
Mar  2 03:01:05.200: INFO: Pod "busybox-scheduling-bc8947c3-6711-4030-8898-79b818f02d3f": Phase="Pending", Reason="", readiness=false. Elapsed: 24.254799ms
Mar  2 03:01:05.200: INFO: The phase of Pod busybox-scheduling-bc8947c3-6711-4030-8898-79b818f02d3f is Pending, waiting for it to be Running (with Ready = true)
Mar  2 03:01:07.216: INFO: Pod "busybox-scheduling-bc8947c3-6711-4030-8898-79b818f02d3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040989362s
Mar  2 03:01:07.216: INFO: The phase of Pod busybox-scheduling-bc8947c3-6711-4030-8898-79b818f02d3f is Pending, waiting for it to be Running (with Ready = true)
Mar  2 03:01:09.214: INFO: Pod "busybox-scheduling-bc8947c3-6711-4030-8898-79b818f02d3f": Phase="Running", Reason="", readiness=true. Elapsed: 4.039145616s
Mar  2 03:01:09.215: INFO: The phase of Pod busybox-scheduling-bc8947c3-6711-4030-8898-79b818f02d3f is Running (Ready = true)
Mar  2 03:01:09.215: INFO: Pod "busybox-scheduling-bc8947c3-6711-4030-8898-79b818f02d3f" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Mar  2 03:01:09.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2202" for this suite. 03/02/23 03:01:09.287
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","completed":351,"skipped":6411,"failed":0}
------------------------------
• [4.309 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 03:01:05.005
    Mar  2 03:01:05.005: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename kubelet-test 03/02/23 03:01:05.008
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 03:01:05.074
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 03:01:05.095
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Mar  2 03:01:05.175: INFO: Waiting up to 5m0s for pod "busybox-scheduling-bc8947c3-6711-4030-8898-79b818f02d3f" in namespace "kubelet-test-2202" to be "running and ready"
    Mar  2 03:01:05.200: INFO: Pod "busybox-scheduling-bc8947c3-6711-4030-8898-79b818f02d3f": Phase="Pending", Reason="", readiness=false. Elapsed: 24.254799ms
    Mar  2 03:01:05.200: INFO: The phase of Pod busybox-scheduling-bc8947c3-6711-4030-8898-79b818f02d3f is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 03:01:07.216: INFO: Pod "busybox-scheduling-bc8947c3-6711-4030-8898-79b818f02d3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040989362s
    Mar  2 03:01:07.216: INFO: The phase of Pod busybox-scheduling-bc8947c3-6711-4030-8898-79b818f02d3f is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 03:01:09.214: INFO: Pod "busybox-scheduling-bc8947c3-6711-4030-8898-79b818f02d3f": Phase="Running", Reason="", readiness=true. Elapsed: 4.039145616s
    Mar  2 03:01:09.215: INFO: The phase of Pod busybox-scheduling-bc8947c3-6711-4030-8898-79b818f02d3f is Running (Ready = true)
    Mar  2 03:01:09.215: INFO: Pod "busybox-scheduling-bc8947c3-6711-4030-8898-79b818f02d3f" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Mar  2 03:01:09.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-2202" for this suite. 03/02/23 03:01:09.287
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 03:01:09.321
Mar  2 03:01:09.321: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename secrets 03/02/23 03:01:09.323
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 03:01:09.422
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 03:01:09.44
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
STEP: creating secret secrets-9584/secret-test-dfcb1044-a2dd-4564-94ac-1c89731cabe0 03/02/23 03:01:09.465
STEP: Creating a pod to test consume secrets 03/02/23 03:01:09.501
Mar  2 03:01:09.578: INFO: Waiting up to 5m0s for pod "pod-configmaps-ee9e3fe1-455c-4d6e-8c4a-cf2c21fedae9" in namespace "secrets-9584" to be "Succeeded or Failed"
Mar  2 03:01:09.595: INFO: Pod "pod-configmaps-ee9e3fe1-455c-4d6e-8c4a-cf2c21fedae9": Phase="Pending", Reason="", readiness=false. Elapsed: 17.420456ms
Mar  2 03:01:11.610: INFO: Pod "pod-configmaps-ee9e3fe1-455c-4d6e-8c4a-cf2c21fedae9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032323341s
Mar  2 03:01:13.610: INFO: Pod "pod-configmaps-ee9e3fe1-455c-4d6e-8c4a-cf2c21fedae9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031848981s
Mar  2 03:01:15.611: INFO: Pod "pod-configmaps-ee9e3fe1-455c-4d6e-8c4a-cf2c21fedae9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032837461s
STEP: Saw pod success 03/02/23 03:01:15.611
Mar  2 03:01:15.611: INFO: Pod "pod-configmaps-ee9e3fe1-455c-4d6e-8c4a-cf2c21fedae9" satisfied condition "Succeeded or Failed"
Mar  2 03:01:15.625: INFO: Trying to get logs from node 10.132.92.143 pod pod-configmaps-ee9e3fe1-455c-4d6e-8c4a-cf2c21fedae9 container env-test: <nil>
STEP: delete the pod 03/02/23 03:01:15.663
Mar  2 03:01:15.712: INFO: Waiting for pod pod-configmaps-ee9e3fe1-455c-4d6e-8c4a-cf2c21fedae9 to disappear
Mar  2 03:01:15.750: INFO: Pod pod-configmaps-ee9e3fe1-455c-4d6e-8c4a-cf2c21fedae9 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Mar  2 03:01:15.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9584" for this suite. 03/02/23 03:01:15.809
{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","completed":352,"skipped":6429,"failed":0}
------------------------------
• [SLOW TEST] [6.522 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 03:01:09.321
    Mar  2 03:01:09.321: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename secrets 03/02/23 03:01:09.323
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 03:01:09.422
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 03:01:09.44
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:94
    STEP: creating secret secrets-9584/secret-test-dfcb1044-a2dd-4564-94ac-1c89731cabe0 03/02/23 03:01:09.465
    STEP: Creating a pod to test consume secrets 03/02/23 03:01:09.501
    Mar  2 03:01:09.578: INFO: Waiting up to 5m0s for pod "pod-configmaps-ee9e3fe1-455c-4d6e-8c4a-cf2c21fedae9" in namespace "secrets-9584" to be "Succeeded or Failed"
    Mar  2 03:01:09.595: INFO: Pod "pod-configmaps-ee9e3fe1-455c-4d6e-8c4a-cf2c21fedae9": Phase="Pending", Reason="", readiness=false. Elapsed: 17.420456ms
    Mar  2 03:01:11.610: INFO: Pod "pod-configmaps-ee9e3fe1-455c-4d6e-8c4a-cf2c21fedae9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032323341s
    Mar  2 03:01:13.610: INFO: Pod "pod-configmaps-ee9e3fe1-455c-4d6e-8c4a-cf2c21fedae9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031848981s
    Mar  2 03:01:15.611: INFO: Pod "pod-configmaps-ee9e3fe1-455c-4d6e-8c4a-cf2c21fedae9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032837461s
    STEP: Saw pod success 03/02/23 03:01:15.611
    Mar  2 03:01:15.611: INFO: Pod "pod-configmaps-ee9e3fe1-455c-4d6e-8c4a-cf2c21fedae9" satisfied condition "Succeeded or Failed"
    Mar  2 03:01:15.625: INFO: Trying to get logs from node 10.132.92.143 pod pod-configmaps-ee9e3fe1-455c-4d6e-8c4a-cf2c21fedae9 container env-test: <nil>
    STEP: delete the pod 03/02/23 03:01:15.663
    Mar  2 03:01:15.712: INFO: Waiting for pod pod-configmaps-ee9e3fe1-455c-4d6e-8c4a-cf2c21fedae9 to disappear
    Mar  2 03:01:15.750: INFO: Pod pod-configmaps-ee9e3fe1-455c-4d6e-8c4a-cf2c21fedae9 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Mar  2 03:01:15.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-9584" for this suite. 03/02/23 03:01:15.809
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 03:01:15.844
Mar  2 03:01:15.844: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename svcaccounts 03/02/23 03:01:15.845
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 03:01:15.964
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 03:01:15.977
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
STEP: creating a ServiceAccount 03/02/23 03:01:15.991
STEP: watching for the ServiceAccount to be added 03/02/23 03:01:16.074
STEP: patching the ServiceAccount 03/02/23 03:01:16.085
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 03/02/23 03:01:16.11
STEP: deleting the ServiceAccount 03/02/23 03:01:16.153
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Mar  2 03:01:16.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8287" for this suite. 03/02/23 03:01:16.609
{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","completed":353,"skipped":6441,"failed":0}
------------------------------
• [0.795 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 03:01:15.844
    Mar  2 03:01:15.844: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename svcaccounts 03/02/23 03:01:15.845
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 03:01:15.964
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 03:01:15.977
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:646
    STEP: creating a ServiceAccount 03/02/23 03:01:15.991
    STEP: watching for the ServiceAccount to be added 03/02/23 03:01:16.074
    STEP: patching the ServiceAccount 03/02/23 03:01:16.085
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 03/02/23 03:01:16.11
    STEP: deleting the ServiceAccount 03/02/23 03:01:16.153
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Mar  2 03:01:16.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-8287" for this suite. 03/02/23 03:01:16.609
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 03:01:16.641
Mar  2 03:01:16.642: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename projected 03/02/23 03:01:16.644
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 03:01:16.739
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 03:01:16.751
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
Mar  2 03:01:16.879: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-0f21fa0a-e1a4-435f-86c5-44c8e0962abf 03/02/23 03:01:16.879
STEP: Creating secret with name s-test-opt-upd-ad6c72c4-e773-43d4-8ef5-533dffbb7a68 03/02/23 03:01:16.942
STEP: Creating the pod 03/02/23 03:01:17.182
Mar  2 03:01:17.253: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-96100868-4156-402d-b3f0-6c20d5a5f938" in namespace "projected-4981" to be "running and ready"
Mar  2 03:01:17.265: INFO: Pod "pod-projected-secrets-96100868-4156-402d-b3f0-6c20d5a5f938": Phase="Pending", Reason="", readiness=false. Elapsed: 12.511909ms
Mar  2 03:01:17.265: INFO: The phase of Pod pod-projected-secrets-96100868-4156-402d-b3f0-6c20d5a5f938 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 03:01:19.284: INFO: Pod "pod-projected-secrets-96100868-4156-402d-b3f0-6c20d5a5f938": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031679595s
Mar  2 03:01:19.284: INFO: The phase of Pod pod-projected-secrets-96100868-4156-402d-b3f0-6c20d5a5f938 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 03:01:21.293: INFO: Pod "pod-projected-secrets-96100868-4156-402d-b3f0-6c20d5a5f938": Phase="Running", Reason="", readiness=true. Elapsed: 4.040120181s
Mar  2 03:01:21.293: INFO: The phase of Pod pod-projected-secrets-96100868-4156-402d-b3f0-6c20d5a5f938 is Running (Ready = true)
Mar  2 03:01:21.293: INFO: Pod "pod-projected-secrets-96100868-4156-402d-b3f0-6c20d5a5f938" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-0f21fa0a-e1a4-435f-86c5-44c8e0962abf 03/02/23 03:01:21.477
STEP: Updating secret s-test-opt-upd-ad6c72c4-e773-43d4-8ef5-533dffbb7a68 03/02/23 03:01:21.533
STEP: Creating secret with name s-test-opt-create-e8075132-863f-4095-b4f1-fc88bfbb62fc 03/02/23 03:01:21.553
STEP: waiting to observe update in volume 03/02/23 03:01:21.584
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Mar  2 03:02:37.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4981" for this suite. 03/02/23 03:02:37.484
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":354,"skipped":6468,"failed":0}
------------------------------
• [SLOW TEST] [80.868 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 03:01:16.641
    Mar  2 03:01:16.642: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename projected 03/02/23 03:01:16.644
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 03:01:16.739
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 03:01:16.751
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:214
    Mar  2 03:01:16.879: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating secret with name s-test-opt-del-0f21fa0a-e1a4-435f-86c5-44c8e0962abf 03/02/23 03:01:16.879
    STEP: Creating secret with name s-test-opt-upd-ad6c72c4-e773-43d4-8ef5-533dffbb7a68 03/02/23 03:01:16.942
    STEP: Creating the pod 03/02/23 03:01:17.182
    Mar  2 03:01:17.253: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-96100868-4156-402d-b3f0-6c20d5a5f938" in namespace "projected-4981" to be "running and ready"
    Mar  2 03:01:17.265: INFO: Pod "pod-projected-secrets-96100868-4156-402d-b3f0-6c20d5a5f938": Phase="Pending", Reason="", readiness=false. Elapsed: 12.511909ms
    Mar  2 03:01:17.265: INFO: The phase of Pod pod-projected-secrets-96100868-4156-402d-b3f0-6c20d5a5f938 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 03:01:19.284: INFO: Pod "pod-projected-secrets-96100868-4156-402d-b3f0-6c20d5a5f938": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031679595s
    Mar  2 03:01:19.284: INFO: The phase of Pod pod-projected-secrets-96100868-4156-402d-b3f0-6c20d5a5f938 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 03:01:21.293: INFO: Pod "pod-projected-secrets-96100868-4156-402d-b3f0-6c20d5a5f938": Phase="Running", Reason="", readiness=true. Elapsed: 4.040120181s
    Mar  2 03:01:21.293: INFO: The phase of Pod pod-projected-secrets-96100868-4156-402d-b3f0-6c20d5a5f938 is Running (Ready = true)
    Mar  2 03:01:21.293: INFO: Pod "pod-projected-secrets-96100868-4156-402d-b3f0-6c20d5a5f938" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-0f21fa0a-e1a4-435f-86c5-44c8e0962abf 03/02/23 03:01:21.477
    STEP: Updating secret s-test-opt-upd-ad6c72c4-e773-43d4-8ef5-533dffbb7a68 03/02/23 03:01:21.533
    STEP: Creating secret with name s-test-opt-create-e8075132-863f-4095-b4f1-fc88bfbb62fc 03/02/23 03:01:21.553
    STEP: waiting to observe update in volume 03/02/23 03:01:21.584
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Mar  2 03:02:37.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4981" for this suite. 03/02/23 03:02:37.484
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 03:02:37.516
Mar  2 03:02:37.516: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename services 03/02/23 03:02:37.518
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 03:02:37.601
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 03:02:37.615
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-677 03/02/23 03:02:37.629
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 03/02/23 03:02:37.685
STEP: creating service externalsvc in namespace services-677 03/02/23 03:02:37.685
STEP: creating replication controller externalsvc in namespace services-677 03/02/23 03:02:37.758
I0302 03:02:37.784221      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-677, replica count: 2
I0302 03:02:40.835674      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 03/02/23 03:02:40.849
Mar  2 03:02:40.899: INFO: Creating new exec pod
Mar  2 03:02:40.956: INFO: Waiting up to 5m0s for pod "execpodfcsl6" in namespace "services-677" to be "running"
Mar  2 03:02:40.968: INFO: Pod "execpodfcsl6": Phase="Pending", Reason="", readiness=false. Elapsed: 11.473994ms
Mar  2 03:02:42.984: INFO: Pod "execpodfcsl6": Phase="Running", Reason="", readiness=true. Elapsed: 2.028323755s
Mar  2 03:02:42.985: INFO: Pod "execpodfcsl6" satisfied condition "running"
Mar  2 03:02:42.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-677 exec execpodfcsl6 -- /bin/sh -x -c nslookup clusterip-service.services-677.svc.cluster.local'
Mar  2 03:02:43.343: INFO: stderr: "+ nslookup clusterip-service.services-677.svc.cluster.local\n"
Mar  2 03:02:43.343: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nclusterip-service.services-677.svc.cluster.local\tcanonical name = externalsvc.services-677.svc.cluster.local.\nName:\texternalsvc.services-677.svc.cluster.local\nAddress: 172.21.39.202\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-677, will wait for the garbage collector to delete the pods 03/02/23 03:02:43.343
Mar  2 03:02:43.430: INFO: Deleting ReplicationController externalsvc took: 23.625125ms
Mar  2 03:02:43.631: INFO: Terminating ReplicationController externalsvc pods took: 201.032909ms
Mar  2 03:02:47.219: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar  2 03:02:47.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-677" for this suite. 03/02/23 03:02:47.593
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","completed":355,"skipped":6511,"failed":0}
------------------------------
• [SLOW TEST] [10.215 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 03:02:37.516
    Mar  2 03:02:37.516: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename services 03/02/23 03:02:37.518
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 03:02:37.601
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 03:02:37.615
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1481
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-677 03/02/23 03:02:37.629
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 03/02/23 03:02:37.685
    STEP: creating service externalsvc in namespace services-677 03/02/23 03:02:37.685
    STEP: creating replication controller externalsvc in namespace services-677 03/02/23 03:02:37.758
    I0302 03:02:37.784221      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-677, replica count: 2
    I0302 03:02:40.835674      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 03/02/23 03:02:40.849
    Mar  2 03:02:40.899: INFO: Creating new exec pod
    Mar  2 03:02:40.956: INFO: Waiting up to 5m0s for pod "execpodfcsl6" in namespace "services-677" to be "running"
    Mar  2 03:02:40.968: INFO: Pod "execpodfcsl6": Phase="Pending", Reason="", readiness=false. Elapsed: 11.473994ms
    Mar  2 03:02:42.984: INFO: Pod "execpodfcsl6": Phase="Running", Reason="", readiness=true. Elapsed: 2.028323755s
    Mar  2 03:02:42.985: INFO: Pod "execpodfcsl6" satisfied condition "running"
    Mar  2 03:02:42.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1535626242 --namespace=services-677 exec execpodfcsl6 -- /bin/sh -x -c nslookup clusterip-service.services-677.svc.cluster.local'
    Mar  2 03:02:43.343: INFO: stderr: "+ nslookup clusterip-service.services-677.svc.cluster.local\n"
    Mar  2 03:02:43.343: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nclusterip-service.services-677.svc.cluster.local\tcanonical name = externalsvc.services-677.svc.cluster.local.\nName:\texternalsvc.services-677.svc.cluster.local\nAddress: 172.21.39.202\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-677, will wait for the garbage collector to delete the pods 03/02/23 03:02:43.343
    Mar  2 03:02:43.430: INFO: Deleting ReplicationController externalsvc took: 23.625125ms
    Mar  2 03:02:43.631: INFO: Terminating ReplicationController externalsvc pods took: 201.032909ms
    Mar  2 03:02:47.219: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar  2 03:02:47.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-677" for this suite. 03/02/23 03:02:47.593
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 03:02:47.732
Mar  2 03:02:47.732: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename resourcequota 03/02/23 03:02:47.733
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 03:02:48.132
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 03:02:48.215
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
STEP: Counting existing ResourceQuota 03/02/23 03:02:48.257
STEP: Creating a ResourceQuota 03/02/23 03:02:53.282
STEP: Ensuring resource quota status is calculated 03/02/23 03:02:53.305
STEP: Creating a Service 03/02/23 03:02:55.338
STEP: Creating a NodePort Service 03/02/23 03:02:55.402
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 03/02/23 03:02:55.515
STEP: Ensuring resource quota status captures service creation 03/02/23 03:02:55.621
STEP: Deleting Services 03/02/23 03:02:57.651
STEP: Ensuring resource quota status released usage 03/02/23 03:02:57.832
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar  2 03:02:59.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8091" for this suite. 03/02/23 03:02:59.866
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","completed":356,"skipped":6516,"failed":0}
------------------------------
• [SLOW TEST] [12.161 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 03:02:47.732
    Mar  2 03:02:47.732: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename resourcequota 03/02/23 03:02:47.733
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 03:02:48.132
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 03:02:48.215
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:90
    STEP: Counting existing ResourceQuota 03/02/23 03:02:48.257
    STEP: Creating a ResourceQuota 03/02/23 03:02:53.282
    STEP: Ensuring resource quota status is calculated 03/02/23 03:02:53.305
    STEP: Creating a Service 03/02/23 03:02:55.338
    STEP: Creating a NodePort Service 03/02/23 03:02:55.402
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 03/02/23 03:02:55.515
    STEP: Ensuring resource quota status captures service creation 03/02/23 03:02:55.621
    STEP: Deleting Services 03/02/23 03:02:57.651
    STEP: Ensuring resource quota status released usage 03/02/23 03:02:57.832
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar  2 03:02:59.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-8091" for this suite. 03/02/23 03:02:59.866
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 03:02:59.895
Mar  2 03:02:59.895: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename configmap 03/02/23 03:02:59.896
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 03:02:59.97
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 03:02:59.982
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
Mar  2 03:03:00.026: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-e836cd83-7df5-4c0b-932f-0b1a78390415 03/02/23 03:03:00.026
STEP: Creating the pod 03/02/23 03:03:00.049
Mar  2 03:03:00.140: INFO: Waiting up to 5m0s for pod "pod-configmaps-dc71e335-ca91-4d0c-bbf5-8fa875e8e8e0" in namespace "configmap-1682" to be "running and ready"
Mar  2 03:03:00.157: INFO: Pod "pod-configmaps-dc71e335-ca91-4d0c-bbf5-8fa875e8e8e0": Phase="Pending", Reason="", readiness=false. Elapsed: 16.686319ms
Mar  2 03:03:00.157: INFO: The phase of Pod pod-configmaps-dc71e335-ca91-4d0c-bbf5-8fa875e8e8e0 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 03:03:02.172: INFO: Pod "pod-configmaps-dc71e335-ca91-4d0c-bbf5-8fa875e8e8e0": Phase="Running", Reason="", readiness=true. Elapsed: 2.031964688s
Mar  2 03:03:02.172: INFO: The phase of Pod pod-configmaps-dc71e335-ca91-4d0c-bbf5-8fa875e8e8e0 is Running (Ready = true)
Mar  2 03:03:02.172: INFO: Pod "pod-configmaps-dc71e335-ca91-4d0c-bbf5-8fa875e8e8e0" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-e836cd83-7df5-4c0b-932f-0b1a78390415 03/02/23 03:03:02.225
STEP: waiting to observe update in volume 03/02/23 03:03:02.25
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar  2 03:03:04.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1682" for this suite. 03/02/23 03:03:04.358
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":357,"skipped":6531,"failed":0}
------------------------------
• [4.488 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 03:02:59.895
    Mar  2 03:02:59.895: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename configmap 03/02/23 03:02:59.896
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 03:02:59.97
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 03:02:59.982
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:123
    Mar  2 03:03:00.026: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating configMap with name configmap-test-upd-e836cd83-7df5-4c0b-932f-0b1a78390415 03/02/23 03:03:00.026
    STEP: Creating the pod 03/02/23 03:03:00.049
    Mar  2 03:03:00.140: INFO: Waiting up to 5m0s for pod "pod-configmaps-dc71e335-ca91-4d0c-bbf5-8fa875e8e8e0" in namespace "configmap-1682" to be "running and ready"
    Mar  2 03:03:00.157: INFO: Pod "pod-configmaps-dc71e335-ca91-4d0c-bbf5-8fa875e8e8e0": Phase="Pending", Reason="", readiness=false. Elapsed: 16.686319ms
    Mar  2 03:03:00.157: INFO: The phase of Pod pod-configmaps-dc71e335-ca91-4d0c-bbf5-8fa875e8e8e0 is Pending, waiting for it to be Running (with Ready = true)
    Mar  2 03:03:02.172: INFO: Pod "pod-configmaps-dc71e335-ca91-4d0c-bbf5-8fa875e8e8e0": Phase="Running", Reason="", readiness=true. Elapsed: 2.031964688s
    Mar  2 03:03:02.172: INFO: The phase of Pod pod-configmaps-dc71e335-ca91-4d0c-bbf5-8fa875e8e8e0 is Running (Ready = true)
    Mar  2 03:03:02.172: INFO: Pod "pod-configmaps-dc71e335-ca91-4d0c-bbf5-8fa875e8e8e0" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-e836cd83-7df5-4c0b-932f-0b1a78390415 03/02/23 03:03:02.225
    STEP: waiting to observe update in volume 03/02/23 03:03:02.25
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar  2 03:03:04.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1682" for this suite. 03/02/23 03:03:04.358
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 03:03:04.385
Mar  2 03:03:04.385: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename emptydir 03/02/23 03:03:04.386
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 03:03:04.448
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 03:03:04.479
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
STEP: Creating a pod to test emptydir 0666 on node default medium 03/02/23 03:03:04.489
Mar  2 03:03:04.583: INFO: Waiting up to 5m0s for pod "pod-b0bf87c6-508b-4810-89b4-60acf6981506" in namespace "emptydir-8349" to be "Succeeded or Failed"
Mar  2 03:03:04.597: INFO: Pod "pod-b0bf87c6-508b-4810-89b4-60acf6981506": Phase="Pending", Reason="", readiness=false. Elapsed: 13.574704ms
Mar  2 03:03:06.613: INFO: Pod "pod-b0bf87c6-508b-4810-89b4-60acf6981506": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029297384s
Mar  2 03:03:08.611: INFO: Pod "pod-b0bf87c6-508b-4810-89b4-60acf6981506": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027794067s
Mar  2 03:03:10.616: INFO: Pod "pod-b0bf87c6-508b-4810-89b4-60acf6981506": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032301579s
STEP: Saw pod success 03/02/23 03:03:10.616
Mar  2 03:03:10.616: INFO: Pod "pod-b0bf87c6-508b-4810-89b4-60acf6981506" satisfied condition "Succeeded or Failed"
Mar  2 03:03:10.628: INFO: Trying to get logs from node 10.132.92.143 pod pod-b0bf87c6-508b-4810-89b4-60acf6981506 container test-container: <nil>
STEP: delete the pod 03/02/23 03:03:10.659
Mar  2 03:03:10.693: INFO: Waiting for pod pod-b0bf87c6-508b-4810-89b4-60acf6981506 to disappear
Mar  2 03:03:10.707: INFO: Pod pod-b0bf87c6-508b-4810-89b4-60acf6981506 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar  2 03:03:10.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8349" for this suite. 03/02/23 03:03:10.765
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":358,"skipped":6557,"failed":0}
------------------------------
• [SLOW TEST] [6.406 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 03:03:04.385
    Mar  2 03:03:04.385: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename emptydir 03/02/23 03:03:04.386
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 03:03:04.448
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 03:03:04.479
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:176
    STEP: Creating a pod to test emptydir 0666 on node default medium 03/02/23 03:03:04.489
    Mar  2 03:03:04.583: INFO: Waiting up to 5m0s for pod "pod-b0bf87c6-508b-4810-89b4-60acf6981506" in namespace "emptydir-8349" to be "Succeeded or Failed"
    Mar  2 03:03:04.597: INFO: Pod "pod-b0bf87c6-508b-4810-89b4-60acf6981506": Phase="Pending", Reason="", readiness=false. Elapsed: 13.574704ms
    Mar  2 03:03:06.613: INFO: Pod "pod-b0bf87c6-508b-4810-89b4-60acf6981506": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029297384s
    Mar  2 03:03:08.611: INFO: Pod "pod-b0bf87c6-508b-4810-89b4-60acf6981506": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027794067s
    Mar  2 03:03:10.616: INFO: Pod "pod-b0bf87c6-508b-4810-89b4-60acf6981506": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032301579s
    STEP: Saw pod success 03/02/23 03:03:10.616
    Mar  2 03:03:10.616: INFO: Pod "pod-b0bf87c6-508b-4810-89b4-60acf6981506" satisfied condition "Succeeded or Failed"
    Mar  2 03:03:10.628: INFO: Trying to get logs from node 10.132.92.143 pod pod-b0bf87c6-508b-4810-89b4-60acf6981506 container test-container: <nil>
    STEP: delete the pod 03/02/23 03:03:10.659
    Mar  2 03:03:10.693: INFO: Waiting for pod pod-b0bf87c6-508b-4810-89b4-60acf6981506 to disappear
    Mar  2 03:03:10.707: INFO: Pod pod-b0bf87c6-508b-4810-89b4-60acf6981506 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar  2 03:03:10.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8349" for this suite. 03/02/23 03:03:10.765
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 03:03:10.797
Mar  2 03:03:10.797: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename dns 03/02/23 03:03:10.799
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 03:03:10.946
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 03:03:10.96
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 03/02/23 03:03:10.974
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7189.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-7189.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7189.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7189.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7189.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-7189.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7189.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-7189.svc.cluster.local;sleep 1; done
 03/02/23 03:03:11
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7189.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-7189.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7189.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-7189.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7189.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-7189.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7189.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-7189.svc.cluster.local;sleep 1; done
 03/02/23 03:03:11
STEP: creating a pod to probe DNS 03/02/23 03:03:11
STEP: submitting the pod to kubernetes 03/02/23 03:03:11
Mar  2 03:03:11.162: INFO: Waiting up to 15m0s for pod "dns-test-d97012ac-ebbd-4ea5-8c9a-b8fa1bd40b79" in namespace "dns-7189" to be "running"
Mar  2 03:03:11.176: INFO: Pod "dns-test-d97012ac-ebbd-4ea5-8c9a-b8fa1bd40b79": Phase="Pending", Reason="", readiness=false. Elapsed: 13.749224ms
Mar  2 03:03:13.194: INFO: Pod "dns-test-d97012ac-ebbd-4ea5-8c9a-b8fa1bd40b79": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031636424s
Mar  2 03:03:15.190: INFO: Pod "dns-test-d97012ac-ebbd-4ea5-8c9a-b8fa1bd40b79": Phase="Running", Reason="", readiness=true. Elapsed: 4.028349252s
Mar  2 03:03:15.191: INFO: Pod "dns-test-d97012ac-ebbd-4ea5-8c9a-b8fa1bd40b79" satisfied condition "running"
STEP: retrieving the pod 03/02/23 03:03:15.191
STEP: looking for the results for each expected name from probers 03/02/23 03:03:15.204
Mar  2 03:03:15.235: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7189.svc.cluster.local from pod dns-7189/dns-test-d97012ac-ebbd-4ea5-8c9a-b8fa1bd40b79: the server could not find the requested resource (get pods dns-test-d97012ac-ebbd-4ea5-8c9a-b8fa1bd40b79)
Mar  2 03:03:15.265: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7189.svc.cluster.local from pod dns-7189/dns-test-d97012ac-ebbd-4ea5-8c9a-b8fa1bd40b79: the server could not find the requested resource (get pods dns-test-d97012ac-ebbd-4ea5-8c9a-b8fa1bd40b79)
Mar  2 03:03:15.291: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7189.svc.cluster.local from pod dns-7189/dns-test-d97012ac-ebbd-4ea5-8c9a-b8fa1bd40b79: the server could not find the requested resource (get pods dns-test-d97012ac-ebbd-4ea5-8c9a-b8fa1bd40b79)
Mar  2 03:03:15.311: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7189.svc.cluster.local from pod dns-7189/dns-test-d97012ac-ebbd-4ea5-8c9a-b8fa1bd40b79: the server could not find the requested resource (get pods dns-test-d97012ac-ebbd-4ea5-8c9a-b8fa1bd40b79)
Mar  2 03:03:15.333: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7189.svc.cluster.local from pod dns-7189/dns-test-d97012ac-ebbd-4ea5-8c9a-b8fa1bd40b79: the server could not find the requested resource (get pods dns-test-d97012ac-ebbd-4ea5-8c9a-b8fa1bd40b79)
Mar  2 03:03:15.354: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7189.svc.cluster.local from pod dns-7189/dns-test-d97012ac-ebbd-4ea5-8c9a-b8fa1bd40b79: the server could not find the requested resource (get pods dns-test-d97012ac-ebbd-4ea5-8c9a-b8fa1bd40b79)
Mar  2 03:03:15.374: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7189.svc.cluster.local from pod dns-7189/dns-test-d97012ac-ebbd-4ea5-8c9a-b8fa1bd40b79: the server could not find the requested resource (get pods dns-test-d97012ac-ebbd-4ea5-8c9a-b8fa1bd40b79)
Mar  2 03:03:15.394: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7189.svc.cluster.local from pod dns-7189/dns-test-d97012ac-ebbd-4ea5-8c9a-b8fa1bd40b79: the server could not find the requested resource (get pods dns-test-d97012ac-ebbd-4ea5-8c9a-b8fa1bd40b79)
Mar  2 03:03:15.394: INFO: Lookups using dns-7189/dns-test-d97012ac-ebbd-4ea5-8c9a-b8fa1bd40b79 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7189.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7189.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7189.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7189.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7189.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7189.svc.cluster.local jessie_udp@dns-test-service-2.dns-7189.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7189.svc.cluster.local]

Mar  2 03:03:20.557: INFO: DNS probes using dns-7189/dns-test-d97012ac-ebbd-4ea5-8c9a-b8fa1bd40b79 succeeded

STEP: deleting the pod 03/02/23 03:03:20.557
STEP: deleting the test headless service 03/02/23 03:03:20.598
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar  2 03:03:20.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7189" for this suite. 03/02/23 03:03:20.673
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","completed":359,"skipped":6600,"failed":0}
------------------------------
• [SLOW TEST] [9.928 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 03:03:10.797
    Mar  2 03:03:10.797: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename dns 03/02/23 03:03:10.799
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 03:03:10.946
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 03:03:10.96
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 03/02/23 03:03:10.974
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7189.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-7189.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7189.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7189.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7189.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-7189.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7189.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-7189.svc.cluster.local;sleep 1; done
     03/02/23 03:03:11
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7189.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-7189.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7189.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-7189.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7189.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-7189.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7189.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-7189.svc.cluster.local;sleep 1; done
     03/02/23 03:03:11
    STEP: creating a pod to probe DNS 03/02/23 03:03:11
    STEP: submitting the pod to kubernetes 03/02/23 03:03:11
    Mar  2 03:03:11.162: INFO: Waiting up to 15m0s for pod "dns-test-d97012ac-ebbd-4ea5-8c9a-b8fa1bd40b79" in namespace "dns-7189" to be "running"
    Mar  2 03:03:11.176: INFO: Pod "dns-test-d97012ac-ebbd-4ea5-8c9a-b8fa1bd40b79": Phase="Pending", Reason="", readiness=false. Elapsed: 13.749224ms
    Mar  2 03:03:13.194: INFO: Pod "dns-test-d97012ac-ebbd-4ea5-8c9a-b8fa1bd40b79": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031636424s
    Mar  2 03:03:15.190: INFO: Pod "dns-test-d97012ac-ebbd-4ea5-8c9a-b8fa1bd40b79": Phase="Running", Reason="", readiness=true. Elapsed: 4.028349252s
    Mar  2 03:03:15.191: INFO: Pod "dns-test-d97012ac-ebbd-4ea5-8c9a-b8fa1bd40b79" satisfied condition "running"
    STEP: retrieving the pod 03/02/23 03:03:15.191
    STEP: looking for the results for each expected name from probers 03/02/23 03:03:15.204
    Mar  2 03:03:15.235: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7189.svc.cluster.local from pod dns-7189/dns-test-d97012ac-ebbd-4ea5-8c9a-b8fa1bd40b79: the server could not find the requested resource (get pods dns-test-d97012ac-ebbd-4ea5-8c9a-b8fa1bd40b79)
    Mar  2 03:03:15.265: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7189.svc.cluster.local from pod dns-7189/dns-test-d97012ac-ebbd-4ea5-8c9a-b8fa1bd40b79: the server could not find the requested resource (get pods dns-test-d97012ac-ebbd-4ea5-8c9a-b8fa1bd40b79)
    Mar  2 03:03:15.291: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7189.svc.cluster.local from pod dns-7189/dns-test-d97012ac-ebbd-4ea5-8c9a-b8fa1bd40b79: the server could not find the requested resource (get pods dns-test-d97012ac-ebbd-4ea5-8c9a-b8fa1bd40b79)
    Mar  2 03:03:15.311: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7189.svc.cluster.local from pod dns-7189/dns-test-d97012ac-ebbd-4ea5-8c9a-b8fa1bd40b79: the server could not find the requested resource (get pods dns-test-d97012ac-ebbd-4ea5-8c9a-b8fa1bd40b79)
    Mar  2 03:03:15.333: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7189.svc.cluster.local from pod dns-7189/dns-test-d97012ac-ebbd-4ea5-8c9a-b8fa1bd40b79: the server could not find the requested resource (get pods dns-test-d97012ac-ebbd-4ea5-8c9a-b8fa1bd40b79)
    Mar  2 03:03:15.354: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7189.svc.cluster.local from pod dns-7189/dns-test-d97012ac-ebbd-4ea5-8c9a-b8fa1bd40b79: the server could not find the requested resource (get pods dns-test-d97012ac-ebbd-4ea5-8c9a-b8fa1bd40b79)
    Mar  2 03:03:15.374: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7189.svc.cluster.local from pod dns-7189/dns-test-d97012ac-ebbd-4ea5-8c9a-b8fa1bd40b79: the server could not find the requested resource (get pods dns-test-d97012ac-ebbd-4ea5-8c9a-b8fa1bd40b79)
    Mar  2 03:03:15.394: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7189.svc.cluster.local from pod dns-7189/dns-test-d97012ac-ebbd-4ea5-8c9a-b8fa1bd40b79: the server could not find the requested resource (get pods dns-test-d97012ac-ebbd-4ea5-8c9a-b8fa1bd40b79)
    Mar  2 03:03:15.394: INFO: Lookups using dns-7189/dns-test-d97012ac-ebbd-4ea5-8c9a-b8fa1bd40b79 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7189.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7189.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7189.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7189.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7189.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7189.svc.cluster.local jessie_udp@dns-test-service-2.dns-7189.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7189.svc.cluster.local]

    Mar  2 03:03:20.557: INFO: DNS probes using dns-7189/dns-test-d97012ac-ebbd-4ea5-8c9a-b8fa1bd40b79 succeeded

    STEP: deleting the pod 03/02/23 03:03:20.557
    STEP: deleting the test headless service 03/02/23 03:03:20.598
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar  2 03:03:20.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-7189" for this suite. 03/02/23 03:03:20.673
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 03:03:20.726
Mar  2 03:03:20.726: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename custom-resource-definition 03/02/23 03:03:20.727
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 03:03:20.819
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 03:03:20.838
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Mar  2 03:03:20.861: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar  2 03:03:24.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9423" for this suite. 03/02/23 03:03:24.401
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","completed":360,"skipped":6607,"failed":0}
------------------------------
• [3.717 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 03:03:20.726
    Mar  2 03:03:20.726: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename custom-resource-definition 03/02/23 03:03:20.727
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 03:03:20.819
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 03:03:20.838
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Mar  2 03:03:20.861: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar  2 03:03:24.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-9423" for this suite. 03/02/23 03:03:24.401
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 03:03:24.444
Mar  2 03:03:24.445: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename kubelet-test 03/02/23 03:03:24.446
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 03:03:24.514
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 03:03:24.533
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Mar  2 03:03:24.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6330" for this suite. 03/02/23 03:03:24.792
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","completed":361,"skipped":6632,"failed":0}
------------------------------
• [0.380 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 03:03:24.444
    Mar  2 03:03:24.445: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename kubelet-test 03/02/23 03:03:24.446
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 03:03:24.514
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 03:03:24.533
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Mar  2 03:03:24.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-6330" for this suite. 03/02/23 03:03:24.792
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/02/23 03:03:24.829
Mar  2 03:03:24.829: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
STEP: Building a namespace api object, basename subpath 03/02/23 03:03:24.83
STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 03:03:24.908
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 03:03:24.922
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/02/23 03:03:24.959
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-l5bt 03/02/23 03:03:25.029
STEP: Creating a pod to test atomic-volume-subpath 03/02/23 03:03:25.029
Mar  2 03:03:25.178: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-l5bt" in namespace "subpath-9745" to be "Succeeded or Failed"
Mar  2 03:03:25.204: INFO: Pod "pod-subpath-test-downwardapi-l5bt": Phase="Pending", Reason="", readiness=false. Elapsed: 26.302529ms
Mar  2 03:03:27.236: INFO: Pod "pod-subpath-test-downwardapi-l5bt": Phase="Running", Reason="", readiness=true. Elapsed: 2.058372653s
Mar  2 03:03:29.220: INFO: Pod "pod-subpath-test-downwardapi-l5bt": Phase="Running", Reason="", readiness=true. Elapsed: 4.042150225s
Mar  2 03:03:31.227: INFO: Pod "pod-subpath-test-downwardapi-l5bt": Phase="Running", Reason="", readiness=true. Elapsed: 6.049007482s
Mar  2 03:03:33.220: INFO: Pod "pod-subpath-test-downwardapi-l5bt": Phase="Running", Reason="", readiness=true. Elapsed: 8.041396484s
Mar  2 03:03:35.230: INFO: Pod "pod-subpath-test-downwardapi-l5bt": Phase="Running", Reason="", readiness=true. Elapsed: 10.051461861s
Mar  2 03:03:37.224: INFO: Pod "pod-subpath-test-downwardapi-l5bt": Phase="Running", Reason="", readiness=true. Elapsed: 12.046291743s
Mar  2 03:03:39.220: INFO: Pod "pod-subpath-test-downwardapi-l5bt": Phase="Running", Reason="", readiness=true. Elapsed: 14.04202545s
Mar  2 03:03:41.220: INFO: Pod "pod-subpath-test-downwardapi-l5bt": Phase="Running", Reason="", readiness=true. Elapsed: 16.041850133s
Mar  2 03:03:43.227: INFO: Pod "pod-subpath-test-downwardapi-l5bt": Phase="Running", Reason="", readiness=true. Elapsed: 18.048686355s
Mar  2 03:03:45.221: INFO: Pod "pod-subpath-test-downwardapi-l5bt": Phase="Running", Reason="", readiness=true. Elapsed: 20.043024904s
Mar  2 03:03:47.222: INFO: Pod "pod-subpath-test-downwardapi-l5bt": Phase="Running", Reason="", readiness=false. Elapsed: 22.043442045s
Mar  2 03:03:49.220: INFO: Pod "pod-subpath-test-downwardapi-l5bt": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.041540808s
STEP: Saw pod success 03/02/23 03:03:49.22
Mar  2 03:03:49.220: INFO: Pod "pod-subpath-test-downwardapi-l5bt" satisfied condition "Succeeded or Failed"
Mar  2 03:03:49.235: INFO: Trying to get logs from node 10.132.92.143 pod pod-subpath-test-downwardapi-l5bt container test-container-subpath-downwardapi-l5bt: <nil>
STEP: delete the pod 03/02/23 03:03:49.295
Mar  2 03:03:49.329: INFO: Waiting for pod pod-subpath-test-downwardapi-l5bt to disappear
Mar  2 03:03:49.343: INFO: Pod pod-subpath-test-downwardapi-l5bt no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-l5bt 03/02/23 03:03:49.343
Mar  2 03:03:49.344: INFO: Deleting pod "pod-subpath-test-downwardapi-l5bt" in namespace "subpath-9745"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Mar  2 03:03:49.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9745" for this suite. 03/02/23 03:03:49.398
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]","completed":362,"skipped":6683,"failed":0}
------------------------------
• [SLOW TEST] [24.593 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/02/23 03:03:24.829
    Mar  2 03:03:24.829: INFO: >>> kubeConfig: /tmp/kubeconfig-1535626242
    STEP: Building a namespace api object, basename subpath 03/02/23 03:03:24.83
    STEP: Waiting for a default service account to be provisioned in namespace 03/02/23 03:03:24.908
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/02/23 03:03:24.922
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/02/23 03:03:24.959
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-l5bt 03/02/23 03:03:25.029
    STEP: Creating a pod to test atomic-volume-subpath 03/02/23 03:03:25.029
    Mar  2 03:03:25.178: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-l5bt" in namespace "subpath-9745" to be "Succeeded or Failed"
    Mar  2 03:03:25.204: INFO: Pod "pod-subpath-test-downwardapi-l5bt": Phase="Pending", Reason="", readiness=false. Elapsed: 26.302529ms
    Mar  2 03:03:27.236: INFO: Pod "pod-subpath-test-downwardapi-l5bt": Phase="Running", Reason="", readiness=true. Elapsed: 2.058372653s
    Mar  2 03:03:29.220: INFO: Pod "pod-subpath-test-downwardapi-l5bt": Phase="Running", Reason="", readiness=true. Elapsed: 4.042150225s
    Mar  2 03:03:31.227: INFO: Pod "pod-subpath-test-downwardapi-l5bt": Phase="Running", Reason="", readiness=true. Elapsed: 6.049007482s
    Mar  2 03:03:33.220: INFO: Pod "pod-subpath-test-downwardapi-l5bt": Phase="Running", Reason="", readiness=true. Elapsed: 8.041396484s
    Mar  2 03:03:35.230: INFO: Pod "pod-subpath-test-downwardapi-l5bt": Phase="Running", Reason="", readiness=true. Elapsed: 10.051461861s
    Mar  2 03:03:37.224: INFO: Pod "pod-subpath-test-downwardapi-l5bt": Phase="Running", Reason="", readiness=true. Elapsed: 12.046291743s
    Mar  2 03:03:39.220: INFO: Pod "pod-subpath-test-downwardapi-l5bt": Phase="Running", Reason="", readiness=true. Elapsed: 14.04202545s
    Mar  2 03:03:41.220: INFO: Pod "pod-subpath-test-downwardapi-l5bt": Phase="Running", Reason="", readiness=true. Elapsed: 16.041850133s
    Mar  2 03:03:43.227: INFO: Pod "pod-subpath-test-downwardapi-l5bt": Phase="Running", Reason="", readiness=true. Elapsed: 18.048686355s
    Mar  2 03:03:45.221: INFO: Pod "pod-subpath-test-downwardapi-l5bt": Phase="Running", Reason="", readiness=true. Elapsed: 20.043024904s
    Mar  2 03:03:47.222: INFO: Pod "pod-subpath-test-downwardapi-l5bt": Phase="Running", Reason="", readiness=false. Elapsed: 22.043442045s
    Mar  2 03:03:49.220: INFO: Pod "pod-subpath-test-downwardapi-l5bt": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.041540808s
    STEP: Saw pod success 03/02/23 03:03:49.22
    Mar  2 03:03:49.220: INFO: Pod "pod-subpath-test-downwardapi-l5bt" satisfied condition "Succeeded or Failed"
    Mar  2 03:03:49.235: INFO: Trying to get logs from node 10.132.92.143 pod pod-subpath-test-downwardapi-l5bt container test-container-subpath-downwardapi-l5bt: <nil>
    STEP: delete the pod 03/02/23 03:03:49.295
    Mar  2 03:03:49.329: INFO: Waiting for pod pod-subpath-test-downwardapi-l5bt to disappear
    Mar  2 03:03:49.343: INFO: Pod pod-subpath-test-downwardapi-l5bt no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-l5bt 03/02/23 03:03:49.343
    Mar  2 03:03:49.344: INFO: Deleting pod "pod-subpath-test-downwardapi-l5bt" in namespace "subpath-9745"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Mar  2 03:03:49.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-9745" for this suite. 03/02/23 03:03:49.398
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
{"msg":"Test Suite completed","completed":362,"skipped":6705,"failed":0}
Mar  2 03:03:49.432: INFO: Running AfterSuite actions on all nodes
Mar  2 03:03:49.432: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
Mar  2 03:03:49.432: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
Mar  2 03:03:49.432: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
Mar  2 03:03:49.432: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Mar  2 03:03:49.432: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Mar  2 03:03:49.432: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Mar  2 03:03:49.432: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
Mar  2 03:03:49.432: INFO: Running AfterSuite actions on node 1
Mar  2 03:03:49.432: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.001 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Mar  2 03:03:49.432: INFO: Running AfterSuite actions on all nodes
    Mar  2 03:03:49.432: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
    Mar  2 03:03:49.432: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
    Mar  2 03:03:49.432: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
    Mar  2 03:03:49.432: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
    Mar  2 03:03:49.432: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
    Mar  2 03:03:49.432: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
    Mar  2 03:03:49.432: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Mar  2 03:03:49.432: INFO: Running AfterSuite actions on node 1
    Mar  2 03:03:49.432: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:146
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:146
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:559
------------------------------
[ReportAfterSuite] PASSED [0.146 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:559
  << End Captured GinkgoWriter Output
------------------------------

Ran 362 of 7067 Specs in 6971.251 seconds
SUCCESS! -- 362 Passed | 0 Failed | 0 Pending | 6705 Skipped
PASS

Ginkgo ran 1 suite in 1h56m12.070449705s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.1.6[0m

